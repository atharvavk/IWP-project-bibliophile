<!DOCTYPE html>
<html>
<head>
	<title></title>
	<style>
		.white-space-pre {
    white-space: pre-wrap;
}
	</style>
</head>
<body oncopy="return false" onpaste="return false" oncut="return false">
	<div class="white-space-pre">
	<blockquote>
		A Brief History of Time - Stephen Hawking

Chapter 1 - Our Picture of the Universe
Chapter 2 - Space and Time
Chapter 3 - The Expanding Universe
Chapter 4 - The Uncertainty Principle
Chapter 5 - Elementary Particles and the Forces of Nature
Chapter 6 - Black Holes
Chapter 7 - Black Holes Ain't So Black
Chapter 8 - The Origin and Fate of the Universe
Chapter 9 - The Arrow of Time
Chapter 10 - Wormholes and Time Travel
Chapter 11 - The Unification of Physics
Chapter 12 - Conclusion
Glossary
Acknowledgments & About The Author

FOREWARD
I didn’t write a foreword to the original edition of A Brief History of Time. That was done by Carl Sagan. Instead,
I wrote a short piece titled “Acknowledgments” in which I was advised to thank everyone. Some of the
foundations that had given me support weren’t too pleased to have been mentioned, however, because it led to
a great increase in applications.
I don’t think anyone, my publishers, my agent, or myself, expected the book to do anything like as well as it did.
It was in the London Sunday Times best-seller list for 237 weeks, longer than any other book (apparently, the
Bible and Shakespeare aren’t counted). It has been translated into something like forty languages and has sold
about one copy for every 750 men, women, and children in the world. As Nathan Myhrvold of Microsoft (a
former post-doc of mine) remarked: I have sold more books on physics than Madonna has on sex.
The success of A Brief History indicates that there is widespread interest in the big questions like: Where did
we come from? And why is the universe the way it is?
I have taken the opportunity to update the book and include new theoretical and observational results obtained
since the book was first published (on April Fools’ Day, 1988). I have included a new chapter on wormholes
and time travel. Einstein’s General Theory of Relativity seems to offer the possibility that we could create and
maintain wormholes, little tubes that connect different regions of space-time. If so, we might be able to use
them for rapid travel around the galaxy or travel back in time. Of course, we have not seen anyone from the
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/A Brief History in Time.html (1 of 2) [2/20/2001 3:13:58 AM]

ASTRONOMY AND
ASTROPHYSICS LIBRARY
Series Editors:

G. Börner, Garching, Germany
A. Burkert, München, Germany
W. B. Burton, Charlottesville, VA, USA and
Leiden, The Netherlands
M. A. Dopita, Canberra, Australia
A. Eckart, Köln, Germany
T. Encrenaz, Meudon, France
B. Leibundgut, Garching, Germany
J. Lequeux, Paris, France
A. Maeder, Sauverny, Switzerland
V. Trimble, College Park, MD, and Irvine, CA, USA

Peter Hoyng

Relativistic Astrophysics
and Cosmology
A Primer
With 114 Figures, 16 in color and 12 Tables

123

Peter Hoyng
SRON Netherlands Institute for Space Research
Sorbonnelaan 2
3584 CA Utrecht, The Netherlands
p.hoyng@sron.nl

Cover picture: 'Zwarte Kom op Geel Vlak' 2002 by Olav Cleofas van Overbeek, picture Galerie Lieve Hemel,
Amsterdam.

Library of Congress Control Number: 2006920111

ISSN 0941-7834
ISBN-10 1-4020-4521-2 Springer Berlin Heidelberg New York
ISBN-13 978-1-4020-4521-9 Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned,
specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilm or in any other way, and storage in data banks. Duplication of this publication or parts thereof is permitted
only under the provisions of the German Copyright Law of September 9, 1965, in its current version, and permission
for use must always be obtained from Springer. Violations are liable to prosecution under the German Copyright Law.
Springer is a part of Springer Science+Business Media
springer.com
© Springer-Verlag Berlin Heidelberg 2006
Printed in The Netherlands
The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply, even in
the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and
therefore free for general use.
Typesetting by author and SPI Publisher Services using a Springer LATEX macro package

Cover design: design & production GmbH, Heidelberg
Printed on acid-free paper

SPIN: 11301257

3144/SPI - 5 4 3 2 1 0

Preface

This textbook oﬀers a succinct and self-contained introduction into general
relativity and its main areas of application: compact objects, gravitational
waves and cosmology. It has evolved from lecture courses I have taught at the
University of Utrecht since 1990. The book is intended for advanced undergraduate and beginning graduate students in physics and astrophysics.
The past decades have seen spectacular new developments in our knowledge of cosmology, the physics of compact objects and in high precision gravity
experiments. As a result, relativistic astrophysics and cosmology have become
a very attractive element in the (astro)physics curriculum, and there is a variety of excellent textbooks. But most of these are either too advanced, too
elementary, or too voluminous for my purpose, or they do not cover all topics. My object in writing this book has been to provide a concise text that
addresses general relativity and its applications homogeneously, at an intermediate level, conveying a maximal physical insight with a minimal amount
of formalism. It is often a revelation for students to see that it is possible, at
least for the range of subjects addressed here, to cut down the usual tangle of
math to manageable proportions without watering down the discussion. My
guiding principle has been to keep only what is really useful, but that does
not mean that it is always the more diﬃcult topics that have been eliminated.
For example, I kept very little formal tensor calculus as it is not really needed
– only the basics are indispensable. But variational calculus is used extensively because it is by far the simplest way to compute Christoﬀel symbols
and therefore very useful.
The approach is theoretical, but the text is interlaced with discussions
of observational, instrumental and historical aspects where appropriate. The
book is divided into (1) preparatory material: special relativity, geometry of
Riemann spaces, and general relativity, (2) Schwarzschild metric and applications: classical tests, binary pulsars, gravitational lenses, neutron stars and
black holes, (3) experimental gravity: gravitational waves and their detectors, Gravity Probe B, and ﬁnally (4) cosmology: Robertson-Walker metric,
evolution of the universe, observational cosmology, and inﬂation. Due to the
self-imposed restrictions several topics had to be skipped. But in view of their
current interest, extra attention has been given to the operation of interfer-

VI

Preface

ometer detectors for gravitational waves, to the Gravity Probe B mission, and
to structure formation in relation to the results of the Wilkinson Microwave
Anisotropy Probe (WMAP).
The reader is supposed to be familiar with linear algebra and calculus,
ordinary diﬀerential equations, and with elementary thermal physics, electrodynamics, special relativity and quantum mechanics - in other words, the basic
education of advanced physics undergraduates. Prior knowledge of diﬀerential
geometry, general relativity and astrophysics is helpful but not required. The
necessary mathematical techniques are introduced informally, following geometrical intuition as much as possible. The admirable texts of Dirac (1975),
Price (1982) and Schutz (1985) have been a source of inspiration for me in
this regard. And the astrophysical concepts are likewise brieﬂy introduced to
a level where they should be intelligible for physics students. There are about
145 exercises with hints for their solution. These exercises are an indispensable element in helping students to come to grasp with the subject matter,
and to train them to solve elementary problems independently. In my experience 40 − 45 lectures (45 min.) of oral instruction would suﬃce to expound
all material, excluding tutorials for exercises.
References to the literature are eclectic rather that complete, and appear
as footnotes in the text. General references (mostly textbooks) are given in
Appendix A. The ﬁniteness of the alphabet did cause some problems of notation. The reader is alerted to my propensity for the symbol a. There are many
diﬀerent constants a in the text, but confusion is unlikely as they have only a
local meaning. Likewise h has three diﬀerent meanings, (H0 /100, the constant
of the motion r2 ϕ̇ in the Schwarzschild metric, and Planck’s constant).
The cover picture is a still life by the Dutch artist Olav Cleofas van
Overbeek entitled Black bowl on yellow plane (2002). Its simplicity and wellbalanced design epitomize the rotational and translational symmetries that
are so ubiquitous in physics, and in this book embodied in the Schwarzschild
metric and the Robertson-Walker metric, respectively. The cartoons opposite to the chapter headings have been drawn by Roeland van Oss, and I am
grateful for his permission to reproduce them here. The drafting of the ﬁgures
reﬂects the technical developments of the period, and began on rice paper, to
proceed entirely by electronic means in the end. I wish to thank Hans Braun,
Arjan Bik, and in particular Artur Pfeifer for their assistance in this area.
There are instances where we have been unable to trace or contact the copyright holder of some of the reproduced ﬁgures. If notiﬁed the publisher will
be pleased to rectify any errors or omissions at the earliest opportunity.
I want to express my gratitude to Jan van der Kuur for his help in solving my Latex problems, and to Constance Jansen who generously provided

Preface

VII

library assistance. Lucas van der Wiel has helped me with the ﬁrst English
translation. In the course of the years that led up to this book I have beneﬁtted from discussions and correspondence with many colleagues. I cannot
name them all, but I do wish to thank Bram Achterberg and Ed van den
Heuvel and several unknown referees who read sections of the manuscript. I
am in particular indebted to my friend and colleague John Heise who since
many years is my discussion partner on matters relativistic and other. His
inﬂuence is pervasive throughout the book. And last but not least, I should
thank all the students who continually forced me to improve the presentation
of the material, from my ﬁrst notes in 1990 (to which I think in slight embarrassment), to the present text which is, I hope, of some use to the reader.

Utrecht,
February 2005

Peter Hoyng

Contents

1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1 Special relativity (SR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 General relativity (GR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.3 The need for GR in astrophysics . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

2

Geometry of Riemann Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1 Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 The tangent space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Tensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Parallel transport and Christoﬀel symbols . . . . . . . . . . . . . . . . . .
2.5 Geodesics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.6 The covariant derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.7 Riemann tensor and curvature . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19
19
21
23
26
30
33
35

3

General Relativity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1 Co-ordinates, metric and motion . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Weak ﬁelds (1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Conservation of mass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4 The ﬁeld equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5 Weak ﬁelds (2) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

43
43
47
50
52
56
59

4

The Schwarzschild Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1 Preliminary calculations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 The Schwarzschild metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 Geodesics of the Schwarzschild metric . . . . . . . . . . . . . . . . . . . . . .
4.4 The classical tests of GR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.5 Gravitational lenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

65
65
69
72
77
82

X

Contents

5

Compact Stars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5.1 End products of stellar evolution . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5.2 The maximum mass Mc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
5.3 The Tolman-Oppenheimer-Volkoﬀ equation . . . . . . . . . . . . . . . . . 97
5.4 A simple neutron star model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
5.5 Realistic neutron star models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103

6

Black Holes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
6.2 Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
6.3 Elementary properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
6.4 Kruskal-Szekeres co-ordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
6.5 Rotating black holes: the Kerr metric . . . . . . . . . . . . . . . . . . . . . . 125
6.6 Hawking radiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128

7

Gravitational waves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
7.1 Small amplitude waves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
7.2 The eﬀect of a gravitational wave on test masses . . . . . . . . . . . . 136
7.3 Generation of gravitational radiation . . . . . . . . . . . . . . . . . . . . . . . 138
7.4 Bar detectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
7.5 Interferometer detectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145

8

Fermi-Walker Transport . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
8.1 Transport of accelerated vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
8.2 Thomas precession . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
8.3 Geodesic precession . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
8.4 Gravity Probe B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164

9

The Robertson-Walker Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
9.1 Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
9.2 Deﬁnition of co-ordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
9.3 Metric and spatial structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
9.4 Equations of motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
9.5 The cosmological constant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
9.6 Geodesics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185

10 The Evolution of the Universe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
10.1 Equation of state . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
10.2 The matter era . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
10.3 The radiation era . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
10.4 The formation of structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203

Contents

XI

11 Observational Cosmology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
11.1 Redshift and distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
11.2 The visible universe and the horizon . . . . . . . . . . . . . . . . . . . . . . . 218
11.3 Luminosity distance and Hubble relation . . . . . . . . . . . . . . . . . . . 224
11.4 The microwave background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
11.5 Light-cone integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
12 The Big Bang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
12.1 Nuclear reactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
12.2 The ﬁrst 100 seconds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
12.3 The synthesis of light elements . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
13 Inﬂation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
13.1 The horizon problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
13.2 Evolution of a universe with a scalar ﬁeld . . . . . . . . . . . . . . . . . . . 258
13.3 Chaotic inﬂation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
13.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266

1
Introduction

From the earliest days of history mankind has shown an avid interest in the
heavenly phenomena, and astronomers have good reasons to claim that theirs
is the oldest profession of the world but one. This interest arose largely from
practical needs. In a diﬀerentiated society where rituals play an important
role it is useful to know the direction of the North and to be able to predict the turn of the seasons, days of festivities, and so on. Astronomy was still
tightly interwoven with religion and astrology. The Babylonians had an extensive knowledge of practical mathematics and astronomy. The two important
issues were the calendar (i.e. the question of the relative length of the year,
months, days and the time of important feast-days), and the ephemeris (the
positions of the Sun, Moon and the planets, lunar and solar eclipses, etc., as
a function of time). In parallel to this practical knowledge, a whole variety
of mythological ideas developed about the origin of the world around us. It
is a peculiar coincidence that the Hindus arrived at time scales close to what
we now think to be the age of the universe. The Hindus believed in a cyclic
universe. It was created by Brahma, and exists in an orderly state for a period
of one Brahma day (4.32 × 109 year).1 At the end of the day Brahma will go
to rest, and the universe will turn into chaos. Light, orderly motion and life
only return when Brahma wakes up again. Ultimately Brahma himself will
die, and the universe and the Hindu pantheon will perish with him. A new
Brahma will then be born, and the endless cycle of creation and destruction
will repeat itself.
The Greek were the ﬁrst to develop rational concepts about the world.
According to Pythagoras and his followers (ca. 500 B.C.) the Earth is spherical. The Sun, Moon and planets reside on concentric spheres revolving around
the central ﬁre Hestia. The stars are located on the outermost sphere. The
idea that the Earth is not at the centre of the universe is therefore very
old. Eudoxus (about 408-355) and Aristotle (384 - 322) developed a spherical world model consisting of a great number of concentric spheres with the
1

Thomas, P.: 1975, Hindu religion, customs and manners, Taraporevala Sons &
Co, Bombay.

2

1 Introduction

Earth located at the centre. Each celestial body (Sun, Moon, and the ﬁve
known planets) has a set of spheres associated with it, and is located on the
innermost sphere of its own set. Each sphere of a set revolves around an axis
attached to the sphere directly within. Because the axes of the spheres are
not aligned, the apparent motions of the planets could be reproduced approximately. To the Greek, esthetic considerations played an important role, and
this trend has persisted in physics to this day because it is often productive
(‘a theory is plausible because it is elegant’). Religious aspects played a role
as well, and this has also lingered on for a very long time (cf. for example
Newton). And haven’t we all at times been overwhelmed by the beauty of the
night sky – a strong emotional experience bordering to a religious experience?
In a letter to his brother Theo, Vincent van Gogh wrote ‘.. It does not prevent
me from having a terrible need of – shall I say the word – of religion, then I
go outside in the night to paint the stars ..’ 2
Based on Babylonian observations Hipparchus (ca. 190 - 125) catalogued
some 850 stars and their positions. He also invented the concept of epicycles
to explain the brightness variations associated with the apparent motion of
the planets. It should be kept in mind that in those days stars and planets were regarded as independent light sources of a divine nature, and that
only the Earth and the Moon were thought to be lit by the Sun. The insight
that the Earth and the planets are actually comparable objects came much
later. Geocentric world models with epicycles were gradually reﬁned. Ptolemy
(87 − 150) recorded his version in the Almagest3 , a summary of ancient astronomy and one of the most inﬂuential texts in the development of Western
thinking. Much earlier, Aristarchus (ca. 310 − 230) had proposed a simpler,
truly heliocentric model with the Earth rotating around its axis and around
the Sun. He was therefore 1800 years ahead of his time, but his ideas did not
prevail. The history of astronomy would arguably have been quite diﬀerent if
they had, and this example may serve as a consolation for those who feel that
the world does not hear their voice. The heliocentric theory became gradually
accepted only after the publication of the work of Copernicus in 1543. For
more information on these matters see Koestler (1959), Dijsterhuis (1969),
Pannekoek (1989), Evans (1998), and Bless (1995).
The transition from a geocentric to a heliocentric world model meant that
mankind had to give up its privileged position at the centre of the universe.
This development continued well into the last century, one might say, until
Hubble proved in 1924 that the spiral nebulae are actually galaxies located
far outside our own galaxy, as Kant had already postulated in 1755. As a
result, our galaxy became one among many. This led to the formulation of
the cosmological principle, which says that our position in the universe is in
no way special – the complete antithese of the geocentric view.

2

3

J. van Gogh-Bonger (ed.), Verzamelde brieven van Vincent van Gogh, Wereldbibliotheek, Amsterdam (1973), Vol III, letter 543, p. 321.
From the Arabic-Greek word Kitab al-megiste, the Great Book.

1.1 Special relativity (SR)

3

Saturn

Jupiter

Sun
Venus
Mercury
Earth
Mars

Fig. 1.1. Ptolemy’s world model, very much simpliﬁed and not to scale. The centres
of the epicycles of the inner planets are on the Sun-Earth line, while the radii of
the epicycles of the outer planets run parallel to this line. The innermost sphere
around the Earth (the ‘sublunary’) belongs to the Moon. The stars are located on
an outermost sphere (not shown). The whole system operates like a clockwork as
the Sun moves around the Earth. To the modern eye, a strange aspect of the model
is that the motion of the other planets is connected with the motion of the Sun
around the Earth. This coincidence is removed in Copernicus’s heliocentric model.
After Dijksterhuis (1969).

1.1 Special relativity (SR)
Modern cosmology is based on the theory of general relativity (GR), which is
a natural generalisation of the theory of special relativity (SR). This section
recapitulates the main ideas of special relativity, that is, physics in the absence
of gravity. For a more thorough discussion we refer to Schutz (1985). We
consider space and time to be a 4-dimensional continuum, called Minkowski
spacetime. A (global) co-ordinate system in Minkowski spacetime is usually
called a reference frame or just a frame. A point P with co-ordinates {xα } is
called an event. The motion of a particle can be represented by its worldline,
Fig. 1.2. SR is based on two postulates:
- The principle of relativity, which states that the laws of physics must have
the same form in every inertial frame.
- The speed of light has a constant value c in all inertial frames.

4

1 Introduction

t
or

{ xa + Dxa }

Q

x0 = ct

worldline

V
P

{ xa }

{ xi }

Fig. 1.2. The Minkowski spacetime, with events P and Q, a vector V connecting
these events, and the worldline of a particle.

An inertial frame is a rigid system of spatial co-ordinates with synchronised
clocks to measure t, in which test particles on which no forces are exerted
move uniformly with respect to each other. An example of an inertial frame is
a frame that does not move (no rotation, no translation) with respect to the
distant galaxies. Inertial frames in SR are global, and they all move uniformly
with respect to each other. In this section we admit only inertial frames.
The principle of relativity is very old and goes back to Galilei. The second
postulate is Einstein’s innovative step, which he based, among other things, on
Michelson and Morley’s experiment which demonstrated the impossibility of
measuring the velocity of the Earth with respect to the ether. The consequence
is that invariance for Galilean transformations, as e.g. Newton’s laws possess,
no longer applies.
Simultaneity exit
SR often evokes major conceptual problems due to the fact that some very
deeply rooted (Newtonian) ideas about space and time are not consistent
with observations. Paramount among these is the fact that simultaneity has
no longer an invariant meaning. Consider an inertial observer W , who tries to
locate the events in his co-ordinate system (x, t) that are simultaneous with
the origin x = t = 0, see Fig. 1.3, left. W argues: all events P that reﬂect
light such that the moments of emission and detection are symmetrical with
respect to t = 0 (emission at t = −t0 , detection at t = t0 for all t0 ). W ’s
conclusion is: all events on the x-axis. Now consider observer W who moves
uniformly to the right in W ’s frame, Fig. 1.3, right. At t = 0, W and W are
both at the origin. W ’s worldline serves as the t-axis of his frame, and t = 0 is
chosen at the common origin. W repeats W ’s experiment, but since the value
of c is frame-independent, W identiﬁes a diﬀerent set of events, eﬀectively his
x-axis, as being simultaneous with the origin. The x-axis lies tilted in W ’s
frame, and the tilt angle depends on W ’s velocity. Diﬀerent observers W will

1.1 Special relativity (SR)
t

worldline W
t

worldline W

t0

t

5

worldline W

t0

P

P
x

x
x

-t0

-t0
observer W

observer W

Fig. 1.3. As explained in the text, an invariant deﬁnition of simultaneity is impossible in SR.

nucleus

A
B

A
B

B

Fig. 1.4. Thomas precession of an electron orbiting a nucleus explained in the spirit
of Fig. 1.3. After Taylor and Wheeler (1966).

therefore disagree as to which events are simultaneous with the origin.
Inaccurate reasoning in SR has led to many paradoxes (clock paradox,
car-in-garage paradox). A vivid illustration of how drastically SR turns our
perception of space and time upside down is the Thomas precession of the
spin of an electron in an atom, a purely special-relativistic eﬀect. Fig. 1.4
shows the classical orbit, approximated by a polygon. The heavy line is the
projection of the spin axis on the plane of the orbit. After the electron has
rounded a corner, its spin axis has turned. An analysis of what happens during
the acceleration at the corner can be avoided by replacing electron A there
by electron B, demanding that the spin vectors are aligned in a frame moving with A (A’s rest-frame; right ﬁgure). But in the laboratory frame these
orientations are diﬀerent – this is a consequence of the relative meaning of
simultaneity as explained in Fig. 1.3. Note that the electron is subject to

6

1 Introduction

before SR

after SR
lightcone of A

t

t
A’s future

A

A’s future
elsewhere
A’s past

elsewhere

A
x

A’s past

x

Fig. 1.5. The causal structure of Minkowski space. In SR every event A has its
own invariant light-cone that divides Minkowski space into a past, a future and an
elsewhere.

an additional precession due to electromagnetic interaction with the nucleus.
The question arises whether a gyroscope in orbit around the Earth will also
exhibit a precession. At the time of writing, the Gravity Probe B mission is
performing the experiment, see further Ch. 8.
Lorentz metric
An important concept in SR is the interval ∆s2 between two events P and Q
with co-ordinates xα and xα + ∆xα :
∆s2 = c2 ∆t2 − ∆xi ∆xi = ηαβ ∆xα ∆xβ ;
⎛
⎞
1
∅
⎜ −1
⎟
⎟ .
ηαβ = ⎜
⎝
⎠
−1
∅
−1

(1.1)

(1.2)

Notation:
x0 = ct ,

∆t2 ≡ (∆t)2 .

(1.3)

Relation (1.1) deﬁnes the metric, i.e. the distance between two events in
Minkowski space, and is called the Lorentz metric. Here and everywhere else:
summation convention ; Roman indices run from 1 to 3 and Greek indices
from 0 to 3. Note that we adopt the signature : 1, −1, −1, −1.4
4

The sign convention is important as it leads to sign diﬀerences everywhere, but
it has of course no inﬂuence on the physics. The advantage of the present choice
is that for timelike geodesics the curve parameter p, the interval length s and
proper time τ are proportional, see § 2.5.

1.1 Special relativity (SR)

7

particle
m>0

photon
m=0
Fig. 1.6. The worldline of a particle with nonzero mass is located inside the lightcone, that of a photon is tangent to it.

Two events connected by a light ray have ∆s2 = 0, irrespective of their
spatial distance. No matter how enormous the distance of some objects in
the universe may be, the interval ∆s2 between them and the telescope is
zero. The value of ∆s2 is also invariant: if some other observer W computes
∆s2 ≡ ηαβ ∆xα ∆xβ in his rest-frame (i.e. in a comoving inertial frame), the
value he ﬁnds is equal to ∆s2 (proof: e.g. Schutz (1985), p. 11). This leads to
an important relation between events, see Fig. 1.5. Prior to the advent of SR,
all events were located either in the future, in the past, or they were simultaneous with a given event A. In SR there is A’s light-cone ∆s2 = 0 that divides
Minskowski space into a past and a future (with which A can have causal relations), and an ‘elsewhere’ (with which A cannot have any interaction). This
division is independent of the reference frame because ∆s2 is invariant. Hence
we can speak of the light-cone. The worldline of a particle with non-zero mass
is always located inside the light-cone, see Fig. 1.6. Depending on the value
of ∆s2 , the vector connecting events P and Q in Fig. 1.2 is called a
timelike vector :
null vector :
spacelike vector :

⎫
when ∆s2 > 0 ; ⎬
when ∆s2 = 0 ;
⎭
when ∆s2 < 0 .

(1.4)

The proper time interval ∆τ between two (timelike connected) events on the
worldline of a particle is deﬁned as:
c2 ∆τ 2 ≡ ∆s2 = c2 ∆t2 − ∆xi ∆xi .

(1.5)

For positive ∆s we may deﬁne ∆s ≡ (∆s )
and proper time intervals
as ∆τ = ∆s/c. Proper time intervals are invariant because ∆s2 is. By
transforming to the rest-frame of the observer W , so that ∆xi = 0, we
2
ﬁnd that ∆τ 2 = ∆t , which shows that the proper time is just the time
of a clock moving with the observer (his own wristwatch). Now substitute
∆xi = (∆xi /∆t) ∆t = v i ∆t in (1.5) and compute the limit:
2

2 1/2

dτ =

1 − (v/c)2 dt ,

(1.6)

8

1 Introduction

B

Dt1

A

Dt 2
clock
paradox
Dt1 > Dt2

Fig. 1.7. The clock paradox. Two clocks moving from event A to B along diﬀerent
worldlines indicate diﬀerent readings ∆τ for the duration of the trip.

where v is the speed of the particle (the ordinary 3-velocity). The proper time
∆τ elapsed between two events can be found by integrating (1.6) along the
worldline connecting the events. The answer will depend on the shape of the
worldline, which leads to the famous clock paradox, Fig. 1.7, explained in
detail in Schutz (1985), § 1.13.
Lorentz transformations
The co-ordinates xα and xα of an event with respect to two diﬀerent inertial
frames can be expressed into each other by means of a Lorentz transformation:
xα = Lαν xν .

(1.7)

Lαν

The
are constants that depend only on the relative velocity v of the two
frames. Relation (1.7) is a linear transformation that leaves ∆s2 invariant. If
the co-ordinate axes (x, t) and (x, t) are deﬁned as in Fig. 1.3 the transformation is
⎛
⎞
γ −βγ 0 0
⎜ −βγ γ 0 0 ⎟
⎟
(1.8)
Lαν = ⎜
⎝ 0
0 1 0⎠
0
0 0 1
with β = v/c and γ = (1 − β 2 )−1/2 . The mathematical formulation of SR
proceeds in terms of 4-vectors and tensors, that transform according to a
Lorentz transformation. The trick is to try and write the laws of physics as
relations between scalars, vectors and tensors only, because in that case they
are automatically invariant for Lorentz transformations.
Lorentz transformations are global. In GR we allow arbitrary curvilinear

1.1 Special relativity (SR)

W

d

9

W



v Dt
Fig. 1.8. An Einstein clock consists of photons traveling between two parallel mirrors at a distance d; the time for a round trip ∆t = 2d/c serves as the time unit. This
clock will run slower if it moves with respect to the observer because the photons
traverse a distance  > d while c is constant. The merit of this example is that the
time dilation is immediately obvious, but it is not so evident that it is impossible to
eliminate the eﬀect by using another clockwork. However, it can be shown that the
eﬀect is quite general and independent of the way the clock is constructed.

reference frames. As we shall see in § 2.3, the eﬀect is that the global Lorentz
transformation is replaced by a mesh of local Lorentz transformations that
are diﬀerent at each position in spacetime.

Exercise 1.1: Explain the time dilation with the help of Einstein’s clock,
Fig. 1.8:
(∆t)measured by W
.
(1.9)
(∆t)measured by W =
1 − v 2 /c2
Hint: W observes W ’s clock as it travels to the right at velocity v. W measures ∆t = 2/c, and 2 = d2 + (v∆t/2)2 = d2 + (v/c)2 , from which
 = d/ 1 − (v/c)2 , and (∆t)measured by W = 2d/c.
Exercise 1.2: Below relation (1.6) it was said that the proper time elapsed
between events depends on the worldline connecting the two events. Doesn’t
that contradict the fact that dτ is invariant?
Hint: In a given set of dτ , each dτ is invariant under co-ordinate transformations, but another integration path simply implies a diﬀerent set of dτ .

10

1 Introduction

1.2 General relativity (GR)
If we extend SR to arbitrarily moving reference frames, we would be able to do
physics from the point of view of an accelerated observer. There is, however,
another important motivation. Since in doing so apparent forces appear that
are closely related to gravity, we may perhaps also be able to address gravity. And this turns out to be true. But if we are only after gravity, it would
seem more straightforward to try and incorporate gravity in the framework of
SR. Unfortunately, that doesn’t work. Newtonian gravity may be summarised
by ∇2 Φ = 4πGρ and K = −m∇Φ. It follows that gravity operates instantaneously – a change in ρ alters Φ everywhere at the same moment. This is
inconsistent with SR because what is instantaneous in one frame is no longer
so in another. This theory holds therefore only in one preferred frame. The
problem might be overcome by replacing the equation for the potential by
Φ = (c−2 ∂ 2 /∂t2 − ∇2 )Φ = −4πGρ, for example, but then other diﬃculties
appear, see e.g. Robertson and Noonan (1969) and Price (1982). Special relativistic theories of gravity using a ﬂat spacetime and a single global reference
frame don’t work because they cannot accommodate the gravitational redshift
and the weak equivalence principle. A diﬀerent approach is needed.
Weak equivalence
At this point we need to be more precise about the concept of mass. A force
K acting on a particle with inertial mass mi causes an acceleration a given
by Newton’s law K = mi a. The inertial mass expresses the fact that objects resist being accelerated. To compute the force K we need the ﬁeld(s)
in which the particle moves, and the charge(s) that couple to those ﬁeld(s).
For example, K = q(E + v × B/c) for a particle with electric charge q moving with speed v in an electric ﬁeld E and magnetic ﬁeld B. For a particle
with a gravitational charge mg , usually called the gravitational mass, we have
K = −mg ∇Φ. It follows that a = −(mg /mi )∇Φ.
It is an experimental fact that materials of diﬀerent composition and mass
experience exactly the same acceleration in a gravitational ﬁeld. Eötvös veriﬁed that with an accuracy of 10−8 in 1896, and Dicke attained 2 × 10−11
in 1962. Both experiments used a torsion balance. Presently, torsion balance
and free-fall experiments achieve an accuracy of ∼ 10−12 . 5 6 Hence mg /mi is
5

6

Chen and Cook (1993) § 4.8; Will (1993) Ch. 14. With the help of lunar laserranging an accuracy of 7 × 10−13 has been achieved (Dickey, J.O. et al., Science
265 (1994) 482). The idea is that the lunar orbit as a whole must be displaced
along the Earth-Sun line in case the Moon and the Earth experience a slightly
diﬀerent acceleration with respect to the Sun.
The gravitational constant G, however, is only known with a precision of a few
times 10−4 .

1.2 General relativity (GR)

11

a universal constant, taken to be unity in classical mechanics. This is called
the weak principle of equivalence. It follows that the concept of gravity loses
its meaning, as the ﬁeld can be made to vanish by transforming to a freely
falling reference frame. For an electromagnetic ﬁeld this is impossible as q/mi
is most certainly not a universal constant. From this Einstein (and others
before him) concluded that light must be deﬂected by a gravitational ﬁeld,
because it moves along a straight line in a freely falling frame where there is
no gravity. This trick of transforming gravity away works only locally. In a
frame that moves with a freely falling particle, neighbouring particles will initially move uniformly with respect to each other, but not after some time. In
the famous elevator thought experiment it is impossible to distinguish locally
gravity from an externally imposed acceleration. But a distinction is possible
by observing two test particles at some distance from each other, because the
latter is homogenous while the former is not. The so-called tidal forces cannot
be transformed away, because a ‘real’ gravitational ﬁeld is inhomogenous.
The fact that inertial and gravitational mass are identical is an unexplained coincidence, in some sense comparable to the unexplained coincidence
in Ptolemy’s world model, Fig. 1.1. Einstein took that as a basis for a new
theory. The fact that motion in a gravitational ﬁeld depends neither on the
composition nor on the mass of the particles suggests that the particle orbits
might perhaps be determined by the structure of spacetime. In SR the worldlines of free particles are straight, independent of the nature of the particles.
If we now switch on gravity, maybe a more general formulation is possible, in
which the worldlines remain ‘straight’ (i.e. geodesics) in a curved spacetime.7
In that case gravity would no longer be a force, but rather a consequence of
the curvature of spacetime. The elaboration of this idea is what we now know
as the theory of General Relativity (GR). Global inertial frames no longer
exist, only local inertial frames do. For according to GR there are no forces
working on freely falling particles, while it is at the same time not possible to
deﬁne a reference frame in which two freely falling particles move uniformly
with respect to each other.
Curvature
That curvature is the way to go ahead may be gleaned, for instance, from
the experiments of Pound, Rebka and Snider.8 Photons moving vertically in
the Earth’s gravity ﬁeld turn out to be slightly redshifted, see Fig. 1.9. The
7

8

A space is said to be ﬂat when Euclides’s 5th postulate on the existence of a
single parallel holds (in metric terms: the Riemann tensor is zero). A space is said
to have Euclidean geometry if the metric can be cast in the form ds2 = dxα dxα .
The Minkowski spacetime of SR is ﬂat but not Euclidean.
Pound, R.V. and Rebka, G.A. Phys. Rev. Lett. 4 (1960) 337; Pound, R.V. and
Snider, J.L. Phys. Rev. B 140 (1965) 788.

12

1 Introduction

t
Dt1

Dt0
g
z0

z1

z

Fig. 1.9. The Pound-Rebka-Snider experiment. Photons move vertically upwards
over a distance of z1 − z0 = 22.5 meters and get redshifted.

required precision could be attained with the help of the Mössbauer eﬀect.
The worldlines of subsequent wave crests in the Minkowski diagram must
be congruent because the gravity ﬁeld does not depend on time. Therefore
∆t0 should be equal to ∆t1 , regardless of the shape of the worldlines, but
the experiment shows that ∆t1 > ∆t0 (a redshift). This suggests (but does
not prove) that one can no longer assume that the Minkowski spacetime is
globally ﬂat in the presence of gravity. A curved spacetime is descibed by a
local metric:
(1.10)
c2 dτ 2 = ds2 = gαβ dxα dxβ ,
and ds2 is the interval (‘distance’) between two events at xα and xα + dxα ; gαβ
is the metric tensor. The relation ds2 = c2 dτ 2 between interval and proper
time is taken to remain valid (for particles with mass), but the relation between dt and dτ is no longer as simple as in (1.6) because gαβ = ηαβ . The
possibility of transforming gravity away locally amounts to the following requirement: at any point {xµ } of spacetime there should exist a transformation
that casts (1.10) into the SR form ds2 = ηαβ dxα dxβ . In doing so we have
constructed a local inertial (i.e. freely falling) frame in {xµ } where gravity
does not exist9 – provided the frame is not too big, otherwise we will notice
the eﬀect of curvature in the form of tidal forces. Suﬃciently small sections of
spacetime are ﬂat, ‘small’ meaning small compared to the typical dimension of
the system (the Schwarzschild radius, the scale factor S of the universe, etc.).
Spacetime curvature and tidal forces will be the hallmark of a real gravitational ﬁeld. Weight is merely a pseudo-force caused by being in the wrong (not
freely falling) frame, just as centrifugal and Coriolis forces are pseudo-forces
caused by being in a wrong (rotating) frame.
9

The terms ‘local inertial frame’ and ‘local freely falling frame’ will be used interchangeably. A local rest-frame is a local inertial frame in which a particle or an
observer is instantaneously at rest.

1.2 General relativity (GR)

13

Strong equivalence and general covariance
In order to generalise existing physical laws to GR we broaden the scope of the
weak equivalence principle, and assume that it is impossible to detect locally
any eﬀect of gravity in a freely falling frame, whatever other forces may be
acting. In other words, in a freely falling frame all laws of physics have the
form they have in SR in the absence of gravity. This is called the strong principle of equivalence. These laws / equations are then generalised by replacing
the tensors that appear in them by tensors that are invariant for arbitrary
co-ordinate transformations instead of only for Lorentz transformations. This
is called the principle of general covariance. The application of this principle
is somewhat arbitrary, as we shall see, but the obvious way out of adopting
the simplest possible generalisation has sofar proven to be eﬀective. The term
‘principle of general covariance’, incidentally, is misleading in that it has nothing to do with the covariant form of tensors. Principle of general invariance
(for arbitrary co-ordinate transformations) would have been a much better
name. Note also that general covariance has no deeper signiﬁcance of its own
(Friedman, 1983). It is a self-imposed regime of great heuristic value in ﬁnding
physically correct equations, in some way comparable to checking the correct
dimension of an expression.
Mach’s principle
A number of ideas, collectively known today under the name Mach’s principle,
have strongly inﬂuenced Einstein in his formulation of GR. Mach rejected the
Newtonian concept of absolute space, as Leibniz had done earlier. Mach was
struck by the fact that the frame deﬁned by the distant matter in the universe
happens to be an inertial frame, and that inertia manifests itself only if masses
are accelerated with respect to this frame. He argued that this cannot be just
a coincidence, and that the inertial mass may somehow be ‘induced’ by the
gravitational mass of all matter in the universe. This led Einstein to seek
a theory in which the geometry of spacetime, i.e. gαβ , is determined by the
mass distribution. The frame-dragging eﬀect near rotating massive objects, for
example (Ch. 6), may be seen as a manifestation of Mach’s principle. However,
Gödel’s solution10 of the ﬁeld equations indicates that Mach’s principle is only
partially contained in GR as it is presently formulated, see Friedman (1983)
for more information.

Exercise 1.3: GR and cosmology are ﬁelds of many principles. Formulate
in your own words the meaning of these principles: relativity, strong and
weak equivalence principle, Mach, general covariance, cosmological and the
anthropic principle (§ 13.4).
10

Gödel, K., Rev. Mod. Phys. 21 (1949) 447.

14

1 Introduction

1.3 The need for GR in astrophysics
While SR was born out of the need to resolve a major conﬂict, namely the
failure to measure the velocity (of the Earth) with respect to the ether, GR
was created rather for esthetic reasons: the wish to have a relativistic theory of gravity. But there was no compelling conﬂict with observations that
called for a solution. The problem of the perihelium precession of Mercury
was known at the time, but was considered to be a nut for the astronomers to
crack – not as a stumble block to progress in physics. Consequently, after its
conception, GR remained for a long time what is was: an elegant but rather
inconsequential theory that was accepted by the physics community precisely
because of its elegance. After the correct prediction of the perihelium shift
and the spectacular conﬁrmation of the deﬂection of starlight in 1919, there
weren’t many other things that could be measured. The technology of the day,
for example, was inadequate to detect the gravitational redshift in the solar
spectrum. SR on the other hand, led to many observable consequences and
was soon completely integrated in the framework of physics as an indispensable basic element. It was recognised that GR was relevant for cosmology,11
but in the ﬁrst half of 20th century cosmology was very much a slightly esoteric ﬁeld that a decent physicist did not touch, because there were very few
observations that could show the way. Notions such as a hot big bang, light
element synthesis and structure formation were as yet unheard of. And so
GR remained outside the mainstream of physics. That state of aﬀairs began
to change only in the second half of the 20th century. In particular the 60ies
saw a rapid succession of novel developments and discoveries. Technological
advances led to a demonstration of the gravitational redshift in the laboratory
(1960), soon followed by a measurement in the solar spectrum (1962). Radar
reﬂections from Venus (1964) showed that the travel time of light increases
when it moves closely past the Sun. This eﬀect had been predicted by GR
as a consequence of the warping of spacetime near a massive object, causing
distances to be generally longer.

Astrophysics, too, began to proﬁt from several new developments. Most
important were the emergence of radio astronomy, and the possibility to deploy instruments in space which opened up the ﬁeld of X-ray astronomy.
Non-solar X-rays were ﬁrst detected in 1962 and led to the discovery of Xray binaries. The X-ray emission is believed to be due to accretion of matter
onto a neutron star or black hole, two objects whose existence is predicted by
GR. The energy released per unit mass by accretion on such a compact object
depends on various parameters, and is of the order of 10% of the infalling rest
mass energy – a factor 10-20 more than hydrogen fusion. As the matter falls
into the deep potential well, it is heated to X-ray temperatures and serves as
11

In particular the work of Lemaı̂tre was inﬂuential in this regard (Lemaı̂tre, G.,
Ann. Soc. Sci. Bruxelles 47A (1927) 49 and M.N.R.A.S. 91 (1931) 483).

1.3 The need for GR in astrophysics

0

black holes (BH)
neutron stars (NS)

merging BH,NS
collapsing stars
cosmology

accretion
flows
gravitational waves
binary compact
objects
PN
ap
p

ati
xim
ro

gravitational potential rs / R

1

15

classical
mechanics

velocity (v / c)2

special
relativity
1

Fig. 1.10. A classiﬁcation of some applications of General Relativity. For weak
ﬁelds there is only the horizontal axis. The world of GR unfolds as we move upward
to stronger gravitational potential, measured by rs /R = 2GM/Rc2 ∼ |Φ|/c2 , where
Φ ∼ −GM/R and R = typical size of the object (rs = its Schwarzschild radius). The
post-Newtonian approximation gives ﬁrst order corrections to classical mechanics.
Neutron stars and black holes are in the strong ﬁeld corner. Binary objects have
Φ ∼ −v 2 and are approximately on the grey diagonal, slowly moving up to their
eventual merger and generating gravitational waves as they do so. The latter may
also be generated to the left of the diagonal (oscillating / rotating neutron stars)
or to the right (close encounters). To position cosmology the universe is considered
to be a compact object with expansion velocities approaching c near the horizon
(though ﬁelds and velocities are locally small).

a bright probe of conditions very close to the compact object. In some cases
the mass of the object could be shown to be larger than 3M . Since this
is larger than the theoretical maximum mass of a neutron star, the object
is, in all likelihood, a black hole. Accretion ﬂows thus provide an important
diagnostic tool of these compact systems, but it is not the only one. Direct
proof of the existence of neutron stars came in 1967 with the discovery of
pulsars. It was soon realised (1968) that pulsars are spinning neutron stars
equipped with a radio beacon, a feat no one had ever dreamt of. Neutron
stars had been hypothesized by Baade and Zwicky (1934) following the discovery of the neutron (1932). They suggested that neutron stars are formed
in a supernova explosion, a gravitational collapse of a heavy, evolved star that
has run out of nuclear fuel. In 1939 Oppenheimer and Volkoﬀ calculated the
structure of a neutron star and showed that it is completely determined by

16

1 Introduction

GR. Now, after 33 years, it turned out that these objects actually did exist.
And if stellar evolution, that great creator, can make neutron stars, it may
very well produce black holes too. These and other developments led to a
revival of theoretical studies in GR which had been stagnant for years. The
properties of these mysterious black holes and the generation of gravitational
waves, for example, drew much attention. Experimental gravity received a
boost as well, leading to the development of detectors for gravitational waves
and Gravity Probe B, a space mission for detecting relativistic precession effects – to name only two.
The ﬁrst binary pulsar was discovered by Hulse and Taylor in 1975. This
system turned out to be a perfect cosmic experiment featuring two neutron
stars in a tight orbit, one of which is a precision clock. Since the system is
clean, application of GR permitted determination of all system parameters. In
1979 it was shown that the system loses energy at a rate that is consistent with
energy loss by gravitational waves. This is a strong if indirect argument for
the existence of gravitational waves. Several of these binaries have now been
found, and there should be many more out there that we cannot see because
they contain no pulsar. However, the gravitational waves they emit should
be detectable. As the binary loses energy it shrinks and moves slowly along
the diagonal in Fig. 1.10 until the components merge in a gigantic explosion,
unleashing a ﬁnal burst of gravitational radiation and γ-rays into space which
should be visible throughout the universe. Perhaps this is the explanation of
the so-called short-duration γ-ray bursts, whose nature is still not understood.
And the hunt for gravitational waves is on: detectors for gravitational waves
are in an advanced state of development and several are operating in science
mode.
The discovery of quasi-stellar objects or quasars (1963) showed that there
are distant objects that are typically 100 times brighter than ordinary galaxies
in our neighbourhood. It was gradually understood that these and other objects (Seyferts, BL Lac objects,..) are diﬀerent visual manifestations of active
galactic nuclei (AGNs) with a huge power release, up to 1048 erg s−1 . Rapid
variability pointed to a small gravitational powerhouse casting as the main
actors a black hole of 106 − 109 M , a surrounding disc swallowing matter (in
some cases as much as 10−100 M per year), and collimated bipolar outﬂows.
Another line of evidence for the existence of massive black holes comes from
galactic rotation curves which demonstrate that many galaxies contain heavy
objects (106 − 109 M ) within a small radius at the centre, very likely a black
hole. And there is very strong evidence that a ∼ 3.6 × 106 M black hole is
lurking at the centre of our own galaxy, which is currently not accreting any
appreciable amount of mass.
The gravitational deﬂection of light by the Sun discovered in 1919 received a spectacular follow-up in 1979 when the quasars Q0957+561 A and

1.3 The need for GR in astrophysics

17

B were identiﬁed as two images of the same object whose light is deﬂected by
an intervening galaxy. Many gravitational lenses have been found since then.
In principle this opens the possibility to weigh the lens including the dark
matter it contains, and to study magniﬁed images of very distant objects.
There have been many other advances in cosmology, but there are two that
outshone all others. The ﬁrst is the cosmic microwave background (CMB),
discovered in 1965, with suggestions as to its existence dating back to 1946.
The CMB was a monumental discovery that marked the beginning of cosmology as a quantitative science. It put an end to the so-called steady state
model and permitted for example a quantitative prediction of the synthesis
of the light elements in the universe (1967), which has been conﬁrmed by
observations. The latest highlight is the WMAP mission which has measured
the tiny ﬂuctuations in the temperature of the CMB across the sky. This
has resulted in a determination of the basic parameters that ﬁx the structure
and evolution of our universe. The second very important development was
of a theoretical nature and took place in 1981: the discovery of the possibility
of an inﬂation phase right after the Big Bang. The inﬂation concept repairs
some basic defects of the classical Friedmann-Robertson-Walker cosmology
that had to do with causality. The inﬂation paradigm is very powerful but
speculative. Pending some unsettled ‘ﬁne-tuning’ it seems to explain why the
universe expands, why it is homogeneous and ﬂat, as well as the origin of the
density ﬂuctuations out of which galaxies evolve later.
This overview illustrates that GR is nowadays being studied in all corners of the diagram of Fig. 1.10. The ﬁeld has really opened up and there is
a great sense of anticipation and promise of new results every day. Particle
physicists turn to cosmology in the hope to ﬁnd answers to questions that
particle accelerators seem unable to address. This symbiosis of cosmology and
particle physics has sparked oﬀ the new ﬁeld of astroparticle physics. And
although it may take years before the detectors for gravitational waves currently in operation actually observe a wave, it may also be tomorrow! This
element of suspense and impending surprise renders GR and its application
to astrophysics and cosmology a highly attractive ﬁeld, and some of the thrill,
it is hoped, will transpire in the following chapters.

2
Geometry of Riemann Spaces

The fact that the geometry of the space in which we live is Euclidean is a very
basic daily experience. This may explain why it took so long before it was
realised that this may actually not be correct, and that the question of the
geometry of the space around us is a matter of empirical assessment. Early in
the 19th century Gauss studied the geometry of curved surfaces, and showed
that all references to a ﬂat embedding space could be eliminated. In the same
way Riemann formulated in 1854 the geometry of 3D spaces. He found that
Euclidean geometry is merely one possibility out of many. Riemann’s method
could be generalized to spaces of arbitrary dimension. The geometry of these
curved Riemann spaces is wholly described within the space itself, by the use
of co-ordinates and the metric tensor. No embedding is required. These geometrical concepts gradually spread beyond the mathematical incrowd, and in
the last quarter of the 19th century the idea that a fourth (spatial) dimension might exist had mesmerized the public’s imagination, perhaps even more
so than black holes did a century later. One of the products of that period
was Abbott’s famous Flatland.1 The ﬂatland analogy is nowadays a standard
technique of teachers to explain some of the intricacies of curved spaces.
The theoretical framework of Riemann spaces is also the starting point
for the mathematical formulation of GR. In this chapter we discuss the tools
that any student should master in order to be able to deal with GR beyond the level of handwaving. In doing so we have deliberately chosen to stay
close to intuition as that outweighs the merits of rigour, certainly on ﬁrst
acquaintance.

2.1 Deﬁnition
A Riemann space has the following properties:
1

Abbott, E.A.: 1884, Flatland: A Romance of many Dimensions, by a Square,
Seeley & Co. (London).

20

2 Geometry of Riemann Spaces
p

j

q

q

0
geometrical picture

j

2p

co-ordinate picture

Fig. 2.1. A geometrical picture and the corresponding co-ordinate picture of the
space deﬁned by (2.2). Co-ordinate pictures will be frequently used.

1. Any point can be identiﬁed by a set of co-ordinates {xµ }; the number of
independent xµ is called the dimension.
2. It is possible to deﬁne continuously diﬀerentiable functions of {xµ }, in
particular one-to-one co-ordinate transformations {xµ } ↔ {xν }.
3. There is a metric that speciﬁes the distance ds2 between two nearby points
xµ and xµ + dxµ :
ds2 = gαβ dxα dxβ ;

gαβ = gβα .

(2.1)

An antisymmetric part of gαβ does not contribute to ds2 . Example: a spherical
surface with radius 1 and co-ordinates θ, ϕ:
ds2 = dθ2 + sin2 θ dϕ2 .

(2.2)

Notation: dθ2 ≡ (dθ)2 , dϕ2 ≡ (dϕ)2 , but ds2 = (ds)2 only if ds2 > 0 as in
(2.2). But the metric is in general not positive deﬁnite! In this simple case the
geometrical structure may be visualised through embedding in an Euclidean
space of one higher dimension, but for Riemann spaces of higher dimension
this is no longer possible. Moreover, a Riemann space of dimension D cannot
always be embedded in a ﬂat space of dimension D + 1. It is often useful
to draw a co-ordinate picture of a suitably chosen subspace, even though it
contains no information on the geometry, see Fig. 2.1.
An important point is that the metric determines the local structure of the
space, but reveals nothing about its global (topological) structure. A plane,
a cone and a cylinder all have the same metric ds2 = dx2 + dy 2 , but entirely
diﬀerent global structures.

2.2 The tangent space

21

4
ey

ei
3

ex

{ xa + dxa }
2

ey
ex
x =1

P

ds

{ xa }
y=1
2

3

ej

4

Fig. 2.2. Co-ordinate lines and base vectors spanning the tangent space. The choice
of the co-ordinates is entirely free, and in practice dictated by the question which
co-ordinates are the most expedient to use.

2.2 The tangent space
In each point we construct a set of base vectors tangent to the co-ordinate
lines, as in Fig. 2.2. The arrow points towards increasing xi . The base vectors span the ﬂat tangent space, which has the same dimension as Riemann
space. This construction evidently requires the existence of a ﬂat embedding space, but that can be avoided as follows. Consider the curves {xα (p)}
through a point P in Riemann space (p = curve parameter), and construct
Aσ = [dxσ /dp]P . These vectors Aσ span the abstract tangent space of P ,
which exists independent of any embedding. Usually, however, the abstract
tangent space may be identiﬁed with the tangent space constructed in Fig. 2.2.
For our discussion there is no real advantage in making the distinction and
we shall work with the intuitive picture of Fig. 2.2.
We may use any metric we like in the tangent space, but there exists a
preferred metric. Consider an inﬁnitesimal section of Riemann space. This section is ﬂat and virtually coincides with the tangent space. To an inﬁnitesimal
vector ds = dxα eα in the tangent space we may therefore assign the length of
the line element ds in Riemann space, i.e. we require ds · ds = ds2 :
ds · ds = (dxα eα ) · (dxβ eβ ) = eα · eβ dxα dxβ
= gαβ dxα dxβ ,

(2.3)

and it follows that
gαβ ≡ eα · eβ .

(2.4)

Here · represents the vector inner product. This may be the usual inner
product, for example when we deal with 2D surfaces embedded in a ﬂat R3 .
But in case of the Minkowski spacetime of SR, and in GR, the inner product is

22

2 Geometry of Riemann Spaces

not positive deﬁnite, and we may have that A · A < 0 (for spacelike vectors).
By taking dxα = 1 in (2.3) and all other dxβ = 0 we see that eα · eα = ds · ds
(no summation). It follows that the ‘length’ of eα corresponds to a jump
∆xα = 1, at constant value of the other co-ordinates. Due to the curvature
this is of course only approximately correct. These base vectors are called a
co-ordinate basis because they are deﬁned entirely by the co-ordinates and
the metric. The length of the base vectors depends on the choice of the coordinates, and is in general a function of position. Consider for example polar
co-ordinates in a plane, Fig. 2.3. The length of er is constant, while |eϕ | ∝ r :
ds2 = 1 · dr2 + r2 dϕ2 .
↑
↑
er · er
eϕ · eϕ

(2.5)

Now that we have deﬁned the basis we may construct ﬁnite vectors A = Aα eα
in the tangent space through the usual parallelogram construction. These so
called contravariant components Aα are the components of A along the basis.
The next step is to deﬁne another (covariant) representation Aα of A by
demanding that A · A = Aα Aα , for every A:
A · A = (Aα eα ) · (Aβ eβ ) = gαβ Aβ Aα ≡ Aα Aα ,

(2.6)

which leads to:
Aα = gαβ Aβ .

(2.7)

In a more advanced treatment a distinction is made between tensors as geometrical objects, their contravariant representation located in an abstract tangent space, and the dual tangent space, in which the covariant representations
reside. In the current, more primitive context the following interpretation suggests itself. Since Aγ = gγβ Aβ = eγ · eβ Aβ = (Aβ eβ ) · eγ = A · eγ , it follows
that Aγ is the projection of A on eγ . Hence, the contravariant components
Aβ are the components of A along the base vectors eβ (parallelogram construction), while the covariant component Aα is the projection of A on the
base vector eα , Fig. 2.3, right:
contravariant (Aβ ) : A = Aβ eβ ,
covariant (Aα ) : Aα = A · eα .

(2.8)
(2.9)

Finally, the concept of index raising and lowering. We can lower an index
with the help of (2.7). The inverse operation of raising is deﬁned as:
Aγ = g γα Aα .

(2.10)

The meaning of g γα can be gleaned from:
Aγ = g γα Aα = g γα gαν Aν ,

(2.11)

2.3 Tensors

Aj

ej
Dj = 1

Aj

ej
ei

er

A

P

Dr = 1

23

Ai
Ai

Fig. 2.3. Left: polar co-ordinates and the base vectors er and eϕ . Right: interpretation of the contravariant and covariant representation of a vector A.

so that g γα gαν = δγν , i.e. {g γα } is the inverse of {gαν }. In summary:
⎫
⎪
index lowering : Aα = gαβ Aβ ,
⎪
⎪
⎬
γ
γν
index raising :
A = g Aν ,
⎪
⎪
⎪
⎭
{g γν } = {gαβ }−1 .

(2.12)

We have silently adopted the summation convention: if an index occurs twice,
once as a lower and once as an upper index, summation over that index is
implied. Note that the rules for index raising and lowering are always valid,
and have nothing to do with the question whether one is dealing with a tensor
or not. The tensor concept is related to behaviour under co-ordinate transformations, which was not an issue above, and to which we turn our attention
now.

2.3 Tensors
We are now in a position to do linear algebra in the tangent space, but we
leave that aside and study the eﬀect of co-ordinate transformations. Consider

two overlapping sets of co-ordinates {xµ } and {xµ }. The notation is sloppy


µ
– it would be more appropriate to write {x } instead of {xµ }, but {xµ } is

much more expedient if used with care. A displacement δxµ is related to a
displacement δxν through:
µ

δx




∂xµ
=
δxν ≡ xµ,ν δxν .
∂xν

(2.13)

24

2 Geometry of Riemann Spaces

Notation:
X,ν ≡

∂X
;
∂xν

X,νρ ≡

∂2X
,
∂xν ∂xρ

etc.

(2.14)

where X can be anything (Aα , g αβ , ...). We may freely interchange indices
behind the comma: X,αβγ = X,αγβ = X,γαβ etc.
Any set Aµ transforming according to (2.13) is called a contravariant tensor of rank 1:


(2.15)
Aµ = xµ,ν Aν ↔ Aν contravariant.
Hence δxν is a contravariant tensor. Tensors of rank 1 are often referred to
as vectors, and henceforth we shall use the word vector in this sense only. A
function such as the temperature distribution T (x) is called a scalar, a tensor
of rank zero. Its value in a point is independent of the co-ordinate system, i.e.
invariant for co-ordinate transformations: T  (x ) = T (x), where T  is the new
function prescription. The derivative of a scalar Q,
Bµ =

∂Q
≡ Q,µ
∂xµ

(2.16)

transforms like Bµ = Q,µ = Q,ν xν,µ = xν,µ Bν . Every Bν that transforms
in this way is called a covariant vector or tensor of rank 1:
Bµ = xν,µ Bν

↔

Bν covariant.

(2.17)

From two covariant vectors we can form Tµν = Aµ Bν , a covariant tensor of
rank 2. More general tensors can be constructed through summation, Tµν =
Aµ Bν + Cµ Dν + ... This process may be continued: Tαβ C γ and Aµ C ν Bρ are
mixed tensors of rank 3 (provided T, A, B and C are tensors themselves). The
indices of tensors of higher rank transform according to (2.15) resp. (2.17),
for example:




(2.18)
T αβ  γ  δ = xα,µ xν,β  xσ,γ  xδ,τ T µνσ τ .
There is no other choice because (2.18) must hold for the special tensor
T αβγ δ = P α Qβ Rγ S δ , and the transformation rules for vectors have already
been ﬁxed! Note that we get a glimpse here of how the Lorentz transformations of SR will be generalised in GR: relation (1.7) of SR will be replaced by
(2.15). This transformation is still locally linear, but diﬀerent in each point

of Riemann space as the {xµ,ν } are functions of position. The single global
Lorentz transformation will be replaced by a mesh of local Lorentz transformations.
The horizontal position of the indices is important: T µν is diﬀerent from
Tν ! The summation over double indices is called contraction. It lowers the
rank by two. For example T µµ , T αβα γ , Pαβ Qβγ , T αβα β (double contraction).
Double indices are dummies: T αα = T µµ , dummies may occur only twice, once
µ

2.3 Tensors

25

as an upper and once as a lower index. If you encounter expressions like C µµ ,
P αβ Qαγ or Pαβ Qαγ Rδα then you have made a mistake somewhere!
Index raising and lowering, ﬁnally, is done by factors gαβ or g µν for each
upper/lower index, e.g.:
⎫
⎬
T µν = g µα Tα ν ,
(2.19)
T α β γδ = g αµ gβν g δσ Tµ νγ σ , etc. ⎭
Again, like in (2.18), we have hardly any other choice here, because (2.19)
must hold for the special tensors T µν = P µ Qν and T αβγδ = P α Qβ Rγ S δ , and
the rules for index raising and lowering for vectors have already been ﬁxed.
We are now in a position that we can raise and lower indices at liberty. We
emphasise once more that the rules (2.12) and (2.19) for index gymnastics are
generally valid, also for non-tensors. For example, Qµν = Aµ,ν is not a tensor
(exercise 2.4), and yet Qµν = g µα Qαν .

Exercise 2.1: The unit tensor is deﬁned as δαβ = 1 for α = β, otherwise
0. Prove that δαβ is a tensor, and that δαβ = δβ α , so that we may write δα
β
without risk of confusion. Show that δαβ = gαβ . Is ηαβ a tensor? And gαβ ?
One could deﬁne δαβ = 1 for α = β, and 0 otherwise, but then δαβ is not a
tensor.










Hint: δαβ  must be equal to xα,ν xµ,β  δνµ , or δαβ  = xα,ν xν,β  = xα,β  (chain
rule) = 1 for α = β  otherwise 0. Hence δαβ is tensor. And δβ α =
gβµ g αν δµν = gβµ g αµ = g αµ gµβ = 1 for α = β, otherwise 0, i.e. identical
to δαβ ; δαβ = gαν δνβ = gαβ ; ηαβ is a tensor in SR only, i.e. under Lorentz
transformations; gαβ tensor: use (2.1), require that ds2 is also tensor in GR
(invariant scalar), and dxα is tensor, then exercise 2.3. Other deﬁnition δαβ :
δα β  = xν,α xµ,β  δνµ ? No, because the chain rule can no longer be used.
Exercise 2.2: If T αβ and P µν are tensors then P µµ is a scalar, but T αα is
not. The inner product Aν B ν of two vectors is a scalar.




Hint: P µµ = xµ,α xβ,µ P αβ , then the chain rule.
Exercise 2.3: Quotient theorem: If Aλ Pλµν is a tensor for arbitrary vector
Aλ , then Pλµν is a tensor; µν may be replaced with an arbitrary sequence of
upper / lower indices.

26

2 Geometry of Riemann Spaces


Hint: Aλ Pλµν is a tensor, i.e. Aλ Pλ µ ν  = xα,µ xβ,ν  Aσ Pσαβ (λ and σ are

dummies!), then substitute Aσ = xσ,λ Aλ , etc.
Exercise 2.4: The derivative Aµ,ν of a covariant vector Aµ is not a tensor,
as it transforms according to:
Aµ ,ν  = Aα,β xα,µ xβ,ν  + Aα xα,µ ν  .

(2.20)

The problem is in the second term of (2.20). In SR only linear (Lorentz)
transformations are allowed. In that case the second term is zero and Aµ,ν is
a tensor.
Hint: Start from Aµ ,ν  = (xα,µ Aα ),ν  , then use the product rule.
Exercise 2.5: Prove Tα ν Aν = Tαν Aν ; Tα α = T αα ; g νν = 4 ;
g 00 − g 11 − g 22 − g 33 .

η νν =

Hint: We know that g νν = g να gαν = δνν = 4. The following may be illuminating: the scalar g νν is invariant, compute in a freely falling frame: g νν = η νν ,
SR holds in that frame: η νν = η να ηαν = 4. But in GR: η νν = g να ηαν = etc.

2.4 Parallel transport and Christoﬀel symbols
Consider a particle at position P in Riemann space, Fig. 2.4. The vectors
associated with it (velocity, spin, ..) reside in the tangent space of P . At some
later time the particle has moved to position Q, but the tangent space of Q
does not coincide with that of P . To be able to do dynamics, we must develop
a way to compare vectors in the diﬀerent tangent spaces along the worldline
of the particle. In other words, we need something against which to gauge the
concept of ‘change’. This is what parallel transport in GR is about.
Fig. 2.4 shows the curve xσ (p) in Riemann space. The vector A is always
in the tangent space, but the tangent spaces of P, Q, R, .. are disjunct, and
comparison of A(P ) with A(Q) or A(R) is not possible. To this end we deﬁne
a connection between tangent spaces, that is, a mathematical prescription
telling us how a vector A(P ) lies in the tangent space of Q if we ‘transport’
it along a given path from P to Q. This can be done in a variety of ways, but
much of the mathematical freedom that we have is eliminated by the physical

2.4 Parallel transport and Christoﬀel symbols

27

Parallel transport in flat embedding space
Projection onto tangent space

A(P)

A'(Q)
A'(R)
A(Q)
Q

P

A'(S)

R
A(R)

{x a}

{x a + d xa}

S
A(S)

xs(p)

Fig. 2.4. Conceptual deﬁnition of parallel displacement of a vector along a curve
xσ (p) in Riemann space: ﬁrst an ordinary parallel displacement in the ﬂat embedding
space (resulting in the dashed arrows) followed by projection on the local tangent
space. The process is repeated in inﬁnitesimal steps.

requirement that we recover what we ordinarily do when we transport a vector
parallel to itself in a ﬂat space. Imagine the Riemann space embedded in a
ﬂat space of higher dimension. We know how to move A(P ) around parallel
to itself in this embedding space, because it is ﬂat. Having arrived in Q, the
result is projected onto the local tangent space. To order O(dxα ) projection
does not change the length of the vector: the projection angle γ is O(dxα ),
but cos γ = 1 up to O(dxα ). This process is now repeated with inﬁnitesimal
steps, and generates the coloured vector ﬁeld A in Fig. 2.4, starting from
A(P ). In this way we have generalized the concept of parallel transport to
curved spaces, in such a way that it reduces to normal parallel transport for
ﬂat spaces. Not surprisingly, it is also the deﬁnition that turns out to work in
GR. The result of the transport operation depends on the path, see Fig. 2.5.
However, when e in Fig. 2.5 is parallel-transported along a small curve on the
sphere there is virtually no change, because there is hardly any curvature felt
(exercise 2.17).
We now formalise our intuitive approach. The diﬀerence dA = A(Q) −
A(P ) is not deﬁned, but up to order O(dxα ) we have that dA A(Q)−A (Q),
and this is useful as both vectors lie in the same tangent space. The vector
dA may be interpreted as the intrinsic change of A, after correction for the
‘irrelevant’ change in the orientation of the tangent space:
dA

A(Q) − A (Q)
= d(Aµ eµ ) = (dAµ )eµ + Aµ (deµ ) .

(2.21)
(2.22)

28

2 Geometry of Riemann Spaces
N

x0
e
Q

P

x3 = j
x1

=r

Fig. 2.5. Left: Parallel displacement of the vector e along P N Q and along P Q
produces entirely diﬀerent results. To the right, the geodesic precession of a top in
orbit around a central mass, see text.

Here, dA has been split into two contributions: the change dAµ ≡ Aµ (Q) −
Aµ (P ) of the contravariant components of A, and a contribution from the
change of the base vectors. On general grounds we anticipate deµ to be proportional to both {dxβ } and {eα }:
deµ = Γαµβ dxβ eα .

(2.23)

Γαµβ is called the Christoﬀel symbol of the second kind, and as may be expected
it is intimately related to the metric tensor:
Γµνσ =

1 µλ
2g

(gλν,σ + gλσ,ν − gνσ,λ ) ≡ g µλ Γλνσ .

(2.24)

The = sign is proved in § 2.5. The ≡ sign deﬁnes the Christoﬀel symbol of
the ﬁrst kind, simply by raising one index with g µλ . According to (2.23) the
Christoﬀel symbols deﬁne the connection between the base vectors of the
tangent spaces at diﬀerent positions. As pointed out above, there exist more
general connection coeﬃcients than (2.24), but these play no role in GR.
Insert (2.23) in (2.22) and rename the dummy-indices:
dA = (dAµ + Γµνσ Aν dxσ ) eµ ≡ (DAµ ) eµ .

(2.25)

The right hand side deﬁnes the intrisic change DAµ , which apparently obeys
the following equation:
DAµ
dAµ
dxσ
=
+ Γµνσ Aν
Dp
dp
dp

(contravariant);

(2.26)

DAµ
dxσ
dAµ
=
− Γνµσ Aν
Dp
dp
dp

(covariant).

(2.27)

2.4 Parallel transport and Christoﬀel symbols

29

For the second relation (2.27) see exercise 2.8. We may apply these equations
in two ways. For a given vector ﬁeld we may compute DAµ or DAµ for a
displacement dp along xσ (p). On the other hand, one may solve DAµ /Dp = 0
or DAµ /Dp = 0 starting from an initial value Aµ (P ) or Aµ (P ), and construct
a vector ﬁeld along xσ (p) for which dA = A − A = 0. Parallel transport of
a vector along xσ (p) is therefore described by the diﬀerential equation
DAµ
= 0
Dp

or

DAµ
= 0.
Dp

(2.28)

We mention a few properties of the Christoﬀel symbols. They are symmetrical
in the last two indices:
Γµνσ = Γµσν ;

Γλνσ = Γλσν .

(2.29)

By interchanging the indices in (2.24) we may infer Γνλσ , and on adding that
to Γλνσ one obtains
(2.30)
Γλνσ + Γνλσ = gλν,σ .
The Christoﬀel symbol transforms according to






Γµν  σ = Γραβ xµ,ρ xα,ν  xβ,σ + xµ,ρ xρ,ν  σ .

(2.31)

The proof is for diehards (see literature). The ﬁrst term is what we would
expect if the Christoﬀel symbol were a tensor, but the second term makes
that it is actually not a tensor. The concept of parallel transport will be used
in § 2.5 to deﬁne geodesics.
In SR the velocity and spin vector of a particle on which no forces are
exerted are constant. They are transported parallel along the ‘straight’ orbit
of the particle. The idea of GR is that a particle under the inﬂuence of gravity
moves freely in a curved spacetime. A natural generalisation is that velocity
and spin vector of the particle can be found by parallel transport along the
orbit in spacetime. In this way we are able to understand the geodesic precession of a top. Fig. 2.5 shows a co-ordinate picture, with x0 = ct on the vertical
axis and polar co-ordinates x1 = r and x3 = ϕ in the horizontal plane. The
worldline of the top orbiting the central object (vertical bar) is a spiral. The
spin 4-vector (whose spatial part is directed along the spin axis) is paralleltransported along the worldline. After one revolution the top has returned
to same spatial position, but because spacetime is not ﬂat – not visible in a
co-ordinate picture – the spin vector has changed its direction. At this point
one may wonder how the eﬀect is related to the Thomas precession. We refer
to Ch. 8 for a more general treatment, from which both Thomas precession
and geodesic precession emerge in the appropriate limit.

30

2 Geometry of Riemann Spaces

Exercise 2.6: The length of a vector remains constant under parallel transport:
d Aν Aν = d (gµν Aµ Aν ) = 0 .
Hint: First attempt: d = D = intrinsic change: DAν Aν = (DAν )Aν +
Aν (DAν ) = 0, because DAν = (DAν /Dp) dp = 0, etc. But (2.27) must still be
proven, and for that we need d Aν Aν = 0. Second attempt: d = total change:
d gµν Aµ Aν = 2Aν dAν + Aµ Aν gµν,σ dxσ ; (2.26): dAν = −Γνµσ Aµ dxσ ; exercise 2.5 and (2.30): 2Aν dAν = −2Γνµσ Aν Aµ dxσ = −gνµ,σ Aν Aµ dxσ .
Exercise 2.7: Prove that d Aν Bν = 0 under parallel transport.
Hint: The length of Aν + B ν is constant.

Exercise 2.8: For parallel transport of a covariant vector:
dBµ = Γνµσ Bν dxσ .

(2.32)

Hint: 0 = d Aµ Bµ = Aµ dBµ + Bµ dAµ , and dAµ is known.
Exercise 2.9: Prove that
Γµνµ = g,ν /2g =

1
2

log |g|

,ν

;

g = det {gαβ } .

(2.33)

Hint: (2.24): Γµνµ = 12 g λµ gλµ,ν . For a matrix M we have that Tr (M −1 M,ν ) =
(Tr log M ),ν = (log det M ),ν . Take M = {gαβ }.

2.5 Geodesics
Intuitively, a geodesic is a line that is ‘as straight as possible’ on a curved
surface. We say that a curve xµ (p) is a geodesic when the tangent vector
dxµ /dp remains a tangent vector under parallel transport along xµ (p). Therefore ẋµ ≡ dxµ /dp must satisfy (2.28), and we arrive at the geodesic equation:


D dxµ
(2.34)
= 0 → ẍµ + Γµνσ ẋν ẋσ = 0 ,
Dp dp

2.5 Geodesics

31

B
ds 2 = 0

ds 2 > 0

A
Fig. 2.6. A timelike geodesic connecting events A and B is the curve with the
maximum possible interval length between A and B, see text.

with ˙ = d/dp. For timelike geodesics2 the parameter p in (2.34) is proportional to the interval length s. Proof: according to exercise 2.6 the length of
ẋα = dxα /dp is constant along xµ (p), i.e. ẋα ẋα = gαβ ẋα ẋβ ≡ (ds/dp)2 is
constant. For timelike geodesics ds2 > 0, and we may take the square root to
conclude that ds = const · dp. Later, when GR is cast into the geometrical
framework developed here, this result will be connected to proper time (a
physical concept that does not yet exist here): ds = cdτ , so that
dp ∝ ds ∝ dτ

for timelike geodesics.

(2.35)

This is important as it implies that we may, for timelike geodesics, replace
the curve parameter p in (2.34) by the interval length s or the proper time τ .

Intuitively, a geodesic is also the shortest possible route between two
2
points. For a positive deﬁnite metric this is indeed the case, but
 ds  can be
positive as well as negative in GR. Assuming that the interval ds = ṡ dp of
a timelike geodesic is an extremum (see below), it is easy to see that it should
be a maximum:
there always exists an arbitrarily nearby worldline that has a

smaller ds, by letting it jump more or less from light-cone to light-cone, as
in Fig. 2.6 (see e.g. Wald (1984) § 9.3). The construction of Fig. 2.6 fails for
spacelike geodesics.

2

In an analogy with (1.4) we speak of a timelike (spacelike) worldline or geodesic
when ds2 > 0 (ds2 < 0). A null worldline or null geodesic has ds2 = 0. For
spacelike and null geodesics p can no longer be interpreted as an interval length.

32

2 Geometry of Riemann Spaces

Eq. (2.34)
may also be derived from a variational
principle.3 The sim

plest is δ ṡ dp = 0, and this is equivalent
to δ F (ṡ) dp = 0 provided F is

monotonous, F  = 0. We choose δ ṡ2 dp = 0 , or

δ L dp = 0 ; L(xα , ẋβ ) = (ds/dp)2 = gαβ ẋα ẋβ .
(2.36)
The solution is determined by the Euler-Lagrange equations (Appendix C)


∂L
∂L
d
=
.
(2.37)
∂xλ
dp ∂ ẋλ
Now, ∂L/∂xλ = gαβ,λ ẋα ẋβ because only gαβ depends on {xµ }. By using
λ
α
∂ ẋα /∂ ẋλ = δα
λ one gets ∂L/∂ ẋ = 2gαλ ẋ . Substitute this in (2.37):
gαβ,λ ẋα ẋβ = 2(gαλ ẋα )˙
= 2(gαλ,β ẋβ ẋα + gαλ ẍα ) ,
or
gαλ ẍα +

1
2 (2gλα,β

− gαβ,λ ) ẋα ẋβ = 0 .

(2.38)

Now comes a frequently used trick: renaming of dummy indices: 2gλα,β ·
ẋα ẋβ = gλα,β ẋα ẋβ + gλβ,α ẋβ ẋα = (gλα,β + gλβ,α ) ẋα ẋβ . Substitution in
(2.38) and multiplication with g µλ gives:
ẍµ +

1 µλ
2g

(gλα,β + gλβ,α − gαβ,λ ) ẋα ẋβ = 0 .

(2.39)

This is of the form of (2.34) and the factor multiplying ẋα ẋβ must be equal to
Γµαβ , which proves (2.24). Variational calculus is a very eﬃcient tool for this
type of problem. Without much diﬃculty, it permits us to ﬁnd the geodesic
equation directly from the metric, and from this equation one may just read
the Christoﬀel symbols Γµνσ . This is usually a lot faster than calculating them
from (2.24), and this method is therefore highly recommended.
The following result is very helpful when analysing the dynamics of a test
particle in GR (assuming that its orbit is a geodesic), because it allows us to
ﬁnd constants of the motion. From the text below (2.37) we see that ∂L/∂xλ
vanishes if gαβ,λ = 0. And then eq. (2.37) says that ∂L/∂ ẋλ = 2gαλ ẋα is
constant. In terms of the 4-velocity uµ = dxµ /dp we have found that the
covariant 4-velocity uλ = gλα uα is constant:
gαβ,λ = 0

→

uλ = gλν ẋν = constant

(2.40)

with ˙ = d/dp. The fact that uλ is a constant along a geodesic if the metric is
independent of xλ – doesn’t that ring a bell?
3

Here we switch to another deﬁnition of geodesics without proving its equivalence
with (2.34).

2.6 The covariant derivative

33

Exercise 2.10: Show that the geodesics of the Lorentz metric (gαβ = ηαβ )
are straight lines.

Exercise
2.11: Show that the variational problem (2.36) is equivalent to

δ F (L) dp = 0 if F is monotonous, F  = 0.
Hint: Write down (2.37) with L → F (L); use ∂F (L)/∂xλ = F  ∂L/∂xλ , and
(F  ∂L/∂ ẋλ )˙ = (F  )˙ ∂L/∂ ẋλ + F  (∂L/∂ ẋλ )˙. But (F  )˙ = F  dL/dp = 0 (L
is constant on xµ (p) because ẋα ẋα is).

2.6 The covariant derivative
For a given vector ﬁeld Aµ that is not restricted to the curve xσ (p) we can
elaborate dAµ /dp in (2.26) as dAµ /dp = Aµ,σ ẋσ , because we are able to
compute derivatives in other directions than along the curve. This leads to
the introduction of the covariant derivative
DAµ
= Aµ,σ + Γµνσ Aν ẋσ ≡ Aµ:σ uσ ,
Dp

(2.41)

where uσ = ẋσ = dxσ /dp and
Aµ:σ ≡ Aµ,σ + Γµνσ Aν

(2.42)

is the covariant derivative of Aµ . It may be regarded as the ‘intrinsic derivative’, the derivative after correction for the meaningless change in orientation
of the base vectors. In a similar way we may obtain the covariant derivative
of a covariant vector from (2.27):
Aµ:σ = Aµ,σ − Γνµσ Aν .

(2.43)

Important is that both Aµ:σ and Aµ:σ are tensors if Aµ is a vector, even
though neither of the two terms on the right hand sides of (2.42) and (2.43)
are tensors themselves. The proof is a matter of combining relations (2.20)
and (2.31), and is left to the reader.
Next follow a few deﬁnitions. The covariant derivative of a product XY
of two tensors is:
(2.44)
(XY ):σ = X:σ Y + X Y:σ .
For example:

34

2 Geometry of Riemann Spaces

(Aµ Bν ):σ = (Aµ,σ − Γαµσ Aα ) Bν + Aµ (Bν,σ − Γανσ Bα )
= (Aµ Bν ),σ − Γαµσ Aα Bν − Γανσ Aµ Bα .

(2.45)

Accordingly, we deﬁne the covariant derivative of a covariant second rank
tensor as:
Tµν:σ = Tµν,σ − Γαµσ Tαν − Γανσ Tµα .
(2.46)
The recipe for tensors of higher rank should be clear by now. For example, if we
need an expression for Tα βγ:σ , then we merely have to work out (Pα Qβ Rγ ):σ
∗
∗
as in (2.44) and (2.45). The general pattern is T ∗:σ
= T ∗,σ
± Γ-term for every
index. For a scalar:
(2.47)
Q:σ = Q,σ .
Covariant derivatives do not commute, unlike normal derivatives (X,αβ =
X,βα for every X). We calculate Bµ:ν:σ by substituting Tµν = Bµ:ν in (2.46):
Bµ:ν:σ = Bµ:ν,σ − Γαµσ Bα:ν − Γανσ Bµ:α ,

(2.48)

which should be elaborated further with (2.43). After that, interchange ν and
σ and subtract. The result of a somewhat lengthy calculation is:
Bµ:ν:σ − Bµ:σ:ν = Bα Rαµνσ

(2.49)

Rαµνσ = Γαµσ,ν − Γαµν,σ + Γτµσ Γατ ν − Γτµν Γατ σ .

(2.50)

with

Rαµνσ is called the RIEMANN tensor. It is a tensor because (2.49) is valid
for every vector Bα and because the left hand side is a tensor. Then apply the quotient theorem. Apparently, covariant derivatives commute only if
Rαµνσ = 0. The Riemann tensor plays a crucial role in GR because it contains all information about the curvature of space. Note the remarkable fact
that according to (2.49) the diﬀerence of two consecutive covariant diﬀerentiations is proportional to the vector itself. The explanation is given in the next
section.

Exercise 2.12: Show that
T µν:σ = T µν,σ + Γµασ T αν + Γνασ T µα .

(2.51)

Great care is needed in using these relations. For example, let T µν be diagonal. Then it seems evident that T 1µ:µ = T 11:1 , but that is not the case.
Why not?
Hint: Write out (Aµ B ν ):σ as in (2.45). It is due to the action of the invisible

2.7 Riemann tensor and curvature

35

dummy index α.

Exercise 2.13: An important property is that the metric tensor behaves as
a constant under covariant diﬀerentiation:
gµν:σ = 0 .

(2.52)

Hint: Use (2.46) and (2.30).

Exercise 2.14: Prove the following compact form of the geodesic equation:
uσ uµ:σ = 0

or

uσ uµ:σ = 0 .

(2.53)

Hint: The last relation is just 0 = Duµ /Dp = (2.41); the ﬁrst relation with
(2.52): 0 = gλµ uµ:σ uσ = (gλµ uµ ):σ uσ = etc.
Exercise 2.15: A reminder of the linear algebra aspects of tensor calculus.
Given a 2D Riemann space with co-ordinates x, y, a metric and two vectors
in the tangent space of the point (x, y):
 
 
1
y
2
2
2
α
; Bα =
.
ds = dx + 4dxdy + dy ; A =
4
x
Write down gµν and g µν and show that all Christoﬀel symbols are zero. Compute Aν and B ν:ν .
Hint: g11 = g22 = 1; g12 = g21 = 2, use (2.24) for the Christoﬀel symbols;
 


1 −1 2
4
9
µν
.
g =
; B ν:ν =
; Aµ =
6
2
−1
3
3
The Γ’s being zero we have B ν:ν = B ν,ν .

2.7 Riemann tensor and curvature
The metric tensor does not tell us whether a space is ﬂat, because the use
of ‘strange’ co-ordinates is not prohibited. For example ds2 = dr2 + r2 dϕ2

36

2 Geometry of Riemann Spaces

(planar polar co-ordinates) deﬁnes a ﬂat space, but (2.2) deﬁnes a curved
space. The metric tensor contains apparently a mix of information on coordinates and curvature. The intrinsic curvature properties are determined
by the Riemann tensor. We shall illustrate this by transporting a vector
Aµ parallel to itself along two diﬀerent paths to the same ﬁnal position, see
Fig. 2.7. According to (2.26), dAµ = −fσ (x)dxσ with fσ (x) = Γµνσ Aν (the
upper index µ is omitted for brevity as it does not change). The diﬀerence of
the two ﬁnal vectors is:
dAµ = Aµ1 − Aµ2
= −fσ (x)dξ σ − fσ (x + dξ)dη σ + fσ (x)dη σ + fσ (x + dη)dξ σ
−fσ dξ σ − fσ dη σ − fσ,λ dξ λ dη σ + fσ dη σ + fσ dξ σ + fσ,λ dη λ dξ σ
= (fσ,λ − fλ,σ ) dξ σ dη λ .

(2.54)

Now substitute fσ = Γµνσ Aν = Aµ:σ − Aµ,σ . The terms Aµ,σ cancel, and after
some index gymnastics we arrive at (exercise 2.16):
dAµ = (Aµ:σ,λ − Aµ:λ,σ ) dξ σ dη λ
= g µν (Aν:σ,λ − Aν:λ,σ ) dξ σ dη λ
= g µν (Aν:σ:λ − Aν:λ:σ ) dξ σ dη λ
= g µν Rανσλ Aα dξ σ dη λ
= g µν Rναλσ Aα dξ σ dη λ
= Rµαλσ Aα dξ σ dη λ .

(2.55)

On account of (2.24) the Christoﬀel symbols vanish identically in a ﬂat space
with rectangular co-ordinates, since gµν has only constant elements. Therefore
the Riemann tensor (2.50) is zero as well. The transformation properties of
a tensor then ensure that Rαµνσ is zero in a ﬂat space for any choice of the
co-ordinates.4 In that case parallel transport along a closed path leaves a vector unchanged.5 But in a curved space the orientation of the vector will have
4

5

Contrary to the Christoﬀel symbols, which are not tensors. For example, the
Christoﬀel symbols vanish in rectangular co-ordinates in a plane, but not in polar
co-ordinates.
Conversely, if the Riemann tensor is zero, it can be proven that there exist coordinates so that gµν is constant which implies that the space is ﬂat, see e.g. Dirac
(1975) § 12.

2.7 Riemann tensor and curvature

S
dha
{ xa } P

R

37

A1
A2

A
dxa

Q

Fig. 2.7. Parallel transport of the vector A from P to R along path 1 (P QR) and
path 2 (P SR) produces a diﬀerent result.

changed. Once this is accepted intuitively, it is clear that the diﬀerence dAµ
must be proportional to the length of the vector, which explains the factor
Aα in (2.55). The derivation in (2.55) shows that the diﬀerence dAµ is also
proportional to the diﬀerence of two consecutive covariant diﬀerentiations,
and this explains why this diﬀerence is proportional to the vector itself, as in
(2.49).
There are several other ways to illustrate the relation between the Riemann tensor and curvature. One is the equation for the geodesic deviation,
see exercise 2.18. Another is the relation between Gaussian curvature and the
Riemann tensor. Gaussian curvature refers to surfaces embedded in a ﬂat 3D
space. The curvature κ in a point P of a curve on the surface is deﬁned as
the inverse radius of the osculating circle at P . Each point has two principal
curvatures κ1 and κ2 , and the Gaussian curvature K ≡ κ1 κ2 is an invariant
determined by the geometry of the surface, which has several interesting properties.6 Turning now to Riemann spaces, take two orthogonal unit vectors e1
and e2 in the tangent space of a point P which are not null. Now consider
those geodesics in Riemann space that are tangent in P to the plane spanned
by e1 and e2 . These geodesics subtend, locally around P , a 2D curved subspace of Riemann space. The Gaussian curvature of this 2D space at P is
µ ν σ
7
Rαµνσ eα
1 e2 e1 e2 , apart from the sign.
The Riemann tensor obeys several symmetry relations that reduce the
number of independent components from n4 to n2 (n2 − 1)/12 (see literature).
In 4 dimensions Rανρσ has only 20 independent components, and all contractions of Rανρσ are either zero or equal, apart from the sign. We choose

6

7

E.g. Gauss’s theorem on integral curvature: the sum of the three interior angles
of a geodesic triangle (bounded by 3 geodesics) equals π plus the surface integral
of K.
For a proof of these statements see e.g. Robertson and Noonan (1969) p. 216.

38

2 Geometry of Riemann Spaces

the Ricci tensor : 8
Rµν ≡ Rαµνα

(RICCI).

(2.56)

The explicit expression follows from (2.50):
Rµν = Γαµα,ν − Γαµν,α − Γαµν Γβαβ + Γαµβ Γβνα .

(2.57)

We infer from (2.33) that Γαµα,ν = 12 log |g| ,µν so that all terms in (2.57)
are symmetric in µ and in ν. Hence Rµν is symmetric:
Rµν = Rνµ .

(2.58)

R ≡ Rνν = g νµ Rµν = Rαββα .

(2.59)

We may contract once more:

R is called the total curvature. Finally we introduce the Einstein tensor Gµν :
Gµν = Rµν − 12 gµν R

(EINSTEIN).

(2.60)

The Einstein tensor will be useful later because its divergence is zero:
Gµν:ν = (Rµν − 12 g µν R):ν = 0 .

(2.61)

Riemann, Ricci en Einstein tensor contain at most second derivatives of gαβ .
By substituting (2.24) in (2.50) we get:
Rαµνσ =

1 αβ
2g

gβσ,µν − gµσ,βν − gβν,µσ + gµν,βσ
+ g αβ Γτ βσ Γτµν − Γτ βν Γτµσ .

(2.62)

The corresponding expressions for Rµν and for Gµν can be found from this
by contraction. The ﬁrst term contains all second-order derivatives. The ﬁrstorder derivatives are in the second term. The proofs of (2.61) and (2.62) can
be found in the literature, but are not important here.

Exercise 2.16: Provide the missing details of the derivation of (2.55).
Hint: Second = sign: Aµ:σ,λ = (g µν Aν ):σ,λ = (g µν Aν:σ ),λ = g µν,λ Aν:σ +
g µν Aν:σ,λ , but Aµ is parallel transported, hence Aν:σ = 0, etc. Third = sign:
Aν:σ:λ = Aν:σ,λ from (2.48). Fifth = sign: Rανσλ = Rναλσ is a symmetry
relation of the Riemann tensor.
8

Other authors deﬁne Rµν = Rαµαν , another source of sign diﬀerences. For a
complete classiﬁcation of all sign conventions see the red pages in Misner et al.
(1971). In terms of this classiﬁcation we follow the − + − convention.

2.7 Riemann tensor and curvature

39

z

q
ej

ej
eq

x

A(0)

eq

A(j)

y

j

Fig. 2.8. Parallel transport of a vector A over the surface of a sphere with radius
r = 1, see exercise 2.17.

Exercise 2.17: Consider a 2D spherical surface with radius r = 1, see Fig. 2.8.
Calculate the Christoﬀel symbols and the total curvature R. Convince yourself that R ∝ r−2 . Show that a vector A will rotate in the tangent space as
it is parallel-transported along a circle θ = θ0 . Try to understand this with
the intuitive deﬁnition of parallel transport in § 2.4. Start in (θ, ϕ) = (θ0 , 0)
with (Aθ , Aϕ ) = (0, 1/ sin θ0 ). Show that Ai Ai is always 1, i.e. |A| ≡ 1, and
that after one full revolution A has rotated over an angle 2π cos θ0 . Discuss
1.
the limiting cases θ0 = π/2 (geodesic!) and θ0
Hint: (2.2): g11 = 1, g22 = sin2 θ (θ = 1, ϕ = 2). Do not use (2.24), but rather
(2.37) with L(θ, θ̇, ϕ̇) = θ̇2 + sin2 θ ϕ̇2 :
 ∂L 
∂L
=
˙ → θ̈ − sin θ cos θ ϕ̇2 = 0 ;
∂θ
∂ θ̇
 ∂L 
∂L
=
˙ → ϕ̈ + 2 cot θ θ̇ϕ̇ = 0 .
∂ϕ
∂ ϕ̇
By comparing with (2.34) we may just read the Γ’s: Γ122 = − sin θ cos θ ;
Γ212 = cot θ (double product!). All other Γ’s are zero. (2.33) → Γαµα,ν =
(log sin θ),µν → Γα1α,1 = −1/ sin2 θ . And Γα11,α = 0 ; Γα22,α = −(sin θ ·
cos θ),θ = sin2 θ − cos2 θ. Algebra: R11 = −1 and R22 = − sin2 θ. Finally
R = g µν Rµν = g 11 R11 + g 22 R22 = R11 + (1/ sin2 θ) R22 = −2. For a sphere
with radius r: R = −2/r2 (minus sign due to sign convention).
Parallel transport: p is proportional to the arc length (why?), so choose p = ϕ;
(2.28)+(2.26): Aµ,ϕ + Γµνσ Aν xσ,ϕ = 0 with x1,ϕ = dθ/dϕ = 0 and x2,ϕ =
dϕ/dϕ = 1:

40

2 Geometry of Riemann Spaces

B
xm

l + dl

um

A
(observer)

l
p, or arc
length s

Fig. 2.9. The geodesic deviation.

Aθ,ϕ = sin θ0 cos θ0 Aϕ ;

Aϕ,ϕ = − cot θ0 Aθ .

Eliminate Aϕ : Aθ,ϕϕ + cos2 θ0 Aθ = 0, same equation holds for Aϕ . Harmonic
oscillator with frequency cos θ0 . Solution for given initial value:
Aθ = sin(ϕ cos θ0 ) ;

Aϕ = cos(ϕ cos θ0 )/ sin θ0 .

A rotates clockwise when looking down on the tangent space from outside;
θ0 = π/2: Aθ ≡ 0 and Aϕ ≡ 1/ sin θ0 = 1, therefore A remains a tangent
1 (small circle around the north pole): in that case the tangent
vector; θ0
space is always almost parallel to the equatorial plane, with base vectors x en
1 it
y, and eθ x cos ϕ + y sin ϕ and eϕ (y cos ϕ − x sin ϕ) sin θ0 . For θ0
follows that A = Aθ eθ + Aϕ eϕ y, so that A remains virtually unchanged
with respect to a ﬁxed frame.
Exercise 2.18: Given a set of geodesics xµ (p, λ) where p is the curve parameter and λ labels diﬀerent geodesics (λ is constant along one geodesic).
Consider two neighbouring geodesics λ and λ + δλ. The points A and B are
connected by the vector ξ µ = xµ (p, λ + δλ) − xµ (p, λ) (∂xµ /∂λ)δλ ≡ eµ δλ.
Prove that:
D2 ξ µ
= Rµαβν uα uβ ξ ν ;
Dp2

uα = ẋα =

∂xα
.
∂p

(2.63)

This is the equation for the geodesic deviation, that will play an important
role later. In a ﬂat space the Riemann tensor is zero, and then ξ µ is a linear
function of p, and for timelike geodesics also a linear function of the arc
length s, as expected. In a curved space however this is no longer the case.
For example, on a sphere ξ µ (s) will be something like a sine-function.
Hint: The proof comes in three steps:

(a)

∂ 2 xµ
∂uµ
∂xα
∂eµ
=
=
= uµ,α
= uµ,α eα ;
∂p
∂p ∂λ
∂λ
∂λ

(b) eµ:α uα ≡

∂eµ
Deµ
=
+ Γµαβ eα uβ
Dp
∂p

= uµ,α eα + Γµαβ eα uβ = uµ:α eα ;
(c)

D2 eµ
≡ (eµ:α uα ):β uβ = (uµ:α eα ):β uβ
Dp2
= uµ:α eα:β uβ + uµ:α:β eα uβ
= uµ:α uα:β eβ + uµ:α:β uα eβ + uµ:α:β − uµ:β:α uβ eα
= uµ:α uα

:β

eβ + g µν uν:α:β − uν:β:α uβ eα

= g µν uσ Rσναβ uβ eα
= Rσµαβ uσ uβ eα
= Rµσβα uσ uβ eα .
In (c) we have twice used (b), next uµ:β:α eα uβ = uµ:α:β eβ uα is added and
substracted again, and then (2.53) and (2.49). The last = sign is a symmetry relation of the Riemann tensor. Because δλ is constant, the equation also
holds for ξ µ = eµ δλ.
Exercise 2.19: Be aware of some inconsistencies in the notation. We encountered one in exercise 2.12. Meet two more here. In § 2.2 and § 2.3 it was
stressed that the rules for index raising and lowering are always valid. Does
that mean that
?

g µα gαλ,ν = g µλ,ν ;
?

gµα u̇α = u̇µ .

(2.64)
(2.65)

Hint: In exercise 2.12 the trouble was caused by a hidden index; here we
discover that the symbols without derivative had already been deﬁned; one
way to see that (2.64) cannot be correct is to note that g µλ,ν ≡ δµλ,ν = 0, and
since det{g µα } = 0 → gαλ,ν = 0 → gαλ = const. Instead, 0 = (g µα gαλ ),ν =
g µα,ν gαλ + g µα gαλ,ν , etc. Likewise, uµ is deﬁned as gµα uα so that u̇µ =
(gµα uα )˙ = gµα,σ uσ uα + gµα u̇α . Also correct is u̇µ = uµ,α ẋα = uµ,α uα .

3
General Relativity

We shall now put the ideas of GR on solid footing by casting them into
the framework of Riemann spaces. From now on we deal again with a 4dimensional spacetime in which every event is determined by the co-ordinates
x0 , .. , x3 (x0 = ct). First we say a few words about the meaning of these coordinates and their relation to the metric. Then we discuss the ﬁeld equations
for the metric tensor, and the classical limit for weak ﬁelds.

3.1 Co-ordinates, metric and motion
It is important to understand that the co-ordinates serve merely as labels that
identify events in spacetime. They can be chosen arbitrarily, as long as they
are well-behaved (continuous, one-to-one,..), but they have usually no physical
meaning. In particular, diﬀerences in time or spatial co-ordinates are meaningless because they are not invariant. In GR, measurable quantities such as
lengths and times are always expressed in terms of the co-ordinates and the
metric tensor, so that the result is invariant for a co-ordinate transformation.
Consider for example radial distances in the Schwarzschild metric. The diﬀerence r2 − r1 of two radial positions r1 and r2 is not invariant and not equal
to the measured distance. If we travel radially from r1 to r2 and measure
the distance with a measuring rod, the result is equal to ∫rr12 −grr (r) dr.
This strange expression will become clear in a moment. The point is that the
outcome of a measurement is always given by an invariant expression (invariant for co-ordinate transformations) involving the metric tensor. These two
functions of labelling and measuring are frequently confused in daily life, for
example, in the case of cartesian co-ordinates (think of millimetre paper), but
in GR they are strictly separated.
Even though the choice of the co-ordinates is free, some co-ordinates are
much easier to use than others. It is not very wise to use rectangular coordinates for a spherically symmetric system, and this is also very much true

44

3 General Relativity

x0

B"

A''

B'

A'

B

A

x0 + dx20

x0

x0 + dx10
{xi + dxi}

{xi}

{xi}

Fig. 3.1. Experimental determination of the metric of space in terms of the metric
of spacetime. After Landau and Lifshitz (1971), § 84.

in GR. By ‘natural selection’ a few standard co-ordinate systems have emerged
for frequently occurring physical situations that everybody uses because it
saves a lot of work.
Time and distance measurements
To begin with the co-ordinate time t, one way to deﬁne t is to count light
ﬂashes of a beacon, for example a pulsar. The co-ordinate time interval ∆t
between n ﬂashes has the same value everywhere in space (namely n), but the
proper time interval does not. Their relation is determined by the metric:
c2 dτ 2 = ds2 = gαβ dxα dxβ .

(3.1)

For timelike worldlines (ds2 > 0) we interpret ds ≡ (ds2 )1/2 as c × the proper
time interval dτ , like in SR. An observer at rest (dxi = 0, i = 1, 2, 3) has a
timelike worldline1 and hence (cdτ )2 = ds2 = g00 (cdt)2 :
√
(3.2)
dτ = g00 dt .
dτ is the interval read from the clock of the observer at rest, while dt is the
co-ordinate time interval. It follows that g00 must be positive.
1

This need not be a geodesic. In the Schwarzschild metric an observer needs a
rocket to remain at rest. But ‘rest’ (dxi = 0) is not an invariant concept. If one
drops a stone into a black hole once every second, the radial position of any point
can be expressed in terms of the fractional stone number. These co-ordinates are
not stationary, but perfectly legitimate. In these co-ordinates ‘an observer at rest’
is a freely falling observer. Co-ordinates in which a freely falling observer is at
rest are used in cosmology.

3.1 Co-ordinates, metric and motion

45

For the spatial co-ordinates, too, there should exist properly deﬁned measuring procedures to determine their value. As an example we illustrate in
the next chapter how the values of the Schwarzschild co-ordinates may be
determined. Another issue is this: an observer A (co-ordinates xµ + dxµ ) may
determine the metric of the space in his neighbourhood empirically. How is
this metric related to the metric of spacetime? A places a mirror at B at a
distance dl, Fig. 3.1, measures on his clock the time dτ it takes a light signal
to travel from A to B and back again, and argues: dl = cdτ /2. Light signals
must travel along null worldlines.2 The interval ds2 between the events A, B 
and B  , A is zero, or, from (3.1):
gij dxi dxj + 2g0i dxi dx0 + g00 (dx0 )2 = 0 .
Roman indices run from 1 to 3. Solve for dx0 :



1
−g0i dxi ± (g0i dxi )(g0j dxj ) − g00 gij dxi dxj
dx01,2 =
g00

(3.3)

(3.4)

(1 = −; 2 = +). The co-ordinate time interval dt between the events A and
A is

2
cdt = dx02 − dx01 =
(g0i g0j − g00 gij ) dxi dxj .
(3.5)
g00
With the help of (3.2) we infer dl2 = (cdτ /2)2 = g00 (cdt)2 /4. The spatial
metric now follows from (3.5):


g0i g0j
dl2 =
− gij dxi dxj .
(3.6)
g00
Frequently g0i = 0, in which case the metric of space simpliﬁes to 3
dl2 = −gij dxi dxj .

(3.7)

As an application consider a curve along the x1 -axis, so that dx2 = dx3 =
√
0, and dl = −g11 dx1 . The distance between x1 = a and x1 = b is l =
b√
∫a −g11 dx1 , and this explains the formula used earlier.
Strong equivalence
The metric gαβ is determined by the mass distribution and the choice of
co-ordinates, through the ﬁeld equations that are yet to come. When other
co-ordinates are used, the metric becomes diﬀerent as well, in such a way that
ds2 and all other physical quantities remain invariant. The metric tensor gαβ
2

3

We know that ds2 = 0 in a local freely falling frame since SR holds there. But
the way ds2 is written in (3.1) makes it a scalar, hence it is zero in any frame.
But not always. In the Kerr metric (rotating black holes) gtϕ = 0, and then weird
things may happen.

46

3 General Relativity

is symmetric and may therefore be diagonalized locally by a transformation.
Subsequently, the (real) eigenvalues may all be rescaled to ±1 by redeﬁning
the units. Further analysis (omitted here) shows that the metric can always be
brought into the following form, in the neighbourhood of any event x0 = {xµ0 },
by a transformation of co-ordinates:
gαβ (x0 + dx) = ηαβ + O(dxµ dxν ) ,

(3.8)

see e.g. Schutz (1985, p. 154) or Kenyon (1990, p. 24). In x0 the metric has
approximately the SR-form. These co-ordinates deﬁne a local freely falling
frame in x0 . It follows that the possibility to apply the strong equivalence
principle is properly built into the theory.
Geodesic motion
In § 1.2 we anticipated that test masses on which only gravity acts move
along geodesics. An elegant argument due to Weinberg (1972, p. 72) shows
that geodesic motion is an almost inevitable consequence of the principles of
weak equivalence and general covariance. In a freely falling frame a test mass

moves as a free particle in SR. If its co-ordinates are {xµ } we have


d2 xµ
= 0,
dτ 2





and c2 dτ 2 = ηµ ν  dxµ dxν ,

(3.9)

where τ is the proper time of the mass. We now transform to co-ordinates xλ .


Denoting ˙ = d/ds with ds = cdτ we have dxµ /ds = xµ,λ ẋλ , or






0 = (xµ,λ ẋλ )˙ = xµ,λ ẍλ + xµ,λσ ẋλ ẋσ .

(3.10)

On multiplying with xα,µ and summing over µ :
ẍα + Γαλσ ẋλ ẋσ = 0 with



Γαλσ = xα,µ xµ,λσ .

(3.11)

This looks like the geodesic equation. Likewise,
ds2 = gλσ dxλ dxσ

with





gλσ = ηµ ν  xµ,λ xν,σ .

(3.12)

These Γαλσ and gλσ have as yet nothing to do with the Christoﬀel symbols
and the metric tensor, but Weinberg goes on to prove that the quantities
thus deﬁned obey relation (2.24)! This illustrates how neatly GR ﬁts into
the framework of Riemann geometry, and that the metric tensor is the basic
quantity that determines everything else. And it is of course no coincidence
that (3.11) for Γαλσ is equal to the second term in (2.31)!
We know that the geodesic is timelike because ds2 is invariant and positive in SR. According to (2.35) we may choose s for the curve parameter

3.2 Weak ﬁelds (1)

47

p according and then (2.34) tells us that the 4-velocity uµ = dxµ /ds obeys
Duµ /Ds = 0, or, in terms of the 4-momentum pµ = m0 cuµ :
Dpµ
= 0,
Dτ

(3.13)

and this is a suggestive generalisation of the equations of motion dpµ /dτ = 0
of a free particle in SR.
What about null geodesics? We concluded earlier that particles with zero
rest mass must move along null worldlines, because of the invariance of ds2 .
We now assume that those lines are null geodesics. Here, too, we have hardly
any choice because the worldlines are already null geodesics in SR.

Exercise 3.1: Show that the − sign in (3.7) is a consequence of the adopted
signature.
Hint: If we take − + + + in (1.1), then −c2 dτ 2 = ds2 = gαβ dxα dxβ in (3.1).
How does (3.2) look in that case, and how does (3.5) follow from (3.4)?

Exercise 3.2: In a local frame in free fall all Christoﬀel symbols are zero, but
the Riemann tensor is not.
Hint: (3.8) implies that gαβ = ηαβ + Aαβµν (xµ − xµ0 )(xν − xν0 ) in the neighbourhood of x0 with constant Aαβµν . First-order derivatives of gαβ are zero
in x = x0 , second-order derivatives not. Diehards should try to express Aαβµν
in terms of the Riemann tensor Rβµνσ with (2.62).

3.2 Weak ﬁelds (1)
Assume now that we are dealing with weak, time-independent gravity ﬁelds
and that the relevant velocities are non-relativistic, β = v/c
1. Spacetime
is then nearly ﬂat, and it makes sense to do the substitution gµν = ηµν + γµν
with γµν small, and γµν,0 = 0. We take once more p = s and settle ﬁrst the
relation between ds and dt by using the metric (3.1), which we may write as
ds2 = (dx0 )2 − dxi dxi + γµν dxµ dxν . After ‘division’ by dt2 :


ds
dt

2
c2 − v 2 + γ00 c2 + terms O(γvc) or O(γv 2 )

48

3 General Relativity

t
Dt1

Dt0
g
z

z1

z0

Fig. 3.2. The Pound-Rebka-Snider experiment revisited.



= c2 1 − β 2 + γ00 + O(γβ)

c2 ,

(3.14)

plus terms of order β 2 . To order β we may put d/ds = c−1 d/dt in the geodesic
equation (2.34):
0

ν
σ
d2 xµ
µ dx dx
+
Γ
νσ
dt2
dt dt

d2 xµ
+ c2 Γµ00 ,
dt2

(3.15)

since due to dxµ /dt (c, v i ) the summand ν = σ = 0 is at least a factor c/v
larger than all others. Now drop terms of order γ 2 and use γµν,0 = 0 in (2.24):
Γµ00

1 µλ
2 η (2γλ0,0

Hence, Γ000 = 0 and Γi00 =
µ = 0, and for µ = i:
d2 xi
dt2

1
2 γ00,i .

− γ00,λ ) = − 12 η µλ γ00,λ .

(3.16)

Relation (3.15) produces an identity for

− 12 c2 γ00,i = −∇i Φ .

(3.17)

The second = sign follows from the classic equation of motion of a test particle
in a gravitational potential. The obvious choice is: Φ = 12 c2 γ00 . We may now
draw two conclusions for weak time-independent gravity ﬁelds:
1. Since the classical ﬁeld equation is ∇2 Φ = Φ,ii = 0, the ﬁeld equation of
GR must imply that
(3.18)
∇2 γ00 = γ00,ii = 0 .
2. We know one component of the metric tensor:
g00 = 1 + γ00 = 1 +

2Φ(r)
.
c2

(3.19)

3.2 Weak ﬁelds (1)

49

Table 3.1. Parameters of characteristic objects.
Object
Earth
Sun
Procyon B
neutron star

M

R (km)

5.98 × 1027 g
1.99 × 1033 g
0.43 M
1.5 M

6370
6.96 × 105
8800
∼ 10

rs
0.89 cm
2.95 km
1.3 km
4.4 km

−Φ/c2
7 × 10−10
2 × 10−6
7 × 10−5
∼ 0.2

Gravitational redshift
At this point we sidestep to a related issue: the gravitational redshift. It
follows from (3.19) that light passing through a gravitational ﬁeld is subject
to a frequency shift, see Fig. 3.2. In fact we compare proper time intervals
dτ at two locations z0 and z1 in the gravitational ﬁeld. Physically, the proper
time is deﬁned by identical oscillators at z0 and z1 (atoms of one species).
With the help of (3.2) and (3.19) we obtain
dτ (z0 )
=
dτ (z1 )



g00 (z0 )
g00 (z1 )

1/2
1+

Φ(z0 ) − Φ(z1 )
.
c2

(3.20)

Use was made of dt0 = dt1 because the geodesics must be congruent as the
gravity ﬁeld is time-independent. Let n waves be emitted at z0 and be detected at z1 . The measured frequencies ν0 at z0 and ν1 at z1 follow from
n = ν0 dτ (z0 ) = ν1 dτ (z1 ). Consequently
∆ν
ν1 − ν0
dτ (z1 )
=
= 1−
ν
ν1
dτ (z0 )

Φ(z0 ) − Φ(z1 )
.
c2

(3.21)

Note the diﬀerence between the SR and the GR point of view. In § 1.2 we
tried to ﬁt the Pound-Rebka-Snider experiment into the framework of SR,
that is, in one global frame in which the source and the detector are both at
rest. Under these circumstances we expect no diﬀerences in proper time, i.e.
dt(z0 ) = dt(z1 ), but this is refuted by the experiment. In GR the reasoning
is diﬀerent. We now interpret Fig. 3.2 as a co-ordinate picture, a picture that
displays the co-ordinates but conveys no information about the geometry, see
Fig. 2.1. The null geodesics must of course be congruent because the ﬁeld
is time-independent, i.e. dt(z0 ) = dt(z1 ). However, a measurement refers to
proper time, which in GR follows from (3.2).

Exercise 3.3: Show that the relative redshift in the Pound-Rebka-Snider
experiment is about 10−15 (h = 22.5 m). For details on the experimental
conﬁrmation see also Adler et al. (1965, p. 129) and Misner et al. (1971,

50

3 General Relativity

p. 1056).
Hint: (3.21): ∆ν/ν = ∆Φ/c2 = −gh/c2 .
Exercise 3.4: Henceforth, 4-velocity and 4-momentum of a particle with rest
mass m0 are deﬁned by uµ = dxµ /ds and pµ = m0 cuµ . Prove that
uµ is a vector ;

uµ uµ = 1 ;

pµ pµ = (m0 c)2 .

Show that in the SR limit and for small velocity v i /c
1/ 1 − β 2 ):
uµ = (γ, γv i /c)


(1, v i /c) ;



(3.22)

1 (E = γm0 c2 ; γ =

pµ = (E/c, pi )

(3.23)



Hint: vector: uµ = dxµ /ds = xµ,ν dxν /ds etc.; uµ uµ = 1: ‘divide’ (3.1) by
ds2 ; SR: u0 = dx0 /ds = dt/dτ = γ according to (1.6); ui = dxi /ds =
c−1 (dxi /dt)(dt/dτ ), etc.
Exercise 3.5: Estimate the values of Φ/c2 in Table 3.1.
Hint: Φ/c2 = −rs /2R with rs = 2GM/c2 = Schwarzschild radius.

3.3 Conservation of mass
In preparation for § 3.4 we analyse how conservation of mass is formulated in
GR. The volume element d4 x = dx0 dx1 dx2 dx3 transforms according to
d4 x = |J| d4 x ,




(3.24)



where J = det{xµ,α }. From g√αβ = xµ,α xν,β gµ ν  and g ≡ det{gαβ } it follows
√
that g = J 2 g  , or −g = |J| −g  , 4 because g and g  < 0. As a result,
√
−g d4 x =

−g  d4 x ≡ proper volume element.

(3.25)

This is important for integrations. It is physically not very
to
 meaningful
4
integrate a scalar S over a section of spacetime,
because
S
d
x
is
not
in √
 √
variant, even though S(x) = S  (x ). But S −g d4 x = S  −g  d4 x is
4

It follows, incidentally, that g is not a scalar.

3.3 Conservation of mass

51

invariant. The proper volume element is the physical volume element corresponding to the meaningless (i.e. not invariant) co-ordinate volume element d4 x. As an example consider spherical co-ordinates in a ﬂat R3 :
ds2 = dr2 + r2 dθ2 + r2 sin2 θ dϕ2 and g = r4 sin2 θ > 0, so that the invariant
√
volume element equals g d3 x = r2 sin θ drdθdϕ.
The divergence Aµ:µ of a vector Aµ is a scalar. With (2.42) and (2.33):
Aµ:µ
because g,σ /2g =

=

Aµ,µ

√
−g

,σ

+

Γµσµ Aσ

=

Aµ,µ

+

√
−g ,σ Aσ
√
,
−g

√
/ −g. We may write this as follows:

√
√
Aµ:µ −g = (Aµ −g ),µ .
Consequently,



(3.26)

√
Aµ:µ −g d4 x =



√
(Aµ −g ),µ d4 x

(3.27)

(3.28)

is invariant. For the volume of integration we choose a 3-volume V times
an inﬁnitesimal dx0 that we subsequently eliminate again from the equation.
Assuming now that Aµ:µ = 0 we infer that


√
0√
3
0 =
(A −g ),0 d x +
(Ai −g ),i d3 x ,
(3.29)
V

V

or, with Gauss’s theorem



√
A0 −g d3 x
= −
V

,0

√
Ai −g dσi .

(3.30)

∂V

At this point we make a connection with physics by choosing Aµ = ρuµ where
ρ = rest mass density and uµ = 4-velocity. Exercise 3.6 invites the reader to
show that for this Aµ the classical limit of (3.30) coincides with the continuity
equation in integral form. On this ground we accept (3.30) with Aµ = ρuµ as
the integral form of the continuity equation in GR. The diﬀerential form is
then
(3.31)
(ρuµ ):µ = 0 .

Exercise 3.6: Show that with Aµ = ρuµ (3.31) and (3.30) in the nonrelativistic limit reduce to


∂ρ
∂
3
+ ∇ · ρv = 0 ;
ρd x = −
ρv · dσ .
∂t
∂t V
∂V

52

3 General Relativity

Hint: For weak ﬁelds and β
√
Furthermore −g 1.

1 we have (3.23); in this limit ρuµ → (ρ, ρv i /c).

Exercise 3.7: Prove the equation for the rate of change of the density ρ along
the worldline xµ (s):
(3.32)
dρ/ds = −ρuµ:µ .
Hint: (3.31) → ρ:µ uµ + ρuµ:µ = 0 and ρ:µ = ρ,µ .

3.4 The ﬁeld equations
We now have to generalise the classical ﬁeld equation ∇2 Φ = 4πGρ to one
that determines the metric tensor. The story how this is done has been told
many times. The basic idea is that the local energy density ﬁxes the local
curvature of spacetime:
curvature ∝ energy density .

(3.33)

The left hand side of (3.33) will involve the Riemann tensor as that determines the curvature. The Riemann tensor contains second and lower-order
derivatives of the metric tensor gαβ , which is attractive on general grounds.
So it appears that we must relate ∇2 Φ to the Riemann tensor. The energy
density on the right hand side could just be ρc2 , but as we shall see, things
aren’t that simple.
To ﬁnd the relation between the potential Φ and the Riemann tensor,
consider two test particles A and B moving along their respective geodesics
xµ (s, λ) and xµ (s, λ + δλ). A is for example an observer on board the Space
Station, who sees satellite B passing at some distance. This situation has been
analysed in exercise 2.18. The vector ξ µ connecting A and B satisﬁes equation
(2.63) for the geodesic deviation:
D2 ξ µ
= Rµαβν uα uβ ξ ν ;
Ds2

uα =

∂xα
.
∂s

(3.34)

We elaborate this tensor equation in the local rest-frame of A. In that frame
gµν = ηµν according to (3.8), and dt = dτ . This means that A promotes
his clock to the master clock indicating co-ordinate time. Furthermore, all
Γ’s are zero (exercise 3.2), so that D/Ds = c−1 D/Dτ = c−1 d/dt. Moreover
xµ = (ct, 0, 0, 0) in this frame → uµ = (1, 0, 0, 0). We are left with

3.4 The ﬁeld equations

53

B
xm

l + dl

um

A
(observer)

l
p, or arc
length s

Fig. 3.3. The geodesic deviation as an aid in ﬁnding the vacuum equations.

d2 ξ µ
= c2 Rµ00ν ξ ν .
dt2

(3.35)

At this point A recalls that according to classical mechanics both he and B
move in a stationary gravitational ﬁeld: r̈A = F (rA ) and r̈B = F (rB ). Setting
i
i
− rA
we have
ξ i = rB
d2 ξ i
= F i (rA + ξ) − F i (rA )
dt2

i
F ,k
ξ k = −Φ,ik ξ k ,

(3.36)

since the force is related to the gradient of the gravitational potential as
F i = −Φ,i . By comparing (3.35) and (3.36) we ﬁnd
Φ,ik = −c2 Ri00k .

(3.37)

This is the relation between the second derivatives of Φ (that determine the
tidal forces) and the Riemann tensor. The use of the indices is sloppy, but
that’s all right as long as we are in the exploratory stage. The classical ﬁeld
equation is Φ,ii = −c2 Ri00i = 4πGρ. It follows quite generally from (2.50)
that R0000 = 0, and so we have found that
R00 = Rα00α = −

4πGρ
.
c2

(3.38)

However, this is not a tensor equation. The simplest guess would be that in
an arbitrary reference frame we should use Rµν = Rαµνα . In vacuum (ρ = 0)
we would then get
Rµν = 0
(3.39)
Although derived for weak ﬁelds, this is indeed the correct vacuum ﬁeld equation, also for the strong ﬁelds in the vicinity of compact objects and black
holes. An equivalent form is (see exercise):
Gµν ≡ Rµν −

1 µν
2g R

= 0

(3.40)

54

3 General Relativity

Eq. (3.39) or (3.40) cannot be proven in the strict sense of the word. Their
status of vacuum equations rests on the fact that predictions inferred from
them are sofar in agreement with the observations. The deeper reason why
this is so remains a mystery – apparently this is how Nature works. We refer
to Pais (1982) for an account of Einstein’s Werdegang to arrive at the correct
ﬁeld equation.
Nonzero energy density
What to do if the matter energy density ρc2 = 0? Evidently, we cannot simply
replace the right hand side of (3.39) or (3.40) by a constant times ρ. To proceed
we study how ρ behaves under Lorentz transformations.5 Consider a volume
V0 at rest, containing point masses with number density n0 , rest mass m0
and negligible random motion (‘cold dust’). The mass density is ρ0 = n0 m0 .
Observe V0 from a frame moving with velocity v. The mass of each particle
becomes m = γm0 with γ = (1 − β 2 )−1/2 and β = v/c. Lorentz contraction
makes that volume and number density transform as V = V0 /γ and n = γn0 ,
respectively. Hence, the mass density transforms as ρ = γ 2 ρ0 , and is therefore
not a scalar. Nor is it a component of a vector, as that produces one factor
γ at most, according to (1.8). However, the transformation of a second rank
tensor may yield a factor γ 2 since








T α β = Lαµ Lβν T µν ,

(3.41)

and if we take ρ as the 00-element of a second rank tensor of which it is
the only nonzero element in the local rest-frame, T µν = ρ δµ0 δν0 , we obtain
 

T 0 0 = (L00 )2 ρ = γ 2 ρ in the moving frame. It seems therefore that ρ is also
part of a second rank tensor. But which? The crucial step is to recognise that
one should take T µν = ρuµ uν where ρ is now the rest mass density and uµ
the 4-velocity. Since uµ = c−1 dxµ /dτ (γ, v i /c) we have T 00 = γ 2 ρ.
It would seem now that (3.38) is to be replaced by Rµν = −(4πG/c2 )Tµν
or equivalently Rµν = −(4πG/c2 )T µν . However, that leads to inconsistencies.
The trouble is that Rµν:ν is in general nonzero, so that T µν:ν would also be
nonzero. And as explained in a moment, conservation of mass or geodesic
motion would no longer be guaranteed. The proper continuation turns out to
be to replace (3.40) by
Gµν = Rµν −

1 µν
2g R

=−

8πG µν
T
c2

(3.42)

with T µν ≡ ρuµ uν = stress-energy tensor; ρ = rest mass density and uµ =
bulk 4-velocity of the cold dust. The value of the constant −8πG/c2 will be
derived in the next section by considering the classical limit.
5

We follow Price (1982). This beautifully written article is highly recommended.

3.4 The ﬁeld equations

55

Equation (3.42) has several attractive features. Because Gµν:ν = 0 according to (2.61), we have T µν:ν = (ρuµ uν ):ν = 0, or
uµ (ρuν ):ν + ρuν uµ:ν = 0 .

(3.43)

Multiply this by uµ , use uµ uµ = 1 and uµ uµ:ν = 0 (exercise 3.10). We are
left with (ρuν ):ν = 0, which is the continuity equation. A second consequence
is that uν uµ:ν = 0, i.e. the matter moves along geodesics according to (2.53).
Thus, equation (3.42) with T µν = ρuµ uν describes the dynamics of a collection of particles with a rest mass density ρ in their own gravitational ﬁeld
(‘cold dust’). Mass is conserved, and there is only gravitational interaction
between the mass elements because each moves along a geodesic. For that
reason, too, there are no collisions and the gas pressure is negligible. This
simple form of matter corresponds to the current state of the universe, with
the galaxies serving as the particles. Other forms of matter in which for example the pressure is important can be handled by adapting the stress-energy
tensor T µν accordingly. We return to this issue in § 3.6.
This may be the right place to draw attention to the power of the principle
of general covariance. The ﬁeld equations have the same form in all reference
frame, rotating, accelerating or other – it does not matter. The reader who
has checked the derivation of the Schwarzschild and Robertson-Walker metric
will have noticed that we make in fact a series of co-ordinate transformations.
We make one whenever it comes in handy, and there is no penalty because
the form of the ﬁeld equations does not change. Whatever co-ordinates we
choose, the ﬁeld equations deliver a metric tensor so that ds2 = gαβ dxα dxβ
is the correct metric in those co-ordinates. But the real advantage lies deeper:
the formulation of GR and the ﬁeld equations would be practically impossible
without exploiting general covariance. Take for example the stress-energy tensor T µν = ρuµ uν of cold dust. It appears on stage by asking how the rest mass
energy density ρc2 transforms in SR, which suggests that it is the 00-element
of a second rank tensor ρuµ uν . Next we declare this form valid in all reference
frames. It follows that what appears as energy density ρc2 in the local rest
frame shows up partly as momentum ﬂuxes in another frame. The conclusion
that all elements of the stress-energy tensor T µν contribute to the curvature
of spacetime is both inescapable and gratifying.

Exercise 3.8: Prove that (3.39) and (3.40) are equivalent.
Hint: Forward: Rµν = 0 hence Rµν = 0 and R = Rµµ = 0, i.e. Gµν = 0
(Gµν = 0 as well). Backward: Gµν = 0 → 0 = Gµµ = Rµµ − 12 g µµ R = −R.
Therefore Rµν = 0 and Rµν = 0.

56

3 General Relativity

Exercise 3.9: Why is ξ i in (3.35) and (3.36) the physical distance between
i
i
− rA
were said to be
A and B? In § 3.1 co-ordinate diﬀerences ξ i = rB
meaningless.
Hint: The physical distance is determined by (3.7), and what is gij ?
Exercise 3.10: Show that uµ uµ:ν = 0 (not to be confused with the geodesic
equation (2.53): uν uµ:ν = 0).
Hint: 1 = uµ uµ = gµν uµ uν → 0 = (gµν uµ uν ):σ = gµν (uµ:σ uν + uµ uν:σ ) =
2gµν uµ uν:σ = 2uν uν:σ . One may likewise prove that uµ uµ:σ = 0.

3.5 Weak ﬁelds (2)
This section is a little technical. We seek an expansion of the ﬁeld equations
in terms of the small parameter γαβ for weak ﬁelds. We need that to be able
to deal with the classical limit of the ﬁeld equations, and later for handling
gravitational waves. Once more we make the substitution 6
gαβ = ηαβ + γαβ ,

(3.44)

with γαβ ‘small’; gαβ and γαβ may now depend on x0 . Take α = σ in (2.62)
and substitute (3.44). The largest term in Rµν turns out to be of the order of
γ:
Rµν =

1 αβ
(γαβ,µν
2η

− γµα,βν − γβν,µα + γµν,αβ ) + O(γ 2 ) .

(3.45)

This can be written in the following form:
Rµν =

1
2 γµν

−

1
2 (τµ,ν

+ τν,µ ) + O(γ 2 ) ,

(3.46)

where  is the d’Alembert operator:

ψ = η

αβ

ψ,αβ =


1 ∂2
2
−∇ ψ
c2 ∂t2

(3.47)

and
6

ηαβ and therefore γαβ is not a tensor in GR. The use of tensors is usually
very convenient but there are exceptions, and is never a must. This is one such
exception.

3.5 Weak ﬁelds (2)

τµ = η αβ (γµα,β −

1
2 γαβ,µ )

.

57

(3.48)

Veriﬁcation is a matter of substitution. The next step is a transformation of
the co-ordinates. An exercise shows that there always exists a transformation
xµ → x̃µ so that τ̃µ = O(γ̃ 2 ). We work now in these new co-ordinates and
omit all terms of second and higher order in γ̃. Then R̃µν = 0 reduces to (we
drop the ˜ again):
Rµν

1
2 γµν

= 0;

τµ

0.

(3.49)

Here ‘ ’ means accurate to ﬁrst order in γ. These co-ordinates are called
harmonic co-ordinates.7 For stationary ﬁelds (3.49) leads to ∇2 γ00 = γ00,ii =
0. Hence Rµν = 0 implies (3.18) in these harmonic co-ordinates. But in (3.18)
no special co-ordinates had been chosen. Exercise 3.11 shows that γ̃00 = γ00
for stationary ﬁelds, so that (3.18) is always valid.
We also need the equivalent of (3.49) for the Einstein tensor Gµν . We
suppress details and give only the result:
Gµν
with

1
2 hµν

hµν,ν

;

0,

(3.50)

hµν = γµν −

1
2 ηµν γ

;

⎫
γ = γ σσ ; ⎬

γµν = hµν −

1
2 ηµν h

;

h = hσσ .

⎭

(3.51)

Since Rµν and Gµν are of order γ, we may raise and lower indices with η αβ
which may be moved through  in (3.49) and (3.50). Therefore we may move
the indices up and down in these expressions as we please. The condition
hµν,ν = 0 is called the Lorentz gauge because of the strong analogy with the
Lorentz gauge in electrodynamics (Aν,ν = 0 with Aν = vector potential). The
ﬁeld equation for weak ﬁeld now follows from (3.42) and (3.50):
hµν = −

16πG µν
T
c2

(3.52)

The one remaining issue is to show that the constant in eq. (3.42) has
the value −8πG/c2 . From the deﬁnition of the Einstein tensor (2.60) we infer
that Gµµ = R − 2R = −R. Substitution back into (2.60) produces Rµν =
Gµν − 12 g µν Gαα . We now write (3.42) as Gµν = kT µν and compute k. In the
(1, v i /c).
classical limit G00 is by far the largest element of Gµν since uµ
αβ
00
µ
µα
00
η0α η0β G = G
kρ, and G µ = ηµα G
G
G00 . It
Hence G00
7

Co-ordinates obeying the 4 restrictions g αβ Γµαβ = 0 are called harmonic. For weak
ﬁelds this amounts to τµ = 0 to ﬁrst order in γ, see Weinberg (1972), p. 161 ﬀ.

58

3 General Relativity

follows that R00
G00 − 12 η00 Gαα
2
yields k = −8πG/c .

1
2 G00

1
2 kρ.

Comparison with (3.38)

Exercise 3.11: Show that τµ can be made of order O(ξ 2 ) by the transformation xα → x̃α = xα + ξ α (x) with ξ α and its derivatives ‘small’.
α
α
β
Hint: x̃α,µ = δα
µ + ξ ,µ ; gµν tensor → gµν = g̃αβ x̃ ,µ x̃ ,ν , from which
2
gµν = g̃µν + ξµ,ν + ξν,µ + O(ξ ). Hence γ̃µν = γµν − ξµ,ν − ξν,µ . Substitute in
τ̃µ = η αβ (γ̃µα,β − 12 γ̃αβ,µ ):

τ̃µ = τµ − ξµ − η σρ (ξσ,µρ −

1
2 ξσ,ρµ

−

1
2 ξρ,σµ )

+ O(ξ 2 ) .

The term η σρ (...) is zero (interchange ρ and σ in the third term – why is
that allowed?). Choose ξµ so that ξµ = τµ , then τ̃µ = O(ξ 2 ) = O(γ̃ 2 )
and R̃µν = 12 γ̃µν + O(γ̃ 2 ). There is still gauge freedom left because ξµ is
determined up to an arbitrary solution of ξµ = 0.
Exercise 3.12: For weak stationary ﬁelds the metric in harmonic co-ordinates
may be written as
gαβ = ηαβ + γαβ = ηαβ +

2Φ(r)
eαβ .
c2

(3.53)

eαβ = 1 if α = β, otherwise 0 (this is not a tensor, see exercise 2.1; because
γαβ is not a tensor it cannot be expressed in terms of known tensors).
Hint: Since η αβ is diagonal and γαβ,0 = 0 we have η αβ γµα,β = −γµi,i and
1 αβ
γαβ,µ = 12 γ00,µ − 12 γii,µ . From τµ = 0: γii,µ −2γµi,i = γ00,µ . Try γij = aδij
2η
and other γ’s zero, except γ00 . Result: a,j = γ00,j . Take a = γ00 = 2Φ/c2 ,
according to (3.19).
Exercise 3.13: Show that Φ ∼ −v 2 for weak stationary ﬁeld, so that γαβ
from (3.53) is of order β 2 (this is not to say that Φ depends on v, but that
its value is of order −v 2 with v = characteristic velocity of a particle at that
position.)
Hint: Planetary orbits: Φ = −GM /r; circular orbit: mv 2 /r = GM m/r2 .
Exercise 3.14: A general invariant deﬁnition of energy does not exist in GR.
However, it does in case of a single test particle. From (3.23) we retrieve the
SR relations pµ = (E/c, pi ) and pµ = (E/c, −pi ) since gαβ = ηαβ . Hence two
possibilities: E = cp0 or E = cp0 . Show that

3.6 Discussion

E = cp0 = m0 c2 u0

59

(3.54)

is the right choice because it has the correct classical limit, and because E is
constant when the metric does not depend on time.
Hint: E = constant from (2.40). Consider the classical limit with (3.22) and
(3.53): (m0 c)2 = pα pα = gαβ pα pβ = g00 (p0 )2 +g11 p2 (p2 = pi pi ). Furthermore
g00 (p0 )2 = g00 (g 00 p0 )2 = g00 (p0 /g00 )2 = p20 /g00 (1−2Φ/c2 )p20 . We now have
(m0 c)2 (1 − 2Φ/c2 )(p20 − p2 ) or π02 − π 2 1 + 2Φ/c2 , with π0 = p0 /m0 c and
π = p/m0 c, or π02 = 1 + π 2 + 2Φ/c2 . Take the square root and use that π and
Φ/c2 are small: π0 1 + 12 π 2 + Φ/c2 , from which E = cp0 m0 c2 + p2 /2m0 +
m0 Φ. The three terms have an obvious classical interpretation.
Exercise 3.15: Invariant deﬁnition of the energy of a test particle. Consider
a particle with 4-momentum pα and an observer W with 4-velocity uα . Show
that from W ’s point of view, the energy of the particle is
E = cpα uα .

(3.55)

Hint: E = cpα uα = cpα uα = cp0 = energy that W assigns to the particle
according to (3.54) (¯ = local rest-frame of W ). Every W assigns the same
function cp0 to the particle, but not the same value.

3.6 Discussion
In this section we deal with more general forms of the ﬁeld equations. In the
ﬁrst place we investigate what the expression for the stress-energy tensor T µν
should be when the gas pressure p is nonzero. It seems reasonable that T µν
will be of the form T µν = ρuµ uν + pAµν . The only symmetric tensors that
are available to build Aµν are g µν and uµ uν . Therefore try
T µν = ρuµ uν +

p
(a uµ uν + b g µν ) .
c2

(3.56)

Here, too, T µν:ν = 0 must hold. For p = 0 that resulted in the continuity
equation and the geodesic equation (i.e. the equation of motion), and something similar will also be the case now. To see what happens we work in the
classical limit and work out {ρuµ uν + (p/c2 ) (auµ uν + bη µν )},ν = 0. The result

60

3 General Relativity

turns out to be the continuity equation and the Navier-Stokes equation provided a = 1 and b = −1, see Foster and Nightingale (1989, p. 73) or Schutz
(1985, Ch. 4). Thus we have found that
p
(3.57)
T µν = ρuµ uν + 2 (uµ uν − g µν ) .
c
In the early universe and in neutron stars the gas pressure reaches values of
the order of p ∼ ρc2 . Such high pressures determine, together with other forms
of energy, the structure of spacetime because pressure is a form of potential
energy. Pressure gradients, on the other hand, occur in the equations of motion
T µν:ν = 0, but they have no inﬂuence on the curvature of spacetime (§ 5.3).
The reasoning leading to (3.57) is an example of how the principle of
general covariance is used in practice. We start with the SR form of T µν and
look for a so-called mimimal generalisation, i.e. it is forbidden to add terms
that are identically zero in SR, such as ρRµναβ uα uβ . Often it amounts to
, → : and η µν → g µν , but it remains a matter of trial and error.
The next issue is the cosmological constant. Einstein considered an extra
term in the ﬁeld equation (3.42):
Gµν + Λg µν = −

8πG µν
T
c2

(3.58)

The historical motivation for a nonzero cosmological constant Λ was that
(1) it is a term that logically may appear in the equation as g µν:ν = 0, so
that T µν:ν = 0 is left intact, and (2) it permitted the possibility of a static
spherical universe. This solution turned out to be unstable, and when it was
subsequently discovered that the universe actually expands, the cosmological
constant was abandoned. But nowadays it is back with ﬂying colours, see
§ 9.5. The magnitude of Λ is of the order of (size universe)−2 which is so
small that (3.42) remains valid for all local physics. A physical explanation of
the cosmological constant is postponed to § 9.5.
Finally, we ask how (3.42) is to be extended to include other ﬁelds. These
ﬁelds have their own stress-energy tensor and it seems obvious that
8πG  µν
Gµν = − 2
T (ﬁeld i) .
(3.59)
c
i
The coupling between Gµν and the electromagnetic ﬁeld, for example, is important for the metric of a charged black hole. Here we shall only have the
opportunity to consider coupling of Gµν to a scalar boson ﬁeld. Models of the
universe based on such equations exhibit inﬂation, a brief period of extremely
rapid expansion, and may provide a solution for some of the problems of the
standard model of the Big Bang.

3.6 Discussion

61

matter tells spacetime how
to curve through (3.42)
g mn

T mn
spacetime tells matter how to
move through T mn:n = 0 or (3.60)

Fig. 3.4. Schematic structure of the equations of general relativity. The novel aspect
is that both matter and spacetime have become active players in the dynamics of
the world. Conceptually, one full round corresponds to one timestep in a numerical
code. If gµν and gµν,0 are given at x0 = 0 and T µν is suﬃciently well behaved, then
gµν is determined for all x0 , up to 4 arbitrary functions. This freedom of choice
corresponds to the freedom of choosing the co-ordinates (Wald, 1984, Ch. 10). After
Rees, M. et al.: 1974, Black Holes, Gravitational Waves and Cosmology, Gordon and
Breach, p. 2.

Structure of the equations
We draw attention to an important peculiarity of eq. (3.59), namely that it
does not handle the various forms of energy on equal footing. All energies other
than gravitational energy contribute to the geometry of spacetime through
their T µν on the right hand side in (3.59). The gravitational ﬁeld itself appears
only in Gµν on the left. Since Gµν:ν = 0 poses 4 extra restrictions, eq. (3.42) or
(3.59) is a set of 6 nonlinear diﬀerential equations for the metric tensor. The
nonlinearity is the mathematical expression of the fact that the energy density
of the gravitational ﬁeld acts as a source of gravity itself. The superposition
principle of classical mechanics (gravitational ﬁeld of two bodies is the sum
of the individual ﬁelds) no longer applies in GR, except when the ﬁelds are
weak, as in eq. (3.52).
The computational scheme of GR is shown in Fig. 3.4. The dynamics of the
(matter) ﬁelds is ﬁxed by T µν:ν = 0 (for example the structure of a relativistic
star, § 5.3). The motion of a test particle follows from a generalisation of (3.13):
Dpµ
= fµ .
Dτ

(3.60)

The extra forces f µ (e.g. the Lorentz force) push the particle oﬀ the geodesic,
and the 4-momentum pµ is no longer parallel transported along the orbit.8
8

For information on numerical relativity see for instance Font, J.A., Living Rev.
Relativity 6 (2003) 4 (http://www.livingreviews.org/lrr-2003-4).

62

3 General Relativity

But if there is only gravity, a particle experiences no acceleration in the parlance of GR.

Exercise 3.16: Show that (3.58) reduces to
∇2 Φ = 4πGρ − Λc2

(3.61)

in the classical limit. A positive Λ is equivalent to a negative mass density,
i.e. a repulsive force.
Hint: Write k = −8πG/c2 for brevity. Combine (3.50) and (3.58): 12 h00 =
−Λ + kρ. Combine (3.50) and (3.58) again, lower one index and contract:
1
−4Λ + kρ (T 00 and G00 are the largest terms). From (3.51): γ00 =
2 h
(h00 − 12 h) = 2Λ+kρ. Stationary ﬁelds and (3.19): ∇2 Φ −Φ − 12 c2 γ00 .

4
The Schwarzschild Metric

The stationary, spherically symmetric solution of the vacuum equations is of
fundamental importance as it describes the ﬁeld of a spherically symmetric
body like a star – the simplest gravitational ﬁeld one may think of. This solution was discovered by K. Schwarzschild in 1916, a few month after the publication of the vacuum equations. The ﬁeld equations (3.39) are complicated
nonlinear diﬀerential equations, but the spherical symmetry greatly simpliﬁes
the mathematics. The solution predicts small deviations from the Newtonian
results for weak ﬁelds like that of the Sun, of which four have been conﬁrmed
experimentally (the perihelium precession, the deﬂection of light, the gravitational redshift, and the time delay of light signals). Together with the recent
discoveries on the binary pulsar these are the most important quantitive veriﬁcations of GR. The solution also applies to the strong ﬁelds of compact objects
such as neutron stars and black holes. In the latter case the solution predicts
the existence of a singularity in spacetime at r = 0, which is, fortunately,
unobservable for a distant observer.

4.1 Preliminary calculations
We employ spherical co-ordinates x0 = ct, x1 = r, x2 = θ and x3 = ϕ, and
simplify the form of the metric (3.1) step by step:
1. Stationarity implies that gαβ,0 = 0.
2. ds2 must be invariant under dx0 → −dx0 , so that terms ∝ dx0 dxi must
be absent → g0i = 0.
3. Spherical symmetry implies that ds2 is invariant under dθ → −dθ and
dϕ → −dϕ. It follows that grθ = grϕ = gθϕ = 0 because terms ∝ drdθ,
drdϕ and dθdϕ must vanish.
4. At this point only the diagonal terms remain, and we write the metric as
ds2 = A c2 dt2 − Bdr2 − Cr2 dθ2 − D r2 sin2 θ dϕ2 .

(4.1)

66

4 The Schwarzschild Metric

Fig. 4.1. A wormhole. Adapted from Misner et al. (1971).

The factors r2 and r2 sin2 θ have been added for convenience.
5. According to (3.7), the subspace x0 , r = constant has the metric of a
sphere dl2 = r2 (Cdθ2 + D sin2 θ dϕ2 ). An inhabitant of this space will not
notice any eﬀect of gravity (spherical symmetry). He concludes that his
world is an ordinary spherical surface and takes C = D.
6. By choosing convenient units we can arrange that C = 1.
Following the literature we write (4.1) as
ds2 = e2ν c2 dt2 − e2λ dr2 − r2 (dθ2 + sin2 θ dϕ2 ) ,

(4.2)

where ν = ν(r) and λ = λ(r).1 Further simpliﬁcation of the metric on the
basis of symmetry considerations is not possible, except that we know that
for r → ∞ (4.2) must coincide with the Lorentz metric, so that λ, ν → 0 for
r → ∞. The elements of the metric tensor are:

g11 = −e2λ ;
g00 = e2ν ;
(4.3)
g33 = −r2 sin2 θ ,
g22 = −r2 ;
and all other gαβ are zero. We have chosen the co-ordinates so that the 2volume of each subspace x0 , r = constant equals 4πr2 , but that is not to say
that r is the distance to the origin. There may not even be an origin. Suppose
you move in the radial direction toward the origin, deﬁned by insisting that
the area of the sphere r = constant decreases. But as you keep moving in that
direction, the area may at some point begin to increase again so that r = 0 is
never reached, Fig. 4.1. The message is that such topological constructions are
in principle possible, and the ﬁeld equations decide whether they are real or
not. As it turns out, any particle entering a black hole must hit a singularity
at r = 0. So a Schwarzschild wormhole, if it exists, is a diﬀerent concept than
Fig. 4.1 suggests, see Misner et al. (1971) and Wald (1984).
Of course one may use diﬀerent co-ordinates. One possibility is to take
1

ν and λ are in this chapter no longer available as indices.

4.1 Preliminary calculations

67

B = C = D in (4.1), leading to isotropic co-ordinates, see Adler et al. (1965).
Another possibility is Kruskal-Szekeres co-ordinates, § 6.4. Physical quantities
are always independent of the choice of the co-ordinates. The calculations that
follow are divided into three steps: ﬁrst we need the Christoﬀel symbols. Then
we use (2.57) to ﬁnd Rαβ . Finally ν(r) and λ(r) are solved from Rαβ = 0.
The Christoﬀel symbols
The easiest way is to ﬁnd the geodesic equation with the help of variational
calculus. We need this equation anyway, and once we have it we may just
read the Γ’s by comparing with (2.34), just like we did in § 2.5. We elaborate
(2.36) with the help of (4.2):
 

δ
e2ν (ẋ0 )2 − e2λ ṙ2 − r2 θ̇2 − r2 sin2 θ ϕ̇2 dp = 0 .
(4.4)
Here ˙ = d/dp, and x0 (p), r(p), θ(p), ϕ(p) is the parametric representation of a
geodesic. Next we write down the Euler-Lagrange equations (2.37). Notation:
L = {· · ·} = integrand of (4.4):
(a). ∂L/∂x0 = (∂L/∂ ẋ0 )˙. Since ∂L/∂x0 = 0 we get
(2 ẋ0 e2ν )˙ = 0 ;

˙ = d/dp .

(4.5)

Now use that ν̇ = dν/dp = (dν/dr) (dr/dp) = ν  ṙ with ν  ≡ dν/dr. Result:
ẍ0 + 2 ν  ẋ0 ṙ = 0 ;



= d/dr .

(4.6)

Comparing with (2.34) we conclude that Γ001 + Γ010 = 2ν  , and since Γ001 =
Γ010 we have
(4.7)
Γ010 = ν  .
The other Γ0αβ are zero.
(b). ∂L/∂r = (∂L/∂ ṙ)˙, or after performing the diﬀerentiations:
2ν  e2ν (ẋ0 )2 − 2λ e2λ ṙ2 − 2r (θ̇2 + sin2 θ ϕ̇2 ) = (−2ṙ e2λ )˙ .

(4.8)

After some cleaning up:
r̈ + ν  e2(ν−λ) (ẋ0 )2 + λ ṙ2 − r (θ̇2 + sin2 θ ϕ̇2 ) e−2λ = 0 .

(4.9)

Consequently:
Γ100 = ν  e2(ν−λ) ;

Γ111 = λ ;

Γ122 = −r e−2λ ;

Γ133 = −r sin2 θ e−2λ ,

⎫
⎬
⎭

(4.10)

68

4 The Schwarzschild Metric

and all other Γ1αβ are zero. We get the remaining Christoﬀel symbols from
the other two geodesic equations (see exercise):
Γ212 =

1
;
r

Γ233 = − sin θ cos θ ;

(4.11)

Γ313 =

1
;
r

Γ323 = cot θ .

(4.12)

At this point all nonzero Christoﬀel symbols have been found. Recall that
Γµαβ = Γµβα .
Computing Rαβ
The Γ’s must now be inserted in (2.57). This requires a little perseverance.
We illustrate that for R00 :
R00 = Γα0α,0 − Γα00,α − Γα00 Γβαβ + Γα0β Γβ0α
= − Γ100,1 − Γ100 Γβ1β + Γα0β Γβ0α
= − Γ100,1 − Γ100 Γβ1β + 2 Γ010 Γ100 .

(4.13)

Γαµα occurs twice in (2.57) and may be calculated with (2.33). But the gain
is minimal because it is just as easy to ﬁnd Γαµα through a summation:
Γα0α = Γα3α = 0 ;

Γα1α = ν  + λ +

2
;
r

Γα2α = cot θ .

From (4.14) we now obtain


2ν 

 
 2
R00 = − ν − ν λ + (ν ) +
e2(ν−λ) .
r

(4.14)

(4.15)

Without proof we mention the other nonzero components of Rαβ :
2λ
;
r

(4.16)

R22 = (1 − rλ + rν  ) e−2λ − 1 ;

(4.17)

R33 = R22 sin2 θ .

(4.18)

R11 = ν  − ν  λ + (ν  )2 −

We may now compute the total curvature

R = Rαα = g αβ Rαβ =
α Rαα /gαα


2λ
1
2ν 
2

 
 2
−
+ 2 e−2λ + 2 .
= − 2 ν − ν λ + (ν ) +
r
r
r
r

(4.19)

4.2 The Schwarzschild metric

69


In
α · · · the summation convention has been switched oﬀ momentarily.
The Einstein tensor Gαβ = Rαβ − 12 gαβ R is also diagonal and we need only
G00 and G11 :
G00 = −
G11 =

e2ν d
r (1 − e−2λ ) ;
r2 dr

(4.20)

1 2λ
2ν 
.
(e
−
1)
−
r2
r

(4.21)

Exercise 4.1: Show that the two remaining geodesic equations for θ and ϕ
are
(r2 θ̇)˙ = r2 sin θ cos θ ϕ̇2

→

θ̈ +

2
ṙθ̇ − sin θ cos θ ϕ̇2 = 0 ;
r

(4.22)

(r2 sin2 θ ϕ̇)˙ = 0

→

ϕ̈ +

2
ṙϕ̇ + 2 cot θ θ̇ϕ̇ = 0 ,
r

(4.23)

and determine the corresponding Christoﬀel symbols (4.11) and (4.12).

4.2 The Schwarzschild metric
It is actually easier to solve Gαβ = 0 than Rαβ = 0. From (4.20) we ﬁnd:
r (1 − e−2λ ) = b

→

e2λ =

1
,
1 − b/r

(4.24)

with b constant. Substitute that in G11 = 0:
2ν  =

b/r2
1 − b/r

→

e2ν = A (1 − b/r) ,

(4.25)

since the expression on the left can be integrated to 2ν = log (1 − b/r) +
const. The constant A must be 1 because (4.2) must be the Lorentz metric
for r → ∞. We insert these results in (4.2):
ds2 = (1 − b/r) c2 dt2 −

dr2
− r2 (dθ2 + sin2 θ dϕ2 ) .
1 − b/r

(4.26)

To determine the constant b we note that the metric (4.26) can only describe the eﬀect of a spherically symmetric mass M . In the classical limit

70

4 The Schwarzschild Metric

the gravitational potential is Φ = −GM/r, and according to (3.19) we
have g00 = 1 − 2GM/c2 r for large r. Comparison with (4.26) shows that
b = 2GM/c2 , or
ds2 = (1 − rs /r) c2 dt2 −

dr2
− r2 (dθ2 + sin2 θ dϕ2 ) .
1 − rs /r

(4.27)

The quantity
rs ≡

2GM
c2

(4.28)

is called the Schwarzschild radius. The Sun has rs 3 km, see also Table 3.1.
The components of the metric tensor, and λ and ν are now given by
g00 = e2ν = e−2λ = 1 − rs /r ;
g22 = −r2 ;

g11 =

−1
;
1 − rs /r

g33 = −r2 sin2 θ .

⎫
⎪
⎬
⎪
⎭

(4.29)

Relations (4.27) – (4.29) describe the standard form of the Schwarzschild
metric. Birkhoﬀ showed in 1923 that the Schwarzschild metric is the only
spherically symmetric solution of the ﬁeld equations exterior to a spherical,
nonrotating, uncharged but not necessarily stationary mass distribution. This
is known as Birkhoﬀ ’s theorem.
The range of validity of the co-ordinates is as follows. Because of stationarity −∞ < t < ∞. Furthermore 0 ≤ θ ≤ π, 0 ≤ ϕ < 2π because we have
chosen θ and ϕ in the ‘usual’ way. For r there are two options:
1. There is a material surface at r = R (and R > rs ). The object is a
(compact) star and the metric is valid in the vacuum region outside the
star, r ≥ R. Inside the star the metric is diﬀerent, see Ch. 5.
2. There is no material surface in the range r > rs . In that case the object
is a black hole. The metric is valid everywhere (no restrictions on r), but
has two singularities at r = 0 and at r = rs . Their physical nature will be
dealt with in Ch. 6.
We brieﬂy dwell on the question how the values of the co-ordinates may be
measured. The co-ordinate time t may be determined for example by counting
pulses from a laser. The laser emits pulses once every second in its own proper
time, say. Now let the laser be at r = ∞ (‘suﬃciently far away’) and have a
proper frequency ν0 . At some ﬁnite r one measures a blueshifted frequency ν.
We repeat the reasoning below (3.20): νdτ = ν0 dt (dτ = dt at r = ∞ because
gαβ = ηαβ ). From (3.2) and (4.29):

4.2 The Schwarzschild metric

71

∆ν
ν − ν0
dt
− 1 = g00−1/2 − 1
=
=
ν0
ν0
dτ

rs −1/2
rs
+ ··· .
(4.30)
= 1−
−1
r
2r
By measuring ∆ν we may determine r/rs everywhere. To ﬁx r and rs separately, we may either put a satellite in a circular orbit and measure its period
∆t (→ r, see (4.46)), or we may construct a sphere with radius r, deﬁned as
the set of all spatial positions having the same frequency ν, and measure its
area O. Then r is also known because r has been chosen so that a surface
r = constant has 2-volume O = 4πr2 . Finally, we may draw a grid of latitute
and longitude circles on these spheres to determine θ and ϕ. This shows that
measuring procedures to determine t, r, θ and ϕ do in principle exist.
No one will notice anything out of the ordinary as long as he or she stays on
a shell r = constant, because the metric is classical: dl2 = r2 (dθ2 +sin2 θ dϕ2 ).
But strange things do emerge as one travels between shells: the 3-volume be3
3
tween two shells at r = r1 and r = r2 is larger than 4π
3 (r2 − r1 ). And the
distance between r1 and r2 is larger than the co-ordinate diﬀerence r2 − r1
(see the exercises below).

Exercise 4.2: Give a qualitative argument to illustrate that rs is proportional
to the mass M and not, for example, to some other power of M .
Hint: Require for example that the escape velocity is c at R = rs . Another
possibility is to say that M c2 ∼ potential gravitational energy ∼ (GM/rs )M .
Exercise 4.3: Calculate the total curvature R. Is spacetime curved or not?
Hint: A close look at (2.59) and (3.39) may spare you a surprise. What does
‘curved’ mean?

Exercise 4.4: Consider two spherical shells at r = r1 and r = r2 . Calculate (1) the 3-volume between the shells; (2) the 2-volume (area) in the plane
θ = π/2 between r1 and r2 ; (3) the 1-volume (length) of a stick pointing
radially to the star, end points at r1 and r2 ; (4) the 2-volume of spacetime
enclosed by t1 ≤ t ≤ t2 , r1 ≤ r ≤ r2 and θ, ϕ = constant.
Hint: Comes down to ﬁnding the invariant volume-element according to § 3.3.
(1). Metric according to (3.7): dl2 = (1 − rs /r)−1 dr2 + r2 (dθ2 + sin2 θ dϕ2 ),

72

4 The Schwarzschild Metric

r
π
hence g ≡ det{gik } = (1 − rs /r)−1 · r2 · r2 sin2 θ; 3-volume = r12 dr 0 dθ ·
 2π √
r
r
3
3
dϕ g = 4π r12 (1 − rs /r)−1/2 r2 dr > 4π r12 r2 dr = 4π
3 (r2 − r1 ).
0
(2). θ = π/2, dθ = 0 and g = (1 − rs /r)−1 · r2 ; 2-volume =
√
g > π(r22 − r12 ).

 r2
r1

dr

(3). dl2 = (1 − rs /r)−1 dr2 because dθ = dϕ = 0. Hence l =
rs /r)−1/2 dr > r2 − r1 .

 2π
0

 r2
r1

dϕ ·

(1 −

(4). Metric: ds2 = (1 − rs /r)c2 dt2 − (1 − rs /r)−1 dr2 → g = −c2 ; 2-volume =
 t2  r2 √
dt r1 dr −g = c(t2 − t1 ) · (r2 − r1 ).
t1
Exercise 4.5: To derive (4.30) we have used eq. (3.2), which is only valid for
observers at rest. Consider an observer moving along r in the Schwarzschild
metric at co-ordinate speed v = dr/dt. Prove the following relation between
the co-ordinate time and the proper time of the observer:
dτ
=
dt



rs
1−
r




−

rs
1−
r

−1  2 1/2
v
.
c

(4.31)

Hint: dθ = dϕ = 0 in (4.27), divide by c2 dt2 . This result is a generalisation
(1.6) of SR and also of (3.2). It serves as a warning that diﬀerent kinds of
redshift (here gravitational and Doppler) do not simply add!

4.3 Geodesics of the Schwarzschild metric
We could set out from the geodesic equations (2.34), but things become a lot
easier if we utilise the constants of motion. These can be found in various
ways. Because gαβ,0 = 0 we infer from (2.40) that u0 = g0α uα = g00 u0 =
(1 − rs /r)cṫ = constant:
(1 − rs /r) cṫ = constant ≡ e .

(4.32)

Recall that t(p), r(p), ϕ(p) is the parametric representation of a geodesic and
that ˙ = d/dp for a null geodesic and ˙ = d/ds = c−1 d/dτ for a timelike
geodesic. For a massive particle, (3.54) says that e = E/m0 c2 = the total
energy in units of its rest mass energy. Relation (4.32) is important because
it ﬁxes the rate of proper time τ of an object in geodesic motion with respect
to co-oordinate time t. We may write it as

4.3 Geodesics of the Schwarzschild metric
m = 0 ; a < 1/8 (large h)

V

m = 0 ; 1/8 < a < 1/6 (medium h)

V
d
a

d
value e2

g

1

1
b
rs r
-

r

r

+

V

d

a

b
rs r-

m = 0 ; a > 1/6 (small h)

r+

r

m = 0 (photon, graviton)

V

d
a

1

73

g

a

rs

r

rs

1.5 rs

r

Fig. 4.2. Classiﬁcation of the four diﬀerent types of orbit in the Schwarzschild
metric, here referred to as α, β, γ and δ-orbits (there is no generally accepted
nomenclature). The ﬁgure shows V (r) and e2 as a function of r (not to scale). For
massive particles with 18 < a < 16 the potential has two extrema, but V (r− ) < 1. In
this case there are two kinds of α-type orbits but no γ-type orbit. See also exercise
4.15.

1
dτ
= (1 − rs /r) .
dt
e

(4.33)

For a photon the meaning of e is not immediately clear, as ṫ = dt/dp and p is
an unspeciﬁed orbit parameter. This issue is elucidated in exercise 4.15. The
metric does not depend on x3 = ϕ either → u3 = g33 u3 = g33 ϕ̇ = const. Due
the spherical symmetry the geodesics must lie in a plane, and we may restrict
ourselves to θ = π/2, i.e. g33 = −r2 :
r2 ϕ̇ = constant ≡ h .

(4.34)

74

4 The Schwarzschild Metric

This is a generalisation of Kepler’s second law – equal areas covered in equal
times – which follows also directly from (4.23).
To ﬁnd the equation for ṙ, it is actually easier to start from (4.27) than
from (4.8). We ‘divide’ by dp2 :
(1 − rs /r) c2 ṫ2 −

ṙ2
− r2 ϕ̇2 =
1 − rs /r



ds
dp

2
≡ κ,

(4.35)

with κ = 0 / 1 for a massless / massive particle, respectively. With the help of
(4.32) and (4.34) we obtain
ṙ2
h2
e2
−
− 2 = κ,
1 − rs /r
1 − rs /r
r

(4.36)

which we may write as

ṙ2 = e2 − V (r) ;

1−

with V (r) =

rs
r



h2
+κ
r2


.

(4.37)

For κ = 0 (massless particles), V has a maximum at 1.5 rs . For massive particles it is necessary to distinguish between a high and a low angular momentum
h, measured by the parameter a ≡ rs2 /2h2 . For a > 16 (low angular momentum) V (r) increases monotonously, and for a < 16 (high angular momentum)
V (r) has two extrema at

√
r±
1 
1 ± 1 − 6a ;
=
rs
2a
By using that a <

1
6

a=

rs2
.
2h2

(4.38)

it is easy to show that
1.5 rs ≤ r− ≤ 3 rs ;

r+ ≥ 3 rs ,

(4.39)

while in the classical limit (rs → 0, i.e. a → 0):
r−

1.5 rs ;

r+

rs /a .

(4.40)

A classiﬁcation of the orbits is given in Fig. 4.2. The particle moves on a
horizontal line e2 = constant in a region where e2 ≥ V (r) to ensure that
ṙ2 > 0 in (4.37). Now ṙ may only change sign where e2 = V (r), and the
particle must reverse its radial direction there, as circular orbits at e2 = V (r)
are unstable (see exercise). There are four diﬀerent types of orbit. Assume for
the sake of argument that we are dealing with a black hole, so that the metric
is valid everywhere. A particle in an α-type orbit will be swallowed by the
hole. A massive particle may be in a β-type orbit, performing an ellipse-like
motion, but the orbit need not be closed – we only know that its r-range is
restricted. Type-γ orbits are hyperbola-like, while particles in a δ-type orbit

4.3 Geodesics of the Schwarzschild metric

75

either fall into the hole or escape (r → ∞). A radially moving photon has
V = 0 (h and κ being zero), and is therefore always in an δ-type orbit.
Massive particles can be in a stable circular orbit at r = r+ , the smallest of
which is at r = 3rs . Massless particles do not have stable circular orbits, see
exercise.
The next step would be to determine r(t) and ϕ(t) by solving eqs. (4.32),
(4.34) and (4.37). A simpler task (and suﬃcient for our purposes) is to derive
the shape r(ϕ) of the orbit. This may be done with Binet’s method known
from classical mechanics. We have ṙ = dr/dp = (dr/dϕ) (dϕ/dp) = r,ϕ ϕ̇ =
hr,ϕ /r2 . Next, we introduce the variable u = rs /r, and u,ϕ = −rs r,ϕ /r2 =
−u2 r,ϕ /rs . Result:
ṙ = hu2 r,ϕ /rs2



r,ϕ = −rs u,ϕ /u2

→ ṙ = −hu,ϕ /rs .

(4.41)

Substitute this in (4.37) and use h2 /rs2 = 1/2a:
(u,ϕ )2 = 2ae2 − (1 − u) (u2 + 2κa) .

(4.42)

Take d/dϕ and rearrange terms:
u,ϕ (2u,ϕϕ + 2u − 2κa − 3u2 ) = 0 .

(4.43)

We discard the solution u = constant. The other solution is
u,ϕϕ + u = κa +

3 2
2u

;

u = rs /r .

(4.44)

κ = 0/1 for photon / massive particle. All relativistic eﬀects are hidden in the
nonlinear term 32 u2 . It may be ignored in the classical limit (since for r  rs →
u), and then (4.44) has the solution u = κa + C cos(ϕ − ϕ0 )
u
1 → u2
or r(ϕ) ∝ [κa + C cos(ϕ − ϕ0 )]−1 , which is an ellipse or hyperbola for κ = 1,
and a straight line for κ = 0. In the next section we shall use eq. (4.44) to
derive the perihelium precession and the gravitational deﬂection of light.

Exercise 4.6: Prove (4.38) to (4.40).
Exercise 4.7: Show that r+ = rs /a is equivalent to Kepler’s third law.
Exercise 4.8: In principle, circular orbits are possible at all locations where
e2 = V (r). Investigate the stability of these orbits.
Hint: In (4.44) the root u,ϕ = 0 has been divided out, so it is safer to use

76

4 The Schwarzschild Metric
V

1
m0 = 0
m0 = 0
0

r+

r

Fig. 4.3. The classical potential V (r) may be obtained by taking the appropriate
limit, see exercise. A massive particle can now only be in a β- or γ-type orbit, and
a photon only in a γ-type orbit (a straight line); δ-orbits are only possible if h = 0
(head-on collision).

(4.37). Take r = r0 + δr and e2 − V (r0 ) = 0; result: δṙ2 = −V0 δr − 12 V0 δr2 ,
where δṙ2 ≡ (δṙ)2 ; V0 ≡ dV (r0 )/dr0 , etc. Two possibilities:
1. V0 = 0 → δṙ2 = −V0 δr → δr̈ = − 12 V0 → δr always moves to a region
where V is smaller, hence always unstable.
2. V0 = 0 → δṙ2 = − 12 V0 δr2 → δr̈ = − 12 V0 δr → stable if V0 > 0 (in a
minimum), otherwise unstable.
Conclusion: for m0 = 0 only stable if r = r+ , i.e. at the bottom of the
potential well; for m0 = 0 always unstable.
Exercise. 4.9: We ﬁre a bullet from r = ∞ towards a black hole with impact
parameter d (= shortest distance between particle and hole if the orbit were
a straight line). The bullet is in a γ-orbit and will miss the hole. Next we put
the bullet in a δ-orbit by ﬁring it at a higher velocity (e ↑), see Fig. 4.2. The
bullet will now fall into the hole. Explain this paradox.
Hint: Fig. 4.2 is deceptive in that V (r− ) changes as well. Show that V (r− )
2/(27a) for a
1. Hence V (r− )/e2 ∝ (h/e)2 . Calculate this constant with
(4.32) and (4.34) by analysing the orbit at large r, where ϕ d/r. Show that
h/e
v∞ d/c, so that V (r− )/e2 increases as we ﬁre faster, and the bullet
remains in a γ-orbit.

Exercise 4.10: Show that Fig. 4.2 reduces to Fig. 4.3 in the classical limit.
Hint: The transition to classical mechanics is obtained by letting c → ∞.
Show that a, rs , r− → 0, V (r− ) → ∞, while r+ remains ﬁnite.

4.4 The classical tests of GR

77

Exercise 4.11: Show that for a massive particle in a circular orbit with radius
r

1/2
rrs /2
1 − rs /r
h =
.
(4.45)
;
e =
1 − 3rs /2r
1 − 3rs /2r
Hint: A circular orbit is only possible when r = r+ , hence r/rs = (1 +
√
1 − 6a )/2a. Solve for a = rs2 /2h2 ; e from e2 = V (r).
Exercise 4.12: Show that the period of a satellite in a circular orbit with
radius r is given by (∆t = co-ordinate time, ∆τ = proper time satellite):
2πr
∆t =
c



2r
rs

1/2
;

2πr
∆τ =
c



2r
rs



3rs
1−
2r

1/2
.

(4.46)

Hint: (4.34) → 2πr2 /c∆τ = h; (4.33) → ∆t/∆τ = e/(1 − rs /r); h and e
√
from (4.45). Two points: (1). Apparently dτ /dt = (1 − 3rs /2r)1/2 = g00 .
Why is (3.2) not valid? (2). (2πr/c) 2r/rs = 2πr(GM/r)−1/2 is the classical
expression. Does an observer at r = ∞ (who measures dt) notice any deviation
from classical mechanics?

4.4 The classical tests of GR
The classical tests of GR, in order of their conﬁrmation, are (1) the perihelium
precession of Mercury, (2) the deﬂection of light in a gravitational ﬁeld, (3)
the redshift of light escaping from a gravitational well. Much later came (4)
the delay in radar signals reﬂected by planets. A ﬁfth experiment to measure
the geodesic and Lense-Thirring precession of a gyroscope is now operational
(Gravity Probe B, Ch. 8). For detailed information on these matters we refer
to Will (1993).
In the middle of the 19th century it became apparent that the perihelium precession of Mercury had an unexplained diﬀerence of 43 ± 0.5 per
century. The total precession is 5600 per century, of which 5025 is caused
by the precession of the equinox (due to the precession of the Earth’s rotation axis), and 532 by other planets, mostly Jupiter and Venus. Around that
time Adams (1845) and Leverrier (1846) were able to predict the location of
a then unknown planet (Neptune) from perturbations in the orbit of Uranus.
By analogy it was assumed that the perihelium precession was caused by
zodiacal dust or by an unknown planet (Vulcan) located inside the orbit of

78

4 The Schwarzschild Metric

Fig. 4.4. The Einsteinturm near Potsdam, an elegant design of Erich Mendelsohn,
became operational in 1924, ﬁve years after the discovery of the gravitational deﬂection of light. It was originally built to measure the gravitational redshift in the
solar spectrum. Photo: R. Arlt, AIP.

Mercury. However, the mass of the zodiacal dust was far too small and Vulcan
has never been found.
Another possibility for a classical explanation of the perihelium precession is the fact that the mass distribution of the Sun has a nonzero quadrupole
moment due to rotation. The gravitational potential has a small extra term:
Φ(r) = −(GM/r)[1 + 12 J2 (R /r)2 ] in the equatorial plane, where J2 is the
dimensionless quadrupole moment. This extra term ∝ r−3 causes a precession. Modern measurements place the value of J2 in the range 10−6 − 10−7 , in
which case the quadrupole moment will contribute at most 0.1 per century.2
The dependence of the precession rate on the semi-major axis  is −7/2 for
a quadrupole moment and −5/2 for GR. The observed perihelium precession
of Venus and the Earth suggests an −5/2 dependence.3
The fact that light rays are deﬂected in a gravitational ﬁeld was demonstrated during the solar eclipse of 1919, and in those days after World War
I that achievement generated a media-hype avant la lettre. The observations
are very diﬃcult and later experiments resulted in values ranging from 1.4
to 2.7 . In 1952 a value of 1.7 ± 0.1 was obtained. Nowadays Very Long
Baseline Interferometry (VLBI) is used to measure the change in the position
of a number of bright quasars in the ecliptic plane as the Sun passes by. These
2
3

see Godier, S. and Rozelot, J.-P., A & A 350 (1999) 310; Will (1993) § 7.3.
see Adler et al. (1965) p. 202; Foster and Nightingale (1989) p. xiii.

4.4 The classical tests of GR

79

observations have the advantage of being very accurate, and it is no longer
necessary to wait for an eclipse. The result is 1.760 ± 0.016 .4
Attempts to measure the gravitational redshift initially employed the solar spectrum. The eﬀect is small: the solar spectral lines have a width of ∼ 10
km s−1 , while the redshift is only 600 m s−1 . Moreover, there is a convective
blueshift (rising gas being hotter than sinking gas) of about the same magnitude. Calibration is possible with the help of the Doppler shift induced by
the known motion of the Earth with respect to the Sun. But due to lack of
stability and other systematic eﬀects the redshift could not be measured. It
was only in 1962 that the gravitational redshift was convincingly detected
in the solar spectrum. The Pound-Rebka experiment (1960) was actually the
ﬁrst quantitive measurement of the eﬀect. In 1971 portable caesium clocks
have been ﬂown around the world on commercial jet ﬂights, eastbound and
westbound, and their readings were compared with a reference clock on the
ground.5 Such an experiment measures a mix of gravitational and specialrelativistic redshifts. Gravity Probe A, a rocket experiment using a hydrogen
maser clock measured the gravitational redshift with a precision of 10−4 in
1979.6
Shapiro proposed a fourth test in 1964: the delay of radar signals, and in
retrospect it is amazing that this test had not been thought of earlier. The
idea is that in the Schwarzschild metric the distance between r1 and r2 is
larger than r2 − r1 (exercise 4.4), so that light needs more time to travel the
distance between the points. This has been veriﬁed in radar reﬂection experiments on Mercury and Venus and by observations of the binary pulsars PSR
1534+12 and PSR 1855+09. Radio echo observations of VIKING attained a
precision of 10−3 . Data from the Cassini spacecraft as it passed behind the
Sun on its way to Saturn have recently improved the precision to 2 × 10−5 . 7
Gravitational deﬂection of light
We now calculate the deﬂection angle of starlight from eq. (4.44), see Fig. 4.5.
The classical photon orbit follows by omitting the nonlinear term: u,ϕϕ + u =
0 → u = const·cos ϕ. We write this zero-order solution as u0 (ϕ) = ζ cos ϕ, and
ζ = rs /r0 is the small parameter in the problem. This solution is a straight
line r cos ϕ = r0 . Now substitute u = u0 + δu in (4.44) and linearise:
δu,ϕϕ + (1 − 3u0 ) δu = q(ϕ) ;

q(ϕ) =

3 2
2 u0

.

(4.47)

We need the solution of this equation with initial conditions δu(0)
= δu,ϕ (0) = 0, which we may ﬁnd with the help of the method of variation of
4
5
6
7

see Misner et al. (1971) p. 1104 and Will (1993) Ch. 1 and 7.
Hafele, J.C. and Keating, R.E., Science 177 (1972), 166 and 168.
Vessot, R.F.C. and Levine M.W., General Relat. & Gravit. 10 (1979) 181.
Bertotti, B. et al., Nature 425 (1993) 374.

80

4 The Schwarzschild Metric
d

r
M

j

r0

classical orbit
r cos j = r0

Fig. 4.5. Geometry of the deﬂection of light by the gravitational ﬁeld of a central
mass M . The deﬂection angle is 2δ.

constants (see exercise):

δu = s(ϕ)



ϕ

ϕ

c(ψ)q(ψ) dψ − c(ϕ)
0

s(ψ)q(ψ) dψ ,

(4.48)

0

where s(ϕ) and c(ϕ) are two independent solutions of the homogenous
equation f,ϕϕ +(1−3ζ cos ϕ) f = 0. These are the so-called Mathieu functions:
s(ϕ) = sin ϕ + O(ζ) ;

c(ϕ) = cos ϕ + O(ζ) .

(4.49)

The O-terms are actually series in sin nϕ or cos nϕ of order ζ or smaller, whose
explicit expression we fortunately don’t need. The whole solution is now

 ϕ
3ζ 2
u = ζ cos ϕ +
c(ψ) cos2 ψ dψ
s(ϕ)
2
0




ϕ

− c(ϕ)

s(ψ) cos2 ψ dψ

.

(4.50)

0

We demand that u = 0 (r → ∞) for ϕ = ±(π/2 + δ). As we expect δ ∼ ζ
1,
we expand up to ﬁrst order in δ and ζ. Then cos(π/2 + δ) = −δ. Since there is
already a factor ζ 2 in front of {· · ·} we may take ϕ = π/2, s = sin and c = cos
inside {· · ·}. This simpliﬁes matters considerably:
0

− ζδ +

3ζ 2
2



π/2

cos3 ψ dψ = −ζδ + ζ 2 ,
0

or δ = ζ = rs /r0 . The total deﬂection angle δψ is 2δ:

(4.51)

4.4 The classical tests of GR

δψ = 2rs /r0 .

81

(4.52)

This is twice as large as the result of a classical computation8 that treats
the photon as a massive particle with speed c. The deﬂection is therefore
truly determined by the shape of null geodesics in a curved spacetime. For
the eclipse geometry we have δψ = 2rs /R = 2 × 2.95 km/6.96 × 105 km
= 8.48 × 10−6 ∼
= 1.75 .
The HIPPARCOS satellite measured stellar positions with an accuray of
∼ 0.002 . At this level of precision the deﬂection of light by the Sun can
be detected over half of the sky! For let’s suppose HIPPARCOS is looking
perpendicularly to the Sun-Earth line. The deﬂection angle is then δψ = δ =
rs /r0 where r0 = 1 AU: δψ = 3/(1.5 × 108 ) = 2 × 10−8 ∼
= 0.004 .
Binary pulsars
The derivation of the perihelium precession proceeds in a similar fashion as
the deﬂection of light (see exercise). Much larger relativistic precessions have
been measured in binary stellar systems consisting of a neutron star which is
also a pulsar and another neutron star. These binary systems are laboratory
test equipments on a cosmic scale that may be used to verify GR with greater
accuracy and over a wider parameter range than is possible in solar system
experiments. Six pulsars have now been found to be a member of a binary
neutron star system. The most famous one is PSR 1913+16. The masses of
the components are 1.441 M (pulsar) and 1.387 M (companion);  = 0.617;
orbital period = 27907 s ( 7.8 hour); semi-major axis  = 1.95 × 106 km
(1.4 solar diameters); periastron precession = 4.22662◦ per year.9 The recent
discovery10 of a new binary pulsar PSR J0737 3039 caused great excitement as
the companion turned out to be a pulsar as well.11 The system has an orbital
period of only 2.45 hour and is much closer to us than PSR 1913+16, thus
allowing even more precise tests of GR. The periastron precession is predicted
to be 16.9◦ per year!

Exercise 4.13: Show that (4.48) is the required solution of (4.47).
Hint: After substitution in the equation it is found that the Wronskian W ≡
c s,ϕ − s c,ϕ must be equal to 1. It follows from the homogenous equation that
W,ϕ = 0, hence W = 1 is only a normalisation.
8
9

10
11

made by Von Soldner in 1801, see Will, C.M. Am. J. Phys. 56 (1988) 413.
see Taylor, J.H. and Weisberg, J.M., Ap. J. 345 (1989) 434; Will (1993), Ch. 12
and p. 343.
Burgay, M. et al., Nature 426 (2003) 531.
Lyne, A.G. et al., Science 303 (2004) 1153.

82

4 The Schwarzschild Metric

Exercise 4.14: Show that the perihelium precession is given by
ωp =

3(GM )3/2
c2 (1 − 2 ) 5/2

rad s−1 ,

(4.53)

and that ωp 43 per century for Mercury;  = the semi-major axis and  =
the excentricity.
Hint: The classical orbit u0 (ϕ) follows from (4.44): u,ϕϕ + u = a → u =
1 serves now as the small
a(1 +  cos ϕ) ≡ u0 (ϕ). The parameter a ∼ rs /
parameter. The excentricity  need not be small. Insert u = u0 + δu in (4.44)
and linearise → (4.47), except that q(ϕ) and the O-terms in (4.49) are diﬀerent
functions of ϕ. The analysis proceeds quite analogously up to (4.50). We now
need du/dϕ :




du
3a2 ds ϕ
dc ϕ
= −a sin ϕ +
c(ψ)f (ψ) dψ −
s(ψ)f (ψ) dψ ,
dϕ
2
dϕ 0
dϕ 0
with f (ϕ) = (1 +  cos ϕ)2 ; du/dϕ = 0 for ϕ = 0, and we require it to be zero
for ϕ = 2π + δ as well. Anticipate δ ∼ a
1, and include terms up to ﬁrst
order in δ. Take ϕ = 2π, c = cos, s = sin inside {· · ·}:

3a2 2π
0
− aδ +
(1 +  cos ψ)2 cos ψ dψ
2 0

= − aδ + 3a2 

2π

cos2 ψ dψ = − aδ + 3a2 π .
0

Only the term 2 cos2 ψ contributes to the integral. Now ωp = δ/P where
2
2
2
2
P = period and
√ δ = 3πa = 3πrs /2h . And ch = r dϕ/dτ r dϕ/dt = 2O/P
2
2
with O = π 1 −  = area of ellipse (dτ → dt results only in higher order
corrections). Kepler III: 3 /P 2 = GM /4π 2 = c2 rs /8π 2 → h2 = (1−2 ) rs /2.
Mercury:  = 0.387 AU;  = 0.206; P = 88 days; rs = 2.95 km; ωp =
6.60 × 10−14 rad s−1 ∼
= 42.9 per century.

4.5 Gravitational lenses
The relativistic deﬂection of light causes a variety of wonderful eﬀects. The
gravitational ﬁeld of a neutron star is so strong that it distorts and enlarges
its own image to a considerable extent. Fig. 4.6 shows the image of a neutron star as it would look without relativistic eﬀects, and the real image. Star

4.5 Gravitational lenses

83

Fig. 4.6. The relativistic looks of a neutron star with radius R = 2rs . To the left,
the image without relativistic eﬀects. The real image with the light deﬂection by
the strong gravity ﬁeld included is shown on the right. The image magniﬁcation is
computed in exercise 4.16. From Nollert, H.P., et al., A. & A. 208 (1989) 153.

spots are much longer visible.
Einstein noted in 1936 that when two stars are positioned exactly behind
each other, on the same line of sight, the light of the more distant star assumes
the form of a ring around the nearby star, see Fig. 4.7. The chance of such
a coincidence is very small. Chaﬀee (Sci. Am., Nov. 1980) gives a fascinating
account of the discovery of the ﬁrst gravitational lens in 1979. It concerns
two quasars Q0957+561 A and B with an angular separation of 6 that have
the same spectrum (z = 1.41). They turn out to be images of one and the
same quasar whose light is deﬂected by an intervening galaxy at z = 0.36.
It has been shown that gravitational lensing may be treated as a problem in
geometrical optics in ﬂat space with a refractive index 1 − 2Φ(r)/c2 , where Φ
is to be gauged to zero at inﬁnity. Since Φ is negative, the refractive index is
larger than 1, which suggests that gravitational lenses may be modelled by a
properly designed glass lens. According to the theory an odd number of images is formed, distorted and enlarged to diﬀerent degree, but not all images
may be visible. Later, arcs were discovered (images of a galaxy formed by a
cluster located between the source and the Earth), and radio rings (Einstein
ring image of compact radio source formed by intervening galaxy). At present
of the order of 100 gravitational lenses are known.
Gravitational lenses are interesting for a number of reasons. In principle
it is possible to determine the mass of the lens, including all dark matter.
Another application is distance determination. The geometry of the objectlens-images system, see Fig. 4.7, can be derived from the angular distance
between the images, the mass distribution of the lens, and the ratio of the
distances of images and lens (= redshift ratio). The whole system may thus
be drawn to scale. Since most quasars are variable, we may expect to observe

84

4 The Schwarzschild Metric
Einstein ring
dy = 2 rs / r0

r0
dy
source

observer

lens

uneven number
of images
S

O

L
A

time delays
S

L

B

O

microlensing
S

L

O

resolving power
of the telescope

Fig. 4.7. Various gravitational lensing eﬀects, see text.

the eﬀect of path length diﬀerences, and this has now been seen in the double quasar Q0957+561 A and B. Intensity variations of Q0957+561 A lead
those in Q0957+561 B by 417 ± 3 days.12 This provides the missing absolute
distance measure, so that now all distances are known. In principle it should
be possible to measure the distance of the quasar in this way, independently
of traditional astronomical methods. Such an independent measurement is of
great importance for the determination of the Hubble constant H0 , which in
turn sets the age and size of the universe.13

12
13

Kundić, T. et al., Ap. J. 482 (1997) 75.
The literature on gravitational lenses is enormous. Some useful references
are: Schneider, P., Ehlers, J. and Falco, E.E.: 1992, Gravitational lenses;
Blandford, R.D. and Narayan, R.: 1992, A.R.A.A. 30, 311; Paczyński, B.:
1996, A.R.A.A. 34, 419; Wambsganss, J., Living Rev. Relativity 1 (1998) 12
(http://www.livingreviews.org/lrr-1998-12); Mellier, Y.: 1999, A.R.A.A. 37, 127;
Narayan, R. and Bartelmann, M.: 1999, in Formation of Structure in the Universe,
A. Dekel and J.P. Ostriker (eds.), Cambridge U.P., p. 360.

4.5 Gravitational lenses

85

Fig. 4.8. A gravitational lens of the Einstein cross type. The yellow-red galaxy in
the centre of this Space Telescope image acts as a gravitational lens at z = 0.81,
forming four visible images (blue dots) of a quasar at z = 3.4 that lies behind it and
is invisible. The horizontal image size is 6.5 (see Ratnatunga, K.U. et al., Ap. J.
453 (1995) L5; Crampton, D. et al., A & A 307 (1996) L53). Credit: NASA, HST,
K. Ratnatunga and M. Im (JHU).

Microlensing
The lenses referred to above are macrolenses: the lensing is caused by the
smooth mass distribution of the lens. Microlensing of compact (i.e. not extended) sources occurs when a point mass (a stellar-size object) crosses the
light path of one of the images to the observer. For a brief time several subimages are formed, but their separation is so small that only a temporary
change in brightness of the image can be observed. The duration of an event
is hours to ∼ 100 days and is, for a given lensing geometry, mainly determined by the lens mass. Microlensing was ﬁrst discovered in Q2237+0305
(the ‘Einstein cross’). The quasar images show uncorrelated brightness variations believed to be due to individual stars in the lensing galaxy crossing
the line of sight. The most popular application is the search of galactic microlenses, which might reveal otherwise invisible dark objects. The idea is to
use a CCD camera to monitor millions of stars in dense stellar ﬁelds in the
Large Magellanic Cloud (LMC) or the galactic bulge, and to search for the
characteristic brightness variations (symmetric time proﬁle, independent of
colour).
OGLE II saw about 100 events per year, in ﬁelds covering 11 square degrees on the galactic bulge (∼ 2 × 107 stars). The event characteristics are
constistent with the lenses being ordinary low mass stars, but the number of
events is larger than expected. This has been interpreted to indicate that our

86

4 The Schwarzschild Metric

L
g

r

d
ϕ

R

Fig. 4.9. Geometry of the magniﬁcation of a neutron star image in Fig. 4.6.

galaxy has a barred structure at its centre, protruding towards the Sun. The
MACHO project has seen about 15 microlensing events in 5.7 year in ﬁelds
covering the LMC containing 1.2 × 107 stars.14 About 1/4 of these events can
be explained as lensing by ordinary halo stars, and the remainder is reputed
to lensing by MACHOs (= MAssive Compact Halo Object, such as brown
dwarfs, neutron stars, old white dwarfs, black holes, etc.). About 20% of the
expected galactic dark matter halo would be made of 0.3 − 0.7 M MACHOs.
But it cannot be excluded that the lenses are low mass stars in an LMC halo.

Exercise 4.15: Show that for photon orbits of the γ- and δ-type
e = h/d ,

(4.54)

d is the impact parameter, the shortest distance between photon and the origin
if the orbit were a straight line. Both e2 = (h/d)2 and V = (1 − rs /r)(h/r)2
are now ∝ h2 , thus permitting a comparison of their relative values in Fig. 4.2.
Hint: Fig. 4.9 applies to ingoing as well as outgoing photons. A line through
the origin parallel to the orbit at large r (where it is straight) determines d;
d may have any value – at this point we are not interested in the orbit close
d/ϕ or u = rs /r
rs ϕ/d.
to the central object. For r  rs (small ϕ): r
Hence u → 0 and u,ϕ → rs /d. Together with κ = 0 this ﬁxes the value of the
constant 2ae2 in (4.42): 2ae2 = (rs /d)2 ; 2a = (rs /h)2 → e2 = (h/d)2 .
Exercise 4.16: With reference to Fig. 4.6 and 4.9, show that the gravitational
ﬁeld magniﬁes the image of a neutron star by a factor
d/R = (1 − rs /R)−1/2 ,
14

Alcock, C. et al., Ap.J. 542 (2000) 281.

(4.55)

4.5 Gravitational lenses

87

which amounts to 1.41 for R = 2rs , the value corresponding to Fig. 4.6.
Measure the diameters and verify if Nollert et al. did a proper job.
Hint: The image size is determined by the null geodesic g leaving the surface
tangentially (why?) and approaching asymptote L for r → ∞. Infer from the
previous exercise that g obeys the equation
(u,ϕ )2 + u2 − u3 = (rs /d)2 .

(4.56)

Evaluate (4.56) at the point where the ray leaves the surface. Tangential
implies u,ϕ = 0, while u = rs /R → (rs /R)2 − (rs /R)3 = (rs /d)2 , etc.

5
Compact Stars

The Schwarzschild metric is only valid in vacuum, outside the star, but not in
the stellar interior. Inside the star the metric is diﬀerent, and in this chapter
we shall investigate how relativistic eﬀects inﬂuence the structure of a star.
For main-sequence stars and even for white dwarfs the relativistic eﬀects are
small. In neutron stars, however, they play a dominant role. White dwarfs and
neutron stars are two possible end products of stellar evolution. The third
possibility is a black hole, an object that is smaller than its Schwarzschild
radius.

5.1 End products of stellar evolution
The ﬁrst equilibrium state in stellar evolution is the hydrogen burning phase.
For this to happen, the star must have a minimum mass of 0.08 M .1 The
fusion of hydrogen produces helium and during this period the star is on the
main sequence in the Hertzsprung-Russel diagram. When the stellar core runs
out of hydrogen, it will contract and become hotter as it does so. This may
be understood with the virial theorem:
Et = − 12 Ep ;

Etot ≡ Ep + Et =

1
2 Ep

,

(5.1)

where Ep , Et and Etot are the potential, the thermal and the total energy of
the star, respectively (Ep < 0; for deﬁnitions and proof see exercise). Now,
Etot will decrease, because the energy production by nuclear fusion diminishes
while the radiative energy loss continues unabated. Hence Ep ↓ and Et ↑. The
stellar core contracts (Ep ↓), and the density and temperature will rise (adiabatic compression; Et ↑). The star spends half of the liberated potential
energy on radiative losses and the other half on compression (increase of Et ).
1

Stars lighter than 0.08 M are called brown dwarfs – they undergo hardly any
evolution.

90

5 Compact Stars
H

envelope

He

H

He

He

C

C

O
Si
Fe

}

C
O
synthesis of
heavy elements
up to Fe

Fig. 5.1. A heavy evolved star consists of several shells, with fusion reactions in
progress in the boundary layers (not to scale).

After the main sequence phase the core contracts and hydrogen proceeds
in a shell. The outer layers expand, and the star becomes a red giant. The electron gas in the core becomes degenerate, and if M <
∼ 0.5 M the degeneracy
pressure is able to withstand further contraction. In heavier stars the compression of the core continues until Tc ∼ 108 K is reached, at which point fusion
of helium sets in, through the triple-alpha reaction (3 4 He → 12 C + γ ; 16 O is
also formed). Contraction follows once more when the helium in the core gets
depleted. Stars heavier than about 6 M attain a temperature Tc ∼ 8 × 108
K, which is suﬃcient to switch on carbon fusion. The next stage would be
16
O fusion, etc. Because all these reactions are strongly dependent on temperature, the star acquires a shell structure. The more massive the star, the
more layers it will develop in due course of time, see Fig. 5.1.
Apart from rotation and magnetic ﬁelds, mass loss is a major complication when calculating stellar evolution. Mass loss occurs in the giant phase
(by radiative pressure), but also due to instabilities during the shell burning
stages. Stars with an initial mass below ∼ 6 M are thought to lose enough
mass to bring it below the Chandrasekhar limit of 1.4 M . The mass lost is
often visible as a beautiful planetary nebula, see Fig. 5.3. These stars will
end their life as a white dwarf, with a core of He, or C and O, depending
on the initial mass. The remaining energy is radiated away, and the white
dwarf just cools down progressively. The electron degeneracy pressure is almost independent of temperature and remains therefore in equilibrium with
<
the gravitational force. Stars with 6 <
∼ M/M ∼ 8 have a degenerate core at
the beginning of carbon fusion. The fusion switches on explosively because
the pressure is independent of the temperature. This is known as the ‘carbon
ﬂash’. These stars probably evolve into white dwarfs as well.

5.1 End products of stellar evolution

BLACK HOLE

NEUTRON STAR

MASS
LOSS

BROWN DWARF
(M < 0.08 M )

WHITE DWARF

MASSIVE
(M > 8 M )

91

LIGHT
(M < 8 M )

MAIN SEQUENCE
STARS (M > 0.08 M)

I N T E R S T E L L A R

MASS
LOSS

M A T T E R

Fig. 5.2. Stars are born out of the interstellar medium and spend the largest fraction
of their life on the main sequence with hydrogen fusion in the core. After the main
sequence stage nuclear fusion proceeds in shells. The star is then a (super)giant, and
loses mass through a strong stellar wind. Ultimately, light stars (progenitor mass
< 8 M ) shed a planetary nebula and become a white dwarf. Heavy stars explode
as a supernova leaving an expanding remnant and a neutron star or a black hole.
Stars with progenitor mass < 0.8 M evolve so slowly that they are all still on the
main sequence. Stellar evolution thus recycles and enriches the interstellar medium,
and locks up matter in the four types of stellar remnant at the top of the diagram.
After Bless (1995).

Heavy progenitors
The life of stars heavier than about 8M is radically diﬀerent and vaguely
reminiscent of human tragedy – they carry their bulk with dignity until they
can no longer cope and explode. The cores of these stars do not become degenerate and nuclear fusion continues until elements of the iron-group are
formed. Further extraction of energy by fusion reactions is not possible, because nuclei of the iron-group have the largest binding energy per nucleon,
Fig. 5.4. The star is now irrevocably on its way to total destruction. Due to
the large mass of the star the compression of the core (1 − 2 M ) continues until Tc ∼ 5 × 109 K. At that point much energy is lost through photodesintegration of 56 Fe, an endothermic reaction (56 Fe + γ → 13 4 He + 4n − 124
MeV), and by the emission of neutrinos. The latter because the high Fermi
energy of the electrons induces repeated inverse β-decay reactions of the type
e− + (Z, A) → (Z − 1, A) + νe ↑. Nuclei of the type (Z − k, A) are generally unstable and decay under emission of neutrons. The situation is now as follows.
The electron density decreases and so does the associated electron pressure
that sustained the core. At the same time free neutrons are formed in pro-

92

5 Compact Stars

Fig. 5.3. End stages of stellar evolution (1). Hubble Space Telescope image of the
planetary nebula NGC 6543, also known as the Cat’s Eye nebula, at a distance of
about 1 kpc. The nebula is ejected spasmodically from the central bright star as it
developes into a white dwarf. Horizontal image size 1.2 . Credit: NASA, ESA, HEIC,
and the Hubble Heritage Team.

gressively larger quantities. This runaway process seals the fate of the star.
Eventually, the core contracts rapidly and collapses in about 0.1 s until nuclear densities are attained, 1014 − 1015 g cm−3 , and the neutrons become
degenerate. The core now consists of a degenerate neutron gas, with a small
amount of protons and electrons. The neutron degeneracy pressure is suﬃcient to halt further compression and a neutron star is formed. This is in fact
a giant atomic nucleus held together by gravity rather than by the strong nuclear force. The gravitational energy (∼ 1053 erg) is released in the form
of neutrinos. These are exuded by the core in about 10 s, and escape almost all into space. The neutrino luminosity reaches therefore a brief but
impressive peak of ∼ 1045 W. The collapsing outer layers bounce oﬀ the hard
neutron core and a strong shock begins to propagate outwards. This shock
prevents further collapse of the outer layers, aided by the capture of a small
fraction of the escaping neutrinos. The collapse is reversed and a colossal
explosion ensues, marking the birth of a supernova that radiates ∼ 1051
erg in the optical. The supernova remnant continues to expand, Fig. 5.5. In
very heavy stars (for progenitor masses above 30 M ) the outer layers are

5.1 End products of stellar evolution

93

0
energy release by
fusion
fission

binding energy per
nucleon (MeV)

-5
-6
-7
-8
-9

56

0

Fe

100
atomic number

200

Fig. 5.4. Binding energy per nucleon as a function of atomic number. Quantum
eﬀects make that the curve is in reality not as smooth as indicated. Broadly speaking,
energy is released in fusion reactions of nuclei lighter than iron. Beyond the irongroup the curve rises again due to the increasing Coulomb interaction of the protons.
These nuclei release energy by ﬁssion.

only partially stopped. A black hole may form when the mass of the collapsed core exceeds the maximum mass of a neutron star (about 2 M ). The
progenitor mass required for black hole formation is not well known. Compact
objects, ﬁnally, are quite numerous: about 5% of all objects of stellar-size mass
in our galaxy is estimated to be a white dwarf, ∼ 0.5% a neutron star and
(1 − 5) × 10−4 are black holes.
Mass transfer in binaries
In a binary system the evolution of the components may be drastically altered by mass exchange. Matter accreting onto a white dwarf may cause
various phenomena: cataclysmic variables (optical / UV emission of an accretion disc), thermonuclear fusion of the accreting matter, either steady (the
supersoft X-ray sources), or in quasi-periodic explosions (a nova), or complete
disruption of the star leaving nothing behind (type Ia supernova). Evolution
of binary systems may produce quite exotic systems. Neutron star binaries,
for example, such as the double pulsar PSR 1913+16, are believed to evolve
from a binary system consisting of two ordinary massive stars. The more
massive of the two evolves faster and explodes as a supernova, leaving behind a neutron star. After some time the lighter star becomes a red giant.
The neutron star enters into the expanding envelope of its companion and
begins to accrete matter. Tidal friction leads to the formation of a narrow
binary system, and blows away the envelope. The system now consists of the
red giant’s helium core and the neutron star. If the mass of the helium star

94

5 Compact Stars

Fig. 5.5. End stages of stellar evolution (2). The supernova remnant Cassiopeia A,
the relic of a massive star, as imaged by the Chandra X-ray Observatory. This supernova exploded around 1670, at a distance of about 3 kpc. It should have been almost
as bright as Venus, but no ‘new star’ was reported. The green shell is synchrotron
radiation from the outer shock wave. The ejected matter disperses relatively quickly
into the interstellar medium. It is not clear whether the bright dot in the centre is the
neutron star, because it does not show any periodicity. Red is Si emission (1.8 − 2.0
keV), green is 4.2 − 6.4 keV continuum, blue is Fe (6.5 − 7.0 keV). Exposure time
106 s. Horizontal image size 8 . See Hwang, U. et al., Ap. J. 615 (2004) L117; Vink,
J., New Astr. Rev. 48 (2004) 61. Credit: U. Hwang, J.M. Lamming et al.

is above 2.5M it eventually explodes as a supernova as well, and a narrow
neutron star binary may result if the system is not disrupted. If the mass of
the helium star is below 2.5M it evolves into a white dwarf, and the end
product may be a white dwarf-neutron star binary.
Neutron star observations
Our galaxy contains an estimated number of ∼ 108 − 109 neutron stars, and
most of these are invisible to us. There are basically three ways to observe
neutron stars. Radio pulsars are rotating neutron stars equipped with a radio
beacon that sweeps periodically over the Earth. There are about 1500 known
pulsars, with periods ranging from 8.5 s down to 1.55 ms. Most are located

5.2 The maximum mass Mc

95

in or close to the galactic disc. Precise timing of pulsars has yielded a wealth of
information on neutron stars.2 Next are the X-ray binaries, about 200 in our
galaxy, in which a primary star loses mass that swirls around in an accretion
disc, eventually falling onto the compact secondary. In doing so, the matter
gets heated to X-ray temperatures. The secondary is usually a neutron star
and in some cases a black hole.3 Low mass X-ray binaries feature a low-mass
solar-type primary with Roche lobe overﬂow. The X-ray emission is continuous, in most cases with occasional bursts. These so-called X-ray bursters are
due to quasi-periodic runaway nuclear fusion of the accreting matter onto a
neutron star companion. High mass X-ray binaries have a massive primary
star with a strong wind, part of which accretes on a neutron star (these are
all X-ray pulsars), in some cases on a black hole. Finally, X-ray emission of a
few solitary nearby neutron stars has been detected. The nature of the X-ray
emission is not understood – they could be young cooling neutron stars, or
neutron stars accreting from the interstellar medium.4

5.2 The maximum mass Mc
The masses of white dwarfs and neutron stars are bounded by an upper limit
which is a direct consequence of relativistic quantum statistics as shown by
Landau in 1932. The star is a sphere of radius R containing A baryons that
generate the mass and gravity, and ∼ A fermions providing the degeneracy
pressure that balances gravity. White dwarfs: ∼ A/2 protons, ∼ A/2 neutrons
and ∼ A/2 electrons; neutron stars: A neutrons. The fermions are assumed
to be free particles in a potential well with volume V ∼ R3 . Every element
d3 r d3 p ∼ 3 of phase space may contain at most one fermion (we blissfully
ignore spin). In a cold Fermi gas all states with |p| ≤ pf are occupied.5 The
total number of fermions is A ∼ (pf R)3 /d3 rd3 p = (pf R/)3 , so that pf ∼
A1/3 /R. In case of relativistic degeneracy the Fermi energy is Ef ∼ pf c ∼
cA1/3 /R. For non-relativistic degeneracy (that is, for R > certain Rc at
given A) we have Ef ∝ pf 2 ∝ R−2 . The potential energy per baryon is
Eg ∼ −GM mb /R = −Gmb 2 A/R. The total energy per baryon is:6
2
3

4

5

6

Lyne, A.G. and Graham-Smith, F.: 1998, Pulsar Astronomy, Cambridge U.P.
See Compact Stellar X-ray Sources, W.H.G. Lewin, and M. van der Klis (eds.),
Cambridge U.P., to appear.
Treves, A. et al., P.A.S.P. 112 (2000) 297; Haberl, F., Adv. Space Res. 33 (2004)
638.
The Fermi energy turns out to be so large that the energy distribution of the
8
fermions is hardly aﬀected by temperatures in the range <
∼ 10 K.
In white dwarfs Ef refers to the electrons, that have negligible Eg , and Eg to the
baryons, which have negligible Ef . The meaning of E is then the total energy of
all particles divided by the number of baryons.

96

5 Compact Stars
Ef

E = Ef + Eg ; a >> b
R-1

R-2

E = Ef + Eg ; a << b

(small A)

(large A)

R

R

a
R
b

Rc

Rc

R0

Eg (:) R-1

Fig. 5.6. Left: Ef and Eg as a function of R for given A. Center: the total energy
E per baryon for small A, and for large A to the right.


E

E f + Eg ≈

a/R − b/R

R < Rc ;

aRc /R2 − b/R

R > Rc ,

(5.2)

where a = cA1/3 and b = Gmb2 A, see Fig. 5.6. It may be inferred from (5.2)
that a relativistic star has a limiting mass of about 2 M , and that the radius
of a white dwarf is about 5000 km and the radius of a neutron star about 3
1.4 M for white
km (see exercise). More detailed calculations obtain Mc
dwarfs. This is referred to as the Chandrasekhar limit. The maximum mass of
a neutron star depends on the equation of state p(ρ), which is not well known
for ρ ∼ 1014 − 1015 g cm−3 , see § 5.5.

Exercise 5.1: Prove the virial theorem (5.1) for non-degenerate stars.
R
Hint: If n and 32 nκT are the particle and energy density, then Et = 0 ( 32 nκT )·
4πr2 dr. Now nκT = p = pressure. Substitute and integrate partially: Et =
R
− 12 0 (dp/dr) 4πr3 dr. Hydrostatic equilibrium: dp/dr = −GM (r)ρ/r2 (for
R
r
slow rotation!) → M (r) = 0 4πr2 ρ dr. Result: Et = 12 0 GM (r)·dM (r)/r ≡
− 12 Ep .
Exercise 5.2: Show that compact stars have a maximum mass Mc and a
typical radius Rc which are entirely determined by fundamental constants:

Mc ∼

Rc ∼

c
Gmb2
c
Gmb2

3/2
mb
1/2


.
mc

1.9M ,

(5.3)

(5.4)

5.3 The Tolman-Oppenheimer-Volkoﬀ equation

97

Rc ∼ 3 km for m = mb (neutron star) and 5000 km for m = me (white
dwarf).
Hint: Take R > Rc and dE/dR = 0 → R0 = (2a/b)Rc ∝ A−2/3 ; R0 decreases
as A ↑. But R0 ≥ Rc → a ≥ b (ignore the factor of 2). For a ≤ b we
get therefore Fig. 5.6, right; equilibrium is not possible when A is too large.
Hence A ≤ Ac = (c/G)3/2 m−3
b and Mc = Ac mb . Take mb = neutron mass.
Rc , the value of Rc is roughly the one at which the degeneracy
Since R0
becomes relativistic: Ef ∼ cAc 1/3 /Rc ∼ mc2 . In retrospect we see that Mc =
(Rc /λc )3 mb with λc = /mc = Compton wavelength. Interpretation?

5.3 The Tolman-Oppenheimer-Volkoﬀ equation
We shall now derive the structure equations for spherically symmetric relativistic stars in hydrostatic equilibrium. We note that the interior metric of
the star may still be written in the form (4.2) – (4.3), since these have been
derived solely from symmetry arguments that apply here as well. Our task is
therefore to ﬁnd the new functions λ(r) and ν(r) within the star with the help
of the ﬁeld equations. We begin by elaborating the stress-energy tensor Tµν =
ρuµ uν + (p/c2 )(uµ uν − gµν ) according to (3.57). Since the mass distribution
is stationary we have 1 = uµ uµ = u0 u0 = g 00 (u0 )2 = (u0 )2 /g00 = e−2ν (u0 )2
according to (4.3) (g 00 = 1/g00 because gαβ is diagonal). Consequently,
uµ = (eν , 0, 0, 0) .

(5.5)

With (4.3) we get

It follows that



T00 = ρ e2ν ;

T11 = (p/c2 ) e2λ ;

T22 = pr2 /c2 ;

T33 = (pr2 /c2 ) sin2 θ .

T 00 = T00 /(g00 )2 = ρ e−2ν ;
T 11 = T11 /(g11 )2 = (p/c2 ) e−2λ
T 22 = p/(c2 r2 ) ;
T 33 = p/(c2 r2 sin2 θ) .

(5.6)

⎫
⎪
⎪
⎪
⎪
⎪
⎬
; ⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎭

(5.7)

Both p and ρ are functions of r. Use has been made of T 00 = g 0µ g 0ν Tµν =
(g 00 )2 T00 = T00 /(g00 )2 , and likewise for T 11 , etc. Because Gµν:ν = 0 we have

98

5 Compact Stars

T µν:ν = 0, and this equation determines the structure of the star (§ 3.6). An
exercise invites the reader to show that this leads to
(ρc2 + p)

dp
dν
+
= 0.
dr
dr

(5.8)

We use the covariant form of the ﬁeld equations (3.42), Gµν = −(8πG/c2 )Tµν .
Only µ = ν = 0 and µ = ν = 1 turn out to give an independent contribution,
and we begin with µ = ν = 0. From (4.20) and (5.6):
d
8πG 2
r (1 − e−2λ ) =
r ρ.
dr
c2

(5.9)

Deﬁne the functions m(r) and M (r):
m(r) ≡

GM (r)
≡
c2

1
2 r (1

− e−2λ ) ,

(5.10)

and we may now write (5.9) as
dM
= 4πr2 ρ ;
dr

0≤r≤R.

(5.11)

R = stellar radius. Solve (5.10) for e−2λ and use (4.3):


g11 = − e

2λ

2m(r)
= − 1−
r

−1
.

(5.12)

This amounts to a generalisation of g11 from (4.29). Continuity of the interior
int
ext
and exterior metric in r = R requires g11
= g11
, the latter given by (4.29),
and leads to
M (R) = M = mass of the star ;
2m(R) = rs = Schwarzschild radius .

(5.13)

Next comes µ = ν = 1. From (4.21) and (5.6):
8πG p 2λ
2 dν
1 2λ
= − 2
(e − 1) −
e .
r2
r dr
c c2

(5.14)

Multiply (5.14) with e−2λ , then substitute e−2λ = 1 − 2m(r)/r, and solve for
dν/dr:
dν
m + (4πG/c4 ) pr3
=
.
(5.15)
dr
r(r − 2m)
Finally we eliminate dν/dr with (5.8), and after some algebra we get:

5.3 The Tolman-Oppenheimer-Volkoﬀ equation

99




p + ρc2 m + (4πG/c4 ) pr3
dp
= −
dr
r (r − 2m)
= −

G ρ + p/c2 M + 4πr3 p/c2
.
r2 (1 − 2m/r)

(5.16)

This is the Tolman-Oppenheimer-Volkoﬀ (TOV) equation. In the non-relativistic limit (m
r; p
ρc2 ) the classical equation dp/dr = −GM (r)ρ/r2 for
hydrostatic equilibrium re-emerges. Equations (5.11) and (5.16) determine the
structure of a relativistic star in hydrostatic equilibrium. This is elaborated in
the next sections. Outside the star the Schwarzschild metric applies. It turns
out that GR-corrections are very small in white dwarfs, as may be anticipated
from the value of Φ/c2 (cf. Procyon B, Table 3.1). For that reason this chapter
is mainly about neutron stars. However, GR-corrections are important for
topics like the stability and the oscillation frequencies of white dwarfs.
An interesting point is the dual role of the pressure in (5.16). On the one
hand the pressure gradient dp/dr delivers the force that prevents the star from
collapsing. On the other hand p occurs in the stress-energy tensor Tµν and
acts therefore as a source of gravity, because pressure is a form of potential
energy. These are the terms p/c2 on the right side of (5.16). They increase
−dp/dr and therefore p. For a given density proﬁle ρ(r) the gradient −dp/dr
is always larger than in the case of classical gravity. The central pressure is
therefore larger as well. The matter in a relativistic star has to withstand
much larger internal forces to maintain hydrostatic equilibrium. The fact that
neutron stars have a maximum mass is a direct consequence thereof.
Physical mass and bare mass
According to (5.11) and (5.13) the total or gravitational mass of the star is


R

4πr2 ρ dr .

M =

(5.17)

0

This looks identical to the classical expression, but appearances are deceptive. The proper way to compute the mass of the star seems to be to mul√
tiply the density with the proper 3-volume element of space, g d3 x =

−1/2 2
1 − 2m(r)/r
r sin θ drdθdϕ (see exercise 4.4), and to sum up. This
sum-of-all-mass-elements is called the bare mass Mb :

 R
4πr2 ρ dr
√ 3
Mb =
ρ gd x =
1 − 2m(r)/r
V
0

100

5 Compact Stars



R

4πr2 ρ dr = M .

>

(5.18)

0

Mb exceeds M because the 3-volume of a sphere with radius R is larger than
(4π/3)R3 . But Mb cannot be measured. Whatever experiment we conduct
outside the star to determine its mass (for example the period of an orbiting satellite), the result will always be M , because the metric there is the
Schwarzschild metric of a central mass M . One may also say that Mb is larger
than M due the binding energy of the star. A similar thing happens in the
case of the mass defect of atomic nuclei. If the star is cut into small pieces,
it takes an energy of (Mb − M ) c2 to bring these to r = ∞, if their density
is not altered. Almost the same amount of energy is released when a neutron
star is formed.7 We may estimate Mb from the factor {· · ·}−1/2 in (5.18):


−1/2
−1/2

2m(r)
2m(R)
rs −1/2
1−
∼ 1−
= 1−
r
R
R
∼ 1.2 ,

(5.19)

for rs = 3 km, R = 10 km. It would follow that (Mb − M ) c2 ∼ 0.2M c2 ∼
3×1053 erg for a star of M = 1 M . This colossal amount of energy is radiated
in the form of neutrinos, and only a fraction ∼ 10−2 ∼
= 1051 erg in photons
in the optical range of the spectrum (the visible supernova). We conclude
that the birth of a neutron star in a gravitational collapse is accompanied by
wholesale annihilation of mass.

Exercise 5.3: Prove (5.8) from T µν:ν = 0.
Hint: Write out T 1ν:ν = 0 using (2.51), (5.7), (4.10) and (4.14). T iν:ν = 0
(i = 0, 2, 3) does not convey any extra information. Note that T 1ν:ν is not
equal to T 11:1 , see exercise 2.12.
Exercise 5.4: Stellar evolution produces heavy elements up to iron, through
fusion reactions. How are the elements heavier than iron formed?

7

Actually it is less because the energy to compress the matter to ρ ∼ 1015 g cm−3
must still be subtracted, see Misner et al. (1971) p. 603 for details.

5.4 A simple neutron star model

101

5.4 A simple neutron star model
The structure equations can be solved if we assume that ρ is constant. The
fun of this simple and well known model is that it has some basic features in
common with more realistic models. We put:

ρ0
0 ≤ r ≤R;
ρ =
(5.20)
0
r > R.
From (5.11) we get immediately that:
M (r) =

4πρ0 3
r ,
3

(5.21)

while (5.10) tells us that
2m(r)
= ar2
r
Since 2m(R) = rs we have

with

a =

8πGρ0
.
3c2

rs /R = aR2 .

(5.22)

(5.23)

Next we insert (5.22) into the TOV equation:
2ρ0 c2

dp
ar
=−
(p + ρ0 c2 )(3p + ρ0 c2 ) ,
dr
1 − ar2

from which p(r) can be solved (see exercise):
√
√
1 − ar2 − 1 − aR2
2
√
.
p(r) = ρ0 c √
3 1 − aR2 − 1 − ar2

(5.24)

(5.25)

The central pressure in the star is
p(0) = ρ0 c2

1−x
;
3x − 1

x =

1 − aR2 .

(5.26)

Apparently p(0) ↑ ∞ for x ↓ 13 , i.e. when rs /R = aR2 ↑ 89 . In other words,
when R ↓ 98 rs a star with ρ = constant collapses to become a black hole,
because no material can support an inﬁnite pressure. But constant-density
stars, of course, do not exist. However, it has been proven that this result
is generally valid, even when ρ is not constant: A spherically symmetric star
with radius R < 98 rs collapses to become a black hole.8
We may reformulate this as follows. We have M = (4πρ0 /3)R3 ≥ (4πρ0 /3)·
(9rs /8)3 = (4πρ0 /3) · ( 98 · 2GM/c2 )3 . On solving for M we get:
8

E.g. Wald (1984) p. 129; the only conditions are ρ ≥ 0 and dρ/dr ≤ 0, but there
is no requirement on the pressure p.

102

5 Compact Stars


M ≤ Mc =

4c2
9G

3/2 

3
4πρ0

1/2
=

3.60 M
ρ0 /1015

.

(5.27)

Let us step back to discuss these results. Relation (5.27), on its own, says
that if an object of density ∼ ρ0 is to have a radius larger than rs ( 98 rs to
be precise), its mass cannot exceed Mc . But ρ0 may have any value, and the
results of this section cover normal stars and neutron stars. We have a look at
1 and x 1 − aR2 /2, (5.26)
normal stars ﬁrst. In the classical limit, aR2
2
2
2 2
says that p(0)
ρ0 c aR /4 ∼ 2Gρ0 R . Equating that to the pressure p =
ρ0 κT /mb of a classical gas we ﬁnd for the temperature κT ∼ 2Gmb ρ0 R2 ∼
GM mb /2R. In other words, κT ∼ potential energy per baryon at the surface.
Inserting the solar mass M and radius R, the result is that T ∼ 107 K – about
the correct central temperature of the Sun.
Objects more compact than ordinary stars have a higher central pressure,
and if the radius becomes of the order of rs , (5.25) says that the pressure must
2
> 13
be of order ρ0 c2 , which for a classical gas implies that κT >
∼ mb c or T ∼ 10 K.
But such high temperatures are unattainable due to very eﬃcient cooling
mechanisms (for example neutrino losses – a volume eﬀect). Neutron stars
‘solve’ that by resorting to densities so high that degeneracy sets in, and the
Pauli principle forces baryons to relativistic speeds regardless of temperature.
The energy density and therefore the pressure may now attain values of ρ0 c2
and much more. Quantum statistics says that the pressure is ∼ ρ0 c2 if there
is one baryon per cubic Compton wavelength λc , i.e. ρ0 ∼ mb (/mb c)−3 ∼
1017 g cm−3 . And this agrees in turn with (5.3) and (5.4) if we take ρ0 ∼
Mc /Rc3 .

Exercise 5.5: Prove (5.25).


Hint: Change to y = p/ρ0 c2 in (5.24) → dy (y +1)(3y +1) = − 12 ar dr/(1−


−1
ar2 ). Use (y + 1)(3y + 1)
= 12 (y + 13 )−1 − 12 (y + 1)−1 . Integrate from r to
R.
Exercise 5.6: Prove for the ρ = constant model that for r ≤ R :
g00 =

2
√
1 √
3 1 − aR2 − 1 − ar2
;
4
2 −1

g11 = − (1 − ar )

.

The metric of a constant-density star is now given by:

(5.28)

5.5 Realistic neutron star models

ds2 =

1
3
4

1 − aR2 −
−

1 − ar2

2

103

c2 dt2

dr2
− r2 (dθ2 + sin2 θ dϕ2 ) ,
1 − ar2

(5.29)

for 0 ≤ r ≤ R. Verify that this metric has no singularities.
2 −1
Hint: g00 = e2ν(r)
from r to R:
 ; from (5.8):
 dν = −(p + ρ0 c ) dp; integrateν(R)
ν(r)
ν(R)
2
1 + p(r)/ρ0 c . Now use (5.25). Observe that e
=e
equals eν
e
from (4.29) → eν(R) = (1 − aR2 )1/2 .

Exercise 5.7: Show that for a star with ρ = constant
8
rs
=
R
9



M
Mc

2/3
.

(5.30)

Hint: rs /R from (5.23), and R from (5.21); eliminate ρ0 with (5.27).
Exercise 5.8: Consider a neutron star with constant ρ = 1015 g cm−3 and
M = 1.8 M . Compute (a) the Schwarzschild radius; (b) the bare mass Mb ,
and (c) the rate of a clock at the centre of the star with respect to the clock
rate at r = ∞.
Hint: (a): (5.27) → M = 12 Mc ; (5.30): rs /R = 0.560 and rs = 1.8 × 2.95 km
= 5.3 km, so that R = 9.5 km. Not √
bad for such a crude model.
√ R a 2
(b): (5.18) → Mb = (4πρ0 /a a ) 0
x (1 − x2 )−1/2 dx (look up in a table).


√
3
Result: Mb = 2 M arcsin y − y 1 − y 2 /y 3 with y = R a = rs /R →
Mb = 1.25M .
√
(c): From (3.2) and (5.28): dτ (0)/dt = 12 3 1 − aR2 − 1 = dτ (0)/dτ (∞)
because dτ (∞)/dt = 1 (why?) → dτ (0)/dτ (∞) = 0.495. A redshift while
there is no gravity at the centre of the star - isn’t that strange?

5.5 Realistic neutron star models
The structure equations may be integrated when the relativistic equation of
state p(ρ) is known. If the ‘enclosed mass’ Mi , the pressure pi and the density ρi are known at radial position ri , then we obtain Mi+1 and pi+1 at
the next level ri+1 = ri + ∆ri from eqs. (5.11) and (5.16). The equation of

104

5 Compact Stars
r ~ 106 g cm-3
rd ~ 4 x1011 g cm-3
rn ~ 3 x1014 g cm-3

r0 ~ 1015 g cm-3

outer crust: lattice of n-rich heavy
nuclei; degenerate relativistic einner crust:
as above, plus
degenerate non-relativistic n
core: no ion lattice; degenerate
relativistic n; ~ 1% admixture of
p and e- ; phase transitions?

Fig. 5.7. Schematic structure of a neutron star (n = neutrons, p = protons, e− =
electrons). The ﬁgure is not to scale and the crust is in reality much thinner. The
pressure in the crust is determined by the degenerate electrons, in the core by the
degenerate neutrons.

state gives the corresponding density ρi+1 . Starting values are enclosed mass
M0 = 0 and the central density ρ0 . During the integration r > 2m(r) must
hold everywhere. The stellar surface r = R is deﬁned by p(R) = 0. The g11
element of the metric tensor (4.3) and the function λ(r) are ﬁxed by relation
(5.10). The element g00 = e2ν(r) of the metric tensor (4.3) can be obtained
by integrating (5.8) or (5.15). The value of ν is known at the outer boundary
where the internal metric ﬁts smoothly to the exterior Schwarzschild metric,
see exercise 5.6. One may either integrate inwards, or start at r = 0 assuming
an arbitrary value for ν(0) as the structure equations do not depend on ν. In
the end a constant is added to all {νi } to reproduce the known value of ν(R).
The real problem is not the integration of the structure equations, but the
equation of state (EOS). The properties of ultra-dense, cold matter in thermodynamic equilibrium may, broadly speaking, be divided in three regimes
that also correspond to three diﬀerent regions of the neutron star, see Fig. 5.7.
As we already saw in § 5.1, the high Fermi energy of the electrons induces
inverse β-decay (e− + p → n + νe ).9 As a result the nuclei have more neutrons
4 × 1011 g cm−3 the nuclei become oversaturated
than normal. Above ρd
with neutrons, and free neutrons appear. This phenomenon is called neutron
drip. For densities larger than ρn the ion lattice disintegrates, and, as a result, the structure of the core is deceptively simple: a degenerate relativistic
neutron gas with a small admixture of protons and electrons. But there are
major uncertainties as to the occurrence of phase transitions in the inner core.
These include a hypothetical crystallisation of the neutrons, the formation of

9

A neutron star is not a closed system to neutrinos, so complete equilibrium may
not be achieved.

5.5 Realistic neutron star models

105

Table 5.1. Maximum mass Mc and other parameters for three EOS. a
EOS
soft (F) c
medium (FPS)
stiﬀ (L)

Mc
(M )
1.5 (1.7) d
1.8 (2.1)
2.7 (3.3)

Rc
(km)

ρ0 b
(10 g cm−3 )

surface
redshift

7.9
9.3
13.7

5.1
3.4
1.5

0.49
0.53
0.55

15

a
Data from Cook, G.B., et al., Ap. J. 424 (1994) 823. All numbers are for
nonrotating models.
b
central density.
c
The letter code refers to the EOS in Table 2 of Cook et al. (1994).
d
In parenthesis the maximum mass for maximally rotating models.

a pion condensate or a transition to a quark-gluon plasma. The bottom line
is that the EOS is reasonably well known in the crust (ρ < ρn ), but not at all
in the core (ρ > ρn ), the region that largely ﬁxes mass and radius of the star.
Rhoades and Ruﬃni10 computed an upper limit to Mc by assuming that
below a certain reference density ρref of the order of ρn the so-called HarrisonWheeler EOS applies, while for ρ > ρref the EOS is required to obey causality
(p > 0 and dp/dρ ≤ c2 )11 but otherwise unspeciﬁed. They found that
Mc /M ≤ 4.0 · (ρn /ρref )1/2 .

(5.31)

This upper limit is obtained for a maximally stiﬀ EOS (dp/dρ = c2 for
ρ > ρref ). At the time, the importance of this result was the existence of a
mass limit of about 4 M , independent of the details of the EOS. Since then,
more realistic calculations have considerably brought down the value of Mc ,
see Table 5.1. An EOS is said to be soft / hard if it features a relatively low
/ high pressure at typical core densities. As the EOS shifts from soft to stiﬀ,
we see in Table 5.1 that Mc and the radius Rc both increase. Rotation adds
15 − 20% to Mc at most. Not in the table is the fact that the radius of the
star increases as M decreases below Mc .
Constraining the EOS
Observations are on the verge of putting constraints on the EOS. Neutron
star masses may be determined from orbital dynamics if the neutron star
is a member of a binary system. In some X-ray binaries the so-called mass
10
11

Rhoades, C.E. and Ruﬃni, R., Phys. Rev. Lett. 32 (1974) 324.
dp/dρ equals the square of the signal speed of the medium.

106

5 Compact Stars

function of the optical companion and of the X-ray emitting object could be
determined. Six neutron star masses could thus be determined (Shapiro and
Teukolski (1983), § 9.4). Precise timing of pulsars in binary systems has led to
the determination of some 20 neutron star masses.12 The masses range from
1 to 1.5 M , with a strong concentration near 1.35 M .
Neutron star radii may be estimated from the thermal emission of X-ray
burst sources, for example. The idea is that the spectrum yields the temperature, and the radius follows from the observed ﬂux density if we know or
can somehow estimate the distance of the neutron star.13 But due to many
uncertainties precise radius measurements do not yet exist. The distances to
some nearby neutron stars are known from a measurement of their parallax. Good determination of the radii of these neutron stars will be possible
once the interpretation of their spectra is unambiguous. While an independent
measurement of a neutron star’s mass and radius seems asking for too much,
a measurement of M/R is not. The XMM-Newton observatory has recently
found indications for a redshift of z = 0.35 in the X-ray burst spectra of a
neutron star.14 Such a measurement would determine M/R, which in turn
constrains the EOS, see exercise.
Lastly, we mention quasi-periodic oscillations as a possibility to constrain
the EOS. Some low-mass X-ray binaries show quasi-periodic brightness oscillations (QPOs in the jargon of the X-ray community). There are often two
well-deﬁned frequencies in the range of 300 to 1200 Hz. These frequencies
are so high that the oscillations are likely to be a byproduct of the accretion
process very close to the neutron star. QPOs are therefore telling us something about the inner regions of the accretion disc, but there is no concensus
about meaning of the message. One possibility is accretion onto a neutron
star with a spin frequency of a few 100 Hz, and a moderately strong magnetic
ﬁeld (107 − 1010 G), so that the accretion disc penetrates into the magnetosphere. The faster periodicity may result from clumpiness of matter due to
instabilities near the so-called sonic point of the disc, where the radial accretion ﬂow becomes supersonic. The QPO frequency would be the Keplerian
frequency in the neighbourhood of the sonic point, and the lower frequency a
beat phenomenon of the Keplerian frequency close to the sonic point and the
spin frequency of the star. There many uncertainties and ramiﬁcations, but
if these problems can be overcome QPOs may help to put contraints on the
EOS.15

12

13
14
15

Thorsett, S.E. et al., Ap. J. 405 (1993) L29; Thorsett, S.E. and Chakrabarty, D.,
Ap. J. 512 (1999) 288.
Van Paradijs, J. and Lewin, W.H.G., Class. Quantum Grav. 10 (1993) S117.
Cottam, J. et al., Nature 420 (2002) 51.
See e.g. Miller, M.C. et al., Ap. J. 508 (1998) 791.

5.5 Realistic neutron star models

107

Exercise 5.9: Show that the gravitational redshift z of a non-rotating spherical object is
(5.32)
1 + z = g00 (emission site)−1/2 .
Hint: 1 + z = λ0 /λ = ν/ν0 (λ, λ0 = emitted, observed wavelength); follow
reasoning of (3.20) and g00 (∞) = 1.
Exercise 5.10: Show that the gravitational redshift of a non-rotating object
cannot exceed z = 2, and that the measurement of z = 0.35 implies that
M/R = 0.15 M /km. Suppose we know on astrophysical grounds that the
star has a mass of 1.5 M . Which EOS in Table 5.1 are tenable?
Hint: g00 (emission site) has a minimum because R > 98 rs (§ 5.4); M must be
below Mc (because if M ↓ then R ↑ and z ↓); R = 10 km → incompatible
with EOS L and F, possibly compatible with FPS.

6
Black Holes

In the previous chapter we saw that a star may collapse completely. No force
can prevent this, not even one we haven’t discovered yet: any additional pressure would only generate more gravity than supporting force, and will accelerate the collapse. A complete collapse produces a black hole. Such a black
hole would have a stellar-size mass, due to its formation history, but from a
theoretical point of view there are no restrictions, and black holes of any mass
may exist. Black holes were predicted by John Michell in 1784. He noticed
that the escape velocity (2GM/R)1/2 of a spherical mass may be greater than
c if M/R is suﬃciently large. He argued that such objects must be invisible
because light cannot escape. In 1939 Oppenheimer and Snyder analysed the
collapse and discovered that the collapsing matter cuts oﬀ all communication
with the outside world. Gravitational collapse and black holes began to be
seriously studied only after 1960. In 1963 Kerr found an axisymmetric solution of the vacuum equations, which was later realised to be the metric of a
rotating black hole. In 1967 Wheeler coined the term ‘black hole’, and in 1975
Hawking discovered that a black hole emits black body radiation. Black holes
may be thought of as lumps of pure gravity. They belong to the more advanced topics in GR, and we shall consider only a few elementary properties.

6.1 Introduction
Operationally, a black hole may be deﬁned as an object that is smaller than
its Schwarzschild radius. To an observer at r = ∞ a black hole appears as a
hole in spacetime, that behaves like a black body with mass M and radius
rs = 2GM/c2 , and strong lensing eﬀects near the edge. A black hole is entirely
speciﬁed by 3 parameters: its mass M , angular momentum L, and charge Q
(theoretically, there is a fourth parameter: the magnetic monopole charge). All
other information about the parent body is lost. A black hole may inﬂuence the
outside world only through these parameters. This property led Wheeler to his
famous aphorism ‘black holes have no hair’. Magnetic ﬁeld lines, incidentally,

110

6 Black Holes
Table 6.1. Massive dark objects in galactic nuclei

system type
Milky
way
M31
M106
M32
M87

S
S
S
E
E

b

Mass and radius
(pc)
(106 M )

a

Reference

3.6 ± 0.3

0.005 Eisenhauer et al., Ap. J. 628 (2005) 246.

170 ± 60
39 ± 3
3.4 ± 1.6
(2.4 ± 0.7) · 103

0.11
0.13
0.3
18

Bender et al., Ap. J. 631 (2005) 280.
Miyoshi et al., Nature 373 (1995) 127.
Van der Marel et al., Nature 385 (1997) 610.
Ford et al., Ap. J. 435 (1994) L27.

a
For reviews see Ferrarese, L. and Ford, H., Space Sci. Rev. 116 (2005) 523;
Kormendy, J., in Coevolution of Black Holes and Galaxies, L.C. Ko (ed.), Cambridge U.P. (2004), p 1.
b
S = spiral, E = elliptical galaxy.

do not count as hair: a charged rotating hole has a magnetic dipole moment
QL/M c and an exterior magnetic ﬁeld. The ﬁeld is weak as the charge Q
is expected to be very small. The Schwarzschild metric is the simplest black
hole solution of the vacuum equations (mass M , nonrotating and uncharged).
There are more general black hole solutions, for example the axisymmetric
Kerr solution for a rotating uncharged hole with parameters M and L that we
shall brieﬂy consider in § 6.5. For the most general black hole characterised
by M, L and Q see Wald (1984). Although GR allows black holes of any
mass to exist, stellar and galactic evolutionary processes lead naturally to the
formation of stellar-mass black holes and supermassive holes (106 − 109 M ).
The mean density of a hole is ρ = M/(4πrs3 /3) ∼ 2 × 1016 (M/M )−2
g cm−3 . For a supermassive hole of ∼ 108 M this is only ∼ 1 (density of
water). Black holes are therefore not necessarily associated with extremely
dense matter. Having said that, the mean density of a hole of 1010 kg (rs ∼
10−15 cm!) is ρ ∼ 7 × 1056 g cm−3 . This is so high that these small holes are
believed to form only during the Big Bang – if at all. But evidence for the
existence of such primordial black holes is lacking.

6.2 Observations
Black holes can be observed only indirectly, when they interact with their
47
environment. It is generally believed that the enormous luminosity L >
∼ 10
erg s−1 of quasars and active galactic nuclei (AGNs) is caused by accretion of
1 − 100 M per year onto a massive black hole (106 − 109 M ), for the following reasons. The emission is often variable on time scales tv of days to hours,
in some cases even 103 s. Causality puts an upper limit to the source size of

6.2 Observations

111

Fig. 6.1. Astrometric observations using adaptive optics techniques have shown
that S2, a massive main-sequence star, is in a highly elliptic Keplerian orbit around
the compact radio source Sgr A∗ at the galactic centre (large cross), with a period
of about 15 yr. The combination of radial velocities (from the spectrum of S2) and
proper motion data allows a precise determination of the orbital parameters and
the distance of Sgr A∗ . The data strongly indicate that the gravitational potential is
that of a point mass from 0.8 light days to 2 ly. The only compelling interpretation is
a supermassive black hole. Horizontal image size: 15 . See Schödel, R. et al., Nature
419 (2002) 964; Eisenhauer, F. et al., Ap. J. 597 (2003) L121. The most recent mass
determination of the hole is 3.6 ± 0.3 M , and the distance to the galactic centre
is 7.6 ± 0.3 kpc (Eisenhauer, F. et al., Ap. J. 628 (2005) 246). Credit: R. Genzel
(private communication).

ctv , which must therefore be small. An object radiating L = 1047 erg s−1 will,
9
by the Eddington limit argument,1 have a mass of >
∼ 10 M . Another way to
estimate the mass of the central object is to say that if it converts mass into
energy with an eﬃciency η to sustain its luminosity L for a time ∆t, then
it must have acquired a mass M = L∆t/ηc2 . Accretion onto a black hole is
the most eﬃcient mechanism known for releasing gravitational energy. For a
1

The radiation of the source exerts a radiation pressure on the infalling electrons
through Thomson scattering which by charge neutrality is mediated to all accreting matter. The limiting (Eddington) luminosity LEdd is attained when the
radiation pressure equals the acceleration of gravity. For steady spherical accretion L < LEdd  1.3 × 1038 (M/M ) erg s−1 , see Frank et al. (1992).

112

6 Black Holes

non-rotating hole η = 0.057 (see exercise) and for a maximally rotating hole
η may be as large as 0.42 (while fusion of hydrogen to helium liberates only
0.007 of the rest mass energy). A reasonable estimate for accretion-powered
sources is η ∼ 0.1. Many AGNs (such as double radio lobe sources) must have
existed at least 107 yr. Hence, M ∼ 108 M . One way or another, we have
a very large mass in a small volume, and such mass distributions, whatever
their nature, are believed to develop quickly into a black hole.
Observations of stellar dynamics reveal that many nearby galaxies harbour ‘heavy dark objects’ within a very small radius around their centres
(Table 6.1). Among these, the case for a massive black hole in the galactic
centre at the location of the compact radio source Sgr A*, and in the nuclei
of M31 and M106 (= NGC 4258) is very strong. Sgr A* is arguably the most
convincing black hole candidate we have, see Fig. 6.1. Near-infrared and X-ray
observations of Sgr A* reveal variability on a timescale of 10 minutes, indicating that the object cannot be larger than ∼ 20 Schwarzschild radii (of a
3.6 × 106 M hole). As VLBI techniques improve, they will eventually permit
to resolve the Schwarzschild radius of the hole (∼ 10 µ arcsec). Perhaps we
may one day observe the shadow cast by the event horizon of ‘our’ black hole
in Sgr A*.2
The arguments for the existence of black holes are admittedly indirect, in
the sense that they do not address the immediate vicinity of the hole. But
that situation is beginning to change rapidly. For example, X-ray spectroscopy
of the 6.4 keV iron line of MCG-6-30-15 (a Seyfert 1 galaxy) indicates that
the emission comes from a hot disc around a spinning black hole. The inner
radius of the emission appears to lie at about one Schwarzschild radius.3
Recently, a new class of X-ray sources has been discovered, the ultraluminous X-ray sources, in star forming regions of nearby galaxies. They may
point to the existence of intermediate-mass black holes of 102 − 104 M .4
Moving down the mass scale to stellar-mass black holes, evolution calculations indicate that they should be numerous: a fraction ∼ 10−4 of the stellar
population. But only a few have been found, in bright X-ray binaries with
Lx ∼ 1037 − 1038 erg s−1 . This points towards accretion onto a compact object of mass Mx ∼ 1 M . A lower limit for the mass of the X-ray source can
be inferred if the radial velocity proﬁle of the companion star can be measured. If Mx >
∼ 3M it has to be a black hole because the mass exceeds the
maximum mass of a rotating white dwarf or neutron star (∼ 3M ). There

2

3
4

On Sgr A* see Schödel, R. et al., Nature 419 (2002) 694; Genzel, R. et al., Nature
425 (2003) 934; Melia, F. and Falcke, H., A.R.A.A. 39 (2001) 309.
Fabian, A.C., Mon. Not. R. Astron. Soc. 335 (2002) L1.
E.g. Miller, M.C. and Colbert, E.J.M., Intl. J. Mod. Phys. D 13 (2004) 1.

6.3 Elementary properties

113

are now 18 conﬁrmed stellar mass black hole candidates.5 Their companion is
often a solar-type star, in three cases a massive O or B star.

6.3 Elementary properties
From the previous section it seems that the case for the existence of black
holes, while formally still open, is tightening rapidly. We move on to review
some of their properties. To this end we study the orbit of a test mass falling
radially into the hole. The test mass moves along a geodesic in the Schwarzschild metric, with h = 0 because ϕ̇ = 0. From (4.37) we see that
1
c2



dr
dτ

2
= e2 − 1 +

or, for e = 1:
dr
= −c
dτ



rs
r

rs
,
r

(6.1)

.

(6.2)

1/2

Apparently, the choice e = 1 means
that the mass has zero velocity at r = ∞.
√
Equation (6.2) is of the type r dr = const · dτ and is readily integrated:
2rs
τ = −
3c



r
rs

3/2
+ const.

(6.3)

The singularity
It follows that the test mass traverses the distance between any ﬁnite value
r0 and r = 0 (where the collapsing matter has accumulated earlier) in a ﬁnite
proper time ∆τ , Fig. 6.2 (left). This remains so if we do a more complete
complete calculation with e = 1 to allow for nonzero velocity in r = ∞.
We assumed that the vacuum metric is everywhere correct, but r = 0 is a
singularity where the density becomes formally inﬁnite. The test mass will be
crushed by inﬁnitely large forces as it arrives there. But before that happens
quantummechanical eﬀects take over, as classical GR loses its validity for
length scales near the Planck length Lp 1.6 × 10−33 cm.6 But nothing out
of the ordinary happens when an observer crosses r = rs . This is merely a coordinate singularity, a consequence of the way the co-ordinates are deﬁned
5

6

McClintock, J.E. and Remillard, R.A., in Compact Stellar X-ray Sources, W.H.G.
Lewin and M. van der Klis (eds.), Cambridge U.P. (to appear), also astroph/0306213; see further Frank et al. (1992) § 6.7.
The Planck mass and length are deﬁned as the mass and Schwarzschild radius
of a black hole whose Compton wavelength equals the Schwarzschild radius, see
§ 13.2.

114

6 Black Holes
future
light-cones

photons
ct

t

t

N
W

0

rs

r

0

rs

r

⬁

0

rs

S

E

r

Fig. 6.2. The worldline of a test mass falling into a black hole. It reaches the origin
r = 0 in a ﬁnite amount of proper time τ . To an observer at r = ∞ (whose clock
runs synchonously with co-ordinate time t), the object turns dark and freezes to
immobility just ouside the horizon r = rs . To the right: the future light-cone as a
function of r, see text.

in the Schwarzschild metric (4.27). By choosing diﬀerent co-ordinates the
singularity may be avoided (§ 6.4). Note that an extended observer will be
torn to pieces by the tidal forces long before r = rs is reached (see exercises).
The event horizon
A completely diﬀerent picture emerges if we analyse the situation as seen by
an observer at r = ∞. He uses his own proper time to describe the fall, but
that is identical to the co-ordinate time t.7 With e = 1 and (4.32) we can
transform proper time into co-ordinate time: dr/dτ = (dr/dt) · (dt/dτ ) =
(1 − rs /r)−1 dr/dt. Insert this in (6.2):

or, with y ≡ r/rs :


 1/2
rs
dr
rs
= −c 1 −
,
dt
r
r

(6.4)

√
y y
c
dy ,
dt = −
rs
y −1

(6.5)

which can be integrated to


√
y −1
ct
√
√
= − 23 y y + 2 y + log √
+ const.
rs
y +1

(6.6)

When r  rs only the ﬁrst term contributes. For r rs we put r = rs + δ or
y = 1 + δ/rs . In this way we obtain the following approximation:

7

This follows from (3.2): dτ (∞)/dt =

g00 (∞) = 1.

6.3 Elementary properties

ct
rs

⎧
 3/2
⎪
2 r
⎪
⎨−
+ const.
3 rs
⎪
⎪
⎩
− log δ + const.

(r  rs ) ;

115

(6.7)

(r = rs + δ) .

For large r we thus recover (6.3), but not for r rs , as t ↑ ∞ for δ ↓ 0. By
inverting (6.7) we see that δ = const · exp(−ct/rs ), or
⎫
r = rs + const · exp(−t/tc ) ;
⎪
⎪
⎬
(6.8)
2GM
rs
⎪
−5 M
⎪
⎭
tc =
=
10
sec
.
c
c3
M
According to an observer in r = ∞ the test mass slows down and hovers
just outside r = rs , never actually reaching r = rs , Fig. 6.2 (middle panel).
The time scale tc for this to happen is very small, about 10 µs for M = 1M .
Light emitted by the test mass will shift progressively to the red (see exercise).
Measured in proper time, the number of photons emitted before crossing r =
rs is ﬁnite, and that number of photons also arrives at r = ∞, but spread
out over time to t = + ∞. The object will therefore turn dark and vanish
from sight. As we shall see, no signal from the interior region r < rs will
ever reach the exterior r > rs . For that reason r = rs is called the horizon.
There exist diﬀerent kinds of horizon in GR, so we need to be more precise.
The Schwarzschild metric has an event horizon: signals emitted by events
inside the event horizon r = rs will never be visible to external observers –
however long they wait. The other main type of horizon is the particle horizon
in cosmology. The particle horizon is the distance of particles beyond which
an observer cannot see at this moment in time (but at a later moment he
can), see § 11.2. In the professional jargon the term horizon is often employed
without any type indication.
The future light-cone
Fig. 6.2 (middle panel) shows the worldline of the test mass in Schwarzschild
co-ordinates r and t also inside the horizon. From (4.32): dt/dτ = e/ 1 −
rs /r we see that dt < 0 for dτ > 0 in r < rs : t appears to run backwards.
No deep signiﬁcance should be attached to this – it merely means that the
Schwarzschild co-ordinates (event labels) t and r are awkward to use when
r ≤ rs . The right panel of Fig. 6.2 displays the future light-cone as a function
of r. We may ﬁnd that by putting ds2 = dθ = dϕ = 0 in (4.27):
$
rs $$−1
cdt
$
= ± $1 − $ .
(6.9)
dr
r
Relation (6.9) divides the r, t plane in 4 sections N, S, E, W . It is intuitively
clear that N is the future light-cone in the exterior region r > rs . The lightcone becomes progressively narrower close to r = rs . The co-ordinate velocity

116

6 Black Holes
M

dM

M + dM

M

dM'

Fig. 6.3. Growth of a nonrotating black hole as seen by an external observer. Top
left: a spherically symmetric shell collapses onto the hole. Bottom left: asymmetric
collapse with emission of gravitational radiation. Right: the ﬁnal product may be
the same in both cases: a black hole of mass M + δM (δM  < δM on account of the
energy lost by radiation).

dr/dt of a photon and of a particle becomes zero near the horizon, but the
locally measured velocity does not, see exercise. But inside the hole the future
light-cone is W (exercise). Even if a particle in r < rs emits a photon radially
outwards (as seen in its own rest-frame), then drphoton < 0, as the light-cone,
by deﬁnition, contains all future worldlines. The exercise uses that dr < 0
somewhere, but we may take dr > 0 in r < rs as initial condition. Such
particles/photons must come from r = 0 and they will ﬂy through the horizon
into the outside world (in their own perception – it takes until after t =
∞ before an observer in r = ∞ sees them). These are the time-reversed
orbits, which do exist as a mathematical possibility, for example according to
eq. (6.1). They are referred to as ‘non-causal’ because they depend on things
happening in r = 0, about which we cannot say anything.
Growth of a black hole
How can a black hole ever grow if an observer at r = ∞ sees falling test
masses ‘freeze’ on the horizon? The crux is that this is true for test masses,
which do not aﬀect the metric, but not for ﬁnite masses. Consider a spherically symmetric shell of matter falling into a black hole of mass M , Fig. 6.3.
Because of the spherical symmetry an external observer ﬁnds himself in a
Schwarzschild metric with mass M + δM – provided the shell is suﬃciently

6.3 Elementary properties

117

far from the hole so that the gravitational interaction between hole and shell
is small. Birkhoﬀ’s theorem, § 4.2, says that the metric is stationary. This
means that regardless of how the shell collapses, a black hole of mass M + δM
must form. The details of the collapse are quite complicated,8 but Birkhoﬀ’s
theorem allows us to infer the ﬁnal result. It has further been proven that the
surface A = 4πrs2 of the horizon cannot decrease, dA ≥ 0, regardless of the
(a)symmetry of the collapse, and that a near spherically symmetric collapse
produces a black hole with the Kerr metric, see Schutz (1985) § 11.3 for more
information.

Exercise 6.1: Consider the following derivation: dr/dτ = (dr/dt)(dt/dτ ) =
−1/2
(dr/dt) g00 , so that
dr
=
dt


1−

rs
r

1/2


1/2  1/2
rs
rs
dr
= −c 1 −
,
dτ
r
r

which is diﬀerent from (6.4). Which of the two is wrong and why?
Exercise 6.2: Throw a stone radially into a 1M black hole from r = ∞ at
30 km s−1 . How much proper time does the stone need to travel the interval
[10 rs , 0]? Does the initial speed matter? How much proper time does it take
a photon to traverse that distance?
Hint: From (6.1) and the initial condition: e2 − 1 = 10−8 . Hence put e = 1!
∆τ 210 µs. The photon is a catch.

Exercise 6.3: What speed does an observer at rest in r = 1.1 rs measure for
the stone and the photon as they rush by into the hole?
Hint: The locally measured speed v ≡ d(locally measured distance)/d(locally
√
√
√
measured time) = −grr dr/d(τ observer) = ( −grr dr)/( g00 dt) = (1 −
rs /r)−1 (dr/dt), and dr/dt is known. Stone: (6.4) → v = −c rs /r −0.95 c.
Photon: ds2 = 0 in (4.27) → dr/dt = −c(1 − rs /r) → v = −c.
Exercise 6.4: Consider stable circular orbits in the Schwarzschild metric.
Prove that the diﬀerence in binding energy of an orbit at r = ∞ and smallest
possible orbit r = r+ = 3rs is given by:

√ 
5.7 × 10−2 m0 c2 .
(6.10)
∆E = 1 − 23 2 m0 c2
8

See e.g. Shapiro and Teukolski (1983) § 17.5.

118

6 Black Holes

This implies that at most 5.7% of the rest mass energy is liberated as the
mass is processed through an accretion disc. For a maximally rotating black
hole this ﬁgure may rise to 42%.
Hint: ∆E = m0 c2 ∆e , see below (4.32); e(∞) − e(3rs ) from (4.45).
Exercise 6.5: Convince yourself about the future light-cones in Fig. 6.2.
Hint: Consider a timelike worldline (not necessarily a geodesic). From (4.27)
and ds2 > 0 → |cdt/dr| > |1 − rs /r|−1 for r > rs (and < for r < rs ).
Together with dt > 0 (future light-cone) this leaves only N in the exterior
region. Note that dr cannot be zero inside the hole; a particle must move
there, whatever forces are applied → dr either positive or negative. For the
subclass of geodesics this follows also from (6.1). On passing r = rs we know
that dr < 0 → dr < 0 everywhere → W remains (see text for dr > 0).
Exercise 6.6: The diﬀerence in gravitational acceleration over a length  at
a distance r from a black hole with mass M is
δg ∼

M Ra2 
ga .
Ma r 3

(6.11)

Ma , ga = mass of Earth, acceleration at the Earth’s surface. Use a classical
estimate. How large is this tidal acceleration δg for  = 1.8 m, M = 1 M ,
r = 1000 km?
Hint: δg/ga

24.

Exercise 6.7: A laser with proper frequency ν0 falls into a black hole on a
radial geodesic r0 (t0 ) with e = 1, see Fig. 6.4, emitting photons to r = ∞,
also radially. Prove that the frequency ν observed at r = ∞ equals
ν/ν0 = 1 −

rs /r0 ∝ exp(−t/2tc ) .

(6.12)

Hint: Tricky, because we must allow for the extra redshift due to the photons
escaping from an ever deepening gravitational well. If the laser would be at
rest in r0 we have ν0 dτ = νdt0 → ν/ν0 = dτ /dt0 = (1 − rs /r0 )1/2 . But now
ν0 dτ = νdt1 :


ν
dτ
dτ dt0
rs dt0
=
=
= 1−
,
(6.13)
ν0
dt1
dt0 dt1
r0 dt1
see Fig. 6.4; dτ /dt0 from (4.32) with e = 1 and not from (3.2). The relation
between t0 and t1 follows from the outgoing null geodesics: dr/dt = c(1−rs /r)

6.3 Elementary properties

119

t
dt 1
r0(t0)

dt 1
Dt0
dt 1

dt 0
t0

dr 0
ds = c dt

dt 2
dt 2

rs

r0

⬁

r

Fig. 6.4. This ﬁgure covers several situations, all involving masses falling into a black
hole along a radial material geodesic r0 (t0 ): (1) a falling laser, emitting photons to
r = ∞; (2) a falling detector observing photons arriving from r = ∞; (3) a falling
mirror reﬂecting photons back to r = ∞.

((4.27) with ds = dθ = dϕ = 0) → all radial null geodesics are congruent and
can be mapped onto each other by a vertical translation:

dt1 = dt0 + ∆t0 = dt0 − dr0 ·

dr
dt

−1
.

(6.14)

null geodesic in r0 ,t0

dr0 is negative and follows from (6.4). Result:

dt0
= 1+
dt1

rs /r0

−1

,


dr0
dr0 dt0
=
= −c 1 −
dt1
dt0 dt1

and

(6.15)


rs /r0

rs /r0 .

(6.16)

This proves the middle part of (6.12). Now let r0 = rs + δ with δ small, then
1 − rs /r0 δ/2rs . From (6.16): dδ/dt1 (−c/2rs )δ → δ ∝ exp(−t1 /2tc ).

Exercise 6.8: As you fall into a black hole along geodesic r0 (t0 ) with e = 1
you spend your last moments observing photons emitted by a laser at r = ∞
(proper frequency ν0 ), see Fig. 6.4. Show that the observed frequency ν is:

120

6 Black Holes

mass in a-orbit
proper time t

observer at rest
proper time t*
M

Q

P

0

r

R

Fig. 6.5. Computing the gravitational acceleration in the Schwarzschild metric, see
text and Fig. 4.2.


ν/ν0 =

1+

rs /r0

−1

∼

1
2

as r0 ↓ rs .

(6.17)

Hint: Variant of the previous problem, using incoming instead of outgoing null
geodesics, and now νdτ = ν0 dt2 . The photons get blueshifted as they fall, but
the redshift due to the observer’s motion with respect to the laser is apparently
stronger (for e = 1). You may also calculate the redshift ν/ν0 = dt2 /dt1 of
light from r = ∞ reﬂected back to r = ∞ oﬀ a radially falling mirror. Observe
that redshifts do not simply add!

Exercise 6.9: Prove that the gravitational acceleration g at the surface of a
neutron star with mass M and radius R is equal to
g =

−1

GM
.
1 − rs /R R2

(6.18)

Hint: More generally, the question is what acceleration a rocket must deliver
to keep a mass in P at rest in the Schwarzschild metric, see Fig. 6.5. Strategy:
put a test mass in a radial α-type orbit (§ 4.3). The test mass is dropped
in P at zero velocity and an observer at rest in Q measures the acceleration
of the mass as it ﬂies by. There are three times in this problem: τ , τ ∗ and
co-ordinate time t. The speed measured by an observer at rest in Q equals:
√
d
−grr dr
1 dr
,
(6.19)
=
=
dτ ∗ dt
dτ ∗
e dτ
dτ
dt dτ
√
since dτ ∗ /dt = g00 and dt/dτ from (4.33). Diﬀerentiate once more:
d2 
1 d2 r dτ
1 d2 r dτ dt
=
=
,
∗2
2
∗
dτ
e dτ dτ
e dτ 2 dt dτ ∗

(6.20)

and dτ /dt and dt/dτ ∗ are known. Use (6.1) to show (1) that e2 = 1 − rs /R

6.4 Kruskal-Szekeres co-ordinates

121

(P is apex of the orbit → dr = 0 there), and (2) by diﬀerentiation:
(2/c2 ) (dr/dτ ) (d2 r/dτ 2 ) = −(rs /r2 )(dr/dτ ) → (d2 r/dτ 2 ) = −(c2 rs /2r2 ).
Insert everything in (6.20) and let r → R.

6.4 Kruskal-Szekeres co-ordinates
Schwarzschild co-ordinates are useful when r  rs , but become inconvenient
near rs . The co-ordinate singularity at r = rs prevents one from stepping
smoothly over the horizon. In 1960 Kruskal and Szekeres found a system of
co-ordinates that does not suﬀer from this problem and is very expedient for
use in the neighbourhood of r = rs . The idea is to use a mesh of radial null
geodesics as the co-ordinate lines of a new co-ordinate system. Since these are
photon paths that actually cross the horizon we hope to eliminate in this way
some of the odd behaviour of Schwarzschild co-ordinates. According to (4.27)
radial null geodesics are given by (1 − rs /r)c2 dt2 − (1 − rs /r)−1 dr2 = 0, which
integrates to:
x± ≡ ct ∓ {r + rs log(r/rs − 1)} = constant ,

(6.21)

and x+ = constant describes outgoing null geodesics, x− = constant the
incoming null geodesics. For simplicity we restrict ourselves momentarily to
r > rs . The θ and φ co-ordinates remain unchanged and play no role. The
next step is to introduce new co-ordinates u and v:
u + v = f (x− ) ;

u − v = g(x+ ) ,

(6.22)

for arbitrary (well-behaved) functions f and g. We have now deﬁned a class of
co-ordinates in which u ± v = constant represents outgoing and incoming null
geodesics. These null geodesics are therefore straight lines making an angle of
±45◦ with the u and v co-ordinate axes – just like in the Minkowski space of
SR. The ﬁnal step is to choose appropriate functions f and g. Kruskal and
Szekeres took f (x) = 1/g(x) = exp(x/2rs ), leading to
⎫
1/2

r
ct ⎪
⎪
⎪
−1
exp(r/2rs ) cosh
u =
⎪
rs
2rs ⎬
for r > rs ,
(6.23)

1/2
⎪
r
ct ⎪
⎪
⎪
v =
−1
exp(r/2rs ) sinh
⎭
rs
2rs
⎫

1/2
r
ct ⎪
⎪
⎪
u = 1−
exp(r/2rs ) sinh
⎪
rs
2rs ⎬
for r < rs .
(6.24)

1/2
⎪
r
ct ⎪
⎪
⎪
v = 1−
exp(r/2rs ) cosh
⎭
rs
2rs

r = 3 rs / 4

t = r s/c

t=0

t=

{ rv == 0

- rs / c

6 Black Holes

÷1+u 2

122

{ tr == +r s⬁

v

r = 1.05 rs
t = r s/c

u

t=0

t=

- rs / c

r = 1.5 r s

r=0; v=

÷1+u

2

{ tr == r- ⬁
s

Fig. 6.6. Kruskal diagrams (1). Schwarzschild co-ordinates r and t as a function of
the Kruskal-Szekeres co-ordinates u and v. The solid toothed line is the singularity
r = 0.

The angles θ and ϕ remain unchanged. The inverse transformation is (exercise):


r
− 1 exp(r/rs ) = u2 − v 2 ;
(6.25)
rs
$
$
$u + v$
ct
$.
$
(6.26)
= log $
rs
u − v$
The metric in these new co-ordinates is (see exercise):
⎫
4rs3
⎪
2
2
2
2
ds =
exp(−r/rs ) (dv − du ) − r dΩ ; ⎪
⎬
r
⎪
⎪
⎭
dΩ 2 = dθ2 + sin2 θ dϕ2 .
2

(6.27)

From (6.25) we conclude that r/rs is a function of u2 − v 2 .
Properties
Kruskal-Szekeres co-ordinates have a number of interesting properties. First
of all, note that the u, v co-ordinates are a mix of the spatial co-ordinate r

6.4 Kruskal-Szekeres co-ordinates

123

v
light-cone
worldline
photon
u

worldline
particle

Fig. 6.7. Kruskal diagrams (2). Worldlines of photons and a massive particle. The
light-cone has everywhere an opening angle of 45◦ .

and co-ordinate time t. The co-ordinates u and v have no obvious physical
interpretation, and the reader is once more reminded of the fact that coordinates are merely event labels. Important is that the metric (6.27) is no
longer singular at r = rs , but the singularity in r = 0 remains. It follows from
(6.27) that radial null geodesics (ds = dθ = dϕ = 0) are given by:
dv = ± du ,

(6.28)

so that they are indeed lines of ± 45◦ inclination in the u, v diagram (‘Kruskal
diagram’). From (6.25) we see that lines with r = constant are hyperbolae, u2 − v 2 = constant. And (6.26) says that the lines t = constant have
(u+v)/(u−v) = constant, i.e. v = const · u, that is, they are lines through the
origin. The transformation is drawn in Fig. 6.6. Since cosh x+sinh x = ex > 0,
we infer from (6.23) and (6.24) that u + v > 0: the Schwarzschild co-ordinates
r, t are mapped onto region I (‘our universe’ r > rs ) + region II (the black
√ hole
r < rs ). According to (6.25) the singularity r = 0 is located at v = + 1 + u2 .
Fig. 6.7 shows the worldline of a particle falling into the hole (emitting a
photon as an ultimate farewell message), and of an incoming photon. Clearly
neither particles nor photons have the possibility to return to the exterior
region I once they have entered II. All worldlines in II hit the singularity –
there is no escape. The regions III and IV exist because we may also deﬁne
(6.23) and (6.24) with an overall minus sign, the inverse transformation (6.25)
and (6.26) being invariant for (u, v) → (−u, −v). Regions III + IV have no
clear physical meaning – they contain the time-reversed orbits discussed below (6.9), and region IV is accordingly referred to as a white hole For more
information see Misner et al. (1971) Ch. 31; Wald (1984) § 6.4.

124

6 Black Holes

v

3

1

u
5
2
4

Fig. 6.8. Kruskal diagrams (3). Light reﬂected oﬀ a mirror on the surface of a
collapsing star will, after a certain moment, no longer return to r = ∞. The grey
part of the diagram has no physical meaning, see text.

Exercise 6.10: Prove the relations (6.25) – (6.27).
Hint: Use cosh2 − sinh2 = 1 and arctgh x = 12 log{(1 + x)/(1 − x)} for x2 < 1.
For (6.27) diﬀerentiate (6.25): (r/rs2 ) exp(r/rs )dr = 2(udu − vdv) → dr =
(2rs2 /r) exp(−r/rs )(udu − vdv); (6.26): cdt = 2rs (u2 − v 2 )−1 (udv − vdu) =
(2rs2 /r)(1 − rs /r)−1 exp(−r/rs )(udv − vdu). Substitute in (4.27).
Exercise 6.11: We send a light signal towards a mirror lying on the surface of
a collapsing star, in an attempt to let a black hole reﬂect light. Show that the
mirror will always see the beam and reﬂect the light, even inside the horizon.
Nonetheless, light emitted after a certain time t0 will never reach the outside
world, whence the name ‘a hole in spacetime’.
Hint: See Fig. 6.8: 1 = worldline external observer; 2 = worldline stellar surface; 3 = last possibility for reﬂected light to escape; 4 = corresponding null
geodesic; 5 = deﬁnition of t0 .
Exercise 6.12: Let line 2 in Fig. 6.8 be the worldline of the collapsing stellar surface. Show that the grey part of the Kruskal diagram has no physical
relevance.

6.5 Rotating black holes: the Kerr metric

125

Hint: Grey part of I and II contains worldlines of the stellar matter, but by
shifting worldline 2 towards t = −∞ regions I and II remain as a whole.

6.5 Rotating black holes: the Kerr metric
The Kerr metric is a stationary axisymmetric solution of the vacuum equation
(3.39). We shall not actually solve the vacuum equations here, nor shall we
engage in any detailed calculations. In the case of axial symmetry the metric
tensor can no longer be made globally diagonal, and we intend to explain here
one of the more spectacular consequences of the non-diagonality of the metric:
the frame-dragging eﬀect.
The metric tensor now depends on r and θ: gαβ = gαβ (r, θ). Furthermore
ds2 should be invariant under the transformation (dt, dϕ) → (−dt, −dϕ)
which implies gtθ = gtr = gϕθ = gϕr = 0. It would be incorrect to require
dt → −dt and dϕ → −dϕ separately, because that does not correspond to the
physical situation. Two cross terms remain: gtϕ and grθ . It turns out that grθ
can also be made zero, but for gtϕ this is not possible (without proof). The
metric has the following form:9
ds2 = gtt dt2 + 2gtϕ dtdϕ + gϕϕ dϕ2 + grr dr2 + gθθ dθ2 .

(6.29)

The co-ordinates r and θ are no longer the same r and θ of the Schwarzschild
metric; they coincide only in the limit r → ∞. In that case (6.29) should be the
Lorentz metric, that is: gtt ∼ c2 , grr ∼ −1, gθθ ∼ −r2 , gϕϕ ∼ −r2 sin2 θ, and
furthermore gtϕ → 0. Besides the mass M , the metric (6.29) contains a second
parameter a, which occurs everywhere quadratically, except in gtϕ which is
linear in a. It turns out that a = L/M c where L = total angular momentum
0.28 km). It follows that ds2 is invariant under
(for the Sun a = 0.092 rs
(a, dt) → (−a, −dt) which is as it should be if (6.29) is the metric of a
rotating black hole. Moreover (6.29) turns out to possess equatorial symmetry
(invariance for θ → π − θ).
Sofar we have always dealt with metric tensors that were diagonal, so that
g αα = 1/gαα (no summation), but here we encounter for the ﬁrst time a
nontrivial 2×2 submatrix:
% tt tϕ &
%
&−1
g g
gtt gtϕ
=
g tϕ g ϕϕ
gtϕ gϕϕ
9

See Shapiro and Teukolsky (1983) p. 357, Wald (1984) p. 312 ﬀ and Schutz (1985)
p. 297.

126

6 Black Holes

P1
P1 < P2
P2

Fig. 6.9. A satellite in orbit around a rotating black hole. Due to frame-dragging
the period P1 of a prograde orbit is smaller than the period P2 of a retrograde
orbit. To ﬁrst approximation Ω1 − Ω2 is given by (6.32). A polar orbit will show
a precession of the orbital plane. In this way the LAGEOS geodetic satellites have
measured the frame-dragging due the rotation of the Earth to a precision of 10%
(Ciufolini, I. and Pavlis, E.C., Nature 431 (2004) 958).

1
=
gtt gϕϕ − gtϕ2

%

gϕϕ −gtϕ
−gtϕ gtt

&
,

(6.30)

from which expressions for g tt , g ϕϕ and g tϕ follow. We now consider the
constants of the motion. Because gαβ,t = gαβ,ϕ = 0 we know that ut and uϕ
are constant according to (2.40). Consider an ingoing particle with uϕ = 0.
We have:
uϕ ≡ g ϕα uα = g ϕϕ uϕ + g ϕt ut = g tϕ ut ,
(6.31)
t
tα
tt
tϕ
tt
u ≡ g uα = g ut + g uϕ = g ut .
With the help of these relations we calculate the rotation Ω of the particle at
r, as measured by an observer at r = ∞:
Ω ≡

dϕ
dϕ/ds
uϕ
g tϕ
gtϕ
=
= t = tt = −
dt
dt/ds
u
g
gϕϕ

2GL
,
c2 r3

(6.32)

for r  rs and θ = π/2. Use has been made of (6.31), and of (6.30) at the
last = sign; the last expression in (6.32) is given without proof. At r = ∞ we
have Ω = 0 and uϕ = 0. The choice of uϕ = 0 implies that the particle begins
its inward journey in the radial direction. But as it moves to ﬁnite r, the
gravity of the rotating hole forces the particle to rotate with the hole. This is
called frame-dragging. The hole ‘drags space along’ and this may be regarded
as a manifestation of Mach’s principle. One of the consequences of framedragging is that the period of a satellite moving in a prograde orbit is smaller
than the period of a retrograde satellite, Fig. 6.9. In classical mechanics the
gravitational ﬁeld of a sphere is independent of its rotation. Not so in GR!

6.5 Rotating black holes: the Kerr metric

127

event horizon (grr = ⬁ ) at
r = [r + (r 2 - 4a2)½ ] 2
ergosphere

s

event
horizon

s

/

rs = 2GM / c2 ; a = L / Mc
static limit (gtt = 0) at

rs

r = [rs + (rs2 - 4a2 cos2 q)½ ] / 2

Fig. 6.10. A rotating black hole, with its event horizon and static limit. Rotation
reduces the radius of the event horizon to a value between rs and rs /2. The space
between the static limit and the horizon is called the ergosphere. Objects inside
ergosphere may escape to r = ∞, but are forced to corotate with the hole. Black
holes cannot spin arbitrarily fast: the angular momentum L is restricted by a ≤ rs /2.
Holes with a = rs /2 are said to be maximally rotating.

Next we have a look at circular photon orbits, ds = dr = dθ = 0 in (6.29):
gtt dt2 + 2gtϕ dtdϕ + gϕϕ dϕ2 = 0 .

(6.33)

These orbits are not null geodesics, so one would need some optical contraption
like a set of mirrors to actually force the photon into a quasi-circular, polygon
orbit. Dividing by dt2 and solving for dϕ/dt produces



dϕ
1
2
=
− gtϕ ± gtϕ − gtt gϕϕ .
(6.34)
dt
gϕϕ
Suppose that gtt = 0, then dϕ/dt = −2gtϕ /gϕϕ = 2Ω or dϕ/dt = 0. The
former solution is a photon rotating with the hole, the latter is a retrograde
photon. It is just able to beat the frame-dragging and is eﬀectively at rest –
for an observer at r = ∞, not for a local observer. And massive particles are
forced to rotate when gtt = 0. The surface gtt = 0 is called the static limit
and is located outside the horizon, which is deﬁned by grr = ∞. The space
between the horizon and the static limit is called the ergosphere. Rotation is
compulsory in the ergosphere. Whatever force the rocket of a test particle in
the ergosphere may exert, it cannot prevent the test particle from rotating
with the hole (for an observer at r = ∞). But a particle may still escape from
the ergosphere to r = ∞. In the Schwarzschild metric the surfaces gtt = 0 and
grr = ∞ coincide.
A discussion of the geodesics of the Kerr metric would be out of place
here. We only mention that the smallest stable circular orbit of a test particle,

128

6 Black Holes

at 3rs in the Schwarzschild metric, moves inward (outward) for a prograde
(retrograde) orbit in the equatorial plane. For example, for a hole rotating at
80% of its maximum rate (a = 0.8 · 12 rs = 0.4rs ) we have rprogr = 1.45 rs and
rretrogr = 4.21 rs .

Exercise 6.13: We know that the exterior vacuum of a spherically symmetric
neutron star has the Schwarzschild metric. Does the exterior vacuum of a
rotating neutron star have the Kerr metric? Discuss the occurrence of framedragging in case your answer is negative.

6.6 Hawking radiation
In 1975 Hawking discovered that black holes should emit thermal radiation –
in other words, that black holes are really black from a physical point of view.
The eﬀect is due to the fact that vacuum ﬂuctuations, spontaneous creation
and annihilation of particle anti-particle pairs occurring throughout space,
develop an asymmetry near an event horizon.10 The particle with negative
energy may fall into the hole, and the other must then escape towards r = ∞
as a real particle. The reverse process is forbidden because a particle with
negative energy cannot move as a real particle in the region outside the hole
(but inside the horizon it can). A complete calculation requires quantum ﬁeld
theory near the horizon, see Wald (1984) p. 399 ﬀ. Here we shall resort to an
intuitive approach due to Schutz (1985).
Consider a photon pair created close to the horizon, at r = rs + δ. We
analyse this process in the local rest-frame, i.e. a frame in radial free fall with
zero velocity in r = rs + δ. Special relativity applies there, and the virtual
particles have an energy ± . The observer is on a radial geodesic with h = 0
rs :
and from (4.37) we see that 0 = ṙ2 = e2 − 1 − rs /(rs + δ) , or, using δ
δ/rs . The observer reaches the horizon in a proper time interval ∆τ ,
e2
which we may ﬁnd from (4.37). We transform to the variable x = r − rs :
10

The eﬀect occurs whenever there is an event horizon. For example an observer in
Minkowski space subject to a constant acceleration a has an event horizon: signals
from events in the region beyond the asymptote to his worldline will never be able
to reach him. As a result he ﬁnds himself in a bath of thermal radiation with a
temperature T = a/2πκc. This is called the Unruh eﬀect. The eﬀect has been
measured in electron storage rings, where it shows up in that it is impossible to
achieve 100% polarisation. This is now understood to be the result of the thermal
radiation that the accelerated electrons experience, see Bell, J.S. and Leinaas,
J.M., Nucl. Phys. B284 (1987) 488.

6.6 Hawking radiation



dx
cdτ



2
= e −
2

rs
1−
rs + x



129

δ −x
,
rs

(6.35)

rs δ .

(6.36)

and ∆τ follows by integration:
√
c∆τ = − rs


δ

0

dx
√
= 2
δ −x

If ∆τ  / the pair will annihilate long before reaching the horizon, and
when ∆τ
/ they annihilate long after that (i.e. inside the hole). However,
when ∆τ ∼ / there is a chance that the photon with  < 0 stumbles into
the hole while the other escapes towards r = ∞. This gives an approximate
relation between  and δ:
c
.
(6.37)
 ∼ √
2 rs δ
As the photon arrives in r = ∞, it is redshifted, and we compute its energy
E there from (3.20) and (3.2): E/ = ν(∞)/ν(rs + δ) = dτ (rs + δ)/dτ (∞) =
dτ (rs + δ)/dt = g00 (rs + δ) :
c
E ∼ √
2 rs δ



rs
1−
rs + δ

1/2

c
,
2rs

(6.38)

which is independent of δ! Hawking’s analysis showed that the photons have
a Planck distribution corresponding to a temperature κT = c/4πrs . A black
hole emits thermal radiation with a temperature

−1
M
c3
−8
6.2 × 10
K.
(6.39)
T =
8πκGM
M
Physical consequences
Hawking radiation has two interesting consequences. The ﬁrst is of a thermodynamic nature. Earlier on we said that the area of the horizon cannot
decrease (this was without any consideration of quantum eﬀects). We may
cast this in a form reminiscent of a well-known thermodynamic relation. We
have dA = 8πrs drs = 16πrs (G/c2 )dM , or:
 3 
κc
c4
A .
(6.40)
d M c2 =
dA = T d
16πGrs
4G
This says that dE = T dS with E = M c2 and the entropy S of the hole would
then be equal to
πκc3 2
κc3
A =
r ,
(6.41)
S =
4G
G s
apart from a constant. We may now argue that the hole’s entropy cannot
decrease because A cannot. However, A may decrease when quantum eﬀects

130

6 Black Holes

are taken into account, but in that case it is no longer correct to regard the
hole as an isolated system. The idea is that the total entropy of the hole and
the emitted radiation cannot decrease.
A second consequence is that a black hole will evaporate, because it loses
energy by emission of radiation. The mass of the hole must decrease according
to d M c2 /dt = −4πrs2 σT 4 , or with σ = π 2 κ4 /603 c2 :
dM
a
= − 2 ,
dt
M

⎫
⎪
⎪
⎪
⎬

with

1
c4
a = 10
· 2 = 4.0 × 1024 g3 s−1 .
2 · 15π G

⎪
⎪
⎪
⎭

(6.42)

The evaporation rate is initially slow, but accelerates towards the end and
the last stages proceed explosively. All kinds of particles are emitted, not only
photons, but emission of particles with rest mass m0 becomes important only
2
when κT >
∼ m0 c . From the exercise below we see that of all primordial black
14
holes that may have formed during the Big Bang, those with M <
∼ 2 × 10 g
have evaporated by now – provided they did not accrete mass. According to
11
(6.39) these holes have an initial temperature of >
∼ 6 × 10 K.
Exploding microscopic black holes behave not unlike elementary particles,
with a characteristic emission spectrum of particles and photons. At present
there is no evidence for their existence. They may perhaps be found in cosmic
rays. An intriguing possibility is that microscopic black holes (or something
resembling it) might be created in future particle accelerators, and be detected
through their decay products.

Exercise 6.14: Show that the characteristic wavelength of the Hawking radiation at r = ∞ is rs .
Exercise 6.15: Prove that the lifetime of a non-accreting black hole in vacuum is given by
M3
t =
3a


14 × 10

9

M
1.7 × 1014 g

3
yr .

(6.43)

7
Gravitational waves

Periodic solutions of the vacuum ﬁeld equations correspond to periodic variations in the geometry of spacetime. Because the equations are nonlinear in
gαβ analytic solutions can only be found in a few special cases. The physical
origin of the nonlinearity is that the energy and momentum density of the
gravitational ﬁeld act in turn as a source of gravity. The situation is therefore
much more complicated than in the case of electromagnetic waves in vacuum,
which is a linear problem. However, we expect that the waves are very weak,
and then we may use the linearized theory of § 3.5. According to (3.49) we
have


1 ∂2
2
−
∇
(7.1)
γµν =
γµν = 0 ,
c2 ∂t2
with gµν = ηµν + γµν , showing that there must exist waves in the metric that
propagate at the speed of light. From (2.62) we infer that to ﬁrst order in γ:
Rαµνσ =

1 αβ
2η

(γβσ,µν − γµσ,βν − γβν,µσ + γµν,βσ ) .

(7.2)

The waves show up in the Riemann tensor as well, so that we are really
dealing with ﬂuctuations in the structure (the ‘geometry’) of spacetime, and
not with ﬂuctuations in the deﬁnition of the co-ordinate system, for example.
These waves have recently been detected, albeit indirectly, in the binary pulsar
PSR 1913+16. Here we review their most important properties and detection
techniques.

7.1 Small amplitude waves
We use the linearized theory of § 3.5, where we wrote gαβ = ηαβ + γαβ and
hµν ≡ γµν − 12 ηµν γ σσ . For hµν the following equations were obtained:
hµν = 0 ;

hµν,ν = 0 .

(7.3)

It is recalled that the theory is accurate to ﬁrst order in γ, that we may raise
and lower indices with ηµν and that ηµν commutes with . We seek a plane
wave solution:

134

7 Gravitational waves

hµν = aµν exp(ikα xα )

with

k µ = (Ω/c, k) .

(7.4)

The constants aµν obey aµν = aνµ , so that there are in total 10 independent
numbers. Furthermore, kα xα = k0 x0 + ki xi = k 0 x0 − k i xi = Ωt − k · r. Insert
that in (7.3):


0 = hµν ≡ η αβ hµν,αβ = aµν η αβ exp(ikσ xσ ) ,αβ .
(7.5)
Now {exp(·)},αβ = −kα kβ exp(·) , or
0 = −aµν η αβ kα kβ exp(ikσ xσ ) ,

(7.6)

and we conclude that 0 = η αβ kα kβ = ηαβ k α k β = (k 0 )2 − k2 , or, with (7.4):
Ω 2 = (kc)2 .

(7.7)

A gravitational wave has the same dispersion relation as a plane electromagnetic wave in vacuum. From hµν,ν = 0 we ﬁnd
aµν kν = 0 .

(7.8)

These are 4 restrictions on the 10 constants aµν . But we haven’t chosen a
co-ordinate frame yet, and that yields four more restrictions. These take the
following simple form (see exercise):
aσσ = 0

and

aµν tν = 0 ,

(7.9)

where tν is an arbitrary 4-vector obeying kµ tµ = 0. Relation (7.8) reveals
that k µ aµν = aνµ k µ = ηνσ aσµ kµ = 0, so that k µ aµν tν is already zero. Hence
aµν tν = 0 gives only 3 independent restrictions. From aσσ = 0 it follows that
hσσ = 0, so that γ = h = 0 according to (3.51). The distinction between hµν
and γµν has vanished:
hµν = γµν .
(7.10)
All gauge freedom has now been exhausted and from the 10 free constants
aµν only two are left. To proceed we take tν = (1, 0, 0, 0). Then from (7.9):
aµ0 = 0. In particular a00 = 0 and a0 0 = η 0ν aν0 = 0. It then follows from
aσσ = 0 that ai i = 0 → aii = ηiν aνi = −ai i = 0. Taking k along the x3 axis,
we ﬁnd with (7.7) that k µ = (Ω/c)(1, 0, 0, 1). Finally, we have from (7.8) that
0 = aµν k ν = aµ0 k 0 + aµ3 k 3 = (Ω/c)aµ3 . In summary, aµ0 = aµ3 = aii = 0, so
that aµν has the following format:
⎛
⎞
0 0
0 0
⎜ 0 axx axy 0 ⎟
⎟
aµν = ⎜
(7.11)
⎝ 0 axy −axx 0 ⎠ .
0 0
0 0

7.1 Small amplitude waves

135

Only hxx = γxx and hxy = γxy are = 0. This choice of aµν is called the transverse traceless gauge, or TT-gauge. In the literature it is often denoted as aµν ,
hµν , γ µν , .. to indicate that it refers to a special choice of the co-ordinates.
Only the spatial components of aµν perpendicular to the direction of propagation are nonzero, that is, the wave is transverse. And ‘traceless’ obviously
refers to aσσ = 0. There are two independent wave modes, corresponding to
the constants axx and axy in (7.11).
For a weak gravitational wave propagating along the x3 -axis (z-axis) in the
TT-gauge we may summarize our results as follows, using kα xα = Ω(t − z/c)
and λ = wavelength:
gµν = ηµν + γµν ;

γµν = aµν exp{iΩ(t − z/c)} ,
(7.12)

with

λ = 2π/k = 2πc/Ω ,

and aµν is given by (7.11). The explicit form of the metric is
ds2 = c2 dt2 − (1 − γxx )dx2 − (1 + γxx )dy 2 + 2γxy dxdy − dz 2 . (7.13)

Exercise 7.1: Prove that one may impose the restrictions (7.9).
Hint: In exercise 3.11 a transformation was used to obtain the linearized theory. However, there was still some gauge freedom left. We make once more
a transformation xα → xα + ξ α (x) for which then ξα = 0 must hold.
From the hint in exercise 3.11 we see that γ µν = γµν − ξµ,ν − ξν,µ , so that
γ = γ ρρ = η ρα γ αρ = η ρα (γαρ − ξα,ρ − ξρ,α ) = γ ρρ − 2ξ ρ,ρ = γ − 2ξ ρ,ρ . Hence
γ = γ − 2ξ ρ,ρ . From (3.51): hµν = γ µν − 12 ηµν γ. Show that this leads to:
hµν = hµν − ξµ,ν − ξν,µ + ηµν ξ ρ,ρ .

(7.14)

Now take ξµ = bµ exp(ikα xα ) with k µ from (7.4). This choice obeys ξµ = 0.
We must now show that there exist a bµ so that (7.9) holds. Write hµν =
aµν exp(ikα xα ) and hµν = aµν exp(·), in accordance with (7.4), and substitute
in (7.14):
aµν = aµν − ibµ kν − ikµ bν + iηµν bρ kρ ,
(7.15)
from which aµµ = aµµ − ibµ kµ − ik µ bµ + iη µµ bρ kρ → aµµ = aµµ + 2ibµ kµ (since
η µµ = δµµ = 4). Require aµµ = 0 → ibµ kµ = − 12 aµµ . Substitute in (7.15):
aµν = aµν − ibµ kν − ikµ bν −
Require next that aµν tν = 0 for a given tν :

ρ
1
2 ηµν a ρ

.

(7.16)

136

7 Gravitational waves

ibµ (kν tν ) = aµν tν − ikµ (bν tν ) −

ρ
1
2 tµ a ρ

.

(7.17)

We are done if we can eliminate bν tν on the right hand side. Multiply (7.17)
with tµ . The result is an equation from which bν tν may be solved, if kν tν = 0.
Substitute that again in the right hand side of (7.17). The ﬁnal expression for
bµ is not important – what matters is that it exists.

7.2 The eﬀect of a gravitational wave on test masses
We consider the dynamics of a free test mass in a gravitational wave. Its
worldline is a timelike geodesic, determined by (2.34):
duα
+ Γαµν uµ uν = 0 ;
ds

uα =

dxα
.
ds

(7.18)

We elaborate this in the TT-gauge, to ﬁrst order in γαβ . From (2.24) we see
that Γαµν = O(γ). Therefore it suﬃces to expand uµ and uν in the second
(1, v i /c)
(1, 0, 0, 0). The
term in (7.18) to zeroth order, see (3.23): uµ
equation for the test mass motion reads:
duα
+ c Γα00 = 0 .
dτ

(7.19)

We conclude from (3.16) that Γα00 = 12 η αλ (2γλ0,0 − γ00,λ ) = 0 because γµν =
aµν exp(ikα xα ) in the TT-gauge, and γλ0 = 0 because aλ0 = 0. It follows
that uα (τ ) = uα (0), and if the test mass is at rest at τ = 0, it remains at
rest as the wave passes by.1 Superﬁcially, it seems that the test mass does
not move. However, in the TT-gauge we are using very special co-ordinates.
It turns out that the co-ordinates have been chosen so that they move along
with the particle. Let us look at the behaviour of test masses on a circle in
the x3 = 0 plane, orthogonal to the direction of wave propagation, Fig. 7.1.
The co-ordinates of P are x1 = l0 cos θ, x2 = l0 sin θ and x3 = z = 0. Because
g0i = γ0i = 0 we can ﬁnd the physical distance between the origin O and P
by integrating (3.7) along OP . The integration is trivial because gij does not
depend on x1 and x2 . The distance l between O and P becomes (i = 1 or 2):
l2 = − gij xi xj
= (1 − γxx )l02 cos2 θ + (1 + γxx )l02 sin2 θ − 2γxy l02 sin θ cos θ
= l02 1 − γxx cos 2θ − γxy sin 2θ ,
so that

(7.20)

7.2 The eﬀect of a gravitational wave on test masses
y

O

gx x = 0
gx y = 0

P

l0

137

q

x

gx x = 0
gx y = 0

Fig. 7.1. Left: A ring of test particles perpendicular to a gravitational wave is
periodically deformed as shown to the right. Each test mass moves along a geodesic
and senses no acceleration. However, if O and P are materially connected, they
experience a tidal acceleration ¨
l, see § 7.4.

z

l /2

y
x

Fig. 7.2. A γxx = 0 gravitational wave propagating in space. The whole pattern
moves with the speed of light along the direction of propagation (the z-axis), and
γxx (and γxy ) are independent of position in planes perpendicular to the direction
of propagation. Since the expected wave frequencies are less than a few kHz, the
wavelengths λ are large, at least 100 km.

l

l0 1 −

1
2 γxx

cos 2θ −

1
2 γxy

sin 2θ .

(7.21)

Since γxx , γxy ∝ exp(ikα xα ) = exp iΩ(t − x3 /c) = exp(iΩt) we see that
the ring of test masses is deformed periodically as in Fig. 7.1. There are two
independent linearly polarized waves. The directions of polarization diﬀer by
an angle of 45◦ . From these two waves one may construct circularly polarized waves, as usual. In such a circularly polarized wave, the test particles of
Fig. 7.1 describe small circles around their unperturbed position.

1

This is no longer the case if we would work to second order in γ.

138

7 Gravitational waves

Exercise 7.2: Show that the distance between two test masses on the z-axis
does not change. The wave is therefore transverse at least up to order γ.
Hint: Locate the particles in x1 = x2 = 0, and x3 = 0 and x3 =  → l2 =
−g33 2 = −η33 2 = 2 because γzz = 0.
Exercise 7.3: Estimate the acceleration experienced by an extended body
due to the passage of a gravitational wave.
Hint: (7.12): the action of the wave (i.e. γxx and γxy ) is independent of position
in planes ⊥ z-axis, but diﬀerent in planes at diﬀerent z. A ‘pencil’ along
the z-axis will not feel the wave (exercise 7.2). Pencil ⊥ z-axis: (7.21) →
a = ¨l = − 12 l0 γ̈xx ∼ l0 γΩ 2 assuming γxy = 0, γxx = γ cos Ωt and cos θ = 0.
The wave causes a tidal acceleration ∝ size of object. Take the Space Station
(l0 = 100 m) and γ = 10−6 , Ω/2π = 5 Hz → a ∼ 0.1 m s−2 → Station is
periodically stretched and compressed with a force equivalent to 0.01 g.

7.3 Generation of gravitational radiation
The amplitude of gravitational waves is expected to be extremely small,
γxx , γxy ∼ 10−20 , and the reasons are twofold: the enormous distance of potential sources, and the fact that gravitational radiation is inherently weak
because there is no dipole radiation. To illustrate this, consider electromagnetic radiation of a source of size 2R. The radiation consists of the sum of
the various multipole contributions, the dipole radiation usually being the
strongest. At large distances from the source (r  R), the vector potential in
the Lorentz gauge is given by:
Arad (r, t) =

1
1 
multipoles ,
ḋ(t − r/c) +
cr
cr

(7.22)

where d = Σ ei r i is the electric dipole moment of the source and ˙ = ∂/∂t. The
power emitted in electric dipole radiation is proportional to d̈ · d̈. The next
terms in (7.22) are those of the magnetic dipole moment Σ ei (r × v)i and
the electric quadrupole moment Σ ei (3rr − r2 I)i of the source. The power
emitted in electric quadrupole and magnetic dipole radiation is a factor of
(kR)2 ∼ (R/λ)2 smaller than that in electic dipole radiation. In the case of
gravitational radiation, the (mechanical) dipole moment equals d = Σmi r i .
However, d̈ = Σmi ṙ i ˙ = (P tot )˙ = 0. There is no dipole radiation because

7.3 Generation of gravitational radiation

T

mn

139

=0

r
T mn = 0 (vacuum)

2R

Fig. 7.3. A source of characteristic size R and Schwarzschild radius rs radiates
gravitational waves that are detected at a large distance r. The amplitude of the
waves is given by (7.25).

the total momentum P tot of the system is constant. And the analogon of magnetic dipole radiation is absent because the angular momentum is conserved.
The ﬁrst non-vanishing contribution is generated by a variable quadrupole
moment.2
Assuming that the deviations from the Lorentz metric in the source are
small, the generation of gravitational radiation is described by eq. (3.52): 3
hµν = −

16πG µν
T .
c2

(7.23)

We shall now estimate the order of magnitude of hµν far from the source. The
radiation ﬁeld there consists of a superposition of spherical waves of diﬀerent
frequencies of the type:
hµν =

H µν
exp{i(Ωt − kr)} ,
r

(7.24)

with Ω 2 = (kc)2 , the dispersion relation (7.7), see exercise. We neglect the θ, ϕ
dependence of H µν because all we are interested in is an order of magnitude.
Directly exterior to the source, in r ∼ R (see Fig. 7.3), we have hµν ∼ H µν /R.
Next we estimate in (7.23) for r R :  ∼ R−2 and T µν ∼ ρuµ uν ∼ ρv 2 /c2 ,
so that
2

3

For sources of gravitational radiation see e.g. Schutz, B.F., Class. Quantum Grav.
13 (1996) A219; 16 (1999) A131.
Strictly speaking T µν in (7.23) describes only motion due to other forces than
gravity. Radiation from two compact binary stars whose motion is determined
by gravity should actually be found by solving hµν = 0 with two Schwarzschild
singularities in r 1 (t) and r 2 (t) as a boundary condition. However, it can be shown
that the result coincides with the solution of (7.23) up to O(γ) if one uses in T µν
the velocities following from classical mechanics.

140

7 Gravitational waves
Sun
2

8.

3 +_

1.

4

kp

c

_ 0.7 kpc
7.7 +

pulsar

galactic center

orbital phase shift (s)

50o

0
-2
-4
-6
-8
-10
-12

galactic
rotation

-14
1975

1980

1985

1990

Fig. 7.4. The location in the galactic plane of the compact binary system of which
PSR 1913+16 is a member (left), and the cumulative shift of the periastron passage
since the discovery of the system (right). Adapted from Taylor, J.H., Class. Quantum
Grav. 10 (1993) S167, and Damour, T. and Taylor, J.H., Ap. J. 366 (1991) 501.

hµν
G ρv 2
∼
.
R2
c2 c2
One might object that T µν ∼ ρ when µ = ν = 0, but according to (7.11)
h0α does not contribute. With the help of M ∼ ρR3 we ﬁnd that near the
source hµν ∼ rs (v/c)2 R−1 , and this should also be equal to H µν /R, or H µν ∼
rs (v/c)2 . At the observer we have hµν ∼ H µν /r, and we arrive at
⎧ 
2
ωR
rs
⎪
⎪
⎪
for v = ωR ,
⎨
 v 2 r
c
r
s
µν
µν
∼
(7.25)
γ = h ∼
⎪
c
r
2
⎪
r
⎪
s
⎩
for v 2 = GM/R .
Rr
Here we distinguish two archetypical cases: a bar rotating at a given angular
frequency ω, and a binary system where v can be estimated by the classical
circular orbit speed. This estimate (7.25) is valid if the source is far removed
from spherical symmetry, and v
c. Without proof we mention that the
average energy ﬂux density F of a gravitational wave is given by (see e.g.
Kenyon (1990)):
F =

c3
2
γ̇ 2 + γ̇xy
 ;
16πG xx

˙ = ∂/∂t .

(7.26)

The existence of gravitational waves has been demonstrated indirectly

7.3 Generation of gravitational radiation

141

period T

period T/2

period T

T

Fig. 7.5. Top: gravitational radiation of a rotating object has, theoretically, a period T because the source needs T seconds to return to the same conﬁguration.
Bottom: the radiation is emitted by the equivalent quadrupole, whose time dependence determines the spectrum. For small ellipticity the quadrupole rotates almost
uniformly and the radiation is practically monochromatic with frequency 2/T , since
the quadrupole needs T /2 seconds to return to a physically identical conﬁguration.
Not all periodic sources emit at twice the fundamental frequency: a rotating bar
does, but a harmonically oscillating bar emits at the fundamental frequency. For
binaries in a highly elliptic orbit the radiation takes the form of a series of pulses
separated by the orbital period T . The spectrum features emission of higher harmonics because these are now present in the time dependence of the quadrupole.

but convincingly in the compact binary system of which PSR 1913+16 is a
member. The system loses energy in the form of gravitational radiation, and
this shows up as a slowly decreasing orbital period Pb which is now 27906 s or
7.75 hr, Fig. 7.4. Observations carried out over the past 25 years have shown
that Ṗb = −(2.422 ± 0.006) × 10−12 , which agrees within the measurement
error (0.3%) with the prediction of GR: (Ṗbobs − Ṗbgal )/ṖbGR = 1.0032 ± 0.0035.
The term Ṗbgal is due to a small relative acceleration between the binary
pulsar and the solar system. This is because the binary pulsar is closer to
the galactic centre than the Sun, and is gradually overtaking us on account
of its larger galactic orbital velocity. This causes a measurable correction
Ṗbgal = −(0.012 ± 0.006) × 10−12 . This high precision can be achieved because the pulsar is a very accurate clock, and because the system is clean. 4
The orbital shrinking due to emission of gravitational waves of the newly discovered binary pulsar J0737-3039A/B is expected to be detected soon, and
should permit an even more accurate test.

Exercise 7.4: Prove that (7.24) is a solution of hµν = 0.
4

Taylor, J.H. and Weisberg, J.M., Ap. J. 345 (1989) 434; Damour, T., Class.
Quantum Gravity 10 (1993) S59; Taylor, J.H., Class. Quantum Gravity 10 (1993)
S167.

142

7 Gravitational waves

Hint:  = c−2 ∂t2 − ∇2 = c−2 ∂t2 − r−2 ∂r r2 ∂r , because we neglect the dependence on θ and φ.

Exercise 7.5: Estimate the order of magnitude and the time depencence of
γ µν of the following sources: (a) asymmetric collapse of supernova 1987a in
the Large Magellanic Cloud (r = 52 kpc); take rs ∼ 4 km (∼ 1.4M ) and
R ∼ 10 rs . (b) close encounter of two 1M black holes in the centre of our
galaxy (r = 8 kpc); take rs = 6 km, and R = 104 km, for example. (c) the
compact binary system containing PSR 1913+16 (r ∼ 8 kpc); take rs ∼ 8
km (2 × 1.4 M ), R = semi-major axis of relative orbit = 2 × 106 km. (d) a
rotating egg-shaped neutron star (due a strong magnetic ﬁeld). Take r = 2
kpc (Crab pulsar), rs ∼ 4 km and R ∼ 2rs .
−19
Hint: (a) γ µν <
∼ 2.5 × 10 . On account of (6.8) we expect a brief radiation
pulse of ∼ 10 µs. Unfortunately no detector was operational at the time of
−20
the event, in contrast to neutrino detectors. (b) γ µν <
∼ 10 . The radiation is
−22
a pulse lasting R/v ∼ (R/rs )1/2 (R/c) ∼ 1 s. (c) γ µν <
∼ 10 . The radiation is
1
−17
periodic at 2 × orbital period = 3.88 hours, see Fig. 7.5. (d). γ µν <
∼ 3 × 10 µν!
But the shape of the star will be almost spherically symmetric, hence γ
considerably smaller.

Exercise 7.6: Compute Ṗb of PSR 1913+16 from Fig. 7.4, right.
Hint: Expand the period P (t) = Pb + Ṗb t + · · · . The number of periods n in a
certain time interval equals n = dt/P , and n0 = dt/Pb if the period were
constant. The cumulative shift ∆t of the periastron passage is ∆t (n0 −n)Pb ,
or


 
 
1
1
Pb
∆t
Pb
−
dt
dt
1−
Pb
P
Pb + Ṗb t


Ṗb t2
Ṗb t
dt =
,
Pb
2Pb

(7.27)

and Pb = 27906 s, and from Fig. 7.4 we see that ∆t = −14 s in t = 18 years.
Exercise 7.7: Show that the ﬂux density of a weak gravitational wave with
γ 10−22 and a frequency of Ω/2π = 1 kHz is about equal to the optical ﬂux
density of the full moon (∼ 3 erg cm−2 s−1 at the Earth). In this sense sources
of gravitational waves shine very brightly in the sky! Explain this paradox.

7.4 Bar detectors

143

Fig. 7.6. An idealised detector for gravitational waves consisting of two masses
connected by a spring.

2
2
Hint: Estimate γ̇xx
+ γ̇xy
∼ Ω 2 γ 2 in (7.26). The energy ﬂux in gravitational
waves is large, but the relative amplitude γ is small. The stiﬀer the medium,
the smaller the amplitude of a wave at a given energy ﬂux. Spacetime behaves
as a very stiﬀ medium. Sources of gravitational waves radiate in general considerable amounts of energy, but the waves pass through everything without
leaving hardly any physical eﬀect.

7.4 Bar detectors
Detection of gravitational waves is very diﬃcult because the expected amplitudes are so small. Fourty years ago Weber experimented with aluminium
bars that were isolated from the environment as much as possible. We may
model such a detector as two masses connected by a spring, i.e. as a damped
harmonic oscillator with frequency ω0 /2π, see Fig. 7.6. The equation for the
distance ζ of the masses is: ζ̈ = −2ζ̇ − ω0 2 ζ. The eﬀect of a weak gravitational wave can be described by adding the acceleration ¨l due to the wave on
the right side.5 Let γxy = 0 and θ = 0 in (7.21), i.e. we consider one wave
and a detector aligned along the x-axis of Fig. 7.2, so that ¨l = − 12 l0 γ̈xx . The
equation for ζ is then
ζ̈ + 2ζ̇ + ω0 2 ζ = − 12 l0 γ̈xx .

(7.28)

Since γxx is independent of position along the detector we may put γxx =
γ cos Ωt. The maximum amplitude equals (see exercise):
ζmax =

1
2 l0 γQ

;

Q =

ω0
= quality factor .
2

(7.29)

For Q = 105 , γ = 10−20 and l0 = 2 m we have ζmax ∼ 10−13 cm, about the
size of an atom, which nicely illustrates the detection problem. An additional
5

See e.g. Misner et al. (1971) p. 1004 ﬀ; Schutz (1985) p. 222.

144

7 Gravitational waves

Fig. 7.7. Close up of the MiniGRAIL detector under development in Leiden. It
consists of a CuAl sphere of 68 cm diameter suspended by a thin rod. The resonance
frequency is 2.9 kHz, the bandwidth 230 Hz. The sphere carries several transducers
that mechanically amplify and detect the vibration. The theoretical sensitivity of
this 20 mK cryogenic detector is ∼ 4 × 10−21 . A spherical detector can determine
the direction n of the incoming wave (up to a ±n uncertainty) because the relative
excitation levels of the quadrupole modes of the sphere depends on n. Image credit:
A. de Waard and G. Frossati. See http://www.minigrail.nl/

complication is that of the order of Q waves are needed to excite a resonant
detector to its full amplitude ζmax , which renders detection of bursts of radiation more diﬃcult. And ζmax is independent of l0 since ω0 ∝ sound speed / l0 .
Bar detectors are sensitive in a narrow frequency interval ∆Ω ∼ ω0 /Q around
ω0 , and seem therefore more suited for detection of quasi-periodic radiation,
as emitted by narrow binary systems.
Noise is a problem of overwhelming importance. At room temperature the
amplitude ζ of thermally excited oscillations is also about 10−13 cm. Weber had two detectors operating in coincidence at room temperature, at a
frequency of ω0 /2π = 1660 Hz. Coincidence measurements by independent

7.5 Interferometer detectors

145

detectors at diﬀerent locations are essential to eliminate chance detections
that are actually large noise peaks. There are still a few bar detectors operating at room temperature and they attain a sensitivity of γ ∼ 10−16 . By
cooling to liquid helium temperatures (around 4 K) the NIOBE, EXPLORER
and ALLEGRO bar detectors reached a sensitivity of γ ∼ 6 × 10−19 . This
development took place during the eighties and nineties of the previous century. To detect the bar vibrations they are ampliﬁed, usually by a resonant
transducer that is read out by a squid. In the near future detectors of the third
generation NAUTILUS and AURIGA will become operational. These will be
cooled to ∼ 0.1 K. The MiniGRAIL project develops a spherical cryogenic
(20 mK) detector in the Netherlands, and a similar detector is being built in
São Paulo.6

Exercise 7.8: Prove (7.29).
Hint: Take γxx = γ exp(iΩt) and ζ = ζ̂ exp(iΩt) in (7.28) → (−Ω 2 + 2iΩ +
ω02 )ζ̂ = 12 l0 γΩ 2 . The solution is ζ = Re{ζ̂ exp(iΩt)} = Re{|ζ̂| exp(iφ) ·
exp(iΩt)} = |ζ̂| cos(Ωt + φ) for certain φ. Ergo ζmax = maxΩ |ζ̂|. A good
detector has 
ω0 (Q  1), and then the maximum is located practically
at Ω = ω0 .

7.5 Interferometer detectors
An alternative detection technique is based on Michelson interferometers.
These are more expensive but oﬀer two advantages: the sensitivity can be
higher and they cover a broad frequency band. We analyse the operation of
such a detector, see Figs. 7.8 and 7.9. The laser beam enters the arms through
a beam splitter. The beams then travel back and forth between two mirrors
on each arm that are suspended so that they can move freely in the direction
of the beam. We assume an ideal orientation: the gravitational wave propagates perpendicularly to the plane deﬁned by the arms, that are aligned along
the x and y-axis as in Fig. 7.2. The wave induces a frequency shift7 δν/ν0 =
(ν2 − ν0 )/ν0 = dt0 /dt2 − 1 in the returning beams with respect to the laser,
see Fig. 7.10. The induced phase diﬀerences in the two arms have opposite
6

7

For information on existing and planned bar detectors see Blair (1991); Saulson
(1994); Ricci, F. and Brillet, A, Annu. Rev. Nucl. Part. Sci. 47 (1997) 111, and
Ju, L. et al., Rep. Prog. Phys. 63 (2000) 1317.
Actually dτ0 /dτ2 − 1, but g00 = 1.

146

7 Gravitational waves

4

km

Fig. 7.8. Areal view of the LIGO interferometer at Hanford (WA), showing the
central housing and the two arms of 4 km length. The other LIGO interferometer
is located 3000 km away in Livingstone (LA). Courtesy of California Institute of
Technology.

light bounces 50
times between the
mirrors on each arm

laser
photodetector

beam
splitter

4 km
recycling
mirror

mirror

Fig. 7.9. Principle of the LIGO Michelson interferometer.

sign, and show up as intensity variations upon interference on the detector (a
photodiode). We take once more γxy = 0, γxx = γ cos Ωt, and focus attention
on the x-beam. Then (7.13) reduces to c2 dt2 = (1 − γ cos Ωt)dx2 :
dx = ± c (1 − γ cos Ωt)−1/2 dt

± c (1 +

1
2γ

cos Ωt) dt .

(7.30)

7.5 Interferometer detectors

t

recycling
mirror

147

end
mirror
dt

t

2

2

t

1

dt

t

0

0

x

l

0

0

Fig. 7.10. Null geodesics of photons propagating between the mirrors along the xarm of the interferometer. The geodesics of subsequent wave crests are not congruent
because the metric depends on time. The mirrors have ﬁxed spatial co-ordinates
which we take to be x = 0 and x = l0 .

+, − for beams propagating to the right and left, respectively. Since the motion
of the mirrors in the direction of the beam is free, their co-ordinates x = 0 and
x = l0 , according to § 7.2, do not change when a gravitational wave passes.
Therefore we may integrate (7.30), for a beam propagating to the right in
Fig. 7.10:
 t1
l0
=
(1 + 12 γ cos Ωt) dt
c
t0
= t1 − t0 +

γ
(sin Ωt1 − sin Ωt0 ) .
2Ω

(7.31)

For the returning beam after reﬂection we take the − sign in (7.30), and
an extra − sign because we integrate over x from l0 to 0. As a result, the
expression for a beam propagating to the left emerges by substituting t0 → t1 ,
t1 → t2 . Adding these two gives:
t 2 − t0 =

γ
2l0
−
(sin Ωt2 − sin Ωt0 ) .
c
2Ω

(7.32)

To zeroth order t2 = t0 + 2l0 /c, which we use to eliminate t2 in the ﬁrst order
term on the right:
γ
2l0
−
{sin Ω(t0 + 2l0 /c) − sin Ωt0 }
c
2Ω


Ωl0
γ
2l0
−
sin
=
cos(Ωt0 + const) .
c
Ω
c

t2 − t 0 =

(7.33)

148

7 Gravitational waves

Gravitational Wave Amplitude

10 18

NSNS and BHBH
Coalescence

Coalescence of
Massive Black Holes

10 20
Resolved
Galactic Binaries

SN Core
Collapse

10 22
Unresolved
Galactic
Binaries

10 24

10 4

LISA

10 2

LIGO

10 0

10 2

10 4

Frequency [Hz]
Fig. 7.11. Expected sensitivity of LISA and LIGO. The U-shape reﬂects the factor sin(ΩL/c) in (7.35). From LISA System and Technology Study Report ESASCI(2000)11.

In reality the beam travels back and forth n times between the mirrors, and it
is easy to see that the same relation holds with l0 → L = nl0 = eﬀective arm
length. Diﬀerentiation of (7.33) produces dt2 − dt0 = γ sin(ΩL/c) sin(Ωt0 +
const) · dt0 , or:


ΩL
δν
dt0
=
−1
− γ sin
sin(Ωt + const) .
(7.34)
ν0
dt2
c
We have dropped the index 0 on t0 on the right. The frequency shift is far too
small to be measurable, but the phase diﬀerence δψ is not:

δψ = 2π



ΩL
γω0
sin
δν dt =
cos(Ωt + const) ,
Ω
c

(7.35)

with ν0 = ω0 /2π = laser frequency, Ω/2π = frequency gravitational wave.
The factor sin(ΩL/c) in (7.35) determines a broad frequency range where the
detector is sensitive, centered on ΩL/c = π/2 or Ω/2π = c/4L. LIGO has an
eﬀective arm length L ∼ 500 km and a laser frequency of ν0 = 3 × 1014 Hz
(λ = 1 µ). The maximum sensitivity lies around Ω/2π ∼ 150 Hz, see Fig. 7.11,
and the expected phase shift is very small: δψ
γω0 /Ω = 2 × 1012 γ. The
phase shift δψ of the y-beam has the opposite sign.

7.5 Interferometer detectors

149

Fig. 7.12. A gravitational wave will stretch and compress the wavelength of the
laser beam and the arm length of the interferometer in equal proportion. On this
account no phase diﬀerence would develop, see text.

The physics of interferometer detectors
In view of the interest these interferometer detectors will draw in the coming
decades we analyse their operation in some detail. Fig. 7.12 raises a basic
question. A gravitational wave stretches the arm of the interferometer and
the wavelength of the laser beam proportionally. Hence there are no phase
diﬀerences and the detector will not work. Where is the catch? The argument
is correct in the limit of small L. In that case (7.35) says that δψ → 0. But
when L is so large that the travel time of the laser beam is of the order
of the period of the gravitational wave, then the laser beam is no longer a
standing wave but a travelling wave. The wave train becomes a local entity
travelling with speed c with respect to the local track as it is alternatingly
being stretched and compressed. And then phase diﬀerences do develop.
Consider a beam propagating to the right, assuming cos Ωt > 0. Then
(7.30) tells us that dx > cdt. The co-ordinate speed of light is larger than
c, and it is straightforward to see from (7.13) that the co-ordinate speed in
the y-arm is smaller than c. This generates a time diﬀerence and hence a
phase diﬀerence between the two beams as they interfere on the detector, see
Fig. 7.13, top panels. However, we may also write (7.30) as
dl ≡ (1 −

1
2γ

cos Ωt) dx = c dt ,

(7.36)

where dl is the physical length corresponding to the co-ordinate distance dx
according to (7.21). In other words, dl = cdt and that holds for forward and
backward propagating beams. This says that the photons behave as cyclists
moving at speed c with respect to the local track as it is periodically stretching and shrinking, see Fig. 7.13, lower panels. For cos Ωt > 0 the physical
length of the x-track is reduced, that of the y-track increased by an amount
δL ∼ γL (L = nl0 ). Two things happen now. The wave trains are slightly
compressed and stretched (blue or redshifted), just like the track, but that
is too small to be observable. In the second place there is a diﬀerence in
arrival time δt ∼ γL/c, corresponding to a phase diﬀerence δψ ∼ γω0 L/c,

150

7 Gravitational waves

y
l0

co-ordinate
picture
O

x

y
l0

geometrical
picture
O

x

Fig. 7.13. An interferometer detector as a dual race track for photons. The gravitational wave propagates along the z-axis. A laser beam may be thought of as a
series of wave crests that follow null geodesics as in Fig. 7.10. Here we follow only
one wave crest. The top two panels are co-ordinate pictures. Two laser wave trains
start in O at t = 0. As long as cos Ωt > 0, the co-ordinate speed of light on the
x-track, dx/dt, is larger than c, but smaller than c on the y-track. However, the
co-ordinate length of the track is constant. The top right panel shows the positions
after a time δt = l0 /c. The wave trains arrive in O with a time diﬀerence (in reality the beams bounce back and forth many times). The lower two panels show the
equivalent geometrical pictures, see text, and Fig. 2.1.

which is essentially (7.35) for ΩL/c
1. When ΩL/c ∼ 1 the computation of
δt requires an integration and yields (7.35). Optimal operation (maximal δt)
occurs when the duration c/L of the race comprises a quarter of the gravitational wave period. If the race takes longer (larger L) the relative stretching
and compressing of the tracks reverses and the net δt becomes smaller. If
ΩL/c = π the gain δt accumulated during the ﬁrst quarter of the wave period
is undone during the second quarter, and the net gain δt becomes zero, see
(7.35). This conceptual picture of photons as cyclists on a shrinking or stretching road is also useful for understanding the shape of our past light-cone in
cosmology, see § 11.2.
Detector signal
In order to give the reader some idea of the problems involved in the interferometric detection of gravitational waves, we close this chapter with a (much
simpliﬁed) estimate of the ﬂux on the detector. Denoting the unperturbed
phase as ψ0 = ω0 t, and time averaging as ·, the photodiode measures an
intensity

7.5 Interferometer detectors

151

Fig. 7.14. The Laser Interferometer Space Antenna (LISA), a joint ESA-NASA
project, to be launched around 2015. From LISA System and Technology Study
Report ESA-SCI(2000)11.

Iout = [A cos(ψ0 + δψ + α) + A cos(ψ0 − δψ)]2 
1
2 I0

[1 + cos(2δψ + α)] .

(7.37)

The phase diﬀerence α between the beams is a matter of ﬁne tuning the arm
length. For zero phase diﬀerence the detector sees the full laser power I0 so,
ignoring optical losses, I0 = Iout = (2A cos ψ0 )2  = 2A2 . Relation (7.37) says
that Iout = 0 for α = π and δψ = 0, but optical imperfections will prevent
complete nulling and we should expect rather a dark signal Iout = I0 . So
Iout

1
2 I0

[1 +  + cos(2δψ + α)]

(7.38)

is more realistic. For example, an imbalance δA in the beam amplitudes can be
shown to imply  = 12 (δA/A)2 for α = π. The interferometer should operate
close to α = π because otherwise the detector sees a large fraction of I0 and the
associated laser noise, from which the small superposed signal can no longer
be extracted. But at α = π we have Iout I0 [ + 2(δψ)2 ] which is even worse
since, as derived above, δψ 2×1012 γ 2×10−9 for γ = 10−21 . The signal is
distorted, ∝ (δψ)2 , and so small that it would drown in the dark current. The
solution is rapid phase modulation around α = π. Phase modulators between
the beamsplitter and the ﬁrst mirrors (not shown in Fig. 7.9) add a phase
φ sin ωm t to one beam and −φ sin ωm t to the other. We take α = π, and since
δψ
φ
1 we expand to ﬁrst order in δψ and to second order in φ :
Iout

1
2 I0

[1 +  + cos(2δψ + 2φ sin ωm t + π)]

152

7 Gravitational waves
1
2 I0

[ + φ2 − φ2 cos 2ωm t + 4φ δψ sin ωm t] .

(7.39)

To see that this a much better arrangement, let’s take  ∼ φ2 ∼ 10−6 . The
dark signal is ∼ 10−6 I0 and has a zero and a double frequency component.
The signal δψ is now encoded as the amplitude of a periodic signal at the
modulation frequency (which is in the MHz range). This is a great advantage.
The modulation depth is 4φ δψ/( + φ2 ) ∼ 2δψ/φ ∼ 4 × 1015 γ ∼ 4 × 10−6 for
γ = 10−21 , which is small but not impossible. The diﬀerent frequency dependence allows easy separation of the various components. The phase modulation
has an important extra bonus in that it is very eﬀective in suppressing certain
types of noise.
Suppose we want to keep the phase diﬀerence α constant at the 10−3 radian level. Since the laser wavelength λ is 1 µm, that corresponds to a distance
of only 10−3 λ/2π ∼ 0.1 nm over an arm length of 4 km! It follows that an
active phase locking system is indispensable, as the seismic perturbations are
much larger. The question is how that can be done without disturbing the
measurements. The trick is, brieﬂy, to reset the phase at a rate that is outside
the measuring bandwidth (Fig. 7.11). For more information on these issues
and many other experimental ﬁnesses and complications we refer to Blair
(1991) and Saulson (1994).

Exercise 7.9: Check the details of the derivation of (7.37) and (7.39).
Hint: Take a = ψ0 + δψ + α and b = ψ0 − δψ and use cos a + cos b = 2 cos[(a +
b)/2] cos[(a − b)/2]; (a + b)/2 is a fast variable and cos2 [(a + b)/2] = 12 →
Iout = 2A2 cos2 [(a − b)/2]. Use 2 cos2 x = 1 + cos 2x → Iout = A2 [1 + cos(a −
b)]. For (7.39) write a = 2δψ and b = 2φ sin ωm t. Then cos(a + b + π) =
b
1. Finally
− cos(a + b) −[1 − 12 (a + b)2 ] −1 + 12 (2ab + b2 ), since a
2 sin2 x = 1 − cos 2x.

Projects under development
Interferometers for the detection of gravitational radiation are in an advanced
state of development. The two most important are the LIGO project (USA),
Fig. 7.8,8 and the Italian/French Virgo project, a single 3 km interferometer
8

Abramovici, A. et al., Science 256 (1992) 325; Barish, B.C. and Weiss, R., Physics
Today, October 1999, 44; and http://www.ligo-wa.caltech.edu.

7.5 Interferometer detectors

153

under construction at Cascina near Pisa.9 These projects should be taking
science data on a regular basis within a few years. Two smaller projects are the
British-German GEO-600, and the Japanese TAMA-300 (both operational).
The seismic background renders measurements below ∼ 10 Hz impossible
on Earth. Detection of low frequency gravitational waves must be done from
space. Through Doppler tracking of the ULYSSES and GALILEO spacecraft
an upper limit of γ ≤ 10−15 has been set in the range 0.1 − 10 mHz. ESA and
NASA are studying the ambitious LISA project (Laser Interferometer Space
Antenna),10 see Fig. 7.14.

9

10

Ricci, F. and Brillet, A., Annu. Rev. Nucl. Part. Sci. 47 (1997) 111, and
http://www.virgo.infn.it/
LISA System and Technology Study Report, ESA-SCI(2000)11, July 2000;
websites: http://sci.esa.int/categories/futureprojects/ and
http://lisa.jpl.nasa.gov.

8
Fermi-Walker Transport

In § 2.4 we investigated parallel transport of a vector along an arbitrary worldline xµ (s). The motivation was that we should be able to compare, at diﬀerent
places along the orbit, the vectors associated with a point mass, such as the
speed or the spin. The vectors are supposed to be known along the orbit,
and we compare the vector A with A , obtained by parallel transport, see
Fig. 2.4. If these two do not coincide we say that the vector has intrinsically
changed due to inﬂuences other than gravity. The actual change of the vector
A along the worldline is a matter of studying the dynamics. We know that the
4-velocity uµ is by deﬁnition tangent vector and uµ uµ = 1, but the change of
the spin vector for example depends on the applied torque. Here we analyse
a seemingly innocuous question: a spinning top moves along a worldline that
is not a geodesic, i.e. the top experiences an acceleration, but there are no
external torques. How does the spin axis behave? The result will be used to
derive the Thomas precession of the electron and the geodesic precession of a
gyroscope.

8.1 Transport of accelerated vectors
A test mass moves along its worldline W due to gravity and other forces, and
xµ (s) is determined by eq. (3.60), see Fig. 8.1. Now imagine that the test mass
carries orthonormal unit vectors, the 4-velocity uµ and nµi (i = 1, 2, 3). In the
local rest-frame uµ = (1, 0, 0, 0). The nµi = (0, ni ) are spacelike, nµi njµ = −δij
and uµ niµ = 0. The unit vectors ni may be thought of as deﬁned by the
spin axes of ideal precession-free gyroscopes (no external torques). Having
deﬁned the physical situation in the rest-frame, we now seek a mathematical
description of the change or ‘transport’ of uµ and nµi , or rather of Aµ (a linear
combination of uµ and the nµi ) along xµ (s) in an arbitrary reference frame. We
surmise that the transport law is a generalisation of parallel transport, and
try to achieve our goal with an extra term in (2.28). Accordingly, we deﬁne
the following operator on xµ (s):

156

8 Fermi-Walker Transport

ua

W

G
R

va

Q

ua

ua

P
Fig. 8.1. Introducing Fermi-Walker transport. If there is only gravity, a test mass
with initial 4-velocity uα in P moves on a unique geodesic G, but in the presence
of additional non-gravitational forces it moves on a non-geodesic worldline W . The
4-velocity uα = dxα /ds is always tangent to G and to W , and uα uα = 1 (as always).
Parallel transport DAα /Ds = 0 along G carries uα (P ) over into uα (Q) because G
is a geodesic. But parallel transport along W produces some v α (R) = uα (R). We
seek a generalised (Fermi-Walker) transport law δAα /δs = 0 that carries uα over
into itself and preserves the value of the inner product Aα Bα of two vectors along
an arbitrary worldline.

DAµ
δAµ
≡
− K µα Aα .
δs
Ds

(8.1)

D/Ds is the operator (2.26) for parallel transport. We lower the index on
the right hand side of (8.1) by multiplying with gνµ . The result is DAν /Ds −
Kν α Aα (see exercise), and thus we deﬁne for covariant vectors
δAν
DAν
≡
− Kν α Aα ,
δs
Ds

(8.2)

where DAν /Ds is now given by (2.27). The transport law would then be
δAµ
= 0
δs

or

δAν
= 0,
δs

(8.3)

for contravariant and covariant vectors, respectively. With the help of (8.1)
and (2.26) we obtain
δAµ
dAµ
≡
− (K µν − Γµνσ uσ ) Aν = 0 .
δs
ds

(8.4)

This is the explicit form of the so called Fermi-Walker transport law for a
contravariant vector. In order to be able to handle tensors of higher rank we
deﬁne for two vectors X and Y , conform relation (2.44):
δ
δX
δY
XY =
Y +X
.
δs
δs
δs

(8.5)

We now proceed to determine the tensor K µν . The inner product Aµ Bµ
of two vectors Aµ and B µ (i.e. two linear combinations of uµ and the

8.1 Transport of accelerated vectors

157

nµi ) is constant in the local rest-frame. But Aµ Bµ is scalar and therefore
one and the same constant in all frames. This implies according to (2.47)
that DAµ Bµ /Ds = dAµ Bµ /ds = 0, though DAµ /Ds and DB µ /Ds in general do not vanish since they are not parallel-transported. We elaborate
0 = δ(Aµ Bµ )/δs ≡ (δAµ /δs)Bµ + Aµ (δBµ /δs) :
0 = Aµ

=

DBµ
DAµ
+ Bµ
− Aµ Kµα Bα − Bµ K µα Aα
Ds
Ds

D
(Aµ Bµ ) − K µα Aµ Bα − K µα Aα Bµ
Ds

= − (K µα + K αµ )Aµ Bα .

(8.6)

It follows that K µν must be antisymmetric, K µα = −K αµ . It seems natural
to expect that K µα depends on the 4-velocity, and therefore we try
K µν = aµ uν − uµ aν ,

(8.7)

for a certain vector aµ . A component of aµ parallel to uµ does not contribute
to (8.7), so we may impose without restriction that
aµ uµ = 0 ,

(8.8)

K µν uν = aµ .

(8.9)

and then we also have that

The unknown vector aµ may be found by requiring that uµ obey the transport
law δuµ /δs = 0. With the help of (8.1), (8.8) and (8.12) we get:
0 =

Duµ
Duµ
− (aµ uα − uµ aα ) uα =
− aµ ,
Ds
Ds

(8.10)

because of (8.8) and uα uα = 1. Consequently:
aµ =

Duµ
.
Ds

(8.11)

By comparing with (3.60) we see that aν is equal to the non-inertial acceleration f µ of P divided by m0 c2 .
One might object that expression (8.7) is not the most general choice, and
that
K µν = aµ uν − uµ aν + H µν
(8.12)

158

8 Fermi-Walker Transport

with antisymmetric H µν would also satisfy the requirements. We now show
that H µν = 0 implies the absence of any rotation of spatial vectors in the
local rest-frame, hence absence of external torques. To that end we study the
change of a purely spatial vector nµ in the local rest-frame, where nµ = (0, n)
and uµ = (1, 0, 0, 0), so that nµ uµ = gµν nµ uν = ηµν nµ uν = 0, as before. The
Christoﬀel symbols are also zero, and Fermi-Walker transport δnµ /δs = 0
implies
dnµ
= (aµ uν − uµ aν ) nν = − uµ aν nν .
(8.13)
ds
It follows that dni /ds = 0 : the instantaneous rate of change of the spatial
part of nµ is zero, so that there is no instantaneous rotation (but there would
be one if H µν = 0).
This completes the derivation of the Fermi-Walker transport law (8.4),
with K µν given by (8.7), (8.11) and uµ = dxµ /ds. It is a diﬀerential equation
specifying the change of an accelerated vector Aµ on which no torques are
exerted in the local rest-frame. We note the following:
(1). The middle term on the right hand side of (8.4) is of special-relativistic
origin. In SR the Γ’s are zero (in rectangular co-ordinates) but K µν = 0. This
term is responsible for the Thomas precession.
(2). The last term in (8.4) is a general-relativistic eﬀect. If the only force is
gravity, then xµ is a geodesic → Duµ /Ds = 0 → aµ = 0 → K µν = 0. And
in that case eq. (8.4) is identical to parallel transport. One of the consequences
is the geodesic precession. Any additional (non-inertial) force causes an extra
Thomas-like precession.

Exercise 8.1: We are using a spacelike unit vector nµ with nµ nµ = −1.
Negative lengths, how is that again?
Hint: Very simple. For example, in the local rest-frame nµ = (0, n1 , n2 , n3 ) and
nµ = ηµν nν = (0, −n1 , −n2 , −n3 ). The value of the scalar nµ nµ = −|n|2 = −1
is invariant.

Exercise 8.2: Prove the statement between (8.1) and (8.2).
Hint: § 2.6: gνµ DAµ /Ds = gνµ Aµ:σ uσ = (gνµ Aµ ):σ uσ = Aν:σ uσ = DAν /Ds.
Furthermore, gνµ K µα Aα = Kνα Aα = Kν α Aα .
Exercise 8.3: Show that aµ uµ is indeed zero.

8.2 Thomas precession

159

Hint: aµ uµ = 12 uµ Duµ /Ds + 12 uµ Duµ /Ds = 12 D(uµ uµ )/Ds = 0. This last step
requires that uµ Duµ /Ds = uµ Duµ /Ds. See previous exercise for inspiration.

8.2 Thomas precession
This is a problem from SR, and the qualitive explanation has already been
given in § 1.1. An electron moves in a circular orbit in the x1 , x2 plane.
Spacetime is ﬂat and we use Cartesian co-ordinates so that all Γ’s are zero.
According to (8.4), Fermi-Walker transport of the spin vector sµ is described
by
dsµ
= c K µν sν ,
(8.14)
dτ
because d/ds = (1/c)d/dτ . To determine K µν we analyse the circular motion
of the electron and take
x1 = r cos ωτ ;
from which

x2 = r sin ωτ ;

x3 = 0 ,

⎫
u1 = c−1 dx1 /dτ = − (ωr/c) sin ωτ ; ⎪
⎪
⎪
⎬
2
u = (ωr/c) cos ωτ ;
⎪
⎪
⎪
⎭
u3 = 0 .

(8.15)

(8.16)

Here ω is the orbital frequency measured in the proper time of the electron;
u0 can be obtained from 1 = uµ uµ = ηµν uµ uν = (u0 )2 − (u1 )2 − (u2 )2 :
u0 =

1 + (ωr/c)2 = constant ,

(8.17)

and this serves to ﬁnd the relation between proper time τ and laboratory time
t, because u0 = γ = 1/ 1 − β 2 , see (3.23). Therefore ωτ = ωt/γ ≡ Ωt, where
Ω = orbital frequency in laboratory time:
γ =

1 + (ωr/c)2 ;

1 d
1 d
=
.
ω dτ
Ω dt

⎫
Ω = ω/γ ; ⎪
⎬
⎪
⎭

(8.18)

Since the Γ’s are zero, we infer from (8.11) and (2.26) that aµ = Duµ /Ds =
c−1 duµ /dτ . We may now write (8.14) as:

160

8 Fermi-Walker Transport

s

er

eϕ

s

x2

ϕ

eϕ
er

eθ
x1

eθ
Fig. 8.2. Geodesic precession of the vector s analysed in the equatorial plane θ =
π/2 of the rotating reference frame er , eθ , eϕ .

dsµ
duν ν
= c aµ uν − uµ aν sν = − uµ
s ;
dτ
dτ

(8.19)

uν sν = 0 ,

(8.20)

because we know that uν sν is constant (Fermi-Walker transport), and that
uµ = (1, 0, 0, 0) and sµ = (0, s) in the local rest-frame, so that uν sν =
ηνα uα sν = 0. Because uν sν is invariant (8.20) holds in any frame. Since
u3 = 0 we conclude from (8.19) that ds3 /dτ = 0, or
ds3
= 0.
dt

(8.21)

Apparently, the z-component of the spin is constant. The behaviour of s0 follows from (8.20): 0 = ηνσ uσ sν = u0 s0 −u1 s1 −u2 s2 → s0 = (u1 s1 +u2 s2 )/u0 .
However, s0 has no physical meaning – its ‘function’ is to ensure that uν sν
and sν sν are constant. The physics is in the behaviour of s1 and s2 . With
(8.16) and ui = ηiν uν = −ui we obtain:
d
dτ

% 1&
s
s2

ω 3 r2
=
c2

%
sin ωτ cos ωτ
− cos2 ωτ

sin2 ωτ

& % 1&
s

− sin ωτ cos ωτ

s2

.

(8.22)

Express this in laboratory time with (8.18):
d
dt

% 1&
s
s2

= (γ 2 − 1)Ω

%
sin Ωt cos Ωt

sin2 Ωt

& % 1&
s

− cos2 Ωt − sin Ωt cos Ωt

s2

.

(8.23)

8.3 Geodesic precession

161

Exercise 8.4: Verify that the solution of (8.23) with initial values s1 (0) = s
and s2 (0) = 0 is given by
s1 =

1
2s

s2 =

1
2s

Expand for β

'

(
(1 + γ) cos(1 − γ)Ωt + (1 − γ) cos(1 + γ)Ωt ;

'

(
(1 + γ) sin(1 − γ)Ωt + (1 − γ) sin(1 + γ)Ωt .

(8.24)

1:
s1

'
s cos 12 β 2 Ωt −

1 2
4β

(
cos 2Ωt ;

s2

'
− s sin 12 β 2 Ωt +

1 2
4β

(
sin 2Ωt .

(8.25)

Verify that the ﬁrst terms in (8.24) and (8.25) correspond to a rotation of the
spin vector with a frequency
Ω Thomas = (γ − 1) Ω orbit

1 2
2β

Ω orbit ,

(8.26)

with β ωr/c Ωr/c
1. The sense of the rotation is opposite to the orbital
rotation. Both second terms in (8.25) describe a small, fast modulation that
averages to zero.

8.3 Geodesic precession
In § 4.4 we analysed the motion of a test mass moving in the Schwarzschild
metric, and found, among other things, that the orbit precesses. This precession of the perihelium is not the only GR eﬀect. If the test mass behaves as
a vector, as for example a gyroscope, the (spin) vector will also perform a
precession, even when no torque is exerted. We shall now derive this so-called
geodesic precession. Because the body moves along a geodesic we have that
K µν = 0, in which case (8.4) reduces to the equation for parallel transport:
dsµ
+ c Γµνσ uσ sν = 0 .
dτ

(8.27)

Here sν is the unit vector along the spin axis. The following analysis is a
sequel of § 4.3, and we shall employ the notation we used there. The 4-velocity
uµ = dxµ /ds is given by:1
1

For the geodesic precession the rotation of the Earth is irrelevant. So although
Fig. 8.3 suggests otherwise, the satellite may be taken move on the equator r, θ =
constant of the Schwarzschild metric.

162

8 Fermi-Walker Transport

Fig. 8.3. A gyroscope orbiting a rotating mass like the Earth and moving only under
the inﬂuence of gravity should exhibit a geodesic precession and a Lense-Thirring
precession. The experiment is now in progress in the Gravity Probe B satellite,
launched in April 2004 into a polar orbit of 640 km altitude. The star IM Pegasi
(HR 8703) serves as the pointing reference. See text for details. Adapted from: Near
Zero, J.D. Fairbank et al. (eds.) Freeman & Co (1988).

uµ = (cṫ, ṙ, θ̇, ϕ̇) = (cṫ, 0, 0, h/r2 )


−1/2
1/2 
rrs /2
3rs
1
=
1−
, 0, 0, 2
.
2r
r
1 − 3rs /2r

(8.28)

At the second = sign we choose a circular orbit: r = constant and θ = π/2, and
we have used (4.34) as well. The last expression in (8.28) follows immediately
from (4.32) and (4.45). Next we write out (8.27) explicitly, and obtain the
following equations (see exercises):
s0 =

rrs /2 3
s ;
1 − rs /r

ds1
c
=
dτ
r

rrs /2

(8.29)

1 − 3rs /2r s3 ;

ds2
= 0;
dτ
c
ds3
= − 3
dτ
r

(8.30)

(8.31)


rrs /2
1 − 3rs /2r

1/2
s1 .

(8.32)

8.3 Geodesic precession

163

Take d/dτ of (8.32) and eliminate ds1 /dτ with (8.30):
d2 s3
c2 rs 3
+
s = 0,
(8.33)
dτ 2
2r3
and it is easy to verify that the same equation holds for s1 . The solution with
initial value s3 (0) = 0 is:
⎫
s3 = sϕ = − s sin ωτ ;
⎪
⎪
⎪
⎬
1
r
(8.34)
s = s = sr 1 − 3rs /2r cos ωτ ;
⎪
⎪
⎪
⎭
s2 = sθ = constant ,
where


ω = c

rs
2r3



1/2
=

GM
r3

1/2
.

(8.35)

The geodesic precession is a consequence of the fact that the precession frequency ω is a little smaller than the orbital frequency, which is equal to

1/2 
−1/2
rs
3rs
2π
= c
.
(8.36)
1
−
∆τ
2r3
2r
Here we have used expression (4.46) for the orbital period ∆τ . After each
orbit the spin vector has rotated over an angle of
ω∆τ = 2π

1 − 3rs /2r .

(8.37)

The spin vector precesses about an axis orthogonal to the orbital plane, but
the major part of the precession is caused by the fact that the reference
frame itself rotates over an angle of 2π, see Fig. 8.2. When viewed from a
non-rotating frame the precession angle per orbit equals


3πrs
.
(8.38)
δψ = 2π 1 − 1 − 3rs /2r
2r
Actually, we must still transform to co-ordinate time, but that gives rise to a
correction of higher order. The precession has the same sense of rotation as
the orbit. The physical origin of the precession is that a vector that is parallel
transported constantly changes its direction, due to the curvature of spacetime, see § 2.4. This is visible as a small secular angular rotation. The eﬀect
of geodesic precession has been observed in the binary pulsar PSR 1913+16. 2
What if the central object rotates? In that case its exterior metric is replaced
by the Kerr metric (in good approximation), and frame-dragging (§ 6.5) induces an additional precession, called the Lense-Thirring eﬀect. The LAGEOS
satellites have conﬁrmed the Lense-Thirring eﬀect due to the rotation of the
Earth with a precision of 10%.3
2
3

Weisberg, J.M. and Taylor, J.H., Ap. J. 576 (2002) 942.
Ciufolini, I. and Pavlis, E.C., Nature 431 (2004) 958.

164

8 Fermi-Walker Transport

Fig. 8.4. Inside view of a gyroscope of Gravity Probe B and its housing. The rotor
has a diameter of 3.8 cm, and is made of fused quartz coated with niobium. Image
credit: Don Harley.

8.4 Gravity Probe B
The technology for high-precision measurements of the geodesic precession
and the Lense-Thirring eﬀect has been developed in the USA from the beginning of the 1960s. The outcome of this long development programme, the
longest in NASA’s history to date4 , is Gravity Probe B, launched on April 20,
2004, see Fig. 8.3. The satellite carries 4 precision gyroscopes. The geodesic
precession is only 6.6 per year, and the Lense-Thirring precession is much
smaller: 0.04 per year. The gyros consist of quartz rotors coated with superconducting niobium, suspended in an electrostatic ﬁeld, see Figs. 8.4 and 8.5.
The rotation (about 70 Hz) induces a London magnetic moment that generates a magnetic dipole ﬁeld aligned with the spin axis. Its direction, and hence
the orientation of the spin axis can be measured with high precision. 5 There
are many experimental complications. For example, any parasitic torque will
cause the gyroscope to precess, and any non-inertial acceleration induces an
extra Thomas precession. By using a drag-free satellite that literally follows
the inertial motion of one of the the gyroscopes, the residual acceleration will
4

5

For the programmatic and scientiﬁc issues involved see Reichhardt, T., Nature
426 (2003) 380.
For more details see Near Zero, J.D. Fairbank et al. (eds.), Ch. 6.1−6.3 (Freeman
& Co 1988); for theoretical aspects see Will (1993) p. 208; Gravity Probe B
website: http://einstein.stanford.edu/

8.4 Gravity Probe B

165

Fig. 8.5. Gravity Probe B carries four gyroscopes, mounted in a single quartz bloc,
a prototype of which is shown here. The pointing telescope (not shown) is attached
to the ﬂange at the lower end. The whole unit is placed in a much larger helium
dewar. Image credit: Gravity Probe B, Stanford University.

be at the 10−11 g level. The gyroscopes have a pointing stability of better than
5 × 10−4 arcseconds over a period of a year!
In closing, we draw attention to two issues. The ﬁrst is the fact that the
precession angle (8.38) is independent of the spin rate of the gyroscope, and
the same is true for the Lense-Thirring precession.6 This is a reminder of
the physics involved: both eﬀects are a consequence of parallel transport of a
vector in the Schwarzschild or Kerr metric. The nature of the vector is immaterial, and so is the existence of mass currents in the gyroscope. A gyroscope is
for many reasons by far the best technical solution, but a non-rotating pencil
would, as a matter of principle, also do very well – if one could eliminate all
parasitic forces and moments.
6

See Will (1993) p. 210.

166

8 Fermi-Walker Transport

The second issue is the pointing reference. Stellar parallaxes and proper
motions are generally larger than the accuracy required for Gravity Probe B.
Therefore the only suitable pointing references are quasars. Quasars are distant powerful radio sources that are believed to constitute the best available
inertial reference frame. But quasars are too dim in visible light for the small
pointing telescope (aperture 14 cm). Therefore a relatively bright star had to
be found, that is also a strong radio point source, and located suﬃciently close
to a few reference quasars to permit measuring the relative positions with the
method of Very Long Baseline Interferometry (VLBI). The outcome is IM Peg
(HR 8703). The proper motion and parallax of IM Peg with respect to the
quasars have been accurately measured in a VLBI programme extending over
many years. In this way the orientation of the gyroscopes can ultimately be
related to the quasar reference frame.

Exercise 8.5: Write down the explicit expression for the Christoﬀel symbols
necessary to elaborate (8.27).
Hint: From (4.29): 2ν = −2λ = log (1 − rs /r); furthermore θ = π/2. Result:
(4.10) :

Γ100 =

rs
(1 − rs /r) ;
2r2

(4.11) :

Γ212 =

1
;
r

Γ233 = 0 .

(4.12) :

Γ313 =

1
;
r

Γ323 = 0 .

Γ133 = −r (1 − rs /r) .

Exercise 8.6: Show that uµ sµ = 0 holds here as well, just as in the case of
Thomas precession. Use that to derive (8.29).
Hint: 0 = gµν uµ sν = g00 u0 s0 + g33 u3 s3 ; use (4.29) and θ = π/2.
Exercise 8.7: Prove now eqs. (8.30) to (8.32).
Hint: Insert the Γ’s, and u0 and u3 from (8.28), and use (8.29).

Exercise 8.8: Show that a gyroscope in orbit around the Earth at an altitude
of 650 km has a geodesic precession of 6.6 per year.
Hint: (8.38) + Keplerian orbit → 3(GMa )3/2 /(2c2 r5/2 ) rad s−1 , etc.

8.4 Gravity Probe B

167

Exercise 8.9: We wish to compare the precession amplitudes along er and
along eϕ , see Fig. 8.2. But that is not possible as s1 and s3 in (8.34) have
diﬀerent dimensions. How is that?
Hint: Physical lengths follow from (3.7)! Amplitudes along the r-direction:
2
= −gϕϕ (sϕ )2 = r2 s2 .
dlr2 = −grr (sr )2 r2 (1 − rs /2r)s2 ; dlϕ
Exercise 8.10: Does a linearly accelerated electron experience any Thomaslike eﬀect?
Hint: Take the 1-axis in the direction of the acceleration, then u2 = u3 = 0.
According to (8.19) only s0 and s1 will change. To see what actually happens,
assume that the electron experiences a constant acceleration a, and use that
x1 = (c2 /a) cosh(aτ /c) + const, x0 = ct = (c2 /a) sinh(aτ /c), see Rindler
(2001), so that u0 = cosh(aτ /c) and u1 = sinh(aτ /c). Now solve (8.19).

9
The Robertson-Walker Metric

Cosmology is the science that addresses the large-scale structure and evolution
of the universe. Why would that require the framework of GR? Because the
universe as a whole may be regarded as a compact object – in the sense that
its ‘radius’ R is comparable to its Schwarzschild radius! From (4.28) we see
that R ∼ rs if R ∼ 2GM/c2 . Now take M = (4πR3 /3) ρ and use for R the
Hubble radius c/H0 . This is the distance where the expansion speed becomes
formally c according to the primitive Hubble law (9.4). Result:
ρ ∼

3H02
≡ ρc .
8πG

(9.1)

The density of the universe should be comparable to the critical density ρc ,
a concept that will be explained later. And Table 9.2 shows that this is indeed the case. This argument, simple as it may be, does indicate that only
description in terms of GR may be expected to produce meaningful results.
In this chapter we shall review the most important observations, the form of
the metric, the spatial structure of the universe, and the equation of motion
for the scale factor S.

9.1 Observations
On a cosmological scale the smallest relevant unit is a galaxy. Galaxies occur
in aggregates called groups. Our own galaxy and the large spiral galaxy M31
in Andromeda (distance 770 kpc) are the two biggest members of the Local
Group, which has approximately 40 members. Groups in turn form clusters.
Table 9.1 gives some characteristic sizes and distances. From redshift surveys,
Fig. 9.1, it is apparent that matter is distributed in a ﬁlamentary fashion, in
concentrations of widely varying size, with 90% of the matter located in walls,
strings and sheets that occupy a relative volume of the order of 10%, while
90% of space is virtually empty (‘voids’). During the past century there has
been an intense debate on the relative densities of various forms of matter

170

9 The Robertson-Walker Metric
Table 9.1. Characteristic length scales
1 − 50
1
10
100
3
4.5
10

galaxy
group
cluster
distances between clusters
most distant clusters
distance to quasars
distance to horizon

kpc
Mpc
Mpc
Mpc
Gpc
Gpc
Gpc

in the universe. This debate has recently culminated in the publication of
the results of several surveys among which those of the WMAP mission,1 see
Table 9.2. The densities are expressed in terms of the critical density ρc :
ρc =

3H02
= 1.88 × 10−29 h2
8πG

g cm−3

10−29 g cm−3 ,

(9.2)

and h is the Hubble constant in units of 100 km s−1 Mpc−1 , see (9.5). The
present matter energy density is
m = Ωm ρc c2 = 2.4 × 10−9 erg cm−3 .

(9.3)

Only ∼ 2% of all matter in the universe can actually be seen because it is luminous. The remaining 98% is dark, where dark traditionally means optically
dark. It is only indirectly visible through the gravity it exerts, for example
in the rotation curves of galaxies, and in the velocity distribution of galaxies in clusters. To prevent the latter from ﬂying apart they should contain a
lot more matter than we see. Dark matter consists partly of baryons (mainly
hot gas in and between clusters, but also brown dwarfs, old white dwarfs,
etc.). About 80-90% of all baryons is dark. Some of this baryonic dark matter is now beginning to be seen in UV and X-rays.2 But baryons comprise
only a small fraction of all dark matter. Non-baryonic dark matter consists
of weakly interacting massive particles (WIMPs) of unknown identity.3 The
largest constituent in Table 9.2 is dark energy (not be confused with dark
matter), associated with the cosmological constant, whose nature is not understood. It seems unlikely that the debate on the values in Table 9.2 has
1

2

3

Wilkinson Microwave Anisotropy Probe, see Bennett, C.L. et al., Ap. J. S. 148
(2003) 1, and following papers.
Nicastro, F. et al., Nature 433 (2005) 495 and 421 (2003) 719; Kaastra, J.S. et
al., A&A 397 (2003) 445.
WMAP excludes the possibility that they are massive neutrinos, since it ﬁnds
Ων < 0.015. Current contenders are the neutralino (the lightest supersymmetric particle) and axions. For detection attempts see Sumner, T.J., Living Rev.
Relativity 5 (2002) 4.

9.1 Observations
Table 9.2. Relative densities of matter and energy in the universe
b

Type

Ω = ρ/ρc

Matter (Ωm )
- luminous baryons
- dark baryons
- non-baryonic dark matter
Dark energy (ΩΛ )
Total (Ωm + ΩΛ )

0.27 ± 0.04
0.006 ± 0.003
0.038 ± 0.003
0.23 ± 0.04
0.73 ± 0.04
1.02 ± 0.02

a
b
c

171

a

Comment
consists of 3 components
} total baryons:
} Ωb = 0.044 ± 0.004
unknown WIMP c
unknown origin, § 9.5
geometry of universe is ﬂat

See 1 and Fukugita, M. and Peebles, P.J.E., Ap. J. 616 (2004) 643;
ρc = 3H02 /8πG = 1.88 × 10−29 h2  10−29 g cm−3 ;
WIMP = Weakly Interacting Massive Particle.

really ended. The WMAP results conﬁrmed the prevailing theoretical prejudice of the day and were quickly canonized. We shall follow suit, but note that
the future may hold surprises.
An important observation is that the universe is isotropic. The distribution of matter in space is statistically the same in all directions, also as a
function of distance, i.e. within redshift subclasses. There are obvious evolution eﬀects. The morphology of the systems changes gradually with distance,
and at large distances we see only quasars, objects 102 − 103 times brighter
than the average nearby galaxy. The Hubble Deep Field observations illustrate clearly that the universe did look quite diﬀerent in the past.4 Hubble
demonstated in 1929 that the universe expands. All galaxies move away from
us on average with a velocity proportional to the distance, but independent
of direction. This universal expansion is referred to as the Hubble ﬂow:
v = H0 d ,
with
H0 = 100 h

km s−1 Mpc−1

and

(9.4)
h = 0.71 ± 0.04 ,

(9.5)

as measured by WMAP. 1 In fact one measures a redshift z rather than a
velocity. The precise meaning of v and d in (9.4) will be explained in § 11.3.
In physical units:
(9.6)
H0 = (2.3 ± 0.1) × 10−18 s−1 .
The peculiar velocities of the systems, i.e. the deviations from the Hubble
−1
ﬂow, are generally small, <
∼ 500 km s . The Hubble ﬂow is ‘cold’ and this is
because the universe cools adiabatically as it expands.
4

Driver, S.P. et al., Ap. J. 496 (1998) L93; Ferguson, H.C. et al., A.R.A.A. 38
(2000) 667.

172

9 The Robertson-Walker Metric

Fig. 9.1. The 2dF galaxy redshift survey comprises about 220, 000 galaxies and
shows that the distribution of matter in the universe is homogeneous at large, but
clumpy on smaller scales. The left slice measures 75◦ × 10◦ and is located in the
Northern galactic hemisphere, the right slice is 80◦ ×15◦ near the galactic South pole.
Picture taken from the 2dFGRS image gallery. See Colless, M. et al., M.N.R.A.S.
328 (2001) 1039; Peacock, J.A. et al., Nature 410 (2001) 169.

Fig. 9.2. ‘Baby picture’ of the universe: WMAP image of the Cosmic Microwave
Background at λ = 3.2 mm. Monopole and dipole have been subtracted but the
galactic foreground has not. Color coding: black = −200 µK, red = +200 µK. The
minute temperature variations indicate clustering of matter in the early universe.
This is analysed in §§ 10.4 and 11.4. From Bennett, C.L. et al., Ap. J. S. 148 (2003) 1.

9.1 Observations

173

In addition to matter, the universe contains all kinds of radiation, of
which the cosmic microwave background (CMB) has by far the largest energy
density. This radiation had been predicted by Gamov and coworkers in 1948
(T ∼ 5 K), as a remnant of a hot early stage of the universe, and was discovered
by Penzias and Wilson in 1965. Observations of the COBE satellite have
shown that the spectrum is to high accuracy a thermal Planck spectrum in
the wavelength range from 10 cm to 0.1 mm with a maximum at λ ∼ 2 mm.
Temperature and energy density are:
T = 2.725 ± 0.002 K ;
r =

4σ 4
T
c

4.19 × 10−13 erg cm−3 .

(9.7)
(9.8)

The CMB has a dipole anisotropy of |∆T |
3.35 ± 0.02 mK, and this is
interpreted as a Doppler shift due to the velocity of the solar system of 369
km s−1 towards galactic co-ordinates (, b) = (264◦ , 48◦ ) with respect to the
frame deﬁned by radiation.5 After subtraction of the dipole component the
◦
CMB is highly isotropic, ∆T /T 10−5 on angular scales >
∼ 7 (COBE), and
◦
>
WMAP has improved that to angular scales ∼ 0.2 .
On theoretical grounds there should also exist a neutrino background with
a temperature and energy density comparable to those of the CMB (§ 12.2).
If we add that to (9.8), the total radiation energy density is:
r

7 × 10−13 erg cm−3 .

(9.9)

The conclusion seems obvious: the universe is a space of vast expanse, extremely cold (2.7 K), and to our standards almost empty. It is isotropic and
evolves with time. An important aspect of the evolution is the expansion,
14 Gyr ago.
which should have begun approximately a Hubble time H0−1
The microwave background is a remnant of a hot early stage of the universe,
called the Big Bang. For an extensive discussion of the observations see for
example Peebles (1993) and Peacock (1999). We return to observational issues
in Ch. 11.

5

This velocity in turn induces an aligned dipole asymmetry in the observed matter
distribution, see Blake, C. and Wall, J., Nature 416 (2002) 150.

174

9 The Robertson-Walker Metric

time
space

A0

B0

C0

t = t0

B1

C1

t = t1

B2

C2

t = t2

giant
cleft

not observable
for A0
S(t1)
not yet
observable
for A0

S(t2)
t=0

Fig. 9.3. Co-ordinate picture of the spacetime of the universe. Our present position
is A0 , and shown are our past light-cone, the worldlines of a few galaxies (vertical
lines), and a hypothetical inhomogeneity (‘giant cleft’) that we might get to see in
the future.

9.2 Deﬁnition of co-ordinates
Fig. 9.3 shows a spacetime diagram of the universe. We (A0 ) are only able to
see events located on our past light-cone. We experience our light-cone as a
series of nested, ever larger concentric spherical shells around us, showing an
increasingly younger section of the universe. Because of the observed isotropy,
each shell Σ(ti ) must be on average homogenous. Due to our limited technological capabilities we have not yet been able to detect signals from the early
universe, i.e. from the most distant shells. We now make an assumption about
the part of spacetime that is outside our past light-cone and therefore unobservable. To that end we use the cosmological principle, which states that we
(A0 ) occupy no special position in the universe, and that other observers B0 ,
C0 in Fig. 9.3 see on average the same universe as we do. Hence if we translate
our light-cone sideways, the aspect of the shells Σ(ti ) would not change, apart
from statistical ﬂuctuations (the so-called cosmic variance). The implication
is that every subspace t = constant is isotropic and homogenous on average.
Cosmological principle and the isotropy of the universe imply that it is homogeneous.
We now come to the deﬁnition of rest (xi = constant). We are free to
adopt any deﬁnition we like, but there is one that stands out as very natural:
a test mass is at rest if it does not move with respect to the Hubble ﬂow.
That means that the spatial co-ordinates of galaxies are constant (we shall

9.2 Deﬁnition of co-ordinates

um

A0

175

A

L

nm

D

Fig. 9.4. Introducing Gaussian co-ordinates in the spacetime of the universe. The
starting point is a 3-dimensional subspace of spacetime, D, which is spacelike but
otherwise arbitrary (see text). The tangent space of A0 ∈ D is T .

ignore their peculiar velocities). Their worldlines are straight vertical lines in
Fig. 9.3. This ﬁgure is a co-ordinate picture, see Fig. 2.1, and contains no information about the geometry (the geometrical picture appears in Fig. 11.2).
Due to the expansion the geometrical distance between B0 and C0 is larger
than between B1 and C1 . It remains possible that the spacetime that we shall
see in the future contains huge inhomogeneities, and that the cosmological
principle will eventually prove to be incorrect,6 see Fig. 9.3. Presently, however, the assumption that every subspace t = constant is homogeneous and
isotropic is adequate. But it should be clear that very little can be said about
the future of the universe without extra assumptions such as the cosmological
principle.
We assume that spacetime already possesses co-ordinates and a metric,
and we now construct new co-ordinates to simplify the metric. Let the subspace D in Fig. 9.4 be spacelike (but not necessarily of the type t = constant),


i.e. for every vector nµ in the tangent space we have nµ nµ < 0. The primes
denote the old co-ordinates. Consider an event A0 ∈ D with tangent space



T . We deﬁne a vector uµ by requiring uµ nµ = 0 for every nµ ∈ T . These

uµ are unique, apart from an overall factor, and timelike (see exercise). We


normalize them as uµ uµ = 1. Next we construct a geodesic L tangent to uµ
in A0 , and we deﬁne new spatial co-ordinates (x̃1 , x̃2 , x̃3 ) in D (how we do
6

It is a peculiar fact that our universe appears to be homogeneous on the scale of
the Hubble radius c/H0 , but inhomogeneous both on much larger scales (prediction of inﬂation theory) and on much smaller scales (Fig. 9.1).

176

9 The Robertson-Walker Metric

that is immaterial). Finally, we assign the following co-ordinates to an event
A on L :
xi = x̃i ;
(9.10)
x0 = arc length s of A0 A along L .
This construction is possible because L is timelike. In this way we have deﬁned
well-behaved new co-ordinates {xµ } as long as the diﬀerent geodesics L do
not intersect. For dxi = 0 (i.e. along L) we have that ds2 = (dx0 )2 , and
comparing that to ds2 = gαβ dxα dxβ = g00 (dx0 )2 we conclude that g00 = 1
on L. In the new co-ordinates the 4-velocity of a point on L equals
uµ = ẋµ ≡

d
(s, x̃1 , x̃1 , x̃3 ) = (1, 0, 0, 0) ,
ds

(9.11)



and nµ ∈ T is of the form nµ = (0, n), so that 0 = uµ nµ = uµ nµ =
gµα uµ nα = g0i ni . It follows that g0i = 0, because ni is arbitrary. On the
subspace D the metric now has the form
ds2 = (dx0 )2 + gik dxi dxk .

(9.12)

Exercise 9.2 shows that (9.12) holds everywhere. These co-ordinates are called
Gaussian co-ordinates, after Gauss who invented them.
The essence of Gaussian co-ordinates is that the worldlines L of a selected
set of freely falling test masses are taken as the co-ordinate lines of the new
co-ordinate system, and these lines L remain always orthogonal to the subspaces t = constant. Because the derivation is completely general, we may
use Gaussian co-ordinates in any physical situation, also for example in the
Schwarzschild metric. They are not very convenient in that case, but that is
another matter. In cosmology, however, they are very useful. The sections t =
constant are snapshots of the homogeneous and isotropic universe, and the
selected test masses are the galaxies. Because these are at rest (dxi = 0) it
follows from (9.12) that dτ = dt: at any time t all clocks of galaxies tick at
the same rate. This must be so because otherwise a subspace t = constant
would not be homogeneous. In Gaussian co-ordinates the proper time of any
galaxy in this subspace serves as the co-ordinate time t. Since we deal mostly
with objects at rest (galaxies), the notion of proper time plays a minor role
in cosmology. Proper time is only important when we consider motion with
respect to the Hubble ﬂow, as in exercise 9.9.



Exercise 9.1: Prove that uµ introduced above (9.10) is unique and timelike.
Hint: Timelike is invariant, so employ the local rest-frame ¯ of A0 . With 3
independent nµ one may construct 3 orthonormal spacelike unit vectors; uµ

9.3 Metric and spatial structure

177

must be orthogonal to these (in the sense of the inner product) → uµ ∝
(1, 0, 0, 0), therefore timelike.

Exercise 9.2: Prove that (9.12) is valid everywhere.
Hint: Work out (2.34) along the geodesic L with (9.11) → Γµ00 = 0. Now use
(2.24) and g00 = 1 on L → g µλ gλ0,0 = 0; det{g µλ } = 0 → gλ0,0 = 0 on
L → gj0 constant on L → gj0 = 0 on L (i.e. everywhere, q.e.d.).

9.3 Metric and spatial structure
Due to the expansion the metric will depend on x0 , and that dependence must
be the same for every gik , otherwise anisotropies would develop. Therefore
(9.12) can be written as
ds2 = (dx0 )2 + S(t)2 aik dxi dxk ,

(9.13)

with aik constant. We may simplify aik dxi dxk further by noting that the space
is certainly spherically symmetric around an (arbitrarily chosen) origin. The
implications of that have been elaborated as we discussed the Schwarzschild
metric, § 4.1. The spatial metrics associated with (4.2) and (9.13) at time t1
are

e2λ dr2 + r2 dΩ 2
2
dl =
(9.14)
− S12 aik dxi dxk ,
where dΩ 2 = dθ2 + sin2 θ dϕ2 and S1 = S(t1 ). These two metrics describe
the same space, as both are spherically symmetric around the origin. We
conclude that −aik dxi dxk may also be written as (e2λ dr2 + r2 dΩ 2 )/S12 . After
a rescaling S(t)/S1 → S(t) we ﬁnd that (9.13) reads


ds2 = (dx0 )2 − S(t)2 e2λ dr2 + r2 dΩ 2 .
(9.15)
To ﬁnd λ(r) we compute the total curvature Ri i of the subspace t = constant
of (9.15) when S(t) = 1. This Ri i turns out to be equal to Rµµ from (4.19)
with ν = constant (see exercise), or
 



2λ
1
2
2
d
−2λ
−2λ
− 2 e
re
+ 2 = 2 1−
R = 2
,
(9.16)
r
r
r
r
dr
from which it follows that

178

9 The Robertson-Walker Metric

k=0

k =1

k = -1

Fig. 9.5. Two-dimensional analogons of a ﬂat, a spherical and a hyperbolic universe.
After Berry (1978).

d
r e−2λ = 1 −
dr

2
1
2 Rr

.

(9.17)

We now argue that R is constant because the space t = constant is homogeneous, and we may integrate:
e2λ = (1 −

2
1
6 Rr

+ A/r)−1 .

(9.18)

The integration constant A should be zero, otherwise the co-ordinates would
not be locally ﬂat in r = 0. Denoting R = 6k we get:


dr2
2
2
2
2
ds2 = (dx0 )2 − S(t)2
+
r
(dθ
+
sin
θ
dϕ
)
.
(9.19)
1 − kr2
By a co-ordinate transformation r → r̃ we may always make k equal to
0, 1 or − 1. Henceforth we restrict ourselves to k = 0, ±1. Robertson and
Walker have shown in 1936 that (9.19) is the most general metric of a spacetime whose subspaces t = constant are homogeneous and isotropic. Therefore
(9.19) is called the Robertson-Walker metric.
By means of symmetry arguments we have succeeded to ﬁnd the metric up
to an unknown scale factor S(t). The scale factor is determined by the ﬁeld
equations. Before we enter into that we discuss the structure of the spaces
deﬁned by (9.19). According to (3.7) the spatial metric is given by:


dr2
2
2
2
2
2
2
+ r (dθ + sin θ dϕ ) .
(9.20)
dl = S
1 − kr2
It is important to realise that because of the homogeneity all points in the
spaces deﬁned by (9.20) are equivalent, and that the origin r = 0 may chosen
wherever we like. For k = 0 the geometry is Euclidean and the space is ﬂat
– a homogeneous, isotropic and ﬂat universe. For k = ±1 space is no longer
ﬂat and it is useful to make a transformation:
dr2
≡ dχ2 ,
1 − kr2

(9.21)

9.3 Metric and spatial structure

179

which integrates to

r =

sin χ

(k = +1) ;

sinh χ

(k = −1) .

(9.22)

As long as one moves on a surface r = constant one does not notice anything
out of the ordinary, because if we take dr = 0 in (9.20) we obtain the usual
geometry of the surface of a sphere. The surface (2-volume) O of such a sphere
is 4πr2 S 2 . Exercise 9.6 illustrates how we may use that to measure χ and r.
We may also construct a θ, ϕ-grid on the sphere as usual.
Spherical universe with positive curvature
Things are diﬀerent when the radial direction r comes into play. For k = +1
we have:


(9.23)
dl2 = S 2 dχ2 + sin2 χ (dθ2 + sin2 θ dϕ2 ) .
We may visualise this universe as the boundary of a 4-dimensional sphere of
radius S embedded in a 4-dimensional Euclidean space. The boundary of such
a 4-sphere may be parametrized as follows (see exercise):
⎫
x = S sin χ sin θ cos ϕ ⎪
⎪
⎪
0 ≤ χ ≤ π;
⎪
⎪
y = S sin χ sin θ sin ϕ ⎬
0 ≤ θ ≤ π;
(9.24)
⎪
z = S sin χ cos θ
⎪
⎪
⎪
0 ≤ ϕ ≤ 2π .
⎪
⎭
w = S cos χ
The advantage of the χ-co-ordinate is that it is monotonous, contrary to r:
if k = 1, r runs from 0 to 1 and then back again to 0 (see exercise 13.2).
The space (9.24) has no boundary and in the exercises it is shown that its
3-volume is ﬁnite. This is called a closed universe. Note that the embedding
space has no physical reality. The fourth (radial) dimension from the origin of
the embedding space towards the boundary of the sphere and beyond does not
exist: we can only move within the boundary, in the space deﬁned by (9.24),
and have no notion of what is lurking outside. There is no outside.
Hyperbolic universe with negative curvature
For k = −1 we have:



dl2 = S 2 dχ2 + sinh2 χ (dθ2 + sin2 θ dϕ2 ) .

(9.25)

There is now no natural limit to r; both r and χ run from 0 to ∞. This space
is much harder to visualize. The closest analogon is a saddle surface in R3 .
However, in a ﬂat R3 there exists no 2-surface without boundary and with a

180

9 The Robertson-Walker Metric

constant negative curvature.7 And in a ﬂat R4 there exists no 3-surface with
a constant negative curvature. The space (9.25) also has no boundary, and an
exercise shows that its 3-volume is inﬁnite, as in the case of k = 0. This is
called an open universe (k = 0, −1).

Exercise 9.3: Prove (9.16).
Hint: according to (3.7) the metric of the subspace t = constant and S = 1 is
d2 = e2λ dr2 +r2 dΩ 2 . But that is also the metric of the subspace t = constant
of the Schwarzschild metric (4.2). Therefore we calculate R = Ri i of that 3D
space. It is not suﬃcient to restrict the indices α, β in (4.19) to 1, 2, 3: relation
(2.57) shows that we also should set to zero all Γµαβ with one or more zero
indices. According to § 4.1 these are Γ010 , Γ001 and Γ100 , and these can put
to zero by taking ν = constant. Then R0 0 = g 0µ Rµ0 = g 00 R00 = 0 as well
according to (4.15).

Exercise 9.4: Prove that (9.24) is a space with metric (9.23) and that (9.24)
is the boundary of a 4-sphere with radius S in an Euclidean R4 .
Hint: (9.24) should be a sphere (x2 +y 2 +z 2 +w2 = S 2 ) embedded in Euclidean
space, that is dl2 ≡ dx2 + dy 2 + dz 2 + dw2 = (9.23).
Exercise 9.5: Prove that for k = +1 the 3-volume of space is ﬁnite and equal
to 2π 2 S 3 .
Hint: From (9.23):

√
g dχdθdϕ = S 3 sin2 χ sin θ · dχdθdϕ (exercise 4.4).

Exercise 9.6: Consider a sphere with radius r = sin(h)χ around the origin.
Calculate the 2-volume of the boundary (the surface) and the length of the
radius. Prove that

(sin χ/χ)2 < 1
(k = +1) ;
surface sphere
=
(9.26)
4π (length radius)2
(sinh χ/χ)2 > 1 (k = −1) .
Hint: Take for example k = +1; dχ = 0 in (9.23) → metric of the spherical
√
boundary: dl2 = S 2 sin2 χ (dθ2 + sin2 θ dϕ2 ) → g dθdϕ = S 2 sin2 χ sin θ ·
dθdϕ, then integrate. Length along χ : dl = Sdχ dθ = dϕ = 0 in (9.23) .
7

Stillwell, J.: 1992, Geometry of Surfaces, Springer-Verlag.

9.4 Equations of motion

181

9.4 Equations of motion
The derivation of the equations of motion is quite a bit of work: we have
to repeat the entire derivation of §§ 4.1 and 4.2 for the Robertson-Walker
metric. However, we shall pass over many details. The starting point is the
calculation of the Christoﬀel symbols. To this end we write down the equation
for an arbitrary
geodesic with the help of variational calculus, see (2.36) and

(9.19): δ L dp = 0, where L is given by:
L = gαβ ẋα ẋβ
= (ẋ0 )2 −

S 2 ṙ2
− S 2 r2 θ̇2 − S 2 r2 sin2 θ ϕ̇2 ,
1 − kr2

(9.27)

with ˙ = d/dp.8 Note that the co-ordinates x0 = ct, x1 = r, x2 = θ and
x3 = ϕ are functions of the parameter p. The scale factor S depends on t, i.e.
on x0 . All x0 -dependence of L is in S, and S  ≡ dS/dx0 . We elaborate the
Euler-Lagrange equations (2.37) for x0 : ∂L/∂x0 = (∂L/∂ ẋ0 )˙ :


ṙ2
2

2 2
2
2
− 2SS
+ r θ̇ + r sin θ ϕ̇
(9.28)
= (2ẋ0 )˙ .
1 − kr2
After some rearranging:
ẍ0 +

SS 
ṙ2 + SS  r2 θ̇2 + SS  r2 sin2 θ ϕ̇2 = 0 .
1 − kr2

(9.29)

We compare this to (2.34) so that we may read Γ0αβ from the equation (numbering: 1 = r, 2 = θ, 3 = ϕ):
Γ011 =

SS 
;
1 − kr2

Γ022 = SS  r2 ;

Γ033 = SS  r2 sin2 θ ,

(9.30)

and all other Γ0αβ are zero. An exercise invites the reader to prove that
Γν0ν =

3S 
;
S

Γα00 = 0 ;

Γ0ik = −

S
gik .
S

(9.31)

According to (9.19) the metric tensor gαβ is
g00 = 1 ;
g22 = − S 2 r2 ;
8

g11 = −

S2
;
1 − kr2

g33 = − S 2 r2 sin2 θ .

⎫
⎪
⎬
⎪
⎭

(9.32)

Our notation is not very consistent. Sometimes ˙ stands for d/dp and sometimes
for ∂/∂t. Here we are forced to distinguish d/dp (denoted by ˙ ) and ∂/∂t (denoted
as  = d/dx0 ). Later we switch again to Ṡ = dS/dt.

182

9 The Robertson-Walker Metric

The (long) technicalities of the computation of Rµν are left aside, and we
mention only the ﬁnal result
⎫
3S 
⎪
⎪
;
R0i = 0 ;
R00 =
⎪
⎬
S
(9.33)
⎪
⎪
SS  + 2(S  )2 + 2k
⎪
Rik =
gik , ⎭
S2
with  = d/dx0 . Furthermore (see exercise)


3 (S  )2 + k
G00 = −
.
S2

(9.34)

We have expressed Rik and Γ0ik in terms of gik where possible because that
will be useful later.
Next we consider the stress-energy tensor Tµν . The universe is ﬁlled homogeneously with a mixture of matter (galaxies) and radiation (the CMB).
The bulk velocity of that mixture with respect to the Hubble ﬂow is zero:
uµ = (1, 0, 0, 0) → uµ = gµν uν = gµ0 = 0 for µ = 1, 2, 3 and 1 for
µ = 0 → uµ = uµ . With (3.57) we obtain:
T00 = ρ ;

T0i = 0 ;

Tik = −

p
gik .
c2

We conclude from (3.58) that G00 + Λ = − (8πG/c2 ) · T00 , or:
  2
S
k
8πGρ
Λ
− 2 .
=
+
S
3c2
3
S

(9.35)

(9.36)

This is the equation of motion for S, ﬁrst derived by Friedmann in 1922 for
the special case that the cosmological constant Λ is zero.
Adiabatic expansion
There is still information in T µν:ν = 0, but only in T 0ν:ν = 0. From (2.51):
T 0ν:ν = T 0ν,ν + Γ0νσ T νσ + Γνσν T 0σ = 0 .

(9.37)

Now T ik = g iλ g kµ Tλµ = g il g km Tlm = −(p/c2 )g ik , and is straightforward to
see that T 00 = T00 = ρ, and T 0i = 0. This simpliﬁes (9.37) to:
0 = T 00,0 + Γν0ν T 00 + Γ0ik T ik
= ρ +

S p
3S 
ρ +
gik g ik
S
S c2

= ρ +

3S  p
3S 
ρ +
.
S
S c2

(9.38)

9.5 The cosmological constant

183

Here we have made use ﬁrst of Γ00α = 0, then of ρ,0 = ρ and ﬁnally of (9.31).
On multiplying (9.38) with c2 S 3 we get
(ρc2 S 3 ) + p (S 3 ) = 0 .

(9.39)

This equation says that the gas in a volume V ∝ S 3 expands adiabatically:
dQ/dt ≡ dU/dt + pdV /dt = 0 with U ≡ ρc2 V .
The role of the pressure
Equations (9.36) and (9.39) determine the evolution of the universe once we
know the equation of state p(ρ). This is the subject of the next chapter.
Note that (9.36) and (9.39) may be combined into the following relation (see
exercise):


S 
4πG
3p
Λ
= − 2 ρ+ 2 +
.
(9.40)
S
3c
c
3
This equation carries a few important messages. We take Λ = 0 ﬁrst, and
deal with Λ = 0 in the next section. In this case S  < 0, i.e. S  decreases.
In other words, the expansion of the universe is slowing down. The classical
explanation is that this is gravity at work, which is constantly trying to pull
the matter together. Another implication is that the expansion must have been
faster in the past. Perhaps more astounding is that pressure also acts to reduce
the expansion. The intuitive idea that pressure should accelerate expansion is
apparently not correct. The explanation is that a pressure gradient gives rise to
a force, like between the inside and the outside of a balloon. But the universe
is homogeneous and there are no pressure gradients. To continue in the spirit
of the metaphor, we don’t live inside the balloon but on the homogeneous
surface (the interior of the ‘balloon’ does not exist). What remains is that
pressure is a form of potential energy and acts as a source of gravity if it is
suﬃciently large, p ∼ ρc2 . A similar thing happened in the case of the TOV
equation, § 5.3.

9.5 The cosmological constant
Historically, Einstein introduced the cosmological constant Λ because it was
a term that logically should appear in the ﬁeld equations, and it allowed the
existence of a static, zero-pressure spherical universe:
S = Λ−1/2 = c (4πGρ)−1/2 ;

k = +1 .

(9.41)

Remember – this was before it was discovered that the universe expands.
The eﬀect of a positive cosmological constant in eq. (9.40) is to increase the

184

9 The Robertson-Walker Metric

expansion rate. Nowadays we believe we know that ΩΛ ≡ Λc2 /3H02 0.7 and
that the expansion of the universe is actually accelerating. The cosmological
constant is a property of the vacuum since Λ remains in eqs. (9.36) and (9.40)
after ρ and p have been set to zero. It is possible to explain the term Λg µν in
(3.58) in terms of a stress-energy tensor associated with the vacuum. Following
the literature we endow it with a constant energy density ρv c2 of unknown,
probably quantummechanical origin.9 In special relativity, the stress-energy
tensor in the local rest-frame of a ﬂuid is10
⎛
⎞
∅
ρv c2
⎟
1 ⎜
pv
⎟ .
(9.42)
T µν = 2 ⎜
⎝
pv ⎠
c
∅
pv
We have replaced ρ → ρv and p → pv , in anticipation of (9.42) being the T µν of
the vacuum. Now comes the key observation: the vacuum is physically identical
in all inertial frames, so that (9.42) must be the same in all inertial frames,
and it must be Lorentz-invariant. This is only possible if T µν = const · η µν ,
which implies that
Tvµν = ρv η µν ;

pv = −ρv c2 .

(9.43)

A negative pressure is formally in agreement with energy conservation (9.39):
dU/dt + pdV /dt ≡ dρv c2 V /dt + (−ρv c2 )dV /dt = 0 as ρv is constant. The
principle of general covariance suggests that in GR we should take
Tvµν = ρv g µν .

(9.44)

Next, following eq. (3.59), we write the ﬁeld equation (3.42) as
Gµν = −


8πG  µν
µν
Tv + Tm
,
2
c

(9.45)

where the index m stands for matter. After insertion of (9.44) we recover the
ﬁeld equation (3.58) with the Λ-term, and
Λ =

8πGρv
,
c2

or

ΩΛ ≡

Λc2
ρv
=
.
3H02
ρc

(9.46)

The parameters ΩΛ and Ωm ≡ ρ/ρc will play an important role in the next
chapters.
We handle eq. (9.40) in the same spirit: omit the term Λ/3 and split the
ρ + 3p/c3 term in a vacuum part and a matter part. The former equals again
Λ/3:
9
10

Carroll, S.M. et al., A.R.A.A. 30 (1992) 499.
Set uµ = (1, 0, 0, 0) and g µν = η µν in (3.57).

9.6 Geodesics

−


8πGρv
4πG 
Λ
ρ
= +
.
−
3ρ
≡
v
v
2
2
3c
3c
3

185

(9.47)

This also demonstrates that the anti-gravity generated by the negative pressure outweighs the gravity associated with the vacuum density ρv .
In summary, we assign to the vacuum a constant energy density ρv c2 of
unknown origin, referred to as dark energy. Formal arguments such as Lorentz
invariance force us to assign to it a negative pressure −ρv c2 as well. This is
then equivalent with the Λ-term in eq. (9.36). The anti-gravity generated by
the negative pressure makes the expansion of the universe accelerate. Observations suggest that ρv is a little less than ρc . The ultimate explanation of Λ
and ρv must come from a theory of quantum gravity.

9.6 Geodesics
The geodesics of the Robertson-Walker metric are simple in the sense that they
are all eﬀectively radial geodesics. Given a geodesic, spatial homogeneity permits us to move the origin to a point on the geodesic. Seen from this new origin
the geodesic must be a radial geodesic (dθ = dϕ = 0), on account of symmetry.
The situation is therefore simpler than in the case of the Schwarzschild metric.
All optical observations, for example, may be analysed with radial null geodesics, and these are simple: ds2 = 0 in (9.19) → dx0 = ± S(1 − kr2 )−1/2 dr
(§ 11.1). The only non-trivial material geodesics are those having a nonzero
initial velocity, for example a test mass ﬁred into space, see Fig. 9.6 and exercise 9.9. The outcome may be understood right away: the test mass does
not reach spatial inﬁnity, but rather a constant co-ordinate distance r0 . What
happens is that the speed of the test mass decreases with respect to the local
Hubble ﬂow, and after a (formally inﬁnite) time it ﬁnds itself at rest in the
Hubble ﬂow. One of the consequences is that the peculiar motion of a galaxy
superposed on the Hubble ﬂow is generally damped. This is just a manifestation of adiabatic cooling, which we already encountered in connection with
eq. (9.39). It explains why the Hubble ﬂow is cold.

Exercise 9.7: Prove (9.31) and (9.34).
Hint: (9.32) → −g = S 6 r4 sin4 θ/(1 − kr2 ), then (2.33); Γi00 requires the
other geodesic equations from ∂L/∂xi = (∂L/∂ ẋi )˙. According to (2.34) it
comes down to showing that there are no terms ∝ (ẋ0 )2 ; Γ0ik from (9.30)
and (9.32); G00 from (2.60), and R = g µν Rνµ = R00 + g ik Rki . Then (9.33).

186

9 The Robertson-Walker Metric

Fig. 9.6. Star wars. Under the pressure of mounting political tension the Upper
Master of galaxy A decides to ﬁre a bullet K to an unfriendly neighbour with
initial velocity β = v0 /c. The bullet (think of a jet) moves along a radial geodesic
r(τ ), x0 (τ ). Due to the expansion, the bullet reaches a ﬁnite co-ordinate distance
r0 . The computation of exercise 9.9 is only indicative as it does not allow for the
gravitational attraction of A.

Exercise 9.8: Prove (9.40).
Hint: First multiply (9.36) by S 2 . Write ρS 2 = ρS 3 /S, in anticipation of the
substitution of (9.39).

Exercise 9.9: Test mass K in Fig. 9.6 moves on a material geodesic in the
Robertson-Walker metric. Show that
1
λc
dr
dχ
≡ √
= √
,
2
dt
1 − kr dt
S S 2 + λ2

(9.48)

with λ = γβS0 and β = v0 /c, v0 = initial velocity of K and γ = (1 − β 2 )−1/2 .
Hint: Nasty problem. Since the Robertson-Walker metric depends on time,
u0 is not a constant of the motion, as it was in the Schwarzschild metric.
The constants of the motion are θ and ϕ. The equation for x0 is (9.29) with
dθ = dϕ = 0. To obtain the second equation it is easiest to ‘divide’ (9.19) by
ds2 as we did in (4.35):
ẍ0 +

SS  ṙ2
= 0;
1 − kr2

(9.49)

(ẋ0 )2 −

S 2 ṙ2
= 1,
1 − kr2

(9.50)

9.6 Geodesics

187

with ˙ = d/ds and  = d/dx0 . In this problem the proper time plays its usual
role again. Eliminate ṙ2 /(1 − kr2 ):
ẍ0 + (S  /S){(ẋ0 )2 − 1} = 0 .

(9.51)

Multiply with S 2 ẋ0 and use that S  ẋ0 = Ṡ. The result may be integrated to
S 2 (ẋ0 )2 − S 2 = λ2 = integration constant:
ẋ0 =

S 2 + λ2 /S .

(9.52)

Initial condition: at t = t0 we have S = S0 and according to (1.6) ẋ0 ≡
dt/dτ = (1 − β 2 )−1/2 with β = v0 /c → λ = γβS0 . Substitute (9.52) in (9.50):
(1 − kr2 )−1/2 ṙ = λ/S 2 .

(9.53)

Finally, dr/dt = cṙ/ẋ0 . Eq. (9.48) can be integrated once S(t) is known (exercise 11.4). It is easy to see that for S ∝ tα and k = 0 the test mass will
travel a ﬁnite co-ordinate distance if α > 12 .

Worlds in collision. A spectacular merger in progress in NGC 4676, at a distance of
92 Mpc. Analysis shows that we are seeing two spirals some 160 Myr after closest
encounter. Tidal interaction created long tails that contain many associations of young
and hot (blue) stars. The pair will eventually merge into a single elliptical galaxy. The
horizontal image size is about 2 . A similar merger might happen when our galaxy
hits its neighbour M31, a few billion years from now. Image taken by the Advanced
Camera for Surveys on the HST in April 2002. Credit: NASA, H. Ford et al., and the
ACS Science team.

10
The Evolution of the Universe

In the previous chapter we learned that GR opens completely new possibilities
for the spatial structure of the universe, even if we restrict ourselves to homogeneous isotropic spaces. Space becomes a dynamic entity whose topology
and geometry depend on the matter it contains. This is a major conceptual
advance over the Newtonian idea of an absolute, ﬂat and inﬁnite space. This
chapter tells the story of the Friedmann-Robertson-Walker (FRW) model.
That is, the homogeneous isotropic universe with a Robertson-Walker metric whose evolution is determined by the Friedmann equation (9.36). In 1927
Lemaı̂tre proved that Friedmann’s solution implies a linear relation between
distance and redshift. The discovery of the expansion of the universe in the
1920s by Slipher and Hubble did not come out of the blue, but had been anticipated by the theoretical developments of the time. During the second half
of the 20th century it was realised that a FRW universe must have had a hot
start, of which the matter and the cosmic microwave background (CMB) are
ancient relics. More than anything else, the discovery of this CMB by Penzias
and Wilson in 1965 has changed the face of cosmology from a speculative
backyard in the 1950s into the quantitative science it is today.

10.1 Equation of state
Equations (9.36) and (9.39) determine the evolution of the universe as soon
as we know the equation of state p(ρ), or ρ as function of S. In cosmology
it is customary to group all relativistic particles under the name radiation,
regardless of their mass, and to reserve the term matter for all non-relativistic
particles. The reason is that these two groups contribute in rather diﬀerent
ways to the dynamics of the universe.1 The density ρ in (9.36) and (9.39)
1

There are now strong indications that neutrinos have a small mass. The WMAP
data indicate that Ων < 0.015, implying that while some of the neutrinos may
actually be ‘matter’ now, they are likely to be all relativistic at the beginning of
the matter era. For that reason the energy density of the neutrino background
has been added to the radiation density in (9.9).

190

10 The Evolution of the Universe
Table 10.1. Pressure and density in the universe

p = pm + pr
2

Matter
(m  r )

Radiation
(m
r )

0

1

3 r
2

ρ = (m + r )/c

m /c

r /c2

S-dependence

m S 3 = const

r S 4 = const

should be interpreted as /c2 , where  = total energy density, including the
rest mass contribution, and p represents the total pressure. As long as the
temperature is suﬃciently low, m0 c2  κT , the total energy of a particle
with mass hardly exceeds m0 c2 . Such a non-relativistic particle has a constant
contribution to . In the early universe, however, the temperature is very high
and m0 c2 may be much smaller than κT . In that case the rest mass of the
particle is eﬀectively zero and it behaves like a photon, whose wavelength
scales ∝ S (the proof is given in § 11.1). The contribution of such a particle
to  is ∝ S −1 . Since the number of particles in a comoving volume V ∝ S 3
remains constant, we ﬁnd that the energy density is ∝ S −3 for matter and
∝ S −4 for radiation.
A consequence of this matter/radiation deﬁnition is that particles with
m0 = 0 switch gender during the evolution of the universe, from ‘radiation’ to
‘matter’, ﬁrst the heavier particles, subsequently followed by the lighter ones,
since the temperature decreases so drastically. It turns out that the evolution
of the universe can be described by two limiting cases: (1) the recent history of
the universe, during which m  r so that the evolution is entirely determined
r and the radiation
by the matter, and (2) the hot early universe where m
determines the evolution. In the former case the pressure is zero, because
m + r
m
ρc2 → p
ρc2 → p 0,
p = pm + pr ∼ nκT + r
2
since we know that p is only relevant when p ∼ ρc . For particles of zero mass
p = /3 holds generally, see Appendix D. In this way we arrive at the relations
in Table 10.1.

Exercise 10.1: Show that m S 3 = constant and r S 4 = constant from (9.39).
Hint: Matter: trivial. Radiation: (r S 3 ) + 13 r (S 3 ) = 0 → (r S 4 ) = 0.

10.2 The matter era

191

10.2 The matter era
On comparing (9.3) and (9.9) we see that the matter energy density in the
universe is about a factor 3000 larger than the energy density in radiation.
This imbalance will remain in the future as S increases, because m ∝ S −3 ,
while r ∝ S −4 . It is only in the early universe that r > m . During most of its
life the universe evolves according to the limiting case ‘matter’. The equations
for this so-called matter era follow from (9.36), (9.40) and Table 10.1. We also
revert to the notation ˙ = d/dt:
 2
Λc2
kc2
Ṡ
8πGρ
+
− 2 ;
=
S
3
3
S

(10.1)

S̈
4πGρ
Λc2
= −
+
;
S
3
3

(10.2)

ρS 3 = ρ0 S03 .

(10.3)

Here and everywhere else the index 0 indicates the value of a quantity at
the present epoch t = t0 ; ρ is the density of matter (the index m has been
dropped). The ﬁrst step is to rewrite (10.1) in a seemingly complicated way
for t = t0 :
(10.4)
1 = Ωm + ΩΛ + Ωk .
The constants Ωm , ΩΛ and Ωk are deﬁned as:
⎫
8πGρ0
ρ0
⎪
=
;
⎪
⎪
⎪
3H02
ρc
⎪
⎪
⎪
⎪
⎬
2
Λc
ρv
ΩΛ =
=
;
⎪
3H02
ρc
⎪
⎪
⎪

2
⎪
⎪
⎪
c
⎪
. ⎭
Ωk = − k
H0 S 0
Ωm =

and

 
Ṡ
Ṡ0
H0 ≡
=
;
S 0
S0

ρc =

(10.5)

3H02
.
8πG

(10.6)

That (10.4) is the same as (10.1) at t = t0 is just a matter of substitution. The
parameters Ωm , ΩΛ and Ωk indicate the relative importance of the density,
the cosmological constant, and the curvature of space in the evolution of
the universe at the present epoch. We have already met the parameters Ωm
and ΩΛ and the critical density ρc in the previous chapter. The proof that
H0 deﬁned in (10.6) is really the Hubble constant is given in § 11.3. Since
sign(Ωm + ΩΛ − 1) = sign(k), we arrive at an important conclusion:

192

10 The Evolution of the Universe

closed universe (k = +1) ↔ Ωm + ΩΛ > 1 ;
ﬂat universe

(k = 0)

↔ Ωm + Ω Λ = 1 ;

open universe (k = −1) ↔ Ωm + ΩΛ < 1 .

⎫
⎪
⎪
⎬
⎪
⎪
⎭

(10.7)

The spatial structure of an FRW universe is ﬁxed by the matter density,
the cosmological constant and the Hubble constant. And since Ωm + ΩΛ =
1.02 ± 0.02, see Table 9.2, our universe is very likely to have a ﬂat geometry.
After substitution of (10.3), we may cast the evolution equation for the
scale factor (10.1) in a suitable dimensionless form:


u̇2 = H02 Ωm u−1 + ΩΛ u2 + Ωk ,

with

u = S/S0 .

(10.8)

We now discuss the evolution of FRW universes as given by (10.8). At the
present epoch we have Ṡ > 0, hence u̇ > 0 in u = 1. Note that u̇ can only
change sign if the right hand side of (10.8) becomes zero. If we move into the
past, i.e. smaller u, the right hand side of eq. (10.8) becomes larger and u̇
increases, provided ΩΛ is not too large.2 It follows that u will reach zero in
a ﬁnite time. We arrive at a second important conclusion: the expansion of
FRW universes started a ﬁnite time ago from a singularity. The density and
pressure must have been extremely high at that time. This is called the Big
Bang. It turns out that isotropy and m  r are no essential ingredients. The
expansion must have started from a singularity.
FRW models with zero Λ
We now consider the future evolution of FRW models, and take ΩΛ = 0 ﬁrst.
For ΩΛ = 0 we have
|Ωm − 1|1/2 = |Ωk |1/2 =

c
,
H0 S 0

(10.9)

provided Ωm = 1, and


u̇2 = H02 Ωm u−1 + 1 − Ωm ,

u = S/S0 .

(10.10)

At the present epoch u̇ > 0 in u = 1. For Ωm ≤ 1 the right hand side of
(10.10) is positive, so that Ṡ is always positive. The expansion will continue
2

A singularity may not occur if ΩΛ > 1. For a classiﬁcation of universe models as
a function of Ωm and ΩΛ we refer to Peacock (1999) § 3.2. The word singularity
should not be taken too literally. Quantum gravity will probably prevent it, and
even during the earliest phases of the Big Bang the universe never was a ‘point’,
see Ch. 13.

10.2 The matter era

193

4
WL

flat (k = 0)
Wm = 1

hyperbolic (k = -1)
Wm = 0.5

3
S / S0

Big Emptiness

0

2

now
spherical (k = 1)
Wm = 2

1
H0-1

Big Bang
0

1

2

Big Crunch
3

4

5

6

7

time

Fig. 10.1. Three solutions of eq. (10.10): an open, a ﬂat and a closed FRW universe
with Λ = 0, tuned to the same size and expansion rate at the present epoch t0 ,
arbitrarily located at t = 1. Time is in units of H0 −1 .

forever. The density is too low and the associated gravity not strong enough
to stop it. For large S the expansion rate approaches

u̇ →

0

√

H0 1 − Ω m

Ωm = 1 ;
Ωm < 1 ,

(10.11)

or, in terms of S: Ṡ → 0, c for k = 0, −1. When Ωm > 1 the right hand side of
(10.10) may become zero. A horizontal asymptote u → const. is not possible
because (10.2) requires that S̈ < 0. In other words, Ṡ(t) must decrease all
the time. Hence, for Ωm > 1, u will reach a maximum u = Ωm /(Ωm − 1) >
1 after which a contraction follows; u(t) is symmetric with respect to the
maximum (why?). The gravity generated by the matter is suﬃcient to stop
the expansion, after which the universe begins to contract again, and ‘the
movie is shown in reverse order’. The contraction steadily accelerates and
continues until space degenerates formally into a point. This is called the Big
Crunch. Fig. 10.1 shows the evolution of three FRW universes with ΩΛ = 0.
Eq. (10.10) has a simple solution for Ωm = 1. We have u1/2 u̇ = H0 →
u = const · t2/3 :
⎫

2/3
S
⎪
3
⎪
= 2 H0 t
⇒
⎬
S0
for Ωm = 1 and ΩΛ = 0 .
(10.12)
⎪
⎪
⎭
2 −1
−1
6.5 h Gyr
t 0 = 3 H0

194

10 The Evolution of the Universe
Table 10.2. The age of a FRW universe as a function of its size.

S/S0

H0 t
Ωm
ΩΛ

0.01
0.02
0.05
0.1
0.2
0.5
1
2
5
a

0.5
0

1
0

2
0

0.3
0.7

9.4 · 10−4
2.7 · 10−4
0.010
0.029
0.080
0.29
0.75
1.8
5.6

6.7 · 10−4
1.9 · 10−3
7.5 · 10−3
0.021
0.060
0.24
0.67
1.9
7.5

4.7 · 10−4
1.3 · 10−3
5.3 · 10−3
0.015
0.043
0.18
0.57
3.1
−a

1.2 · 10−3
3.4 · 10−3
0.014
0.038
0.11
0.41
0.96
1.7
2.8

The maximum size S/S0 of this closed universe is 2.

This model serves as a kind of reference model in cosmology. The t2/3 dependence can be understood with the help of a classical argument: matter
homogeneously ﬁlling a ﬂat space under its own gravity moves exactly in the
same manner, see exercise. Given that h = 0.71, the age t0 of this universe is
about 9 Gyr, which is too young. The radiation era prior to the matter era
5
lasted only <
∼ 10 yr, and cannot signiﬁcantly aﬀect the value of t0 . It is possible to increase t0 by taking Ωm < 1, but in order to attain a reasonable age,
say t0 ∼ 12.5 Gyr, Ωm must be ≤ 0.1, which is excluded by the observations.
FRW models with non-zero Λ
The solution of this age problem came after the WMAP mission had established the parameters of our universe: (Ωm , ΩΛ ) (0.3, 0.7). The cosmological
constant produces an extra acceleration, and a universe with Λ > 0 expands
forever (unless Ωm√is large), and is usually
√ older. For large u we infer from
(10.8) that u̇ H0 ΩΛ u → u ∝ exp(H0 ΩΛ t). The expansion is exponential. The turning point where the expansion rate changes from decreasing to
increasing can be obtained from (10.2) in dimensionless form:


ü = H02 − 12 Ωm u−2 + ΩΛ u ,
(10.13)
and ü = 0 for u = (Ωm /2ΩΛ )1/3 ∼ 0.6. Such an (Ωm , ΩΛ ) = (0.3, 0.7)
universe is now forever in a state of accelerating expansion. The time evolution
follows from (10.8): du/dt = H0 (· · ·)1/2 or H0 dt = (· · ·)−1/2 du :
 S/S0 
−1/2
Ωm u−1 + ΩΛ u2 + Ωk
du .
(10.14)
H0 t =
0

10.2 The matter era

2

195

S / S0

( Wm, WL ) = (0.3, 0.7)

(1, 0)

H0-1

1

B
t 2/3
0
0

0.5

1.0

1.5

2.0

time

Fig. 10.2. The evolution of a (Ωm , ΩΛ )  (0.3, 0.7) universe, from numerical integration of (10.14). B is the turning point where S̈ = 0. The model is compared with
the (Ωm , ΩΛ ) = (1, 0) reference model, and both are scaled to the same size and
expansion rate at the present epoch, arbitrarily located at t = 1. Time is in units of
H0 −1 .

Some values are given in Table 10.2, and Fig. 10.2 shows the (0.3, 0.7) solution
together with the (1, 0) reference model. The (0.3, 0.7) universe is older than
the (1, 0) model because u begins convex but turns concave later on. The age
t0 of this universe is
 1
−1/2
Ωm u−1 + ΩΛ u2 + Ωk
H0 t 0 =
du
0

−0.3
2
0.7Ωm − 0.3ΩΛ + 0.3
.
3

(10.15)

The approximate expression holds for 0.1 <
∼ 1 and |ΩΛ | <
∼ 1 (Peacock,
∼ Ωm <
1999). Taking the WMAP parameters, the age of our (0.27, 0.73) universe
would be 0.99H0−1 13.6 Gyr, almost exactly a Hubble time, in fair agreement with other age indicators such as globular clusters (12 ± 1 Gyr), and
nuclear dating (15.6 ± 4.6 Gyr).3 It seems therefore that we live in a ﬂat
universe that is forever ﬂying apart, heading faster and faster towards Big
Emptiness. The driving force behind this cosmic inﬂation, the second in the
life of the universe,4 is the anti-gravity associated with an ill-understood vac3
4

Reid, I.N., A. J. 114 (1997) 116; Cowan, J.J. et al., Ap. J. 521 (1999) 194.
The ﬁrst inﬂation phase occurred right after t = 0, see Ch. 13.

196

10 The Evolution of the Universe

uum energy (the cosmological constant Λ). We refer to Adams and Laughlin
(1999) for an eloquent account of what future of our universe may look like.

Exercise 10.2: Prove that the age of an ΩΛ = 0 universe cannot be larger
than H0−1 .
Hint: (10.10) → u̇ ≥ H0 for u ≤ 1 → dt ≤ du/H0 → ∫0t0 dt ≤ H0−1 ∫01 du,
etc. For ΩΛ = 0 the argument no longer applies.
Exercise 10.3: Show that an observer in an FRW universe with ΩΛ = 0
(1, 0, 0) at early times, and
(0, 1, 0) at
now, will measure (Ωm , ΩΛ , Ωk )
late times.
Hint: The values of the Ω’s in (10.4) depend on time. Write out (10.1) for an
arbitrary time:

2
c
Λc2
8πGρ
+
−k
.
(10.16)
1 =
3H 2
3H 2
HS
For S small (10.1) says H 2 ≡ (Ṡ/S)2 ∝ S −3 , i.e. second and third term in
(10.16) approach zero. For large S we have H = Ṡ/S = constant, i.e. ﬁrst and
third term approach zero.

Exercise 10.4: Show that eq. (10.10) describes the dynamics of self-gravitating
matter homogeneously ﬁlling an inﬁnite ﬂat space.
Hint: Choose an origin O and a point M at distance S. Acceleration of M
with respect to O is S̈ = −G(4πρS 3 /3)S −2 (Newton’s law). Take ρS 3 = ρ0 S03
and S/S0 = u → 2ü = −H02 Ωm u−2 ; multiply with u̇ and integrate:
u̇2 = H02 (Ωm u−1 + const.). Integration constant from u̇ = H0 in u = 1.
Weak point: all mass outside the sphere with radius S is ignored.
Exercise 10.5: Conclude from (10.4) and (10.5) that S0 = (c/H0 ) · |Ωm +
ΩΛ − 1|−1/2 . Is S0 a measurable quantity? Why is that no longer the case
when Ωm + ΩΛ = 1, in a ﬂat universe?
Hint: For the second question, see (9.19): for k = 0 there is a redundancy:
S and r appear only in the combination Sr, and there is no room for two
independent parameters S and r.

10.3 The radiation era

197

Exercise 10.6: Show that ρ − ρc cannot change sign, so that an FRW universe cannot change type. Restrict yourself to Λ = 0.
Hint: Follows directly from the equation
(ρ − ρc )˙ = −2H(ρ − ρc ) .

(10.17)

Proof: (ρ − ρc )˙ = ρ̇ − (3H 2 /8πG)˙ = ρ̇ − (6H/8πG)Ḣ; write (9.38) as ρ̇ =
−3H(ρ + p/c2 ), and Ḣ = (Ṡ/S)˙ = (S̈S − Ṡ 2 )/S 2 = −H 2 + S̈/S, then (9.40).
What if Λ = 0?

10.3 The radiation era
In the matter era the energy density m of matter is much larger than the
radiation energy density. But since m ∝ S −3 and r ∝ S −4 , things must have
the other way around in the early universe. During this so-called radiation
era the universe was an almost perfectly homogeneous, rapidly expanding
and cooling ﬁreball. We shall now study this early hot phase of the universe
which lasted only some 105 yr. We write

m = m0

S0
S



3
;

r = r0

S0
S

4
.

(10.18)

And r = m for a ≡ S/S0 = r0 /m0 . From (9.3) and (9.9):
 
S
r0
r0
=
=
a ≡
S0 rad→mat
m0
Ωm ρc c2
= 4.14 × 10−5 (Ωm h2 )−1 .

(10.19)

This parameter a determines the evolution of the universe in the radiation
era. For Ωm
0.27 and h
0.71 we have a
3.04 × 10−4 . The transition
from a radiation-dominated to a matter-dominated universe took place when
the universe was a factor S0 /S = (1/a) ∼ 3000 smaller than today. Because
r is both ∝ S −4 and ∝ T 4 , we have T ∝ S −1 , or:
Tr = Tr0

S0
.
S

(10.20)

This presupposes that photons with a Planck distribution will keep a Planck
distribution as the scale factor S changes (exercise 10.7). If that were not
the case the interpretation of the CMB would be rather problematic. For

198

10 The Evolution of the Universe

radiation
era
1010
reheating

T (K)

106
Tm = Tr (:) S-1

102
10-2

matter
era

Tr
Tm

1028
er (:) S-4

e ( erg cm -3 )

1018
108

recombination

em (:) S-3
em

10-2

er

10-12
1010 108

106 104
S0 / S

102

100

Fig. 10.3. The thermal history of the universe as a function of the scale factor
ratio S0 /S. Top panel: the temperature of matter and radiation; bottom panel: the
energy densities.

S0 /S ∼ 103 the temperature of the CMB is Tr ∼ 2.73 × 103 ∼ 3000 K, which
is about the temperature at which hydrogen gets ionized.5 The ionization
of helium requires a higher temperature and a larger value of S0 /S. Ionized
matter and radiation are in thermal equilibrium through frequent Thomson
scattering of photons on free electrons. We follow the development in forward
5

Collisional ionization requires T ∼ (1 − 2) × 104 K, but since there are 109
photons to every hydrogen atom photo-ionization is important. The ionization
temperature is now lower because at the same temperature there are more photons
in the tail of the Planck distribution than there are particles in the tail of the
Maxwell-Boltzmann distribution, see Peebles (1993) p. 165 ﬀ.

10.3 The radiation era

199

direction. When S0 /S decreases below ∼ 1200 the plasma begins to recombine (a strange term, because neutral atoms had never existed before), and
this process is completed around S0 /S ∼ 103 . 6 The photons experience a last
Thomson scattering, and the universe becomes transparent. The mean value
of S0 /S at recombination is 1100.
The high degree of isotropy of the CMB allows us to draw an important conclusion: the density ﬂuctuations during the recombination must have
been equally small. Therefore we know for certain that the universe at that
time was practically homogeneously ﬁlled with hydrogen and helium. It was a
hot mixture of radiation and matter that expanded and cooled down. In the
next section we shall see that the existing tiny density ﬂuctuations gradually
evolved, in the course of the matter era, into the present structure of the universe, dominated by galaxies.
Fig. 10.3 shows Tr , Tm , r and m as a function of S. Both r and m continue to scale as ∝ S −4 and ∝ S −4 , until energy exchange between radiation
3
and matter begins to play a role in the very early universe.7 For S0 /S >
∼ 10
we have Tm = Tr . One might think that after the recombination the matter
temperature scales as Tm ∝ ρmγ−1 ∝ (S −3 )γ−1 ∝ S −2 (adiabatic expansion,
γ = 5/3). But in reality density ﬂuctuations develop into mass concentrations, each with its own, independent thermal evolution. Eventually, the ﬁrst
generation of stars is born, marking the end of what is sometimes referred to
as the Dark Ages. These stars enrich, reheat and eventually re-ionize the gas,
probably in several stages. Around S0 /S ∼ 7, when the universe was about
1 Gyr old, this re-ionization process had been completed.8
Time evolution
To investigate how S depends on time we note that at the beginning of the
matter era u = S/S0
1, so that eq. (10.8) reduces to u̇ = H0 (Ωm /u)1/2 .
There are no longer three types of universe k = 0, ±1, but space is virtually
ﬂat (even if it were not ﬂat today) and the cosmological constant is eﬀectively
zero, see also exercise 10.3. To describe the transition from radiation to matter
era we may omit the last two terms in (10.8):
 2
8πGρ
Ṡ
,
=
S
3

(10.21)

and replace (10.3) with
6
7
8

The fractional ionisation freezes out at a value of ∼ 10−4 .
This is a consequence of the deﬁnition of matter and radiation, § 10.1.
Loeb, A. and Barkana, R., A.R.A.A. 39 (2001) 19; Fukugita, M. and Kawasaki,
M., M.N.R.A.S. 343 (2003) L25; Wyithe, J.S.B. and Loeb, A., Nature 432 (2004)
194.

200

10 The Evolution of the Universe

3
radiation era

t 2/3

matter
era

S / a S0

2

recombination

1

t 1/2

0
0

1

2

3

4

t = t / tm
Fig. 10.4. The evolution of the scale factor in the early universe, eq. (10.28). As the
radiation era gives way to the matter era, the gravity associated with the pressure
disappears and the expansion changes from ∝ τ 1/2 to ∝ τ 2/3 and slows down less
rapidly. For Ωm = 0.27 and h = 0.71 we have tm  9.4 × 104 yr, and the matter
era begins at tmat  0.59 tm , while recombination is at trec  4 tm . Note that τ = 1
corresponds to t ∼ 10−5 H0 −1 , so that the ﬁgure is a huge magniﬁcation of the origin
of Figs. 10.1 and 10.2.

ρ =

m0
c2



S0
S

3
+

r0
c2



S0
S

4
.

(10.22)

With u = S/S0 and deﬁnition (10.19) of a we get


a
8πGm0 1
+
.
u̇2 =
3c2
u
u2

(10.23)

We introduce the parameter tm :
tm

2
=
3H0



a3
Ωm

1/2
=

1.75 × 103
(Ωm h2 )2

yr ,

(10.24)

which will turn out to be a measure of the age of the universe at the start of
the matter era. For the parameters of our universe we ﬁnd tm 9.4 × 104 yr.
Eq. (10.23) may now be written in terms of dimensionless variables:


where

dx
dτ

2

4
=
9



1
1
+ 2
x
x


(10.25)

10.3 The radiation era

x =

1 S
u
=
,
a
a S0

τ =

t
.
tm

201

(10.26)

We may rearrange equation (10.25) as xdx/(1 + x)1/2 = 23 dτ , to obtain
 u/a
x dx
2τ
√
.
(10.27)
=
3
1+x
0
The integral is simple after substitution of x = (1 + x) − 1 in the numerator:
τ = 2 + (1 + x)3/2 − 3(1 + x)1/2 .
The solution is shown in Fig. 10.4. For large x we have τ
τ 3x2 /4 for small x :
⎧
√
⎨ (2/ 3) τ 1/2 τ
1;
x
⎩ τ 2/3
τ 1.

(10.28)
3/2

x

, while

(10.29)

The expansion begins as S ∝ t1/2 and changes to S ∝ t2/3 in the matter era.
It is often said that this means that the expansion accelerates, but of course
what really happens is that the expansion decelerates less rapidly because the
pressure becomes eﬀectively zero in the matter era, and the associated gravity
disappears.
The age tmat of the universe at the start of the
√ matter era follows from
the fact that u = a, i.e. x = 1, and τ = 2 − 2
0.59. And the age
1100 corresponds to x = (S/S0 )/a
trec at recombination when S0 /S
(1100a)−1 3 or τ 4 :

0.59tm
5.5 × 104 yr ;
tmat
(10.30)
4tm
3.8 × 105 yr .
trec
Equation (10.28) becomes invalid in the very early universe, for t <
∼ 10 s. The
reason is that the extremely high temperature renders some particles relativistic, which then qualify as radiation. This eﬀectively increases the value
of r0 and hence of a from (10.19). The electrons are the ﬁrst to make the
9
<
switch, when Tr >
∼ 6 × 10 K, for t ∼ 10 s.

Exercise 10.7: Prove the following statements on the Planck distribution:
1. r =

4σ 4
T erg cm−3 ;
c

(10.31)

2. nr

20 T 3 cm−3 ;

(10.32)

3. A Planck distribution remains a Planck
distribution as the scale factor S changes.

202

10 The Evolution of the Universe

(σ = π 2 κ4 /(603 c2 ) and nr = photon density).

−1
Hint: The photon density equals n(ν) = (8πν 2 /c3 ) exp(hν/κT ) − 1
cm−3 Hz−1 , from which


∞

nr =

n(ν) dν =
0

1
π2



κT
c

3 
0

∞

x2 dx
,
ex − 1

∞

and r = 0 hν n(ν) dν. The integrals are tabulated; number of photons
dn in a comoving volume V (i.e. V ∝ S 3 ) and in a frequency interval dν
equals dn = V n(ν)dν. Write this as dn = const · ν 2 f (ν/T )V dν. S changes:
S → S  , so ν, V, T → ν  , V  , T  . Use that λ ∝ S and ν ∝ S −1 (proof in
§ 11.1). Write ν = αν  with α = S  /S. Then V = V  /α3 . Substitute: dn =
const · (ν  )2 f (αν  /T )V  dν  . The number does not change: dn = dn , and that
implies T = αT  → a Planck spectrum at temperature T  , consistent with
(10.20).
Exercise 10.8: Deﬁne ρr ≡ r /c2 and prove that early in the radiation era
32πG
ρr t2 = 1 .
3

(10.33)

Hint: Write (10.21) as (Ṡ/S)2 = 8πGρr /3 (early radiation era → ignore m );
ρr ∝ S −4 → S = const · t1/2 , from which (Ṡ/S)2 = 1/(4t2 ).
Exercise 10.9: Early in the radiation era the age t and the density ρr of the
universe are given by
t

1.8 × 1020 Tr−2

s;

(10.34)

ρr

1.4 × 10−35 Tr4

g cm−3 .

(10.35)

Tr is the photon temperature in K. These relations are valid as long as the
radiation consists of photons and neutrinos.
Hint: (10.29): (S/S0 )2 = (4a2 /3tm ) · t, then (10.20); (10.35): combine (10.33)
and (10.34).
3
Exercise 10.10: For S0 /S >
∼ 10 we have Tm = Tr due to thermal equilibrium.
But why should Tm follow Tr ∝ S −1 ? One could also imagine that Tr follows
Tm , i.e. Tr = Tm ∝ S −2 , or something like that.

10.4 The formation of structure

203

Hint: In cosmology ‘matter’ is non-relativistic, so that the rest mass is the
largest contributor to m . That part is not available for energy exchange with
m0 c2 . For the evolution
photons: E
m0 c2 + 12 m0 v 2 , and 12 m0 v 2 ∼ κT
of S(t) the total m matters, but for energy exchange with photons only an
energy reservoir (κT /m0 c2 )m is available. This is much less than the reservoir
r that the radiation has in stock: (κT /m0 c2 )m /r is independent of S and
1 for all
therefore equal to κT /m0 c2 at S0 /S ∼ 3000 where m = r , and is
particles.

Exercise 10.11: Show that the baryon (protons + neutrons) to photon ratio
in the universe is constant, and very small. There are many more photons
than baryons in the universe:
6.1 × 10−10 .

nb /nr = constant

(10.36)

Hint: (10.32) → nr ∝ T 3 ∝ S −3 and nb ∝ ρm ∝ S −3 → nr /nb = const;
nr = 20 (2.725)3 = 405 cm−3 ; nb = Ωb ρc /mproton = 0.044 · 1.88 × 10−29 ·
0.712 /1.67 × 10−24 cm−3 (Table 9.2).

10.4 The formation of structure
The formation of structures takes place in matter era, with roots going back
as far as the inﬂation period. This is a very active ﬁeld of research involving
many complex physical processes. We restrict ourselves here to outlining a
few basic ideas. A density concentration will collapse under its own gravity if
the time for gravitational contraction (Gρ)−1/2 is shorter than the time L/v
required for a pressure correction (L = size of the region, v = sound speed).
This is the Jeans instability (1902). Equating the two gives

LJ = v

π
Gρ

1/2
;

TJ = (Gρ)−1/2 .

(10.37)

Mass concentrations larger than the Jeans length LJ will collapse on√ a
timescale TJ , smaller ones will oscillate with a period TJ . The factor π
emerges from a detailed calculation.
The Jeans instability is slowed down drastically by the expansion. Consider a homogeneous spherical mass concentration with a density ρi and scale
factor Si that diﬀers from the rest of the universe. The idea is that the outside world keeps evolving unperturbed as a k = 0 universe – the outside

204

10 The Evolution of the Universe

r, S

ri , Si

r= 0

Fig. 10.5. A spherical region (density ρi ) contracts in an expanding ﬂat FRW
universe (density ρ).

world will not notice the inner mass concentration as long as it is spherically
symmetric – while the inner region evolves as a k = +1 universe. It does not
expand as fast as the external Hubble ﬂow, reaches maximal expansion and
collapses, leading eventually to the formation of (a cluster of) galaxies. We
may describe the evolution of the disturbance by two equations of the form
(10.2):
S̈/S = − aρ ;
S̈i /Si = − aρi .
(10.38)
For brevity we write a ≡ 4πG/3. The cosmological constant can be ignored
in the early universe. Conservation of mass demands that
ρi Si3 = ρS 3 .

(10.39)

Put ρi = ρ + δρ and Si = S + δS and linearise (10.39) for small δρ and δS:
0 = (ρ + δρ) (S + δS)3 − ρS 3
or

S 3 δρ + 3ρS 2 δS ,
D

δρ/ρ = −3 δS/S = x .

(10.40)

From (10.38):
S̈ + δS̈ = − a (ρ + δρ) (S + δS)
− aρS − aSδρ − aρδS .

(10.41)

Because S̈ = −aρS, we are left with δS̈ = −aSδρ − aρδS. Now insert δρ = ρx
and δS = −Sx/3:
(Sx)¨ = 2aSρx = − 2S̈x .
(10.42)
After some rearranging we ﬁnd

10.4 The formation of structure

ẍ +

3S̈
2Ṡ
ẋ +
x = 0.
S
S

205

(10.43)

For a k = 0 universe in the matter era S ∝ t2/3 , so that Ṡ/S = 2/3t and
S̈/S = −2/9t2 :
4
2
ẍ +
ẋ − 2 x = 0 ,
(10.44)
3t
3t
from which we see that x must be ∝ tα . Substitution: α2 + 13 α − 23 = 0 →
α = 23 or −1. It follows that x = δρ/ρ ∝ t−1 or ∝ t2/3 , i.e. ∝ S. The former
solution is a more rapidly expanding perturbation connecting to the Hubble
ﬂow at t = ∞. The second solution is the one we are looking for, and the con1.
clusion is that in the matter era δρ/ρ grows ∝ t2/3 ∝ S as long as δρ/ρ
The twist in the story is that the CMB gives information on the value of δρ
at the time of recombination. Prior to decoupling, adiabatic compression generates a temperature perturbation in response to a baryon density variation
δρ :
 
1 δρ
δT
=
.
(10.45)
T
3
ρ b
If δρ is located on the last scattering surface we observe this δT in the CMB
today, provided the density perturbation is smaller than the horizon size at re◦
combination, i.e. for angles <
∼ 1 , see exercise. The observed CMB temperature
diﬀerence between two directions separated by 1◦ or less is (δT /T ) ∼ 3 × 10−5
(§ 11.4). Consequently (δρ/ρ)b,rec ∼ 10−4 , which shows that the universe
was very homogeneous during the recombination. We may now compute the
present value of (δρ/ρ)b :
 
 
δρ
δρ
103
∼ 0.1 .
(10.46)
ρ b,0
ρ b,rec
It follows that the density contrast is only ∼ 0.1, so that we would have no
galaxies today, in obvious conﬂict with the facts. The conclusion is that structure formation in a universe ﬁlled with baryons, electrons and photons does
not proceed as observed. The missing link is the non-baryonic dark matter,
which turns out to be able to enhance the initial value of (δρ/ρ)b .
Structure and dark matter
The evolution of density perturbations in the dark matter and the baryonelectron-photon gas in the early universe is a complicated aﬀair where only
numerical simulations can provide reliable answers. We present here a much
simpliﬁed description of the main issues. In this and the next section ‘dark
matter’ is understood to be non-baryonic dark matter, assumed to be cold, 9
9

Non-baryonic matter is said to be hot / cold when the WIMP in question is
relativistic / non-relativistic at the moment of its own decoupling.

206

10 The Evolution of the Universe

(a) t < tmat
non-baryonic dark matter

x
to observer

(b) tmat < t < trec

(c) t = trec

x

x

Fig. 10.6. The evolution of one Fourier component of dark matter and beγ ﬂuid
density perturbations of the same wavelength. The x-axis is along an arbitrary
direction in the surface of last scattering, and comoving with the Hubble ﬂow along
the line of sight. The expansion along the x-axis is suppressed. (a) At the beginning of
the radiation era the modes have the same phase and amplitude, and fast expansion
prevents growth. (b) During the matter era the dark matter mode amplitude grows
while the beγ mode is damped. (c) At recombination the photons propagate freely
in all directions and carry a characteristic angular temperature modulation pattern
that we observe today. The path length diﬀerence ∆ between dark matter and beγ
mode, observed from a distance of the last scattering surface, determines the position
of the maxima in the angular power spectrum of the CMB. See text and § 11.4.

so that the signal speed v is small. The baryon-electron-photon gas is referred
to as the beγ ﬂuid, and includes of course all baryons, also those that develop
into dark baryons later. Frequent electron-photon Thomson scattering and
charge neutrality render √
the beγ ﬂuid a tightly coupled system with a very
high signal speed v = c/ 3, see exercise. The photons provide the pressure
and the baryons the inertia. The only communication between dark matter
and beγ ﬂuid is through perturbations δφ in the gravity potential. The latter
are mainly generated by the density perturbations in the dark matter as it
has a much higher density than the baryons.

10.4 The formation of structure

207

At the end of the inﬂation period the energy of the scalar ﬁeld ψ that
governs the evolution of the universe is converted into (dark) matter, § 13.3,
and ﬂuctuations δψ appear as density ﬂuctuations. Dark matter and beγ ﬂuid
have the same initial relative density distribution, so that the Fourier modes
of δρ/ρ of both have initially the same phase and amplitude (at a given wave
number). But the expansion in the radiation era is so fast that the density
perturbations cannot grow (without proof), Fig. 10.6a.
In the matter era, dark matter modes with wavelengths smaller than the
horizon size grow ∝ S as derived earlier. But beγ modes of similar wavelengths
√
cannot grow, because their Jeans length is much larger since v = c/ 3. Computations show that beγ mode amplitudes decrease and that they are outrunning their virtually stationary dark counterparts, Fig. 10.6b. In conﬁguration
space the dark matter perturbations are seen to grow, and the associated gravity perturbation tries to pull the baryons into the dark matter concentrations.
But photon pressure is able to prevent that, and the beγ ﬂuid perturbations
are actually damped.
At recombination the beγ ﬂuid desintegrates. Free photons depart in all
directions and the baryons now do fall into the gravity wells of the dark matter, after which (δρ/ρ)b grows ∝ S. But since (δρ/ρ)DM has grown relative
to (δρ/ρ)b , the initial value of (δρ/ρ)b is larger than the value 10−4 derived
previously. The observed δT /T of the CMB is further reduced by the required
summing over all waves and directions, the Doppler eﬀect due to modes that
cross the last scattering surface (which we neglected sofar), perturbations
along the path to the observer, etc. The upshot is that in a universe with
non-baryonic dark matter the observed CMB temperature ﬂuctuations correspond to a larger value of (δρ/ρ)b,rec than what one would naively infer from
(10.45).
Simulations10 show that the matter distribution evolves into a cosmic
web of ﬁlaments and voids, more or less as observed, see Fig. 10.7. Models
in which the dark matter is hot (HDM models) predict a preponderance of
large mass concentrations because the small-scale density ﬂuctuations formed
during the inﬂation period are largely wiped out by the fast WIMPs. Models
with cold dark matter (CDM models) with slow WIMPs produce more smallmass concentrations, and agree better with the observed mass distribution. 11
The identity of the dark matter WIMP(s) is unknown.

10
11

Bertschinger, E., A.R.A.A. 36 (1998) 599.
See further Börner (1988) Ch. 10 ﬀ; Kolb and Turner (1990) Ch. 9; Padmanabhan
(1993); Peacock (1999) Ch. 15 ﬀ.

208

10 The Evolution of the Universe

Fig. 10.7. Large-scale structure simulations have become a sophisticated industry.
The computational volume of this CDM (Cold Dark Matter) simulation is a cube
with sides 500/h Mpc at z = 0, about 45 times the distance to the Virgo cluster. The
ﬁgure shows the projected dark matter distribution in a slice of 15/h Mpc (at z = 0)
cut from the periodic simulation volume at an angle, to avoid replicating structures
in the lower two images. The zoom sequence displays consecutive enlargements by
factors of four, centered on one of the many galaxy clusters present in the simulation.
The top frame shows several hundred gravitationally bound dark matter structures
orbiting the cluster. The bottom frame shows a virtually homogeneous isotropic
cosmic web of cold dark matter clusters, ﬁlaments and voids of characteristic size
100/h Mpc. The challenge for observational cosmology in the coming decades is to
detect and chart the baryonic component of this cosmic web.
The bottom frame measures 3/h Gpc horizontally, the top frame 11/h Mpc. Colour
coding: brightness indicates the relative density with respect to ρc , and colour the
velocity dispersion. The simulation comprises 1010 particles of 8.6 × 108 /h M and
began at z = 127 (t  14 Myr). Parameters: Ωm = 0.25, ΩΛ = 0.75, h = 0.73.
Credit: the Virgo consortium. From Springel, V., et al., Nature 435 (2005) 629.

10.4 The formation of structure

209

Imprints on the CMB
Because the last scattering takes place in a relatively short time interval, the
CMB provides a snapshot of the acoustic waves in the beγ ﬂuid catching modes
of diﬀerent wavelengths in diﬀerent phases of their oscillation. At recombination the beγ modes have travelled a distance ∆ with respect to the dark
matter, and Fig. 10.6c shows the mode for which this distance corresponds to
λ/2, together with its dark matter counterpart. CMB photons coming from
direction A will have a higher temperature because they emerge from a region
with underdense dark matter, i.e. δφ > 0, which changes their temperature by
δT /T = δφ/c2 , while their initial δT /T = 13 (δρ/ρ)b is also positive. Likewise,
photons from direction B exhibit a lower temperature because (δρ/ρ)b < 0
and δφ < 0. The result is a spatial modulation of the CMB temperature along
the x-direction. Modes of diﬀerent wavelength will produce a less pronounced
spatial modulation. The weak side of the story is that photons react to the net
gravity perturbation of all modes, and summing over all modes and directions
x smoothes δT /T . Nevertheless we expect a maximum temperature diﬀerence
in the CMB between directions that subtend a distance λ/2 at recombination, where λ is constrained by (n + 12 )λ = ∆. If d is the distance to the last
scattering surface at the time of recombination, the corresponding angles are
{(λ/2)/d}(Ωm + ΩΛ )1/2 , or
θn
θn

∆
(Ωm + ΩΛ )1/2 .
(2n + 1)d

(10.47)

For completeness we have included the factor (Ωm + ΩΛ )1/2 to allow for the
fact that curvature aﬀects the apparent angles θn . Since ∆/d is of the order
of 0.7◦ , relation (10.47) predicts a grainy structure in the CMB temperature
at sub-degree scales, which is clearly visible in Fig. 9.2. The corresponding
peaks and their positions θn have now been observed in the angular power
spectrum of the CMB. Relation (10.47) is not as simple as it looks because
∆ depends on λ, i.e. eﬀectively on n, and we refer to Appendix E for details.
But the bottom line is that ∆ can be accurately computed since it depends
on linear mode physics, and so we have a yardstick of known length that we
observe from a distance d, and we may use (10.47) to determine the cosmological parameters (§ 11.4).
As explained in the next chapter, points whose mutual distance is larger
than the horizon size12 cannot have exchanged any signal yet. The above applies therefore to density perturbations smaller than the horizon size ∼ 2ctrec
at recombination, i.e. for angles smaller than ∼ 2ctrec /d ∼ 1◦ . Temperature diﬀerences between directions subtending larger angles are solely due
to pre-existing gravity perturbations δφ. As adiabatic compression no longer
operates for these long-wavelength perturbations, relation (10.45) becomes
12

i.e. about 2ct in the radiation era – factor 2 due to expansion.

210

10 The Evolution of the Universe

invalid and needs to be replaced. A δφ in the region of emergence induces
a δT /T = δφ/c2 . But the distance the photons have to cover also changes
(this is comparable to the Shapiro delay of radio signals, § 4.4). Consequently
they start their trip to us at some other instant, whence we see a diﬀerent
temperature. It turns out that δT /T = − 23 δφ/c2 . The net result is called the
Sachs-Wolfe eﬀect:
1 δφ
δT
1
=
= −
T
3 c2
3



H
kc

2 

δρ
ρ


.

(10.48)

DM

It is the dominant eﬀect for density perturbations with wavelengths 2π/k
larger than the horizon size at recombination. The potential perturbations are
linked to dark matter density perturbations by Poisson’s equation −k 2 δφ =
∇2 δφ = 4πGδρ, and G is eliminated with (10.21) in the form H 2 = 8πGρ/3.
So δρ/ρ in (10.45) refers to baryons, but in (10.48) to dark matter. Recall that
in this section ‘dark matter’ stands for non-baryonic dark matter. The ﬁnal
step is again a summation over all waves. For a so-called scale-free spectrum
of dark matter perturbations, (δρ/ρ)2k  ∝ k, the result is that the r.m.s. CMB
temperature diﬀerence between two directions subtending an angle θ  1◦ is
approximately independent of θ. 13

Exercise 10.12: Show that a region with a diameter equal to the horizon
size at recombination is now seen under an angle of ∼ 1◦ .
Hint: This exercise and the next require some knowledge of the next chapters.
Let’s work in the subspace t = t0 . The horizon size at trec is about 2ctrec (a
more precise value is given in (11.20)); at t0 this has expanded by a factor
1 + z = 1100. In a ﬂat universe the angle is 2ctrec (1 + z)/d0 ≡ 2ctrec /d
where d0 = 3.3ct0 = 3.3 · 0.96c/H0 is the distance to the last scattering
surface, Table 11.1. For the inﬂuence of expansion on the viewing geometry
see Fig. 13.2.
Exercise 10.13: Show that the diameter of a sphere containing 1015 M at
recombination is now seen under an angle of about 0.25◦ .
Hint: Work again in the ﬂat subspace t = t0 . The angle is 2R/d0 with R ﬁxed
by (4π/3)R3 Ωb ρc = 1015 · 2 × 1033 ; see previous hint for d0 .
√
Exercise 10.14: Show that the signal speed in the beγ ﬂuid is c/ 3.
13

For more information on the physics of CMB temperature ﬂuctuations see Peacock (1999) Ch. 18; Hu and Dodelson, A.R.A.A. 40 (2002) 171.

10.4 The formation of structure

211

Hint: Ignore the baryons and electrons as there are very few of them, see
(10.36). The speed v of small perturbations in a medium with pressure p and
2
ρvth
, so v
vth = thermal
density ρ is v 2 = ∂p/∂ρ (e.g. a gas with p
2
speed). A photon gas has p = /3 and ρ = /c . In reality the inﬂuence of the
baryons can only be neglected in the early radiation era. In the matter
√ era
prior to recombination the signal speed is noticeably smaller than c/ 3.

11
Observational Cosmology

The two previous chapters dealt with the properties of universes of the FRW
type, which in all likelihood includes our own. The perspective was the behaviour of the homogeneous subspaces t = constant, as a function of t. However,
since we are located inside the universe we cannot observe these spaces. We
observe events located on our past light-cone, and that gives us a totally different perspective on the universe. Our view is restricted to a small section of
the universe, as epitomized in the cartoon on the left. The situation resembles observers on Earth who cannot look beyond the horizon. The question
arises how the properties of these spaces t = constant may be determined
observationally. To this end it is necessary consider the meaning of distance
in an expanding universe and to obtain the theoretical form of the Hubble
relation. Attention is paid to the recent breakthrough in the determination
of the cosmological parameters H0 , Ωm and ΩΛ by the observation of distant
type Ia supernovae and the angular correlation spectrum of the CMB. Finally,
we consider the computation of observable quantities by integration over the
light-cone.

11.1 Redshift and distance
The act of observing is analysed in Fig. 11.1. Our worldline is AA0 , and BB0
is the worldline of a distant source B, at a constant co-ordinate distance r0
from us. The geometrical distances of A0 B0 and AB are called d0 and d,
respectively. These are the distances of B to us in the subspace t = constant
at time t0 (‘now’), and at an earlier time t. The expansion makes that d0 > d,
but that is not visible in a co-ordinate picture. The fact that we observe B
means that it emits light propagating to us on a null geodesic, arriving in A0 at
time t0 . We cannot observe B0 because it is not on our light-cone. The shape
of this light-cone is given by d(t).
√ First we determine d0 . Take dθ = dϕ = 0
in (9.20), to ﬁnd that dl = S dr/ 1 − kr2 , and integrate:

d0 = S0
0

r0

dr
√
=
1 − kr2



S0 f (r0 )

(k = ± 1) ;

S0 r0

(k = 0) ,

(11.1)

214

11 Observational Cosmology

d t0

A0

B0

t0 (now)

n0, l0, F0
(observer)

n, l, L
(source)

r0

dt
A

B

t

Fig. 11.1. Co-ordinate picture showing the vertical worldlines of two objects participating in the Hubble ﬂow, at a ﬁxed co-ordinate distance r0 . At time t (t0 ) the
geometrical distance of A and B is d (d0 ). Photons emitted by B travel along null
geodesics (dotted lines) and are detected in A0 . For simplicity B is assumed to be
a monochromatic source (wavelength λ, frequency ν, luminosity L), while A0 sees a
wavelength λ0 , frequency ν0 and a ﬂux density F0 .

with f (x) = arcsin(h) x, but we need that only in § 11.3. If we deﬁne v0 ≡ d˙0
then v0 = Ṡ0 f (r0 ) = (Ṡ0 /S0 )d0 , or
v 0 = H0 d 0 ,

with

H0 ≡ Ṡ0 /S0 .

(11.2)

Apparently, the ‘geometrical speed’ v0 and the geometrical distance d0 obey
the Hubble relation. But this is rather useless as neither v0 nor d0 can be
measured. We can only measure distances of sources that we see, i.e. are
connected to us by a null geodesic, like for example B. But since B and A0
are not in the same subspace t = constant, their distance is not a well deﬁned
concept. Moreover we do not measure a velocity but rather B’s redshift z:
z =

λ0 − λ
,
λ

(11.3)

where λ, λ0 = wavelength at emission by the source B, and at detection in
A0 , respectively. We shall now ﬁrst express z in terms of the scale factor S,
and return to the distance issue later.
Fig. 11.1 shows two neighbouring null geodesics from B to A0 . These are
given by1
1
dr
dt
= − √
,
(11.4)
S
c 1 − kr2
1

Put ds2 = dθ = dϕ = 0 in (9.19), and dt > 0 for dr < 0.

11.1 Redshift and distance

from which it is inferred that

 t0
1 r0
dt
dr
1
√
=
= f (r0 ) .
2
S
c
c
1 − kr
t
0

215

(11.5)

This relation determines the time of emission t for given r0 . By comparing
(11.5) and (11.1) we see that

d0 = cS0
t

t0

dt
.
S

(11.6)

We now have two expressions for d0 . In (11.1) we know only the co-ordinate
distance r0 , but in (11.6) we have exploited the extra information of ‘eye
contact’ to eliminate r0 . Because the right hand side of (11.5) is constant we
have
 t0 +δt0
 t0
δt0
dt
dt
δt
=
→
.
(11.7)
=
S
S
S
S
0
t
t+δt
Furthermore we know that νδt = ν0 δt0 , so that λ0 /λ = ν/ν0 = δt0 /δt = S0 /S,
and
S0
λ0 − λ
=
−1.
(11.8)
z =
λ
S
We observe that z > 0 and this is now seen to be a consequence of the expansion: the scale factor increases, S(t0 ) > S(t). Apparently, the wavelength
of the photon is stretched in proportion to the expansion of the universe.
It is illuminating to derive the redshift from a diﬀerent perspective. According to (9.23) or (9.25) a radial distance d from the origin is equal to
d = Sdχ. The
√ local velocity v of a particle is therefore v = d/dt =
Sdχ/dt = λc/ S 2 + λ2 , according to (9.48). A little algebra shows that
Sv/ 1 − (v/c)2 = constant, or
pS = constant ,

(11.9)

where p is the particle’s momentum. This says that the De Broglie wavelength
h/p of the particle scales ∝ S, and expresses the fact that particles are subject
to adiabatic cooling as the universe expands. Note that relation (11.9) holds
also for photons since p = k = ω/c.
The redshift z is a key observable in cosmology, and astronomers are
habitually given to jargon like ‘the universe at redshift z’. This expression
indicates the spherical shell around us denoted as Σ(t) in Fig. 9.3, containing
all sources at that redshift, assuming that they follow the Hubble ﬂow. It is
the cross section of our past light-cone and the subspace t = constant, where
t is ﬁxed by (11.8) and S = S(t). However, the phrase is also used to indicate the entire homogeneous subspace t = constant – a space that we cannot

216

11 Observational Cosmology

observe (but of course a very convenient theoretical concept).
Alternative explanations for the redshift have been advanced, such as the
tired light concept. The idea is that photons would be subject to a small
systematic energy loss as they propagate through space. That would mimic
Hubble’s law, in the absense of a real expansion. The main problem with this
explanation is that any mechanism that changes the energy of a photon will
also aﬀect its momentum. That is, to some degree it is a scattering process.
Distant objects would be blurred – contrary to what is observed. Furthermore,
in the standard interpretation of the redshift, light curves of distant supernovae should broaden with z, as is observed, but tired light would produce no
such broadening. Other explanations suﬀer from similar objections, and the
conclusion that the universe expands seems inescapable.
Cosmological models
We are now in a position to construct a cosmological model, that is, a listing
of the age t of the universe, of d, d0 and the luminosity distance dL (a concept
deﬁned in § 11.3), as a function of redshift, see Table 11.1. This table is
constructed as follows. We begin by rewriting (11.8):
u =

d
1
S
.
=
=
S0
d0
1+z

(11.10)

Here we have used that d = Sf (r0 ) so that d/d0 = S/S0 . Relation (11.10)
ﬁxes d/d0 , and t since S = S(t). To make this more explicit, start with
dt = (dt/du)du, so that t = ∫0u du/u̇, and:
u
du/u̇
t
.
(11.11)
= 01
t0
du/u̇
0

The upper integration limit u equals 1/(1 + z). The explicit expression for t
and t0 is given in (10.14) and (10.15). Next, we write (11.6) in dimensionless
form with the help of cS0 dt/S = cdt/u = cdu/(uu̇):
d0
=
ct0

1
u

(uu̇)−1 du
.
1
du/u̇
0

(11.12)

And ﬁnally, d = ud0 . The next step is to substitute u̇ from (10.8), after which
numerical evaluation of (11.11), (11.12) and (11.27) is straightforward. We
have normalised distances to ct0 , that is, the light distance corresponding to
the age of the universe. The advantage of using relative quantities in Table 11.1
like t/t0 and distances/ct0 is that there is no longer a big diﬀerence between
the models. This is why the reference model (Ωm , ΩΛ ) = (1, 0) remains very
useful even if (Ωm , ΩΛ ) = (1, 0).

11.1 Redshift and distance
Table 11.1. Two FRW universe models

217

a

Ωm = 1; ΩΛ = 0; H0 t0 = 0.67

Ωm = 0.3; ΩΛ = 0.7; H0 t0 = 0.96

z

t/t0

d/ct0

d0 /ct0

dL /ct0

t/t0

d/ct0

d0 /ct0

dL /ct0

0
0.2
0.5
1
2
5
10
30
100
1000
∞

1
0.76
0.54
0.35
0.19
6.8-2
2.7-2
5.8-3
9.9-4
3.2-5
0

0
0.22
0.37
0.44
0.42
0.30
0.19
7.9-2
2.7-2
2.9-3
0

0
0.26
0.55
0.88
1.3
1.8
2.1
2.4
2.7
2.9
3

0
0.31
0.83
1.8
3.8
1.1+1
2.3+1
7.6+1
2.7+2
2.9+3
∞

1
0.82
0.63
0.43
0.24
8.6-2
3.5-2
7.3-3
1.2-3
4.0-5
0

0
0.16
0.30
0.40
0.42
0.31
0.21
8.9-2
3.0-2
3.3-3
0

0
0.20
0.46
0.80
1.3
1.9
2.3
2.7
3.1
3.3
3.4

0
0.24
0.69
1.6
3.8
1.1+1
2.5+1
8.5+1
3.1+2
3.3+3
∞

a

Notation: a ± b ≡ a × 10±b ;
t = age of universe at the time the object emits the light we now see;
d = geometrical distance of object at time t;
d0 = geometrical distance of object now, at time t0 ;
dL = luminosity distance (11.27) of the object.

Exercise 11.1: Show that the invariant deﬁnition of the redshift is:
1+z =

(kα uα )e
,
(kα uα )o

(11.13)

where the index e, o indicates the emittor and the detector, respectively; uα =
4-velocity (of the emittor or the detector), and k α is the photon wavevector.
Hint: 1 + z = λo /λe = νe /νo = Ee /Eo . Then (3.55) with pα = k α .
Exercise 11.2: Show that the age of an FRW universe at z  1 is independent of ΩΛ :
−1/2
(1 + z)−3/2 .
(11.14)
H0 t = 23 Ωm
1 only the Ωm /u-term in (10.14) matters. If we compute
Hint: For u = S/S0
trec of our universe with (11.14) the result is trec = 4.8 × 105 yr. Why is this
larger than (10.30)?

218

11 Observational Cosmology

Fig. 11.2. Scale model of an (Ωm , ΩΛ ) = (1, 0) FRW universe. On the vertical axis
the age t of the universe in units of the present age t0 . On the horizontal axis, in
green, a 1D cross section along an arbitrary line of sight with equidistant galaxies
(for simplicity). Distance scale: we arbitrarily adopt A0 C0 = 1.2 ct0 . The galaxies
partake in the universal expansion (- - -) and evolve with time as they do so. Also
indicated are the geometrical distances d and d0 from Table 11.1, and A0’s past
light-cone d(t) in red. The wiggly lines are photons travelling locally with speed c.
Adapted from Hoyng, P., Zenit, July/August 1998, p. 340.

11.2 The visible universe and the horizon
We shall now take a closer look at the properties of FRW universes as given
in Table 11.1. Since we discuss issues here that most FRW universes share, we
focus on the (Ωm , ΩΛ ) = (1, 0) model as a typical example. Age and expansion
of this universe are given by (10.12): S/S0 = (t/t0 )2/3 and t0 = 23 H0−1 . For
the shape of the past light-cone d(t), it is easiest to use (11.6) because we
t
t
know S(t): d = (S/S0 )d0 = cS t 0 dt/S = ct2/3 t 0 dt/t2/3 , or
 2/3 
 1/3 
t
t
d
= 3
1−
.
ct0
t0
t0

(11.15)

This leads to the scale model shown in Fig. 11.2. The horizontal axis of this
ﬁgure is a 1D cross section through the universe along an arbitrary line of

11.2 The visible universe and the horizon

219

Fig. 11.3. Photon propagation in an expanding universe may be understood with
the example of a cyclist moving at constant speed c with respect to the local road,
while the road is being stretched like a rubber band. Left: no expansion, the path
is a straight line with inclination c. Middle: the expansion is initially slow, but
accelerates with time. Right: expansion is initially fast, but slows down with time,
as in a real universe (for ΩΛ = 0). Adapted from Hoyng, P., Zenit, July/August
1998, p. 340.

sight. On this axis are located our system A, then B, next C, etc. The broken
lines show how the universe expands ∝ t2/3 . Each 1D cross section may be
generalised to a 3D image of the universe at that age. This is the green section
of Fig. 11.2. It is eﬀectively an external point of view: the observer is located
outside the universe and surveys the entire universe at a glance, as if one is
studying a map.
However, due to the ﬁnite speed of light we (A0 ) do not see our neighbours
at the same time t0 but at some earlier time. All light that we receive at t0
must have travelled along the past light-cone, given by (11.15). Some photons
come from far and began their journey long ago, while others enjoyed only a
brief trip. But all have travelled along the path marked light-cone, indicated
in red in Fig. 11.2. Hence, we see the systems B1 , C2 , D3 ,.., behind each other,
at progressively larger redshift. These systems are juvenile forms of B0 , C0 ,
D0 ,.. located in the universe at time t0 . The upshot is that we experience the
universe as a series of nested spherical shells, each showing a diﬀerent piece
of an increasingly younger universe. This is the internal point of view, that
of an observer inside the universe. Note that Fig. 11.2 is also the geometrical
picture corresponding to the co-ordinate picture in Fig. 9.3.
The shape of the past light-cone may be understood with the help of
Fig. 11.3. A photon in an expanding universe is like a cyclist on a road that
is being stretched like a rubber band. The cyclist moves always at constant
speed c with respect to the road (locally special relativity holds). The right
panel corresponds to the situation in the universe. The expansion is initially
fast and slows down gradually. The cyclist is initially ‘drawn away’ from A

220

11 Observational Cosmology

(us), but may eventually reach any position in the direction of cycling. This
picture is an exact model of photon propagation, as we shall now show. The
distance d between A and the cyclist obeys
d˙ = Hd − c ,

(11.16)

where H = H(t). The ﬁrst term describes the homogenous stretching of the
road, the second term the motion with respect to the road. Substitute H =
Ṡ/S, and (11.16) may be written as
 
d ˙
c
= − .
(11.17)
S
S
t
Integration yields d/S = −c 0 dt/S + const. Initial condition: d = 0 at t = t0 .
t
Result: d = cS t 0 dt/S, which coincides with (11.6) since d = (S/S0 )d0 .
We draw attention to two remarkable properties of FRW models. The ﬁrst
is that according to Fig. 11.2 distant sources at large z were relatively near to
us at the time they emitted the radiation we now see. Formally d → 0 as t → 0.
In spite of this proximity, the light could not reach us any sooner because the
universe was expanding so much faster than it does today – otherwise it would
have long since recollapsed. One might say that new space is created at a very
high rate, which makes that the photon ‘moves away from us as it travels in
our direction’. Only later, when the expansion has slowed down, the photon
is able to reach us. The inward bending of our past light-cone at large z is
therefore caused by the extremely rapid expansion of the early universe.
The particle horizon
The second feature is that d0 → 3ct0 or thereabout for z → ∞, see Table 11.1.
The present distance of the remotest objects that we can see is apparently
not larger than ∼ 3ct0 . Let’s check that for the (Ωm , ΩΛ ) = (1, 0) universe:
d0 = (S0 /S)d = (t0 /t)2/3 d, according to (10.12), and with (11.15): d0 =
3ct0 {1 − (t/t0 )1/3 } → 3ct0 for t → 0. This boundary is called the horizon,
more precisely the particle horizon. Since light travels locally at speed c one
may say that a photon has traversed a distance ct0 from the moment of the
Big Bang. The expansion increases the distance between starting and arrival
point of the photon apparently by another 2ct0 . This extra amount depends
on the details of the expansion, i.e. on S(t), but not very strongly. For a (1, 0)
reference universe t0 = 23 H0−1 , and the horizon distance is 2c/H0 . And for an
(Ωm , ΩΛ ) = (0.3, 0.7) universe the horizon is at 3.4ct0 (Table 11.1), which is
equal to 3.4c · 0.96/H0 ∼ 3.3c/H0 .
The horizon distance in a FRW universe is apparently a few times the

11.2 The visible universe and the horizon

221

Fig. 11.4. The visible universe is the space inside the horizon of an observer. It
contains all matter from which the observer may have received a light signal. The
visible universes of any two observers A and B comoving with the Hubble ﬂow
overlap progressively, but were disjunct at some point in the past. A can only see
B and vice versa after they have entered each other’s horizon. This leads to the
so-called horizon problem: why do A and B begin to participate in the expansion at
the same moment?

Hubble radius c/H0 . The space inside the horizon is called the visible universe,
sometimes just horizon space. Note that each observer has its own visible
universe, see cartoon on p. 212. The name horizon derives from the analogy
with the terrestrial horizon. An object can only have interacted with objects
inside its horizon – anything outside can have had no inﬂuence.2
Consider two point A and B at a ﬁxed co-ordinate distance r0 , Fig. 11.4.
In a k = 0 universe their geometrical distance d at time t is S(t)r0 . It follows
that d ∝ tα with α 1/2 in the radiation era, α 2/3 in the matter era. The
horizon distance at that time is (put t → 0 and t0 → t in (11.6)):

d = cS
0

t

ct
dt
=
,
S
1−α

(11.18)

or 3ct in the matter era and 2ct in the radiation era. The general expression
is given in (11.20). It follows that the horizon distance grows eventually faster
than the geometrical distance, so that the visible universes of A and B will
overlap more and more in the future. Conversely, regardless of the distance of
A and B, if we go back in time, there comes a moment that their horizon spaces
were disjunct. This leads to the so-called horizon problem, a fundamental
defect shared by all FRW universes, that has only been remedied by the
advent of inﬂation theory, see Ch. 13.
2

An ΩΛ = 0 universe possesses also an event horizon, see exercise 11.6.

222

11 Observational Cosmology

Fig. 11.5. The Big Bang is often misinterpreted as a point explosion, with matter expanding into a pre-existing empty space. Adapted from Hoyng, P., Zenit,
July/August 1998, p. 340.

A common mistake
The nature of the Big Bang is often misunderstood. The very name suggests
an analogy with a point explosion, Fig. 11.5. This seems a rather natural idea,
and that may explain why it appears to be so popular. But it is in conﬂict
with the observations. Brieﬂy, the argument is as follows. From the average
density in the universe and the fact that the edge of the explosion is at most
ct0 away, the optical depth to the boundary is inferred to be much smaller
than 1, so that we should be able to see it. But the universe is also observed
to be highly isotropic. These two statements are incompatible unless we are
located at the centre of a spherically symmetric explosion, which is highly
unlikely. Accepting that would mean a relapse to some kind of a geocentric
world model. It would, incidentally, also be impossible to explain the CMB
as a remnant of the Big Bang since any radiation emitted by the explosion is
necessarily ahead of the matter.
The correct picture is that the universe has no boundary, that space
is homogeneously ﬁlled with matter, and that space itself is swelling. This
picture emerges clearly from the derivation of the Robertson-Walker metric
in §§ 9.2 and 9.3. The galaxies have constant co-ordinates (‘do not move’)
and are rather like currants in a rising bun. This picture of a swelling space
should be used with care. The wavelength of a photon (more generally, the
De Broglie wavelength of a particle) is stretched proportional to S, indeed,
stretched with the swelling of space, but that does not imply that extended
material objects expand as well. That would only happen if the various parts
of the object move along geodesics of the Robertson-Walker metric. But this
is usually not the case due to extra forces, for example internal elastic forces
in a measuring rod, or local gravity in galaxies.

11.2 The visible universe and the horizon

223

Exercise 11.3: Show that in an (Ωm , ΩΛ ) = (1, 0) universe the photons we
see today (including those of the CMB) have never been farther away from
us than d = (2/3)2 ct0 0.44ct0 . This happened at t/t0 = (2/3)3 0.30. At
that point the photon just beats the expansion and its geometrical speed d˙ to
us is zero. Any conﬂict with SR?
Hint: See Fig. 11.2, and determine the maximum of (11.15). There is no conﬂict
with SR: the locally measured speed of the photon is always c. The co-ordinate
speed of a photon falling into a black hole also becomes zero near the horizon,
§ 6.3.
Exercise 11.4: Continue exercise 9.9 and prove that in an (Ωm , ΩΛ ) = (1, 0)
universe the bullet travels a co-ordinate distance
∆r =

2β c
,
S 0 H0

(11.19)

provided the initial velocity is small, β
1. This result may be interpreted
as follows. Mark the position that the bullet will eventually reach as A. The
geometrical distance between the point of ﬁring and A is now 2β(c/H0 ).
Hint: (9.48) becomes dr/dt
βcS0 /S 2 or ∆r
S/S0 = (t/t0 )2/3 and integrate. Then use (11.1).

βcS0

∞
t0

dt/S 2 ; (10.12):

Exercise 11.5: Show that the horizon distance in the early universe is given
by:
√

(11.20)
d = 3ctm x x + 1 − 1 ,
in the notation of § 10.3. Show that d = 2ct for early times, 3ct for late times,
and 2.25ctrec at recombination. Does this imply that the speed of light is larger
than c, or that the horizon actually moves at superluminal speed?
t
u
x
x
Hint: d = cS 0 dt/S = cu 0 du/(uu̇) = cx 0 dx/(xẋ) = ctm x 0 dx/
x
√
(x dx/dτ ); insert (10.25): d = 3ctm x 0 dx/(2 1 + x ) → (11.20). For early
3
2
2ctm τ = 2ct. For large x:
times (x
1): d
2 ctm x ; then (10.29): d
3/2
= 3ctm τ = 3ct. At recombination x = 3 → d = 9ctm = 94 ctrec .
d 3ctm x
Twice no.
Exercise 11.6: Show that an ΩΛ = 0 universe has also an event horizon and
compute its size.

224

11 Observational Cosmology

Hint: Horizons delineate spheres of inﬂuence. The particle horizon embraces
all points (at time t0 ) that have been able to interact with us in the past.
Points inside the event horizon will interact with us in the future (however distant). Geometrical distance of starting position of a photon that
T
∞
reaches us at T is d0 = cS0 t0 dt/S. Let T ↑ ∞: d0 = c 1 du/uu̇ =
∞
(c/H0 ) 1 du(Ωm u + ΩΛ u4 + Ωk u2 )−1/2 which converges if ΩΛ = 0. For
∞
√
√
Ωk = 0 (ﬂat universe): d0 < {c/(H0 ΩΛ )} 1 du/u2 = c/(H0 ΩΛ ). Photons departing to us from beyond d0 will never reach us due to the exponential
expansion.

11.3 Luminosity distance and Hubble relation
The geometrical distances d and d0 in Fig. 11.2 are convenient theoretical
concepts but they cannot be measured. We shall not dwell on the issue of
distance determination here, as it is a large topic in its own right. We restrict ourselves to illustrating how d and d0 can be determined through the
method of standard candles, a time-honoured method to ﬁnd distances of remote objects. The idea is that there are classes of objects whose members
all have about the same absolute luminosity. For example, Cepheid variables
with the same oscillation period, the brightest member of a cluster, type Ia
supernovae, etc. Once the distances to a subset of objects have been determined independently, we only have to recognise a source as a member of its
class, and its absolute luminosity L is known, at least in principle. This leads
to the concept of luminosity distance dL , a measurable quantity, deﬁned as
L = 4πd2L F0 where F0 is the ﬂux density of the source measured at t0 , and L
the luminosity of the source at emission, Fig. 11.1.
For convenience we assume that the source is monochromatic. Number
of photons emitted in δt seconds: δN = (L/hν)δt. These are spread over a
spherical surface of area O = 4πS02 r02 , see below relation (9.22), so that
F0 =
=

δt ν0
hν0 δN
L
=
O δt0
4πS02 r02 δt0 ν
1
L
,
2
2
4πS0 r0 (1 + z)2

(11.21)

from which it follows that

dL =

L
4πF0

1/2
= r0 S0 (1 + z) ,

(11.22)

11.3 Luminosity distance and Hubble relation

225

In particular for k = 0:
dL = (1 + z)d0 .

(11.23)

The luminosity distance is a formal quantity in the sense that is not possible
to indicate a space ‘in which dL lies’, as we could in case of d and d0 . The
point is, however, that dL can be measured, and then d0 is also known through
(11.23) or (11.26).3
We shall now derive the Hubble relation, i.e. the relation between the
two observable quantities dL and z. We start from (11.1), and note that the
function f (x) equals arcsin x, x, arcsinh x for k = 1, 0, −1. Relation (11.1) may
now be inverted:
(11.24)
r0 = sinn(d0 /S0 ) ,
with

⎧
⎪
⎨ sin x
x
sinn x =
⎪
⎩
sinh x

(k = 1) ;
(k = 0) ;
(k = −1) .

(11.25)

This means that r0 = sinn |Ωk |1/2 H0 d0 /c , because c/H0 S0 = |Ωk |1/2 according to (10.5). Insert that in (11.22):


H0 d L
H0 d 0
= |Ωk |−1/2 (1 + z) sinn |Ωk |1/2
,
(11.26)
c
c
and we have found the generalization of (11.23) for k = 0. Next we use (11.6)
1
or (11.12) to obtain d0 = c u du/uu̇ and take u̇ from (10.8):

 1
H0 d L
= |Ωk |−1/2 (1 + z) sinn |Ωk |1/2
dx
c
u


Ωm x + ΩΛ x4 + Ωk x2

−1/2 

.

(11.27)

Since u = 1/(1 + z) we have found the theoretical form of the Hubble relation.
Simpliﬁcation is possible if z is small: the integration limits 1 and 1/(1 + z)
are close to each other, so that the argument of the sinn-function becomes
small and we may use sinn x x :
 1
−1/2

H0 dL
(1 + z)
dx Ωm x + ΩΛ x4 + Ωk x2
.
(11.28)
c
(1+z)−1
For k = 0 this relation is exact for all z. For small z (11.27) may be approximated as (see exercise):
3

There are also other distance measures in use, such as the angular diameter
distance. See Lightman et al. (1975), exercise 19.9.

226

11 Observational Cosmology

Fig. 11.6. Hubble diagram of type Ia supernovae. Datapoints within ∆z < 0.01 have
been grouped together into a single average datapoint. Also shown are the theoretical
curves for three FRW universe models. These data provide direct evidence for the
existence of dark energy, i.e. a positive cosmological constant Λ. From Knop, R.A.
et al., Ap. J. 598 (2003) 102.

H0 d L
c


z 1+

1
2 (1

−

1
2 Ωm


+ ΩΛ )z + · · · ,

(11.29)

The parameters H0 , Ωm and ΩΛ determine the structure and the evolution
of the universe, and large eﬀorts have been undertaken to determine their
values, in particular during the last decades. The principle is straightforward.
A ﬁt of observations of z and dL to (11.29) yields H0 and 12 Ωm − ΩΛ . But
the method is plagued by many problems such as selection eﬀects, a limited
redshift range (z <
∼ 0.3) and the fact that standard candles are not perfect.
There is always a spread in intrinsic luminosities. For a long time this caused
astronomers to be at loggerheads about the value of H0 ,4 while 12 Ωm − ΩΛ
could not really be determined. The negative correlation between Ωm and
ΩΛ , incidentally, is easy to understand: more matter (Ωm ↑) means more
4

Weinberg (1972) p. 441 ﬀ; Börner (1988) § 2.2; Peebles (1993) Ch. 5; Fukugita,
M. et al. Nature 366 (1993) 309.

11.3 Luminosity distance and Hubble relation

227

gravity, and that may be compensated by adding antigravity, i.e. more vacuum
energy (ΩΛ ↑). It is unfortunate that quasars which have redshifts up to z ∼ 5
are no good as standard candles: in a given redshift interval their apparent
magnitudes vary greatly. Otherwise the values of H0 , Ωm and ΩΛ would have
long since been known.
These eﬀorts have culminated in an HST Key Project to measure H0 ,
which has led to the value H0 = 72 ± 8 km s−1 Mpc−1 . 5 The subsequent
measurement of H0 by the WMAP mission has conﬁrmed this value with
improved accuracy: H0 = 71 ± 4 km s−1 Mpc−1 . It is encouraging that this
value is now being conﬁrmed by independent techniques, such as the SunyaevZeldovich eﬀect, a method that does not rely on the classic (and slippery)
distance ladder.6 Since the ﬁrst measurements of H0 around 1930 its value
has come down by almost an order of magnitude. 7
The Supernova Cosmology Project has used distant Type Ia supernovae,
bright objects that may be detected out to z <
∼ 1, and have been shown to be
rather reliable standard candles. They are believed to be white dwarfs with
progenitor masses in the range 4 − 6 M , pushed over the Chandrasekar limit
by mass transfer. When they explode they all have (hopefully) the same mass
and composition, which explains the standard candle property. The project
managed to measure Ωm and ΩΛ independently (with low accuracy), Fig. 11.6.
The supernova data clearly demonstrate that the cosmological constant of our
universe is nonzero.

Exercise 11.7: Show that the redshift is given by the Doppler formula z = v/c
for small z, but GR corrections become important at larger z.
Hint: From (9.4) and (11.29); d

dL for small z.

Exercise 11.8: Provide the details of the derivation of (11.29).
Hint: Write (· · ·)−1/2 ≡ g(x) in (11.27); g(1) = 1 on account of (10.4) and a ≡
g  (1) = −1+ 12 Ωm −ΩΛ . Put x = 1−y and expand the integral to second order
2

1
∫0z−z g(1 − y) dy
in z: ∫(1+z)
−1 g(x) dx
And sinn x = x to second order in x.

5
6
7

2

∫0z−z (1 − ay) dy

Freedman, W.L. et al., Ap. J. 553 (2001) 47.
Mason, B.S. et al., Ap. J. 555 (2001) L11.
Trimble, V., P.A.S.P. 108 (1996) 1073.

z(1 − z − 12 az).

228

11 Observational Cosmology

Exercise 11.9: Prove the Hubble relation for an (Ωm , ΩΛ ) = (1, 0) universe:


H0 dL
1
= 2(1 + z) 1 − √
,
(11.30)
c
1+z
and show once more that the distance to the horizon is 3ct0 .
Hint: (11.28) → H0 dL /c = (1+z)
2c/H0 = 3ct0 for z → ∞.

1
(1+z)−1

√
dx/ x; (11.23): d0 = dL /(1+z) →

Exercise 11.10: Given an object at z = 0.5 in an (Ωm , ΩΛ ) = (2, 0) FRW
universe. What are the values of: (1) the co-ordinate r, (2) the distance d of
the object at the time of emission of the light we receive from it today, and
(3) the temperature of the CMB at that particular time.
Hint: For large values of z there is no alternative but to make a new Table 11.1
by numerical integration. For small z (11.29) is an option → H0 dL /c 0.5.
Then (11.26): H0 d0 /c arcsin(1/3) 0.34. Expression for r0 above (11.26):
1/3. Furthermore d = (S/S0 )d0 = d0 /(1 + z) = 2d0 /3. Temperature
r0
CMB: (1 + z)2.725 K = 4.09 K.

11.4 The microwave background
The COBE satellite has measured the spectrum and the angular distribution
of the temperature of the CMB on angular scales of 7◦ and larger, and ∆T /T
was found to be of order 10−5 . The CMB is therefore highly isotropic. The
angular distribution of the CMB temperature is a very important issue as it
carries information on the clustering of matter in the universe at decoupling,
z ∼ 1100. Various groups have measured ∆T /T down to spatial scales of
∼ 0.1◦ in a section of the sky. 8 The WMAP mission launched in 2001 has
mapped the entire sky with a resolution of ∼ 0.2◦ . The maps are cleaned from
foreground eﬀects, and the resulting temperature distribution is decomposed
in spherical harmonics Ym (θ, ϕ) :
∆T (θ, ϕ) ≡ T (θ, ϕ) − T0 =

∞ 



am Ym ,

(11.31)

=1 m=−
8

E.g. De Bernardis, P. et al., Nature 404 (2000) 955; Lee, A.T. et al., Ap. J. 561
(2001) L1.

11.4 The microwave background

229

Fig. 11.7. Angular power spectrum of the CMB temperature as measured by
WMAP (points in black), obtained by processing the data through (11.32) and
(11.35). The angular scale (top) is added afterwards for convenience. The black line
is the best ﬁt to a ΛCDM model (= CDM model with Λ = 0). The red points are
previously published results. From Hinshaw, G. et al., Ap. J. S. 148 (2003) 135.

where T0 = T  is the average temperature, and am is given by

∗
am = dΩ ∆T Ym
.

(11.32)

We consider two averages: (1) an average over an ensemble of maps i (all
possible realisations
of the CMB sky), in terms of which, for example, T  =

limN N−1 i Ti (n); (2) an angular average over one CMB map, and · =
(4π)−1 dΩ.
The three dipole coeﬃcients a1m are dominated by the Doppler signal
due to a net velocity of the solar system of 371 ± 1 km s−1 with respect to
the surface of last scattering. The intrinsic dipole anisotropy of the CMB is
presumably much smaller, but cannot be separated from the total signal. The
angular correlation function C(θ) is deﬁned as:
C(θ) = ∆T (n1 )∆T (n2 ) | n1 ·n2 = cos θ
=

1 
(2 + 1)C P (cos θ) ,
4π


(11.33)
(11.34)

230

11 Observational Cosmology

with
C = |am |2 


1
|am |2 .
2 + 1 m

(11.35)

P (x) is the Legendre polynomial of order . The ﬁrst = sign in (11.35) deﬁnes
C as an ensemble average; |am |2  does not depend on m (spherical symmetry). C may also be estimated by the second expression, obtained by angular
averaging, where am are the expansion coeﬃcients of the one CMB sky we
have. At small  the values of the two expressions diﬀer appreciably due to
cosmic variance.9 For completeness we mention that relation (11.34) may be
inverted with the help of the orthogonality of the Legendre polynomials:
 π
C = 2π
C(θ)P (cos θ) sin θ dθ .
(11.36)
0

The proof of relations (11.34), (11.35) and (11.36) is somewhat technical and
deferred to Appendix E.
Fig. 11.7 shows the measured values of c ≡ ( + 1)C /2π, referred to as
the angular power spectrum. We recognize a ﬂat plateau at low , followed by
a series of peaks at larger , thus conﬁrming the physical explanation given in
§ 10.4. We have found earlier that the directions of maximal CMB temperature
diﬀerence subtend angles θn given by (10.47). In Appendix E it is shown that
this implies that the ﬁrst peak in the power spectrum is at
0

π/θ0

277 ,

(11.37)

while the observed value is  = 220 ± 1. The origin of the discrepancy is that
our treatment ignores two aspects of the physics of beγ modes, see Appendix E.
A comparison of the WMAP data in Fig. 11.7 with model simulations
of the c allows a precise determination of the cosmological parameters, see
Table 9.2. In brief outline the story is as follows. The WMAP data, HST
Key Project and supernova data together determine Ωtot ≡ Ωm + ΩΛ , i.e.
the geometry of space. The height ratio of the ﬁrst and second peak ﬁxes
ωb ≡ Ωb h2 , while ωm ≡ Ωm h2 follows from the height ratio of the ﬁrst
peak and the ﬂat plateau at low . And h follows from (10.47): trec is known
since we know Ωm h2 and zrec (by modelling) and r0 (standard neutrinos),
see § 10.3. Since Ωm = ωm /h2 and ΩΛ = Ωtot − ωm /h2 , the value of d at zrec
depends eﬀectively only on h, cf. Table 11.1. But the position θ0 of the ﬁrst
9

Cosmic variance is cosmologist’s jargon indicating the eﬀect that observed and
theoretically computed mean values of a cosmological quantity may diﬀer considerably because our visible universe is only one possible realisation out of many.
The r.m.s. diﬀerence between the two expressions in (11.35) is (∆C )2r.m.s. /C2 =
2/(2 + 1).

11.5 Light-cone integrals

231

peak is measured, hence h and then also Ωb , Ωm and ΩΛ are known. Readers
interested in the (complex) details are referred to the literature.10
In 2007 ESA’s PLANCK mission will be launched carrying a third generation CMB experiment with a much improved angular resolution and sensitivity. This mission is expected to determine the cosmological parameters H0 ,
Ωm and ΩΛ with a precision of 1%.

Exercise 11.11: Show that (∆T )r.m.s. /T0

3 × 10−5 .



Hint: (11.34): C(0) = (∆T )2r.m.s. =  (2 + 1)C /4π
 c / with c ≡
is
dominated
by
the
low- plateau.
(+1)C /2π plotted in Fig. 11.7. The sum

clow  log L; take the cut-oﬀ
Intelligent handwaving: C(0) clow  ·  −1
at L = 103 : (∆T )r.m.s. /T0 (1000 · 10−12 log 103 )1/2 /2.725.

11.5 Light-cone integrals
The computation of observable quantities requires integration over the past
light-cone, and we consider here a few simple problems. The ﬁrst is what is
the volume of our past light-cone, i.e. what is the proper volume of the space
that we see as we look into the universe? The light-cone may be thought of as
a series of nested shells, but the volume of the shells will ultimately decrease
with z because the expansion was less advanced.
Draw two subspaces t = constant in Fig. 9.3 intersecting the light-cone at
t and t + dt. The 2-volume of an intersection is 4πS 2 r2 , see below (9.22). The
proper volume of the shell is now 4πS 2 r2 × the light distance cdt, and the
proper volume V of the light-cone follows by integration:
 
 t0
 t0
d
V = 4πc
S 2 r2 dt = 4πc
S 2 sinn2
dt .
(11.38)
S
0
0
Here we have applied (11.1): d = Sf (r) or r = f −1 (d/S) and f −1 = sinn. To
avoid the complications of non-Euclidean geometry we assume a ﬂat
 t universe,
and then (11.38) reduces to the transparent expression V = 4πc 0 0 d2 dt. To
keep the calculations simple, we consider the reference model (Ωm , ΩΛ ) =
10

Hu, W. and Dodelson, S., A.R.A.A. 40 (2002) 171; in particular Fig. 4; Page, L.
et al., Ap. J. S. 148 (2003) 233; Spergel, D.N. et al., Ap. J. S. 148 (2003) 175.

232

11 Observational Cosmology

(1, 0). With the help of d = (S/S0 )d0 , u = S/S0 , dt = du/u̇ and u̇ from
(10.8) we obtain:
 1

√
cu 1 du
du
2cu
√ =
=
d = cu
(1 − u ) .
(11.39)
H0 u
H0
u
u uu̇
The calculation may now be completed:
 1 2
d
V = 4πc
du
0 u̇

3  1
√
2c
= 2π
u5/2 (1 − u )2 du
H0
0

3  1
2c
= 4π
x6 (1 − x)2 dx
H0
0
3

4π 2c
· 1.19 × 10−2 .
3 H0

(11.40)

It follows that in a (1, 0) universe the proper volume of the past light-cone is
about 1% of the volume inside the horizon (4π/3)(2c/H0 )3 – a number one
would not easily have guessed otherwise.
Next, we compute the number of objects N that are located on the lightcone (i.e. how many objects do we see regardless of their brightness), assuming
that the universe is homogeneously ﬁlled with objects and that their density
is now n0 . Obviously, the past density is n = n0 (S0 /S)3 = n0 /u3 , and the
answer is found by inserting n in the integrand of (11.40):

N = 2πn0

2c
H0

3 
0

1

3

√
(1 − u )2
4πn0 2c
√
du =
,
3
H0
u

(11.41)

which is the present density × the volume inside the horizon. This should
come as no surprise because there is, by deﬁnition, a one-to-one correspondence between objects on the past light-cone and ojects inside the horizon,
see Fig. 11.2.
Olbers’s paradox
How bright is the sky if the objects of the previous example all have a constant
luminosity L0 ? In a ﬂat universe the number of objects in a shell c dt and in
a solid angle δΩ is n · d2 · δΩ · c dt, and the ﬂux density at the observer of one
object is L0 /4πd2L , by deﬁnition. The shell contributes therefore an amount
δI0 δΩ = nd2 δΩ · c dt · (L0 /4πd2L ) to the total brightness I0 (W m−2 sr−1 or
erg cm−2 s−1 sr−1 ), or, ignoring absorption by intervening matter:

11.5 Light-cone integrals

cL0
δI0 =
4π



d
dL

233

2
n dt .

(11.42)

We integrate (11.42) using that dt = du/u̇, u̇ = H0 u−1/2 for (Ωm , ΩΛ ) =
(1, 0) and d = ud0 = u2 dL according to (11.10) and (11.23):
I0 =

n0 L0 c
4π H0



1

u3/2 du =
0

n0 L0 c
.
10π H0

(11.43)

The extragalactic background intensity in the visible and infrared is estimated
to be I0 ∼ 5 × 10−5 erg cm−2 s−1 sr−1 , and it would follow that the extragalactic luminosity density is n0 L0 ∼ 109 h L Mpc−3 . More realistic computations
including source evolution, extinction, spectral range, etc., conﬁrm that I0 is
ﬁnite.11
The historical roots of the sky brightness problem date back, one might
say, to the days when Newton introduced universal gravity. In correspondence
with Bentley12 he concluded that a stationary universe would have to be inﬁnite (and that it required a supernatural power to subsist). It was gradually
understood that a stationary inﬁnite universe suﬀered from another problem.
The sky would be as bright as the Sun, because any line-of-sight must eventually hit a stellar surface, no matter in which direction one looks. There
would not be a spot in the sky that is not covered by a stellar surface (nonastronomers are reminded that the brightness of a stellar disc is independent
of its distance). This is known as Olbers’s paradox.13 The problem disappears
in relativistic cosmology as it allows for an expanding universe with a beginning in time. The night sky is dark because arbitrarily long lines-of-sight no
longer exist, and the stars within our horizon (the visible universe) cover only
a minute fraction of the sky. Contrary to what is often stated in the older
literature,14 the decisive factor is the ﬁnite age of the universe – the redshift
merely causes an additional reduction of I0 , see exercise.

11
12

13

14

Wesson, P.S., Ap. J. 367 (1991) 399.
Bentley, a priest, was after proving the existence of God by the classic argument
of design, and he took the precaution to ask Newton to comment on his ideas in
the light of the then new theory of universal gravity, see The correspondence of
Isaac Newton, H.W. Turnbull (ed.), Cambridge U.P. (1961), Vol III.
The physician and amateur astronomer H.W. Olbers published this paradox in
1826, but others had raised the issue before him. He also discovered a number of
comets and the asteroids Pallas and Vesta.
E.g. Gamov, G., in Theories of the Universe, M.K. Munitz (ed.), The Free Press
(1957), p. 390.

234

11 Observational Cosmology

Exercise 11.12: Give an alternative computation of I0 by considering the
radiation energy density stored in the subspace t = t0 , and show that the
redshift just adds an extra reduction factor.
t
Hint: The energy emitted by one source is 0 0 L0 dt, but that is not the
energy that is stored in the subspace t0 = constant, as the redshift reduces the energy by an amount S/S0 . The stored radiation energy density
t
is 0 = n0 0 0 L0 (S/S0 ) dt. Since the radiation is isotropic the intensity is
I0 = c0 /4π:

S
cn0 t0
L0
dt ,
(11.44)
I0 =
4π 0
S0
which is the same as (11.43) since u = S/S0 and dt = du/u̇ = u1/2 du/H0 .
The argument is purely local and shows that (11.43) is also valid for k =
0. The redshift may be switched oﬀ by dropping S/S0 in (11.44) → I0 =
(n0 L0 /6π)(c/H0 ), a factor 5/3 more.

12
The Big Bang

During the radiation era the universe was a perfectly homogeneous, rapidly
expanding space ﬁlled with dense, hot matter, but other than that it was a
rather dull period. The universe just expanded and cooled, and that was it –
nothing of importance happened. For more exciting times we have to go back
to the ﬁrst 1000 seconds, when temperature and density were so high that
nuclear reactions took place. The universe started its life as a gigantic fusion
reactor that produced the matter we observe today. Traditionally, this period
is referred to as the Big Bang. The matter and the cosmic microwave background (CMB) are the two main relics of the hot Big Bang. Very soon after
the discovery of the CMB by Penzias and Wilson in 1965 it was shown1 how
nuclear reactions could explain the observed chemical composition of the universe (H, D, 3 He, 4 He, 7 Li). The idea of nucleosynthesis in the early universe
and the concept of a relic thermal background radiation goes back, however,
to Gamov and co-workers.2 Weinberg’s book The First Three Minutes remains
one of the best accounts of this period of the universe, despite the fact that
it was written before inﬂation theory and astro-particle physics made their
impact on cosmology. Although some of the details may be complex and still
unknown, the story of the Big Bang remains, in broad outline, one of sublime
and almost capricious simplicity.

12.1 Nuclear reactions
We shall only summarise the main points, and not engage in explicit calculations. More information can be found in Weinberg (1977), Börner (1988),
Padmanabhan (1993), Kolb and Turner (1990), and Peacock (1999). We begin
with a brief review of the three kinds of elementary particles and the composite particles.
1
2

Wagoner, R.V. et al., Ap. J. 148 (1967) 3.
Alpher, R.A. et al., Phys. Rev. 73 (1948) 803.

238

12 The Big Bang

Table 12.1. Temperature, density and age of the universe as a function of the
energy scale
particle

m0 c2
(MeV)

Tr = m0 c2 /κ
(K)

ρ
(g cm−3 )

W ±, Z 0
p, n
π, µ
e±

9 × 104
940
120
0.5

1015
1013
1012
6 × 109

1025
1017
1013
103

t
(s)
10−10
10−7
10−4
10

Quarks. There are 6 types, called up (u), down (d); charm (c), strange (s) and
top (t), bottom (b). They carry one of the three positive colour charges (‘red,
green or blue’) responsible for the strong nuclear force. In addition they have
a fractional electric charge. The electric charge of u, c, t is 23 , that of d, s, b is
− 13 . Together with their antiparticles (that carry a negative colour charge)
they number 36 in total. They are fermions with rest masses ranging from
mu 1 MeV to mt 175 GeV.
Leptons. There are also 6 types of these: e− (∼ 0.5 MeV), νe ; µ− (∼ 100
MeV), νµ ; τ − (∼ 1.8 GeV), ντ . Together with their antiparticles (e+ , ν e , ..)
12 in total. They are fermions that do not feel the strong nuclear force. The
neutrinos have no electric charge, and zero mass according to the standard
model. Experimental upper limits: νe < 4.7 eV, νµ < 160 keV and ντ < 24
MeV). Measurements of atmospheric and solar neutrinos indicate that neutrinos switch ﬂavour as they propagate. These so-called neutrino oscillations
imply that they should have a nonzero mass.
Gauge bosons take care of the interaction between these particles. There is 1
graviton g (gravity); 3 vector bosons W ± , Z 0 mediating the weak interaction;
the photon γ (electromagnetic force) and 8 gluons for the strong interaction.
The vector bosons have a rest mass of about 90 GeV and an electric charge of
±1, 0. The other gauge bosons are massless. The gluons carry a colour charge.
Hadrons. Free quarks cannot exist – they occur only in combinations of two
or three quarks called hadrons (= heavy particles). Accordingly, there are two
kind of hadrons. The mesons are colour-free particles consisting of
√ a quark
and an antiquark, for example π + = ud, π − = du, π 0 = (uu − dd)/ 2 (∼ 140
MeV). Baryons are colourless combinations of 3 quarks. The lightest are the
proton p = uud and the neutron n = udd (∼ 940 MeV). The mesons and all
heavier baryons (Λ = uds, Σ + = uus, ..) are unstable.
During the extremely hot and dense initial phase of the universe, the particles it contains are continuously subject to interactions of the type

12.1 Nuclear reactions

A+B ↔ C +D;
D ↔ P +Q,

239

(12.1)

etc. As long as κTr > m0 c2 there is enough energy to create particles of rest
mass m0 . The time available for these reaction is of the order the time scale
on which the universe changes due to expansion, τS = S/Ṡ. Because S ∝ t1/2
we get
τS ∼ H −1 ≡ (Ṡ/S)−1 = 2t
= 2 × age of the universe .

(12.2)

The available time is therefore of the order of the age of the universe (the
factor 2 should not be taken too seriously). In view of the values of ρ and t in
Table 12.1 we may suspect that τS is generally longer than the characteristic reaction times between the elementary particles, and detailed calculations
conﬁrm this suspicion. This has a very important consequence: matter and
radiation are in thermal equilibrium. If we wish to know the abundances of
the particles at a certain temperature we may just as well ignore the expansion, as the reactions proceed much faster anyway, and compute the thermal
equilibrium state.
In a non-equilibrium calculation hundreds of rate equation must be advanced in time. That is not really a big deal, but the problem is that many
reaction cross sections are not well known. For equilibrium calculations simpler and reliable techniques are available. It is no longer necessary to know
the reaction cross section. A typical example is relation (12.9) which shows
how the density ratio of protons and neutrons in thermal equilibrium depends
only on their mass diﬀerence and the temperature, but not on the details of
the weak interactions that maintain the equilibrium. It follows that the material composition of the universe is not strongly dependent on previous states.
Even if we make a mistake in the early universe because the particle physics
at these high energies is not well known, it would have little eﬀect on the
material composition at a later time. That is why it is at all possible to make
statements on the material evolution of the early universe with some degree
of conﬁdence.
In broad outline, the situation is as follows. All particles with rest mass
energy m0 c2 smaller than κTr are continuously being created and destroyed,
usually by many diﬀerent types of reaction. They have a thermal Fermi-Dirac
or Bose-Einstein energy distribution, and they have number densities of the
order of those of the massless particles (for example photons). However, as
the universe evolves, κTr becomes smaller than mA c2 , and then things get a
little complicated. It may happen that particle A vanishes completely from
the scene because reactions such as (12.1), top, and the annihilation reaction

240

12 The Big Bang

A + A → 2γ

(12.3)

proceed entirely to the right. Free neutrons for example ultimately disappear
because they are unstable, though the majority of them gets locked up in 4 He,
as we shall see. History plays a role in two ways:
- There are a number of conserved quantities, such as the net electric charge
(probably zero) and the baryon number (= number of quarks minus number
of antiquarks). These quantities are simply passed on from early times to later
evolutionary stages.
- Because temperature and density decrease, all reaction times increase, and
they do so faster than the universe ages. As a result, in the whole network
of reactions creating and/or destroying particle A some connections become
sterile. These paths eﬀectively disappear from the network. This has no immediate inﬂuence on the number of particles A. That happens only when the
last path disappears – assuming A did not vanish earlier due to (12.3), for
example. The jargon is that particle A decouples or freezes out. What remains
must be calculated for every species individually by solving rate equations.

12.2 The ﬁrst 100 seconds
The thermal history of the early universe evolves through several stages that
we brieﬂy describe here, with reference to the overview in Table 12.2.
Quark-gluon plasma
The story begins when the universe was not yet 10−7 seconds old. The temperature was 1013 K or more, and the density was 1017 g cm−3 or higher. The
universe consisted of a quark-gluon plasma, an extremely dense and heavy
stew of quarks, leptons and gauge bosons, all in comparable amounts. The
beginning of the quark era is believed to be at t ∼ 10−30 s, when the temperature had the impressive value of 1024 − 1025 K, and the universe was a linear
factor S/S0 5 × 1024 smaller than it is today. The space within our current
horizon (radius ∼ 10 Gpc, Table 9.1) would, at that time, ﬁt in a sphere with
a radius of 100 meter!
Baryogenesis
The quark-gluon plasma is believed to be subject to a phase transition and
to condense into hadrons at a few times 1012 K. A heavy-ion collision programme at CERN and later at Brookhaven National Laboratory (the Relativistic Heavy Ion Collider (RHIC)) has given hints about the properties

12.2 The ﬁrst 100 seconds

241

Fig. 12.1. A Little Bang. Snapshot from a simulation of a collision of two lead nuclei
5×10−24 s after an oﬀ-centre impact at 17.4 GeV per nucleon pair. Unaﬀected ‘spectator’ nucleons are white and grey. Colliding hadrons are advanced with a hadron
transport model (UrQMD) that handles the ﬁrst collisions and their hadronic products. At full overlap these hadrons are decomposed into (supposedly deconﬁned)
quarks, which are then advanced with a quark molecular dynamics model (qMD).
The colours above indicate the six (anti)colour charges. During the subsequent evolution the quarks quickly team up in colour-neutral clusters that decay into hadrons.
The ﬁgure is stretched in the beam direction by a factor γ (of order 10) to undo
the Lorentz contraction, but time dilation eﬀects are still there. Credit: S. Scherer,
University of Frankfurt. See Scherer, S. et al., New J. Phys. 3 (2001) 8.1.

of the quark-gluon plasma and the phase transition.3 Various groups have
supported these experimental eﬀorts with simulations, one of which is shown
in Fig. 12.1.
Very soon after the phase transition only the lightest hadrons remain
(p, p and n, n and some mesons). As the temperature drops further p, p and
n, n annihilate according to (12.3). Calculations show that the baryon density drops to nb /nr = nb /nr ∼ 10−18 . Therefore we have a conﬂict with the
observations, which tell us that nb /nr ∼ 6 × 10−10 and that there is no antimatter, exercise 10.11. Attempts to resolve this conﬂict include, for example,
models with spatial ﬂuctuations which may result in regions having a slight
excess of matter, alternated by places with a small antimatter excess. After
3

For a non-technical account see Schwarzschild, B., Phys. Today, May 2000, 20;
Ludlam T. and McLerran, L., Phys. Today, October 2003, 48.

242

12 The Big Bang
Table 12.2. Overview of the material evolution of the universe

age

temperatur

size

(s)

(K)

(S / S0)

composition
baryons

lepton

gauge bosons

atoms
galaxies

neutrino, microwave and
graviton background

Boldface printed particles have approximately the same density, which is about 109
times larger than the other particles on the same line.

annihilation in the hadron era there remain regions with matter and antimatter, and we happen to live in a matter region. The idea has been abandoned
because the regions are small and contain much less than a galactic mass.
Moreover, the boundaries produce much more annihilation radiation than is
actually observed. It is now believed that a small quark-lepton excess of the
order of
nq − nq
∼ 6 × 10−8
(12.4)
nq + nq
was created everywhere in the universe. Computations show that after the
nb , as observed.4
annihilations have taken place, nb /nr ∼ 6 × 10−10 and nb
4

One might think that nb /nr ∼ 6 × 10−8 . This is correct if there were only one
non-relativistic quark gas instead of 36 extremely relativistic ones. The photons
and each individual quark type have initially about equal abundance. Moreover,
a sizeable fraction of the kinetic energy of the quarks is ultimately converted into
photons as well. As a result there are about 100 times more photons after the
annihilations than one might think. The proper attack to this type of problem is
to require that the total entropy is constant, as in exercise 12.2.

12.2 The ﬁrst 100 seconds

243

The origin of this excess (12.4), to which we owe our existence, is unknown. A popular speculation is asymmetric decay of leptoquarks X that play
a role in Grand Uniﬁed Theories (GUTs). These supermassive bosons (∼ 1015
GeV) may have existed in the very early universe from ∼ 10−43 s to ∼ 10−34 s.
Around t ∼ 10−34 s they decay into two quarks or a quark-lepton pair:

q +q
(r) ;
X →
(12.5)
q +l
(1 − r) ;

X →

q +q

(r) ;

q +l

(1 − r) .

(12.6)

Between parentheses the branching ratios for each decay channel; X and X
decay at the same net rate, but when r is a little larger than r a small matter
excess will arise, see exercise.5 One way to check this scenario would be to
measure the induced instability of the proton: the uu in p = uud fuse into
a leptoquark by the inverse of the top channel of (12.5), which decays again
into d + e+ through the lower channel. The remaining d and the new d form
π 0 . Net result: p → π 0 + e+ . The predicted decay time is very long, of the
order of 1032 yr, because the intermediate leptoquarks are so massive. The
Japanese Kamiokande facility, well-known for its detection of neutrinos, was
originally designed to measure the lifetime of the proton.
Returning to the Big Bang, at the end of the hadron era, around t = 10−4
s, the last mesons and the heavier leptons have decayed as well. The universe
is now a rapidly expanding ﬁreball consisting of photons, neutrinos, e+ , e−
in approximately equal profusion, with a tiny admixture (∼ 6 × 10−10 ) of
protons and neutrons.
The lepton era
Thermal equilibrium between e± , ν, ν and photons is maintained by scattering
of photons and neutrinos oﬀ e± , and through reactions such as
ν +ν

↔ e+ + e− ↔ 2γ .

(12.7)

The equilibrium of these particle with p and n (and between p and n) is
maintained by the weak reactions

5

More information on the matter-antimatter symmetry problem in Börner (1988)
Ch. 8; Kolb and Turner (1990) Ch. 6, and Peacock (1999) § 9.6. For a summary
of history and current ideas see Ellis, J., Nature 424 (2003) 631.

244

12 The Big Bang

Fig. 12.2. The evolution of the radiation temperature during the decoupling of the
neutrinos and the e± annihilation.

p + e− ↔ n + νe ;
n + e+ ↔ p + ν e ;
n ↔ p + e− + ν e .

⎫
⎪
⎪
⎬
⎪
⎪
⎭

(12.8)

These reactions leave the number of protons plus neutrons invariant. The
physical state of the matter and the radiation is entirely determined by the
temperature. The previous history of the universe is only relevant in that it
determines (a) the ratio (nn + np )/nr and (b) the time t at which a particular
temperature is attained. Three important events take place during the lepton
era:
- At Tr ∼ 3 × 1010 K the neutrinos decouple because the interaction times
between e± and the neutrinos become of the order of τS . For the time being
Tν and Tr remain equal as both continue to scale ∝ S −1 .
- The ratio nn /np is determined by thermal equilibrium:


nn
(mn − mp ) c2
= exp −
;
np
κTr
(mn − mp ) c2

(12.9)

1.3 MeV .

At the beginning of the lepton era we have κTr  (∆m) c2 so that nn np ;
the mass diﬀerence between p and n plays no role yet. Around Tr 3 × 1010
K the ratio nn /np begins to decrease, and soon the reaction rates of (12.8)
become larger than the age of the universe so that thermal equilibrium (12.9)

12.2 The ﬁrst 100 seconds

245

can no longer be maintained. Calculations show that nn /np freezes out at
Tr ∼ 3 × 109 K at value of (Kolb and Turner (1990) § 4.3; Peacock (1999)
Ch. 9):
nn
0.16 .
(12.10)
nn + np
- Electrons and positrons begin to disappear by annihilation when Tr drops
below ∼ 6 × 109 K. A small fraction of the e− remains, equal to the fraction
of protons. The eﬀect of the e± annihilation is that the photon temperature6
Tr decreases for some time less rapidly than ∝ S −1 . In the end Tr becomes
1.4 larger than the neutrino temperature Tν , Fig. 12.2.
a factor (11/4)1/3
During the subsequent evolution of the universe the energy distribution of the
neutrinos remains a thermal (Fermi-Dirac) distribution with Tν ∝ S −1 . The
present temperature of the neutrino background is therefore predicted to be
2.725 K/1.4 = 1.95 K. A measurement of this neutrino temperature would be
a powerful check on the hot Big Bang scenario (and would also secure your
fame in cosmology).

Exercise 12.1: Show that the decay of an X, X pair causes the baryon number B to increase by r − r, and that a matter excess will arise when r > r.
Hint: X and X are ﬁeld quanta and have B = 0, as do the leptons l; quarks
have B = 13 , and three quarks compose a baryon with B = 1. Antiparticles
have opposite B, hence ∆B = 2 · 13 r − 13 (1 − r) − 2 · 13 r + 13 (1 − r) for each
decaying X, X pair.
Exercise 12.2: Explain that Tr = (11/4)1/3 Tν
e+ e− annihilation.

1.40 Tν at the end of the

Hint: During the annihilation the state of the matter is no longer given by a
simple limiting case as in Table 10.1, but by relation (9.39) which says that the
entropy S in a volume S 3 is constant. In the calculation below only extremely
relativistic gases play a role for which p = 13  = 13 ρc2 and S = S 3 (p+ρc2 )/T =
(4c2 /3)S 3 ρ/T (without proof). Let ρ = aT 4 , for example (10.31) for photons,
then for a mixture Σ ai (T S)3i is constant. The entropy of each neutrino gas
remains constant (the neutrinos have no interaction and play no role), while
p, n do not contribute signiﬁcantly to S due to their relatively small density. What remains is photons, e+ , e− prior to annihilation, and only photons
6

It is customary to denote the photon temperature as Tr , and to identify it with
the temperature of (the radiation in) the universe, even though some components
of the radiation, such as the neutrinos, have a diﬀerent temperature.

246

12 The Big Bang

thereafter:
ar (Tr S)3b + a+ (T+ S)3b + a− (T− S)3b = ar (Tr S)3a .

(12.11)

b, a = before, after annihilation; +, − = e+ , e− . Now a− = a+ = 78 ar (see
3
3
literature), and T−b = T+b = Trb whence 11
4 (Tr S)b = (Tr S)a . But since
Tν ∝ S −1 we have (Tr S)b = (Tν S)b = (Tν S)a , so that after the annihilation
11 3
3
4 Tν = Tr . Details in Peebles (1993) p. 160; Padmanabhan (1993); Peacock
(1999) Ch. 9.

Exercise 12.3: Demonstrate that the last scattering surface of the neutrino
background is located at z ∼ 1010 . Explain that the sooner a background
freezes out, the lower its temperature will be today.
Hint: The neutrino temperature now and at freeze-out are known, Fig. 12.2;
furthermore T ∝ S −1 . The earlier a particle A freezes out the more the photon
temperature will rise with respect to that of A due to later annihilations.

12.3 The synthesis of light elements
At the end of the lepton era the structure of the universe is very simple. It
has a ﬂat geometry (k is eﬀectively zero), and it contains a homogeneous
mix of photons and neutrinos, ‘doped’ one might say with a tiny fraction of
e− , p and n. During the next and longest phase of the Big Bang elements
heavier than hydrogen are ‘cooked’. Helium could already have existed in the
lepton era, because its binding energy is so large (28 MeV ∼
= 3 × 1011 K).
However, the lighter nuclei that are needed to get helium fusion going are not
available, because their binding energies are smaller than κTr at that time.
And formation of helium through four-particle collisions is extremely rare.
The upshot is that heavier nuclei may only be generated in sequential twoparticle collisions. The ﬁrst step in this process, deuterium (D), determines
the rate of the synthesis due to its small binding energy (2.2 MeV) and large
cross section for photo-dissociation. Only when Tr ∼ 109 K (t ∼ 100 s) the
equilibrium
n+p ↔ D+γ
(12.12)
begins to shift to the right, Fig. 12.3. Once D is available, other fusion reactions
follow immediately:

D + D → 3 He + n ;
(12.13)
D + 3 He → 4 He + p ;

12.3 The synthesis of light elements

247

Fig. 12.3. Synthesis of the light elements, after Boesgaard, A.M. and Steigman, G.,
A.R.A.A. 23 (1985) 319.

D+D →

3

D + 3H →

4

H+p;


(12.14)

He + n ,

and the result is that virtually all neutrons end up in 4 He, and only a small
fraction in 3 He and D. We are now in a position to estimate the abundance
of 4 He in the universe. The value of nn /(nn + np ) was 0.16 at the freezeout, and decreased slowly thereafter to about 0.13 at the beginning of the
helium synthesis due to β decay of the neutrons. Because almost all neutrons
end up in 4 He, the mass fraction of 4 He equals Y = 2 × nn /(nn + np )
0.26. Calculations give a result between 0.20 and 0.28, dependending on the
assumed value of nb /nr . The mass fraction of the remaining deuterium equals
roughly 10−4 , and that of 3 He is a bit lower. Tritium (3 H) reaches a level of
∼ 10−7 , but decays in 18 years and disappears.
The formation of heavier elements is hampered by the absence of stable
nuclei with mass number N + Z = 5 and 8. Some 7 Li and 7 Be is formed by
the reactions
⎫
4
He + 3 H → 7 Li + γ ; ⎪
⎪
⎪
⎬
4
3
7
He + He → Be + γ ;
(12.15)
⎪
⎪
⎪
7
Be + e− → 7 Li + νe , ⎭

248

12 The Big Bang

5

N

9 Be

4

7 Li

3

6 Li

7 Be

3

4

2
1

n

0

3H

4 He

D

3 He

p
0

1

2
Z

Fig. 12.4. Stable nuclei with Z ≤ 4. A free neutron and tritium are subject to slow
β decay, last line of (12.8), with e-folding times of 900 s (n) and 18 years (3 H); 7 Be
disappears eventually because it is unstable to electron capture, the last reaction in
(12.15).

but it is very little because they require the rare nuclei 3 H and 3 He. As the universe reaches the respectable age of 10 minutes the nucleosynthesis is drawing
to a close and the radiation era begins. Since 7 Be disappears too, Fig. 12.4,
we conclude that the ﬁnal product of the nucleosynthesis in the early universe
is 4 He plus a little D, 3 He and 7 Li. Heavier elements were not formed, broadly
speaking, because there was no time. The early universe expanded very fast
and the reaction rates soon became vanishingly small due to decreasing densities and Coulomb barriers getting too large. The universe had to wait until
the arrival of the stellar era. Stellar interiors have the right density and temperature for the synthesis of carbon and heavier elements.7 And they have
lots of time.
Primordial abundances are diﬃcult to observe because abundances change
with time due to evolutionary eﬀects. The best value for 4 He is Y
0.24 ± 0.015, observed in isolated extragalactic H II regions with little contamination from stellar nucleosynthesis. This agrees well with the theoretical prediction. An important point is that stars cannot deliver these large
quantities of helium. Stellar nucleosynthesis could have produced Y ∼ 0.04 at
most, and the spatial distribution would be clumpy and cluster around regions
of star formation. However, the observed 4 He distribution is rather homoge-

7

There is actually a third production process: spallation by cosmic rays. A fraction
of the 6 Li, 7 Li, 11 B, and all 9 Be and 10 B in the universe has been formed in this
way. See Geiss, J. and Von Steiger, R., in Fundamental Physics in Space, ESA
SP-420 (1997), p. 99.

12.3 The synthesis of light elements

249

neous. It follows that the helium in the universe must be primorial.8
The abundance of deuterium in the interstellar medium is D/H
(1.6 ± 0.1) × 10−5 . Deuterium is special in that it is only destroyed during stellar evolution and never created. Hence all measured abundances
are lower limits to the primordial abundance. The extragalactic deuterium
abundance has recently been measured from absorption lines in the light
of a quasar that passes through a gas cloud at z = 3.6.9 The result is
4 × 10−5 < D/H < 2.4 × 10−4 , nicely consistent with the theoretical prediction. The primordial 3 He abundance is very diﬃcult to get hold of. The
solar 3 He/H value is (1.5 ± 0.4) × 10−5 . A 20-year programme of galactic H
II region observations yielded 3 He/H < (1.1 ± 0.2) × 10−5 for the primordial
abundance.10 The measured 7 Li abundance in some 100 metal poor Population II halo stars is 7 Li/H = (1.6 ± 0.07) × 10−10 , and this number is believed
to be indicative of the primordial 7 Li abundance.11
The correct prediction of the abundances of the light elements is a resounding success for the theory of the hot Big Bang. We saw that the abundance of
4
He does not depend strongly on the assumed value of nb /nr , but that of D,
3
He and 7 Li does. This provides a sensitive method to determine the value of
nb /nr , and because that ratio is constant and nr0 is known, we may infer the
current baryon density ρb0 . The conclusion is that the outcome of the light element synthesis agrees with the observed abundances if ρb0 = (3±1.5)×10−31
g cm−3 , or Ωb = 0.03 ± 0.015. The light element synthesis scenario is therefore in accordance with the recent WMAP measurements (Ωb = 0.044±0.004,
Table 9.2).
More details on these topics may be found in the (extensive) literature, e.g.
Boesgaard, A.M. and Steigman, G., A.R.A.A. 23 (1985) 319; Börner (1988)
Ch. 3; Kolb and Turner (1990) Ch. 4 (FORTRAN code: p. 96); Padmanabhan
(1993) Ch. 3 and 11. There exist also simpliﬁed models of the light element
synthesis.12

8

9
10
11
12

Quasi-steady-state cosmologists, on the other hand, maintain that all 4 He has
been produced in stars. The energy released by the relevant fusion reactions has
a density which is now equal to that of the microwave background. Therefore they
interpret the CMB as thermalised starlight (Burbidge, G., et al., Physics Today,
April 1999, 38).
Songaila, A., et al., Nature 385 (1997) 137.
Bania, T.M., et al., Nature 415 (2002) 54.
Molaro, P., et al., A&A 295 (1995) L47.
Bernstein, J., et al., Rev. Mod. Phys. 61 (1989) 25; Eskridge, B. and Neuenschwander, D.E., Am. J. Phys. 64 (1996) 1517.

250

12 The Big Bang

Exercise 12.4: Prove that at the beginning of the helium synthesis nn /(nn +
np ) 0.13.
Hint: nn = nn0 exp(−t/τ ); t
β decay (n → p + e− + ν e ).

200 s, τ

900 s; nn + np remains constant in

Exercise 12.5: During the helium synthesis the universe was a fusion physicist’s dream: a gigantic fusion reactor that converted some 13% of all hydrogen into helium in about 1000 seconds. In comparison, stars need 1010 year to
fuse a few percent of their hydrogen into helium. Explain why the enormous
amount of energy liberated during the helium fusion had no inﬂuence on the
evolution of Tr – unlike the e+ e− annihilation during the lepton era.
Exercise 12.6: Neutron star model builders have a hard time in ﬁnding a
reasonable equation of state p(ρ) at ρ ∼ 1015 g cm−3 . Cosmologists, however,
who study the universe at far greater densities couldn’t care less. Why is life
so much easier on them?
Hint: at comparable densities the matter in the universe is much hotter than
neutron star matter. If we increase T at constant density, the interaction energy between nuclei becomes progressively less important, and that simpliﬁes
the equation of state. Ultimately, the matter behaves as an ideal gas.
Exercise 12.7: What would be the 4 He abundance if deuterium had a higher
binding energy?
Hint: It could be as large as Y ∼ 2 × 0.16 = 0.32.

13
Inﬂation

The standard model of the Friedmann-Robertson-Walker (FRW) universe
with a hot beginning is very successful and provides a natural explanation
for:
1. the observed expansion velocities of distant galaxies;
2. the microwave background radiation as a relic of the hot Big Bang;
3. the chemical composition of the universe (H, D, 3 He, 4 He and 7 Li) as a
relic of nuclear fusion during the Big Bang.
However, a number of problems remain, and the most important of these will
be investigated here. For example, an obvious question is why does the universe expand? The only answer we have at this stage is: ‘because it expanded
faster in the past’. Other issues are the horizon problem, and the question
why the geometry of the universe is ﬂat. To illustrate the ﬂatness problem,
we know that Ωm + ΩΛ = 1.02 ± 0.02, so that the universe is ﬂat within
the observational errors. But the universe must have been much ﬂatter in the
past. In exercise 10.3 it was shown that Ωm (t) + ΩΛ (t) + Ωk (t) = 1 and
lim [Ωm (t), ΩΛ (t), Ωk (t)] = [1, 0, 0] .

t→0

(13.1)

To ensure that Ωm
0.3 and ΩΛ
0.7 now, the density ρ in the early universe must have been very close to the critical density ρc at that time (but
not exactly equal). And ΩΛ must have have been minimally diﬀerent from
zero in the past, by just the right amount to achieve that ΩΛ 0.7 now. The
fact that Ωk appears to be zero within the error bounds means that Ωk must
have been almost exactly zero in the past. The universe was ﬂat then, and it
still appears to be ﬂat today. Why was the universe born with these special
initial conditions?
In quest for a solution of these problems, cosmologists and particle physicists have increasingly joined forces. The early universe is an ideal place for
particle physicists to test their theories under conditions that can never be
attained in a laboratory. They go to the Great Accelerator in the Sky rather

254

13 Inﬂation

Fig. 13.1. The horizon problem. We observe that the early universe, i.e. a spherical
shell around us at large z (t
t0 ), is isotropic. The distance of the shell to us at
time t is dlk and the size of causally connected regions is dho , the horizon distance at
dlk , the early universe consists of many causally unconnected
that time. Since dho
regions that don’t know about each other’s existence because they have not yet been
able to exchange a light signal. If, however, the very early universe has gone through
a period of inﬂation, then dho  dlk .

than to CERN. This has led to the discovery of the possibility of inﬂation,
a brief period of extremely rapid expansion immediately after the birth of
the universe. Designed originally to alleviate the problem that the universe
would contain too many magnetic monopoles, inﬂation soon turned out to be
a panacea providing a solution for the horizon and ﬂatness problem as well. In
addition, it explained why the universe expands, and it provided the primordial energy density ﬂuctuations from which the large-scale structure in the
universe may develop later. In view of these impressive achievements, and in
spite of its speculative character, the inﬂation concept appears to be the most
important theoretical development in cosmology of the last decades. Here we
shall explain the basic idea of inﬂation with the help of a simple model due
to Linde.1

13.1 The horizon problem
The Friedmann-Robertson-Walker (FRW) universe has the nasty property
that it consists of many diﬀerent regions that are outside each other’s horizon.
And, as explained in § 11.2, the younger the universe is, the worse it gets.
And yet, according to observations, our universe is on average homogeneous
1

See Linde, A.D., Physics Today, September 1987, 61.

13.1 The horizon problem

255

Fig. 13.2. We detect photons of the CMB in the plane θ = π/2 from two directions
subtending an angle ϕ0 . Our worldline is AA0 . The photons originate from P1 and
Q1 on the surface of last scattering at z ∼ 1100. We arrange things so that P1 and
Q1 are just inside each other’s horizon, so that the physical conditions in P1 and Q1
may in principle be the same. Assuming that space is ﬂat, the photons travel along
the sides of a ﬂat isosceles triangle that has expanded a factor ∼ 1100 when they
reach the observer. In the text it is shown that ϕ0 ∼ 1◦ . This leaves the observed
high degree isotropy of the CMB over the entire sky unaccounted for.

and isotropic. Let P and Q in Fig. 13.1 be two distant objects at large z.
We observe that the surroundings of P and Q have the same properties,
within the error bounds. The properties in P and Q depend only on the space
inside their respective horizons. The ﬁgure displays our light-cone, and the
geometrical distance dlk between us and an object at time t,2 as well as the
geometrical distance dho to the horizon at time t. These are given by
 t0
 t
dt
dt
;
dho = cS
.
(13.2)
dlk = cS
S
0 S
t
In Chap. 11 both distances had been indicated with the same symbol d, for
example in (11.15), (11.18) and (11.20), but here a distinction is necessary. In
an (Ωm , ΩΛ ) = (1, 0) universe the angular size of a causally connected region
that we observe at a redshift z is (see exercise):
ϕ0

t
dt/S
1
dho
1
= √
=
=  0t0
 √ ,
dlk
z
1+z − 1
dt/S
t

(13.3)

for large z. It follows that dho /dlk  1, so that the early visible universe
consists of many causally unconnected regions. The problem is innate to all
2

In the notation of § 11.2 dlk equals d = (S/S0 )d0 .

256

13 Inﬂation
S
t 1/2

inflation

0

10-34 - 10-30 s

t

Fig. 13.3. Behaviour of the scale factor in the very early universe. The dotted line
S ∝ t1/2 causes all problems.

FRW models and the outcome of (13.3) is only weakly dependent on Ωm and
ΩΛ . The horizon spaces of P and Q have had no opportunity to interact.
What mechanism is responsible for the neighbourhood of P and Q having
similar physical properties (i.e. why is the universe isotropic)? Why indeed
would P and Q begin to participate in the expansion at the same moment?
The issue is one of causality. An FRW universe seems to behave like someone
who walks along the street although the various parts of his body are unable
to exchange signals.
Let’s take the CMB in our own universe as an example, and t in Fig. 13.1
is the time of recombination trec . We have seen in exercise 11.5 that the
horizon distance at recombination is 2.25ctrec . The distance dlk of the last
scattering surface to us at that time is 3.3 × 10−3 · 0.96c/H0 (Table 11.1).
In fact we are repeating exercise 10.12, with a better value for dho , and the
result is that the angular size of causally connected regions at recombination
is ϕ0 = dho /dlk 1.1◦ in our universe, while (13.3) predicts ϕ0 1.7◦ for a
(1, 0) universe. It follows that the observed isotropy of the CMB on angular
scales > ϕ0 is accidental, as there is no causal connection possible on larger
angular scales. The viewing geometry is further explained in Fig. 13.2.
Origin and remedy of the horizon problem
The physical origin of the horizon problem is that the expansion is arbitrarily fast near t = 0: limt→0 Ṡ = ∞. Since the signal speed is ﬁnite ( <
∼ c) the
universe immediately breaks up into regions that have had no time to communicate, which is unphysical. The fact that limt→0 Ṡ = ∞ is an inevitable
consequence of (10.1). If ρ = 0 the ρ-term in (10.1) is ∝ S −3 or S −4 for radiation, and it follows that Ṡ → ∞ for S → 0.

13.1 The horizon problem

257

The problem would disappear if S(t) approaches zero in a diﬀerent way,
t
as in Fig. 13.3. The numerator 0 dt/S of (13.3), convergent for S ∝ t1/2 ,
would now become much larger,
 t while the denominator remains unaﬀected.
The value of the denominator t 0 dt/S is diﬃcult to tinker with anyhow, because the shape of S(t) is ﬁxed once the radiation era is underway. But right
after t = 0 we can’t be so sure anymore. For example, let’s suppose for the
sake of argument that S ∝ t2 near t = 0. The expansion is then initially slow,
limt→0 Ṡ = 0, so that the various regions may interact and have an opportunity to ‘homogenize’. The subsequent expansion becomes increasingly rapid.
And now dho ∞ according to (13.2). Thus we would achieve that the horizon distance dho in the early universe was already much larger than dlk (=
size of our visible universe scaled down to an early time t), Fig. 13.1, right.
We have now discovered the essence of the inﬂation concept. The scale
factor S(t) is subject to a very rapid accelerating growth just after t = 0, as
in Fig. 13.3. An implication is that the very early universe is extremely small,
much smaller than one would expect on the basis of S ∝ t1/2 .

Exercise 13.1: Verify the details of (13.3).
t
t
 S/S
1
[ 0 dt/S] / [ t 0 dt/S] = [ 0 0 du/(uu̇)] / [ S/S0 du/(uu̇)];
√ S/S
√
(10.10): u̇ ∝ u−1/2 → ϕ0 = [ u ]0 0 / [ u ]1S/S0 . Finally S0 /S = 1 + z.

Hint: ϕ0

=

Exercise 13.2: The horizon problem in a closed ΩΛ = 0 FRW universe,
Fig. 13.4. Assume that photon F starts at the moment of the Big Bang, and
show that since that time it has travelled a co-ordinate distance
 u
dx
χ = Ωm − 1
−1
+ 1 − Ωm )1/2
0 x (Ωm x
= 2 arcsin

S/Sm ,

(13.4)

where Sm is the value of the scale factor at maximum expansion, Sm /S0 =
Ωm /(Ωm − 1), § 10.2. Prove the following statements (1) complete causal
contact of all parts is only attained in the contraction phase, after maximal
expansion, and in the expansion phase the universe has causally disconnected
parts; (2) after maximal expansion an observer begins to see double images,
diametrically opposite on his sky; (3) A will never see his own image.
Hint: Radial null geodesic from (9.19): (dx0 )2 = S 2 dχ2 or dχ/dt = c/S, then
(10.10); substitute x = Ωm y 2 /(Ωm − 1) to get rid of Ωm in the integral; (1)

258

13 Inﬂation

A

S

c

F

Fig. 13.4. Snapshot of a great circle in a closed ΩΛ = 0 FRW universe, showing the
radial co-ordinate r = sin χ, § 9.3. As time progresses, the circle expands together
with the universe and then contracts again. Object A, located at the origin r = 0,
has emitted photon F at the moment of the Big Bang. The photon returns to A at
the moment of the Big Crunch.

complete causal contact requires that photons emitted by A have covered the
entire universe → χ = π; (2) photons travelling in opposite directions may
reach the same observer as soon as χ > π; (3) A sees his own image at the
moment of the Big Crunch. This shows how fast the expansion really is: in a
closed ΩΛ = 0 universe photons just manage to make one round trip!

13.2 Evolution of a universe with a scalar ﬁeld
We have seen that the horizon problem may be solved if the scale factor
S(t) behaves diﬀerently near t = 0. In the 80ies of the last century it was
discovered that scalar ﬁelds that may have been present in the early universe
can do the magical trick. Scalar or Higgs ﬁelds had originally been used in
particle physics because they could endow mass to the quanta of an otherwise
massless vector ﬁeld, without destroying the possibility of renormalization.
Scalar ﬁelds have been very popular since that time, and it was only a natural
development to investigate their role in cosmology. The bosons in question are
believed to be the hypothetical supermassive X-bosons (∼ 1015 GeV) that
occur in Grand Uniﬁed Theories, see § 12.2. Such particles may be abundant
just after the Big Bang, and play a role in many inﬂation models. In practice
the scalar ﬁeld is just postulated and one doesn’t worry too much about its
place in the grander scheme of things. We shall now derive the equations of
motion for the simplest possible model: one scalar ﬁeld, minimally coupled

13.2 Evolution of a universe with a scalar ﬁeld

259

to gravity. We start from the relativistic expression of the energy E of a free
particle:
(13.5)
E 2 = m2 c4 + (pc)2 ,
where p is the particle’s momentum. This is quantized in the usual way by
replacing E and p by operators E → i∂/∂t and p → −i∇:
−2

∂2ψ
= m2 c4 ψ − 2 c2 ∇2 ψ .
∂t2

(13.6)

This is the Klein-Gordon equation for a ﬁeld ψ of bosons with spin zero and
rest mass m. After some cleaning up:
( + µ2 )ψ = 0 ;

µ =

mc
.


(13.7)

But (13.7) is is not an acceptable equation because ψ = η µν ψ,µν is not an
invariant scalar. The simplest generalization is: η µν ψ,µν → g µν ψ:µ:ν . So, we
replace (13.7) by
(13.8)
g µν ψ:µ:ν + µ2 ψ = 0 ,
which is properly invariant. This type of reasoning, incidentally, is another
example of how the principle of general covariance is used in practise.
The explicit expression for g µν ψ:µ:ν may be found with the help of (2.47):
ψ:µ = ψ,µ , and then (2.43):
g µν (ψ,µν − Γαµν ψ,α ) + µ2 ψ = 0 .

(13.9)

We show later that we may restrict ourselves to ψ,i = 0, i.e. to homogeneous
ψ:
(13.10)
g 00 ψ,00 − g µν Γ0µν ψ,0 + µ2 ψ = 0 .
But g 00 = 1 and from (9.31): g µν Γ0µν = g ik Γ0ik = −(S  /S)g ik gik = −3Ṡ/cS:
ψ,00 +

3Ṡ
ψ,0 + µ2 ψ = 0 .
cS

(13.11)

We have landed on familiar territory: ψ evolves as an harmonic oscillator that
is damped by the expansion of the universe.
The ﬁeld equation is G00 = −(8πG/c2 )T00 , or, with (9.34):
 2
Ṡ
kc2
8πG
T00 .
+ 2 =
S
S
3

(13.12)

T00 is the total energy of the harmonic oscillator:3
3

Landau, L.D. and Lifshitz, E.M.:1971, Relativistic Quantum Theory, Pergamon
Press, § 12.

260

13 Inﬂation

T00 =

2
1
2 (ψ,0

+ µ2 ψ 2 ) .

(13.13)

Actually there is another term 12 |∇ψ|2 on the right hand side of (13.13) which
is omitted because of the assumed homogeneity of ψ. The equation for S
becomes:
 2
kc2
4πG
Ṡ
(ψ,02 + µ2 ψ 2 ) .
+ 2 =
(13.14)
S
S
3
We now have a closed set of equations (13.11) and (13.14) for S and ψ.
Before we proceed it is useful to write these equations in dimensionless
form, with the help of Planck units. The Planck mass Mp is the mass of
a black hole whose Schwarzschild radius 2GM/c2 and Compton wavelength
/M c are equal:
 1/2
c
2.2 × 10−5 g ,
(13.15)
Mp =
G
a macroscopic mass of 22 µg, and Mp c2
1.2 × 1019 GeV. The Compton
wavelength /Mp c of this hole is the Planck length Lp :

Lp =
=
Mp c



G
c3

1/2

1.6 × 10−33 cm .

(13.16)

The Planck density ρp ≡ Mp /Lp 3 and the Planck time tp ≡ Lp /c are
 3
c5
4 c
=
M
5.2 × 1093 g cm−3 ;
p
G2


1/2
G

tp =
= Mp−1 2
5.4 × 10−44 s .
c5
c

ρp =

(13.17)

(13.18)

We now substitute G = c/Mp2 in (13.14) and then  = c = 1 in (13.11) and
(13.14):
ψ̈ + 3H ψ̇ + m2 ψ = 0 ;
H2 +

k
4π
=
(ψ̇ 2 + m2 ψ 2 ) ,
S2
3Mp2

(13.19)
(13.20)

with H = Ṡ/S and ˙ = d/dt. The original units may be restored as follows.
From (13.16) − (13.18) we see that [length] = [time] = [mass]−1 ; [density]
= [mass]4 . The dimension of ψ follows by requiring that c2 T00 is the energy density of the ﬁeld → [(µcψ)2 ] = [ρc2 ] → [ψ] = [mass]. This means
that if we compute a time, we may for example ﬁnd that t = m−1 . Since
Mp tp = 1 according to (13.18), t = m−1 Mp tp = Mp /m in units of tp , or
t = (Mp /m) · Mp −1 (/c2 ) = /mc2 sec. Analogous results may be derived for
other quantities.

13.3 Chaotic inﬂation

261

Fig. 13.5. In the chaotic inﬂation scenario a quantum ﬂuctuation of characteristic
size Lp in the metric of an existing spacetime, casually referred to as a ‘quantum
bubble’, will inﬂate to huge proportions in about 10−34 s, if the energy it may contain on the basis of the uncertainty relation resides in one scalar ﬁeld. During the
subsequent reheating phase, the energy of the scalar ﬁeld is converted into particles, the quark-gluon plasma, marking the beginning of the hot Big Bang around
t ∼ 10−30 s (depending on the inﬂation scenario). Our visible universe is a minute
fraction of the original bubble, and therefore homogeneous and ﬂat.

Equations (13.19) and (13.20) do not allow for interaction with other ﬁelds,
because we assumed a free particle. The equations become more complicated
when these interactions are included, and their mathematical form becomes
strongly dependent on the details of the particle physics at the highest energies, about which little is known. This is where the appeal of the chaotic
inﬂation model proposed by Linde comes in. Chaotic inﬂation assumes that
the universe is born out of a quantum ﬂuctuation in which the energy in one
scalar ﬁeld ψ dominates over all other ﬁelds. The evolution is then presumably
well described by (13.19) and (13.20) for a free ﬁeld.

13.3 Chaotic inﬂation
We start oﬀ from an existing spacetime. On a microscopic scale there are
abundant quantum ﬂuctuations in the metric, Fig. 13.5. A causally connected
part (a ‘quantum bubble’) has a characteristic size Lp = ctp and contains
a characteristic energy Mp c2 that is restricted by Heisenberg’s uncertainty
relation to Mp c2 · tp ∼ . This energy is more or less equally divided among
various ﬁelds. These regions are not of interest to us because they do not
inﬂate. We concentrate on one of those rare places and times where one
scalar ﬁeld dominates over all others, and analyse its evolution with (13.19)

262

13 Inﬂation

and (13.20). Because almost all energy resides in the scalar ﬁeld we get
T00 = 12 (ψ̇ 2 + m2 ψ 2 ) ∼ ρp = Mp4 ( = c = 1). Since the ignored term
1
2
4
2 |∇ψ| in (13.13) may also not be larger than ∼ Mp , we infer a restriction
on the typical variation δψ of ψ:
δψ ∼ |∇ψ|Lp ≤ Mp2 Mp−1 = Mp .
But T00

m2 ψ 2 ∼ Mp4 (we shall show later that ψ̇
ψ ∼

δψ
m
∼
ψ
Mp

Mp2
;
m

(13.21)

mψ), so that
1,

(13.22)

since mc2 ∼ 1015 GeV if m is the X-boson, while Mp c2 ∼ 1019 GeV. We
conclude that the assumption that all energy resides in the scalar ﬁeld implies its near-homogeneity. It is therefore reasonable to put ∇ψ = 0 in the
derivation of (13.19) and (13.20), and we take (13.22) as the initial condition
of ψ at t = tp . According to (13.19), ψ is a harmonic oscillator with frequency
m and damping 3H/2. For weak damping (H
m) ψ will oscillate. But the
damping turns out to be strong (H  m), and ψ approaches zero only very
slowly. In that case the inertia term ψ̈ can be neglected. We assume that:
H  m

and

ψ̇

mψ .

(13.23)

Furthermore we omit the curvature k/S 2 term. These approximations will be
justiﬁed later. We are then left with:
3H ψ̇ = − m2 ψ ;
H2 =

4πm2 2
ψ .
3Mp2

(13.24)
(13.25)

The nature of the solution is rather obvious. Relation (13.25) says that H (:) ψ,
and then (13.24) says that ψ̇ is a negative constant which turns out to be
small, so that ψ is also approximately constant. Hence, according to (13.25),
H = Ṡ/S ∼ constant, i.e. exponential expansion. As ψ slowly decreases, so
does H until the weak damping limit is attained. The explicit solution for
the strong damping case is obtained by solving (13.24) and (13.25) for ψ̇ and
H = Ṡ/S:
mMp
ψ̇ = − √
;
12π

(13.26)

Ṡ
4π
= −
ψ ψ̇ .
S
Mp2

(13.27)

These equations may be integrated:

13.3 Chaotic inﬂation

mMp t
;
ψ(t) = ψp − √
12π
)
*
2π  2
2
ψp − ψ(t)
S(t) = Sp exp
,
Mp2
where ψp = ψ(tp )

Mp2 /m and Sp = S(tp )

263

(13.28)

(13.29)

Lp .

An exercise invites the reader
√ to show that the approximations (13.23)
are valid as long as ψ  Mp / 3π. The range of validity of the solution is
therefore
√
Mp2
Mp / 3π < ψ <
(13.30)
∼ m .
With the help of (13.28) the exponent in (13.29) may be expanded as (2π/Mp2 )·
[ψp 2 −(ψp + ψ̇t)2 ] (2π/Mp2 )(−2ψ̇ψp t) = 4π/3 Mp t, as long as t
−ψp /ψ̇.
The expansion is therefore exponential,
+
4π
Mp
S(t) = Sp exp(Hp t) ;
Hp =
2Mp ,
(13.31)
3
as long as t

te where
√
√
ψp
Mp
te = −
= 12π 2 = 12π
m
ψ̇



Mp
m

2
tp .

(13.32)

For large t, S(t) reaches the ﬁnal value


 
2 
2π
Mp
Se
2
∼ exp
ψp
∼ exp 2π
.
Sp
Mp2
m

(13.33)

The numerical value of (13.33) is very uncertain because the boson mass m
is unknown. But since m
Mp it is clear that the scale factor S is blown
8
up by a huge amount, possibly as large as ∼ 10(10 ) if mc2 ∼ 1015 GeV. This
number is so large that even astronomers, not known to be easily impressed
by large numbers, are baﬄed.
And it all happens in a very brief time span. The time when the inﬂation
terminates can be estimated by requiring ψ(te ) = 0 in (13.28), and this leads
we have te ∼ 6 × 108 tp ∼ 3 × 10−35 s.
again to (13.32). For m/Mp = 10−4 √
When ψ becomes of the order of Mp / 3π it is no longer possible to ignore ψ̈,
and ψ(t) becomes oscillatory. This is the reheating phase, during which the
energy in the scalar ﬁeld is converted into matter, which is not included in the
equations. The subsequent expansion of the universe to its present-day size,
by a factor of ∼ 1027 , is relatively modest with respect to (13.33).

264

13 Inﬂation

Fig. 13.6. Numerical solution of eqs. (13.34) - (13.36). Top: scale factor and ﬁeld
amplitude. Bottom: the energy density of the radiation and the total energy density
in units of the Planck energy density ρp c2 . Note that H ∝ (ρtot )1/2 . All logarithms
are base 10. Parameters: m/Mp = 0.01, γ = 0.01 m, ψp = Mp2 /m, timestep ∆t =
0.25 tp . The equations require a small timestep because they are stiﬀ, and we did
not bother to use a special integration routine. This renders integration for more
realistic parameters such as m/Mp = 10−4 diﬃcult.

13.3 Chaotic inﬂation

265

Toy model
To demonstrate the inﬂation and reheating in some detail we add an equation
for relativistic matter (that is, radiation) to eqs. (13.19) and (13.20):
ψ̈ + (3H + γ)ψ̇ + m2 ψ = 0 ,
H2 =

8π
( 1 ψ̇ 2 +
3Mp2 2

1 2 2
2m ψ

(13.34)
+ ρ) ,

ρ̇ + 4Hρ = γ ψ̇ 2 .

(13.35)
(13.36)

These equations are obtained as follows. The interaction of the scalar ﬁeld with
other ﬁelds is modelled by a damping term γ ψ̇ in (13.34). This interaction is
initially not important since H is large, but as ψ and H decrease, the damping
of ψ by coupling with matter ﬁelds becomes more important than expansion.
In physical terms, the energy in the scalar ﬁeld is converted into particles,
for example X-bosons, that subsequently decay into quarks and leptons. The
matter has an energy density ρ, which has been added to the total energy
density in (13.35). The curvature term k/S 2 has been dropped as it is soon
unimportant. For γ = 0, (13.36) says that ρS 4 is constant (relativistic matter).
The choice γ ψ̇ 2 for the matter source term is motivated by the fact that
it makes the interaction between matter and scalar ﬁeld energy-conserving.
We verify that by computing the time derivative of the total energy density
ρtot = 12 ψ̇ 2 + 12 m2 ψ 2 + ρ :
d
dt

1 2
2 ψ̇

+

1 2 2
2m ψ

+ ρ = − H(3ψ̇ 2 + 4ρ) ,

(13.37)

which is independent of the rate γ at which energy is exchanged between ψ
and ρ. The conversion of the scalar ﬁeld into matter proceeds without loss of
energy, since ρtot decreases only insofar expansion dilutes the energy density
of the scalar ﬁeld and the matter.
These phenomenological equations nicely illustrate the key features of
inﬂation and reheating, see Fig. 13.6. There is a huge expansion as long as
the scalar ﬁeld dominates. In this phase the matter evolves quasistationary,
ρ γ ψ̇ 2 /4H (ρ̇ 0), and is energetically unimportant. The end of the inﬂation phase is correctly predicted by (13.32), and the simulations
√ conﬁrm that
the scalar ﬁeld has by then collapsed to a value ψ ∼ Mp / 3π, see (13.30).
The ﬁeld becomes oscillatory thereafter, with a period ∼ m−1 = (Mp /m)tp ,
which is hardly resolved in Fig. 13.6. The expansion continues but slows down
to S ∝ t1/2 , which is no longer visible on the logarithmic scale of the ﬁgure.
The total expansion factor is well reproduced by relation (13.33). The matter energy density surges to an estimated peak value of ρ/ρp ∼ γm/12πMp2

266

13 Inﬂation

(without proof), and scales as ρ ∝ S −4 ∝ t−2 soon thereafter: the beginning
of a hot big bang.

Exercise √13.3: Verify that the assumptions in (13.23) are correct as long as
ψ  Mp / 3π.
Hint: ψ in (13.19) is a damped harmonic oscillator. As long as the damping
3H/2 is supercritical
(i.e. 3H/2 > m) one may neglect ψ̈. With (13.25) →
√
ψ > Mp / 3π; (13.26) → |ψ̇| < mψ.
Exercise 13.4: Prove that during the inﬂation phase the Hubble constant is
∼ 1061 times larger than it is today.
Hint: Hp ∼ Mp = Mp tp /tp = 1/tp and H0 ∼ 1/t0 → Hp /H0 ∼ t0 /tp .
Exercise 13.5: Show that the scalar ﬁeld ψ is equivalent to a density ρ and
a negative pressure p:
⎫
ρ = 12 ψ̇ 2 + 12 m2 ψ 2 ; ⎬
(13.38)
p = 12 ψ̇ 2 − 12 m2 ψ 2 . ⎭
Hint: Set H = Ṡ/S in (13.20), multiply with S 2 , diﬀerentiate, and eliminate
ψ̈ with (13.19):
S̈
4π
= −
(2ψ̇ 2 − m2 ψ 2 ) .
(13.39)
S
3Mp2
Compare with (9.40) → ρ + 3p = 2ψ̇ 2 − m2 ψ 2 in Planck units, and ρ from
(13.20) and (9.36); Λ is completely negligible. The pressure is negative on
account of (13.23).

13.4 Discussion
The horizon problem has disappeared because if we compute once more dho
using (13.31) and (13.33) we ﬁnd, for t  te :
 t
 te
 te
dt
dt
> cSe
eHp te
dho = cS
e−Hp t dt
S
0
0
0 S

13.4 Discussion

 
2 
c Hp te
Mp
∼
e
∼ Lp exp 2π
,
Hp
m

267

(13.40)

because c/Hp ∼ Lp . After the inﬂation phase the value of dho is by any measure enormous. On the other hand, as long as t > te , we know4 that dlk is
1 that
at most of the order of ct0 . In other words, the inequality dho /dlk
caused all the problems has now been reversed to dho /dlk  1.
Since H is approximately constant while S grows rapidly to huge proportions, the term k/S 2 in eq. (13.20) is soon negligible. Inﬂation thus implies
that the universe is ﬂat. Likewise it is correct to ignore the cosmological conH 2 (in Planck units). Our visible
stant term Λ/3 in (13.20) as Λ ∼ H02
universe is a tiny fraction of the original quantum ﬂuctuation and is therefore
homogeneous, regardless how inhomogeneous the initial ﬂuctuation was.
Current status
The inﬂation concept was originally introduced by Guth5 in 1981, and
presently there are a number of diﬀerent models on the market.6 Because
of its many achievements inﬂation has become a paradigm in cosmology that
is likely to stay – even though the nature of the scalar ﬁeld that does the magical trick is unknown. To this comes that all models have loose ends, and none
is wholly accepted. For example, there is no explanation for the ﬁne-tuning
problem of the cosmological constant Λ. The model expounded here has one
advantage over others: it seems not to depend strongly on the (unknown) details of the particle physics, although self-interaction of the ﬁeld is ignored. It
produces an inﬂation factor much larger than other models do, and also much
larger than is needed (see exercise). On the other hand, the fact that under
certain conditions a quantum ﬂuctuation would inﬂate may be regarded as an
instability of the vacuum, and it remains to be seen if a complete quantum
theory of gravity permits such a phenomenon. Another serious objection is
the fact that the model uses a semi-classical formulation right after the Planck
time.
What drives it?
The clue is that the energy density of the scalar ﬁeld is not diluted by expansion like ordinary radiation (∝ S −4 ). This very counterintuitive property
is conﬁrmed by eq. (13.37): for ρ = 0 the energy density of the scalar ﬁeld
decreases as −3H ψ̇ 2 which is of the order of (m/Mp )2 Planck energy density ρp c2 per Planck time. In the present context this is very small, so H
4
5
6

See Table 11.1; dlk ≡ d.
Guth, A.H., Phys. Rev. D 23 (1981) 347.
For more information on inﬂation theory see Börner (1988) Ch. 9; Kolb and
Turner (1990) Ch. 8; and Peacock (1999) Ch. 11.

268

13 Inﬂation

remains roughly constant according to (13.35). And that means that exponential expansion continues unabated. Another way of saying this is relation
1 2 2
(13.38): the scalar ﬁeld is equivalent to a density ρ
2 m ψ and a negative
1 2 2
1 2
pressure p − 2 m ψ since 2 ψ̇ is small. During inﬂation spacetime behaves
approximately as a vacuum with a large cosmological constant Λ ∝ 12 m2 ψ 2 ,
and we conclude that the initial quantum ﬂuctuation is blown up by the huge
anti-gravity associated with the scalar ﬁeld. All inﬂation models have in common that the inﬂation takes place very early – after ∼ 10−30 s at most it is
all over. Inﬂation may therefore be regarded as a physical mechanism that
creates the homogeneous, isotropic, hot, expanding and ﬂat FRW universe
whose existence we took for granted in earlier chapters.
Seed ﬂuctuations
The exponential expansion creates an event horizon. Events further away than
c/H >
∼ Lp cannot communicate with the observer, who will experience the
universe during the inﬂation phase as a kind of black hole turned inside out.
Although this is merely an analogy, there is one consequence that carries
over: the creation of quantum ﬂuctuations in the
√ scalar ﬁeld, which turn out
to have an r.m.s. amplitude δψ/ψ ∼ m/(Mp 3π) per wavelength decade.
These ﬂuctuations in ψ are eventually converted into density ﬂuctuations δρ/ρ
and have the right spectrum to serve as the seeds for structure formation if
m/Mp ∼ 10−4 . This is an important reason for believing that the scalar ﬁeld
of mass m may correspond to the supermassive X-bosons of grand uniﬁed
theories.
Energetics
The energetics of inﬂation is an elusive problem. We start with a total energy
∼ Mp c2 at t = tp and at the end of the reheating we have ∼ Se3 × the
energy density at that time, plus the gravitational energy, which is negative
one would say. However, this is the reasoning of an external observer, § 11.2.
An observer in the universe faces a diﬀerent situation, as he has to perform
an integration over the past lightcone. A proper calculation is called for, but
then we run into the problem that an invariant deﬁnition of the gravitational
energy in a volume does not exist in GR. It does only in the special cases of
asymptotically ﬂat or stationary spacetimes, neither of which applies to the
FRW universe. So, when we make statements about the global energetics of
the expanding universe we cannot be sure to avoid artifacts due to the choice
of the co-ordinates! As a result of these problems no clear answer exists, which
is very unsatisfactory.
Philosophical issues
Speculations on the origin of the universe concern issues that are often impossible to verify, which gives them a metaphysical twist where, depending

13.4 Discussion

269

Fig. 13.7. Chaotic inﬂation may take the form of a hierarchical process creating
many interconnected or decoupled universes. Observer W would just notice a small
defect in his spacetime of size ∼ Lp that may or may not disappear again. Our visible
universe is a very small section of spacetime, for instance the space indicated by the
two markers. Since observations beyond the horizon are impossible, we can only
make ‘reasonable’ assumptions about what lies outside (such as the cosmological
principle). This underlines the highly speculative character of ideas such as these.

on the temperament of the author, sometimes pretty wild extrapolations are
made.7 The model treated here is called chaotic inﬂation because it begins
whenever a suﬃciently large quantum ﬂuctuation materializes in spacetime,
and the standard lore about chaotic inﬂation runs something like this. Suppose that happens close to an observer W . To W such a region would appear
to be a small defect in space – a kind of black hole with radius ∼ Lp , see
Fig. 13.7. Inside, however, the geometry is ‘redeﬁned’ in a drastic manner, as
it contains an entire universe. This idea of inﬂation being a sudden redeﬁnition of the geometry in a very small patch of spacetime may be helpful. W
may nevertheless hold the defect, and thereby an entire universe in his hand
(where perhaps other students study their cosmology books). The ﬁgure is an
attempt to visualize the geometrical structure by letting space ‘bulge out’ into
a ﬂat embedding space, suggesting that one universe is ‘next to’ or ‘below’
another – which is of course not so because the embedding space does not
exist. Our universe may likewise be enclosed inside another spacetime.
7

For example Tegmark, M., Sci. Am. May 2003.

270

13 Inﬂation

In this vision creation is a stochastic process that continues forever. Spacetime would have a kind of hierarchical structure without a beginning in time.
Our universe may be one of many, and the constants of nature and therefore
the physics would be diﬀerent in each universe. The constants of nature in
our universe must have the values they have because we would not exist if
they were much diﬀerent. This type of reasoning is called the anthropic principle, originally introduced by Carter.8 The so-called weak anthropic principle
maintains that what we can expect to observe is restricted by the conditions
necessary for our existence as observers, see Barrow and Tipler (1986) for
more details.
Perhaps the most stunning perspective oﬀered by inﬂation theory is the
idea that the entire universe as we know it originates from a tiny part of
an already tiny quantum ﬂuctuation. Is this a dazzling show of the power of
scientiﬁc reason, or rather a ﬁgment of the mind – a modern Tower of Babel? Don’t say too soon that we shall never know, and recall the example
of Auguste Comte, who argued in earnest in 1835 that it would be forever
impossible to determine the temperature and the internal state of stars.9 And
then of course came spectroscopy.

Exercise 13.6: The solution of the horizon problem requires a minimum
inﬂation factor of Se /Sp ∼ (t0 /tp )(T0 /Te ) ∼ 1061 10−28 = 1033 ∼ e76 .
Hint: Require that the present horizon distance (about ct0 ) rescaled to t = tp ,
i.e. ct0 (Sp /S0 ), is equal to the horizon size Lp at t = tp ; then Se /Sp = (Se /S0 )·
(S0 /Sp ) ∼ (Se /S0 )(ct0 /Lp ) ∼ (T0 /Te )(t0 /tp ) (use T ∝ S −1 ); estimate Te
from κTe ∼ mc2 → Te ∼ 1028 K. This is a very rough estimate; the energy
of the scalar ﬁeld at the end of the inﬂation may not be completely spent on
reheating and then Te is smaller than 1028 K.
Exercise 13.7: Verify that the expansion factor of the universe from the end
of inﬂation to now is ∼ 1027 .
Hint: The scale factor S goes as t1/2 from te ∼ 6 × 108 tp ∼ 3 × 10−35 s to
tmat 5.5 × 104 yr (→ expansion factor 2.4 × 1023 ). Then as t2/3 until now,
14 × 109 yr (expansion factor 4000).

8

9

Carter, B., in Confrontation of Cosmological Theories with Observational Data,
ed. M.S. Longair (Reidel 1974), p. 291.
Comte, A.: 1835, Philosophie Première (cours de philosophie positive), 19e Leçon,
ed. Hermann (Paris 1975).

A
Bibliography

Elementary texts
Adams, F. and Laughlin, G.: 1999, The Five Ages of the Universe, The Free
Press.
Berry, M.: 1978, Principles of Cosmology and Gravitation, Cambridge U.P.
Geroch, R.: 1978, General Relativity from A to B, University of Chicago Press.
Hogan, C.J.: 1998, The Little Book of the Big Bang: A Cosmic Primer,
Springer-Verlag.
Silk, J.: 1980, The Big Bang, Freeman and Co.
Weinberg, S.: 1977, The First Three Minutes, Basic Books, Inc.

Introductory textbooks
Adler, R., Bazin, M. and Schiﬀer, M.: 1965, Introduction to General Relativity, McGraw-Hill.
Dirac, P.A.M.: 1975, General Theory of Relativity, Wiley-Interscience.
Foster, J. and Nightingale, J.D.: 1989, A short course in General Relativity,
Longman.
Frank, J., King, A.R. and Raine, D.J.: 1992, Accretion Power in Astrophysics,
Cambridge U.P.
Kenyon, I.R.: 1990, General Relativity, Oxford U.P.
Landau, L.D. and Lifshitz, E.M.: 1971, The Classical Theory of Fields, Pergamon Press.
Linder, E.V.: 1997, First Principles of Cosmology, Addison-Wesley.
Price, R.H.: 1982, General Relativity Primer, Am. J. Phys. 50, 300.
Rindler, W.: 2001, Relativity, Special, General and Cosmological, Oxford U.P.
Robertson, H.P. and Noonan, T.W.: 1969, Relativity and Cosmology, Saunders.
Schutz, B.F.: 1985, A First Course in General Relativity, Cambridge U.P.

272

A Bibliography

Advanced textbooks on speciﬁc topics
Blair, D.G. (ed.): 1991, The Detection of Gravitational Waves, Cambridge
U.P.
Börner, G.: 1988, The Early Universe (Facts and Fiction), Springer-Verlag.
Chen, Y.T. and Cook, A.: 1993, Gravitational Experiments in the Laboratory,
Cambridge U.P.
Kolb, E.W. and Turner, M.S.: 1990, The Early Universe, Addison-Wesley.
Misner, C.W., Thorne, K.S. and Wheeler, J.A.: 1971, Gravitation, Freeman
and Co.
Padmanabhan, T.: 1993, Structure Formation in the Universe, Cambridge
U.P.
Peacock, J.A.: 1999, Cosmological Physics, Cambridge U.P.
Peebles, P.J.E.: 1993, Principles of Physical Cosmology, Princeton U.P.
Saulson, P.R.: 1994, Fundamentals of Interferometric Gravitational Wave Detectors, World Scientiﬁc.
Schneider, P., Ehlers, J. and Falco, E.E.: 1992, Gravitational Lenses, SpringerVerlag.
Shapiro, S.L. and Teukolsky, S.A.: 1983, Black Holes, White Dwarfs and Neutron Stars, Wiley-Interscience.
Wald, R.M.: 1984, General Relativity, University of Chicago Press.
Weinberg, S.: 1972, Gravitation and Cosmology, John Wiley and Sons, Inc.
Will, C.M.: 1993, Theory and Experiment in Gravitational Physics, Cambridge
U.P.

Foundations and history
Barrow, J.D. and Tipler, F.J.: 1986, The Anthropic Cosmological Principle,
Clarendon Press.
Bless, R.C.: 1995, Discovering the Cosmos, University Science Books.
Dijksterhuis, E.J.: 1969, The Mechanization of the World Picture, Oxford U.P.
Evans, J.: 1998, The History and Practice of Ancient Astronomy, Oxford U.P.
Friedman, M.: 1983, Foundations of Space-Time Theories, Princeton U.P.
Koestler, A.: 1959, The Sleepwalkers - A History of Man’s changing Vision of
the Universe, Hutchinson.

A Bibliography

273

Pais, A.: 1982, ‘Subtle is the Lord...’, the Science and Life of A. Einstein,
Oxford U.P.
Pannekoek, A.: 1989, A History of Astronomy, Dover.

Study reports
LISA System and Technology Study Report, Reinhard, R. and Edwards, T.
(eds.), ESA-SCI(2000)11, July 2000.

Exercises
Lightman, A.P., Press, W.H., Price, R.H. and Teukolsky, S.A.: 1975, Problem
book in Relativity and Gravitation, Princeton U.P. (general relativity and cosmology).
Taylor, E.F. and Wheeler, J.A.: 1966, Spacetime Physics, Freeman and Co.
(special relativity).

B
Useful numbers

Table B.1. Physical and astronomical constants a
electron mass
proton mass
neutron mass

9.109 × 10−28 g
1.673 × 10−24 g
1.675 × 10−24 g

electron charge e
speed of light c
Boltzmann constant κ
radiation constant σ
Planck constant  = h/2π
gravitational constant G

4.803 × 10−10
2.998 × 1010
1.381 × 10−16
5.670 × 10−5
1.055 × 10−27
6.674 × 10−8

c.g.s. (esu)
cm s−1
erg K−1
erg cm−2 K−4 s−1
g cm2 s−1
cm3 g−1 s−2

2.18 × 10−5
1.62 × 10−33
5.39 × 10−44
5.16 × 1093

g
cm
s
g cm−3

1 AU
1 light year (lyr)
1 parsec (pc)

1.496 × 1013 cm
9.461 × 1017 cm
3.086 × 1018 cm

3.262 lyr

microwave background temperature
Hubble constant H0
h
Hubble time 1/H0
Hubble radius c/H0

2.725 ± 0.002 K
100 h km s−1 Mpc−1
0.71 ± 0.04
3.09 × 1017 h−1 s
9.25 × 1027 h−1 cm

Planck
Planck
Planck
Planck

a

mass Mp = (c/G)1/2
length Lp = (G/c3 )1/2
time tp = (G/c5 )1/2
density ρp = c5 /G2

(511.0 keV)
(938.3 MeV)
(939.6 MeV)

3.24 × 10−18 h s−1
9.79 h−1 Gyr
3.00 h−1 Gpc

http://physics.nist.gov/constants

Table B.2. Sun and Earth
Sun
mass (g)
radius (km)
Schwarzschild radius 2GM/c2
luminosity (erg s−1 )

Earth

1.99 × 10
6.96 × 105
2.95 km
3.83 × 1033
33

5.98 × 1027
6.37 × 103
0.887 cm

C
Euler-Lagrange equations
In GR and other ﬁelds one often encounters the following problem. Given a
function L(y1 (p), ẏ1 (p), y2 (p), ẏ2 (p), · · ·) ≡ L({yi , ẏi }), where ˙ = d/dp . For
b
which functions yi (p) is the value of the integral I = a L dp an extremum?
This well known problem is handled by considering the diﬀerence between the
value of I for a neighbouring function set yi + δyi and the original value of I.
We compute the diﬀerence to ﬁrst order in δyi :
 b
 b
L({yi + δyi , ẏi + δẏi }) dp −
L({yi , ẏi }) dp
δI =
a

 b
a

a



∂L
∂L
δyi +
δẏi dp ,
∂yi
∂ ẏi

with a summation over double indices i as usual. We have
d
dyi
=
δyi .
δẏi = δ
dp
dp

(C.1)

(C.2)

δ and d/dp commute, and that enables us to partially integrate the second
term:
*

 b)
∂L
d ∂L
δI
−
(C.3)
δyi dp .
∂yi
dp ∂ ẏi
a
The stock term (∂L/∂ ẏi )δyi |ba vanishes because δyi (a) = δyi (b) = 0. The end
points are held ﬁxed. The requirement that I is an extremum implies that
δI = 0 for arbitrary δyi . It follows that


∂L
d ∂L
=
.
(C.4)
∂yi
dp ∂ ẏi
These are the famous Euler-Lagrange diﬀerential equations from which the
functions yi (p) may be solved. Note that the derivation of (C.4) clearly shows
that ∂L/∂yi and ∂L/∂ ẏi should be computed as if yi and ẏi are independent
variables. Note, too, that we obtain the functions for which L is an extremum,
a wider class than the functions for which I is a maximum or minimum.
Example
Let L = y 2 ẏ + ẏ 2 . Then ∂L/∂y = 2y ẏ and ∂L/∂ ẏ = y 2 + 2ẏ. After insertion in
b
(C.4) we get 2y ẏ = 2y ẏ + 2ÿ, or ÿ = 0. Hence a (y 2 ẏ + ẏ 2 ) dp has an extremal
value when y(p) is a linear function of p connecting the end points y(a) and
y(b).

D
Pressure of a photon gas
The pressure P is the force per unit area due to photons bouncing, say, oﬀ a
reﬂecting mirror, see Fig. D.1. The force on the surface element dA is P dA,
and this is also equal to the rate of change of momentum of the reﬂected
photons:
P dA = d momentum/dt

=

n(ν)dν ·

dV
dΩ
· ∆p ·
.
dt
4π

(D.1)

Here n(ν)dν is the number density of photons in a frequency band dν centered
on ν, and dV /dt = c cos θ dA is the volume ‘swept out’ by the photons per
unit time (Fig. D.1); ∆p is the momentum change per photon, and dΩ/4π the
fraction of the solid angle. The integrations are over frequency and solid angle.
If p is the photon’s momentum, its energy is E = pc. It follows that pc = hν or
p = hν/c, and the momentum change equals ∆p = 2px = 2(hν/c) cos θ. The
solid angle element, ﬁnally, is dΩ = 2π sin θ dθ. Inserting everything yields

P =



π/2

cos2 θ sin θ dθ =

n(ν)hν dν
0

1
3

.

(D.2)

The ﬁrst integral is equal to the photon energy density  and the second integral equals 13 . Note that the energy distribution of the photons is immaterial,
but isotropy is essential.

dV = dA . cdt

. cos q

dA

q

X

Fig. D.1. The volume dV swept out per unit time by photons impinging on a mirror
under an angle θ.

E
The angular power spectrum of the CMB
We derive here three relations from § 11.4. First we consider (11.35), then
(11.36), after which (11.34) is trivial. Write down (11.32) twice and take the
ensemble average:

∗
am a∗ m  =
dΩ1 dΩ2 Ym
(n1 )Y m (n2 ) ∆T (n1 )∆T (n2 ) . (E.1)
Assuming spherical symmetry, the autocorrelation function ∆T (n1 )∆T (n2 )
can only depend on θ12 , where cos θ12 = n1 · n2 . Accordingly, it should be
possible to expand the autocorrelation function in Legendre polynomials Pn :

∆T (n1 )∆T (n2 ) =
constn Pn (cos θ12 ) .
(E.2)
n

The addition theorem of the spherical harmonics,

j

∗
Ynj (n1 )Ynj
(n2 ) =

2n + 1
Pn (cos θ12 ) ,
4π

(E.3)

allows one to express Pn (cos θ12 ) in terms of n1 and n2 . Insert (E.2) in (E.1)
then make use of (E.3) and rename 4π constn /(2n + 1) ≡ Cn . These constants
are the same as those in (11.35). As a result of these operations we ﬁnd



∗
∗
am a∗ m  =
Cn dΩ1 Ym
(n1 ) Ynj (n1 ) dΩ2 Y m (n2 )Ynj
(n2 )
nj

=



Cn δn δmj δ n δm j = C δ δmm .

(E.4)

nj

In the second line we have twice made use of the orthogonality of the spherical
harmonics:

(E.5)
dΩ Ym Y∗ m = δ δmm .
It follows that
|am |2  = C ,

(E.6)

which is relation (11.35). It shows that |am |2  is indeed independent of m.
The next step is that we may now write (E.1) as

∗
C =
dΩ1 dΩ2 Ym
(n1 )Ym (n2 ) ∆T (n1 )∆T (n2 ) .
(E.7)

282

E The angular power spectrum of the CMB
z
n1
z´

n2
q12

y´
y
j12

x

x´

Fig. E.1. The integration over Ω2 in (E.8) is performed ﬁrst, using spherical coordinates in the x y  z  -frame at ﬁxed n1 . In this frame the co-ordinates of n2 are
θ12 , ϕ12 . Because the integrand is axially symmetric around the z  -axis we have
∫ dΩ2 = 2π sin θ12 dθ12 , and the result is (E.9). Spherical symmetry renders the
remaining integrand independent of n1 , so that ∫ dΩ1 produces just a factor 4π.

Summing this relation over m produces a factor 2 + 1 on the left, while on
the right we invoke the addition theorem (E.3):

2 + 1
dΩ1 dΩ2 P (cos θ12 ) ∆T (n1 )∆T (n2 ) . (E.8)
(2 + 1)C =
4π
We now exploit the fact that the integrand depends only on θ12 . As explained
in Fig. E.1, the result is

 π
C = 12 dΩ1
sin θ12 dθ12 P (cos θ12 ) ·
0

∆T (n1 )∆T (n2 ) | n1 ·n2 = cos θ12

≡ 2π

(E.9)

π

C(θ)P (cos θ) sin θ dθ ,

(E.10)

0

where we have dropped the index 12. This proves relation (11.36).

Finally, expand C(θ) in Legendre polynomials, C(θ) = n An Pn (cos θ),
insert that in (E.10) and use the orthogonality of the Legendre polynomials,


1

2
δnm ,
(E.11)
2n
+1
−1

to ﬁnd that A = (2 + 1)C /4π, or C(θ) = (4π)−1 n (2n + 1)Cn Pn (cos θ),
which is (11.34).
Pn (x)Pm (x) dx =

E The angular power spectrum of the CMB

283

The position of the maxima
In real life the peak positions are found by processing temperature maps
through (11.32) and (11.35). Here we are forced to follow a simpler approach,
and we employ the angles (10.47) between directions of maximal temperature
diﬀerence:
∆
(Ωm + ΩΛ )1/2 .
(E.12)
θn
(2n + 1)d
We now focus on the position of the ﬁrst peak:
θ0 =

∆
(Ωm + ΩΛ )1/2 = 1.64 × 103 H0 tm (Ωm + ΩΛ )1/2
d
1.12 × 10−2 ∼
= 0.65◦

(E.13)

The distance ∆ travelled by the beγ mode at recombination√is the horizon
distance 9ctm at recombination,
see below (11.20), divided by 3, to allow for
√
a signal speed of c/ 3. And according to Table 11.1 the distance d to the last
scattering surface at recombination is 3.3×10−3 ct0 = 3.3×10−3 ×0.96 c/H0 .
The numerical value of θ0 follows by inserting tm = 9.4 × 104 yr, see below
(10.24), and Ωm + ΩΛ = 1.
To estimate the maximum in the angular power spectrum we argue that
according to (11.32) C will be maximal if the grid of + and − signs laid out
on the sphere by Ym is commensurate with that of ∆T . There are maximally
2 zeros on the equator, hence 2 θn = 2π. The position of the ﬁrst peak is
therefore expected at
π/θ0
277 ,
(E.14)
0
while the observations give 0 = 220 ± 1. The origin of the discrepancy is
that we have ignored two important eﬀects that alter the value of ∆ and
therefore of θ0 . In the ﬁrst place we have tacidly assumed that the beγ modes
are free, but in fact they are driven by the gravity perturbation δφ generated
by the dark matter modes. This turns out to enhance their eﬀective speed of
propagation and hence also the value of ∆, by an amount that depends on the
wavelength√λ, i.e. on n. In the second place we have assumed that the signal
speed is c/ 3, but in reality baryon loading reduces the speed, in particular
at late times, and that in turn diminishes ∆. Although the two eﬀects partly
cancel, we cannot hope our result (E.14) to be very accurate.

Index

accretion, 14, 16, 110
onto black hole, 110
onto neutron star, 95
onto white dwarf, 93
active galactic nuclei (AGNs), 16, 110
anthropic principle, 270
bar detectors, 143
MiniGRAIL, 145
ongoing projects, 145
baryogenesis, 241
baryon to photon ratio, 203
baryon-electron-photon ﬂuid, 206
baryons, 238
dark, 170
luminous, 170
Big Bang, 173, 192
misconception of, 222
Big Crunch, 193, 258
Big Emptiness, 195
binary pulsars, 16, 81, 93
and gravitational radiation, 141
binding energy
of neutron star, 100
of nucleons, 93, 100
Binet’s method, 75
Birkhoﬀ’s theorem, 70
black hole, 16, 89, 109
and future light-cone, 115
elementary properties, 113
entropy, 129
evaporation, 129
event horizon, 114

growth, 116
hole in spacetime, 124
in galaxy, 112
microscopic, 130
no hair, 110
observations, 110
of intermediate mass, 112
primordial, 110, 130
rotating, 125
speciﬁed by M, L and Q, 109
supermassive, 110
temperature, 129
brown dwarfs, 89
calendar, 1
cataclysmic variables, 93
Chandrasekhar limit, 96
chaotic inﬂation, 261
and beginning of time, 270
duration, 263
expansion factor, 263
toy model, 265
transition to hot big bang, 266
charge
colour, 238
electric, 238
Christoﬀel symbol, 28
computation of, 32, 39
in freely falling frame, 47
nontensor, 29, 36
properties, 29
Robertson-Walker metric, 181
Schwarzschild metric, 67

286

Index

classical tests of GR, 77
clock paradox, 8
closed universe, 179, 192
CMB
angular power spectrum, 209, 229,
230
dipole anisotropy, 172, 229
discovery, 17
energy density, 172
evolution, 197
temperature, 172
temperature ﬂuctuations, 205, 228
CMB (Cosmic Microwave Background),
172
co-ordinate distance, 214, 215
co-ordinate picture, 20, 49, 175, 214,
219
co-ordinates
Gaussian, 176
harmonic, 57
in Riemann space, 19
Kruskal-Szekeres, 121
meaning, 43
Schwarzschild, 70
spatial, 45
time, 44
cold dark matter (CDM) models, 207
conservation of mass, 50
conserved quantities
baryon number, 240
electric charge, 240
constant of the motion, 32
continuity equation, 51
Cosmic Background Explorer (COBE),
172, 228
Cosmic Microwave Background (CMB),
172
cosmic variance, 174, 230
cosmological constant, 60, 183, 227
and dark energy, 170, 183
cosmological parameters, 171
determination, 230
cosmological principle, 2, 174
covariant derivative
of higher rank tensor, 34
of vector, 33
critical density, 169, 170, 191
curvature, 11
and Riemann tensor, 36

Gaussian, 37
total, 38
cyclist analogy
and photon propagation, 150, 219
d’Alembert operator, 56
dark energy, 170, 185
dark matter, 83, 170
baryons, 170
cold, 205
nonbaryonic, 170
decoupling, 240
density ﬂuctuations
and inﬂation, 207
at recombination, 199, 205
distance
co-ordinate, 215
geometrical, 213, 214
luminosity, 224
distribution of matter
evolution eﬀects, 171
ﬁlaments and voids, 169
isotropy, 171
Eddington limit, 111
Einstein clock, 9
Einstein tensor, 38
Robertson-Walker metric, 182
Schwarzschild metric, 69
Einsteinturm, 78
electron-positron annihilation, 245
elementary particles
overview, 237
energetics of inﬂation, 268
energy
of test particle, 58
entropy
black hole, 129
ephemeris, 1
equation of state (EOS)
cold matter, 104, 105, 250
hot matter, 189, 250
ergosphere, 127
Euler-Lagrange equations, 32, 67
event, 3, 43
event horizon
and Hawking radiation, 128
and Unruh eﬀect, 128
evolution of universe, 171

Index
overview, 242
expansion of universe, 171
adiabatic, 183
swelling of space, 222
Fermi energy, 95
Fermi-Walker transport, 155, 156
ﬁeld equations
basic idea, 52
classical limit, 57
general form, 54, 60
structure, 61
vacuum, 53
weak ﬁeld, 57
ﬂat universe, 178, 192
ﬂatness problem, 253
four-momentum, 47, 50
four-velocity, 32, 47, 50
frame-dragging eﬀect, 13, 125
and Mach’s principle, 126
LAGEOS satellites, 126, 163
freeze-out, 240
Friedmann-Robertson-Walker (FRW)
models, 189
FRW models
failures, 253
matter-dominated, 192
radiation-dominated, 197
successes, 253
FRW reference model, 194
Galilean transformation, 4
Gamma-ray bursts, 16
gauge bosons, 238
Gauss’s theorem, 51
Gaussian co-ordinates, 176
general covariance, 13
how to use it, 60
geodesic
extremal property, 32
null, 31
timelike, 31
geodesic deviation, 40, 52
geodesic equation, 30
geodesic motion, 46
geodesic precession, 29, 161
and binary pulsar, 163
geometrical distance, 213, 214, 255
geometrical picture, 20, 175, 219

287

Grand Uniﬁed Theories, 243
gravitational deﬂection of light, 11, 14,
78
gravitational lensing, 17, 82
arcs, 83
Einstein ring, 83
macrolensing, 85
microlensing, 85
of neutron star image, 83, 86
gravitational mass, 10
gravitational redshift, 11, 14, 49, 79
in solar spectrum, 14, 79
of neutron star surface, 106
gravitational time delay, 14, 79
gravitational waves, 16, 133
detectors, 143, 145
dispersion relation, 134
eﬀect on test masses, 136
energy ﬂux density, 140
generation, 138
metric tensor, 135
polarization, 137
quadrupole radiation, 139
TT-gauge, 135
gravity
Newtonian, 10
SR theories, 10
weak, 47, 56
Gravity Probe A, 79
Gravity Probe B, 6, 16, 162, 164
and geodesic precession, 164
and Lense-Thirring eﬀect, 164
hadrons, 238
Hawking radiation, 128
and event horizon, 128
helium synthesis, 247
High mass X-ray binaries, 95, 113
Hipparcos satellite, 81
horizon
event, 114, 223
in cosmology, 220
particle, 115, 220, 223
horizon problem, 221, 254
and causality, 256
in closed universe, 257
origin, 256
remedy, 257
hot dark matter (HDM) models, 207

288

Index

Hubble constant, 171
Hubble ﬂow, 171
cold, 171, 185
Hubble radius, 175, 220
Hubble relation, 171, 214
general form, 225
Hubble time, 173, 195
hyperbolic universe, 179
index
contraction, 24
dummy, 24
lowering, 22
raising, 22
inertial frame
global, 4, 11
local, 11, 12
inertial mass, 10
inﬂation, 17, 60, 195, 221
and cosmological constant, 268
and creation of FRW universe, 268
and density ﬂuctuations, 268
and ﬂatness problem, 267
and horizon problem, 267
and scalar ﬁeld, 258
basic idea, 257
current status, 267
energetics, 268
equations, 260
ﬁrst and second phase, 195
loose ends, 267
philosophical issues, 268
what drives it, 267
interferometer detectors, 145
LIGO, 146, 147, 152
LISA, 147, 151, 152
ongoing projects, 152
operational principle, 149
signal on detector, 150
VIRGO, 152
interval, 6
isotropy of universe, 171
Jeans instability, 203
Kamiokande facility, 243
Kepler’s second law, 74
Kerr metric, 125
Klein-Gordon equation, 259

Kruskal-Szekeres co-ordinates, 121
Legendre polynomials, 230
Lense-Thirring eﬀect, 163
lepton era, 243
leptons, 238
leptoquarks
asymmetric decay, 243
light-cone, 7
past, 215
light-element synthesis, 17, 246
and observations, 249
Lorentz gauge, 57
Lorentz transformation, 8
of rest mass density, 54
Low mass X-ray binaries, 95, 113
luminosity distance, 216, 224
Mach’s principle, 13
MACHOs, 86
mass limit
neutron star, 96, 105
white dwarf, 96
mass transfer, 93
matter
deﬁnition in cosmology, 189
matter era, 191
models, 217
matter-antimatter asymmetry, 242
maximum mass
neutron stars, 95, 99, 105
white dwarfs, 95
mesons, 238
metric tensor
covariant derivative, 35
experimental determination, 45
for weak ﬁeld, 58
gravitational waves, 135
in general relativity, 12
in special relativity, 6
Kerr metric, 125
of space, 45
Riemann space, 20
Robertson-Walker metric, 181
Schwarzschild metric, 66, 70
Minkowski spacetime, 3
neutrino
decoupling, 244

Index
oscillations, 238
neutrino background, 173, 245
neutron, 238
beta-decay, 240, 243, 247
neutron drip, 104
neutron stars, 89
bare mass, 99
binding energy, 100
constant density model, 101
discovery, 15
equation of state, 104
gravitational acceleration, 120
maximum mass, 95, 96, 99, 105
measured mass and radius, 106
minimum radius, 101
physical mass, 99
realistic models, 103
neutron-to-proton ratio, 245
nova, 93
nuclear fusion
in early universe, 237, 246
in stars, 90, 248
nuclear reactions
in early universe, 237
inverse beta-decay, 91
number density of compact objects, 93,
112
Olbers’s paradox, 232
remedy, 233
open universe, 179, 192
orbit classiﬁcation
in Schwarzschild metric, 74
parallel transport
formal deﬁnition, 29
intuitive deﬁnition, 27
on a sphere, 39
parameters
of characteristic objects, 49
past light-cone
cyclist analogy, 219
integrations over, 231
shape, 218
volume, 232
perihelium precession, 14, 77, 82
Planck density, 260
Planck length, 113, 260
Planck mass, 260

289

PLANCK mission, 231
Planck time, 260
planetary nebula, 90
Pound-Rebka-Snider experiment, 12,
49, 79
pressure
degeneracy, 90
dual role, 99
source of gravity, 60, 99, 183
principle
anthropic, 270
cosmological, 2
general covariance, 13, 55, 60, 184,
259
Mach, 13
relativity, 3
strong equivalence, 13, 46
weak equivalence, 10
proper time, 7, 12, 44
minor role in cosmology, 176
proper volume, 51
proton, 238
lifetime, 243
pulsars, 15, 81, 94
X-ray, 95
QPOs, 106
quadrupole moment of the Sun, 78
quality factor, 143
quark-gluon plasma, 240
in laboratory, 241
quarks, 238
quasars, 16, 78, 110, 166, 227
quasi-periodic oscillations (QPOs), 106
radiation
deﬁnition in cosmology, 189
radiation era, 197
time evolution, 199
radio astronomy, 14
re-ionization, 199
recombination, 199
redshift, 214
and astronomical jargon, 215
and scale factor, 215
and tired light, 216
not additive, 72, 120
of De Broglie wavelength, 215
reference frame

290

Index

freely falling, 11, 12, 46
global, 3, 10
quasar, 166
reheating
after inﬂation, 266
in matter era, 199
rest, 44, 174
rest-frame
global, 5, 7
local, 12
Ricci tensor, 38
Robertson-Walker-metric, 182
Schwarzschild metric, 68
Riemann space, 19
deﬁnition, 19
embedding, 19, 20
Riemann tensor, 34
and curvature, 36, 37
and tidal forces, 53
in freely falling frame, 47
Robertson-Walker metric, 178
co-ordinates, 178
Einstein tensor, 182
geodesics, 185
Ricci tensor, 182
scale factor, 178
rotating black hole, 125
ergosphere, 127
static limit, 127
rotation
galactic, 112
Sachs-Wolfe eﬀect, 210
scalar (tensor of rank 0), 24, 25
scalar ﬁeld
and inﬂation, 258
scale factor, 178
evolution equation, 182
Schwarzschild metric, 70
Einstein tensor, 69
geodesics, 72
orbit classiﬁcation, 74
orbit equation, 75
Ricci tensor, 68
singularity, 113
Schwarzschild radius, 70
Shapiro eﬀect, 79
sign convention, 6, 38
signature, 6

simultaneity, 4, 5
singularity
in cosmology, 192
of Schwarzschild metric, 113
spacetime
curvature, 11
Minkowski, 3
spherical harmonics, 228
spherical universe, 179
standard candle, 224
type Ia supernovae, 227
stellar evolution, 89
binary systems, 93
main sequence, 89
mass loss, 90, 93
neutrino losses, 92
nova, 93
nuclear fusion, 90
red giant, 90
supernova, 92
stress-energy tensor, 54, 55, 60, 97, 182
of cold dust, 54
of matter, 60
of scalar ﬁeld, 259
of vacuum, 184
strong equivalence principle, 13, 46
structure formation, 203
and dark matter, 205
imprints on CMB, 209
summation convention, 6, 23
Sunyaev-Zeldovich eﬀect, 227
supernova, 15, 92
type Ia, 93, 227
Supernova Cosmology Project, 227
supernova remnant, 94
supersoft X-ray sources, 93
tangent space
and embedding, 21
base vectors, 21
preferred metric, 21
temperature of universe, 245
tensor
contravariant representation, 24
covariant representation, 24
Einstein, 38
of higher rank, 24
quotient theorem, 25
Ricci, 38

Index
Riemann, 34
stress-energy, 54, 55
unit, 25
thermal equilibrium
in early universe, 239
Thomas precession, 5, 159
Thomson scattering, 198, 206
tidal forces, 11
and curvature, 53
time dilation, 9
Tolman-Oppenheimer-Volkoﬀ (TOV)
equation, 97
transport of accelerated vector, 155
transverse traceless gauge, 135
ultra-luminous X-ray sources, 112
universe
age, 195, 201
age indicators, 195
closed, 179
evolution, 171
expansion, 171
ﬂat, 178
future development, 196
homogeneity, 174
hyperbolic, 179
isotropy, 171, 174
open, 179
scale model, 218
spherical, 179
temperature, 245
thermal history, 198
visible, 221
Unruh eﬀect, 128

291

UrQMD, 241
variational calculus, 32, 67
vector, 4
contravariant, 24
covariant, 24
null, 7
spacelike, 7
tensor of rank 1, 24
timelike, 7
virial theorem, 89, 96
visible universe, 221
VLBI, 78, 166
Vulcan, 78
weak equivalence principle, 10
white dwarfs, 89, 90
maximum mass, 95, 96
white hole, 123
Wilkinson Microwave Anisotropy Probe
(WMAP), 17, 170, 228
WIMPs (weakly interacting massive
particles), 170
world model
geocentric, 2
Greek, 1
heliocentric, 2
Hindu, 1
Ptolemy’s, 3
worldline, 3
wormhole, 66
X-boson, 243, 258
X-ray astronomy, 14
X-ray binaries, 14, 95, 105, 106, 112

ASTRONOMY AND
ASTROPHYSICS LIBRARY
Series Editors:

G. Börner · A.Burkert · W. B. Burton · M. A. Dopita
A. Eckart · T. Encrenaz · B. Leibundgut · J. Lequeux
A. Maeder · V. Trimble

The Stars By E. L. Schatzman and F. Praderie
Modern Astrometry 2nd Edition
By J. Kovalevsky
The Physics and Dynamics of Planetary
Nebulae By G. A. Gurzadyan
Galaxies and Cosmology By F. Combes, P.
Boissé, A. Mazure and A. Blanchard
Observational Astrophysics 2nd Edition
By P. Léna, F. Lebrun and F. Mignard
Physics of Planetary Rings Celestial
Mechanics of Continuous Media
By A. M. Fridman and N. N. Gorkavyi
Tools of Radio Astronomy 4th Edition
By K. Rohlfs and T. L. Wilson
Tools of Radio Astronomy Problems and
Solutions 1st Edition, Corr. 2nd printing By
T. L. Wilson and S. Hüttemeister
Astrophysical Formulae 3rd Edition
(2 volumes)
Volume I: Radiation, Gas Processes
and High Energy Astrophysics
Volume II: Space, Time, Matter
and Cosmology
By K. R. Lang
Galaxy Formation By M. S. Longair
Astrophysical Concepts 2nd Edition
By M. Harwit
Astrometry of Fundamental Catalogues
The Evolution from Optical to Radio
Reference Frames
By H. G. Walter and O. J. Sovers
Compact Stars. Nuclear Physics, Particle
Physics and General Relativity 2nd Edition
By N. K. Glendenning
The Sun from Space By K. R. Lang
Stellar Physics (2 volumes)
Volume 1: Fundamental Concepts
and Stellar Equilibrium
By G. S. Bisnovatyi-Kogan

Stellar Physics (2 volumes)
Volume 2: Stellar Evolution and Stability
By G. S. Bisnovatyi-Kogan
Theory of Orbits (2 volumes)
Volume 1: Integrable Systems
and Non-perturbative Methods
Volume 2: Perturbative
and Geometrical Methods
By D. Boccaletti and G. Pucacco
Black Hole Gravitohydromagnetics
By B. Punsly
Stellar Structure and Evolution
By R. Kippenhahn and A. Weigert
Gravitational Lenses By P. Schneider,
J. Ehlers and E. E. Falco
Reflecting Telescope Optics (2 volumes)
Volume I: Basic Design Theory and its
Historical Development. 2nd Edition
Volume II: Manufacture, Testing, Alignment,
Modern Techniques
By R. N. Wilson
Interplanetary Dust
By E. Grün, B. Å. S. Gustafson, S. Dermott
and H. Fechtig (Eds.)
The Universe in Gamma Rays
By V. Schönfelder
Astrophysics. A New Approach 2nd Edition
By W. Kundt
Cosmic Ray Astrophysics
By R. Schlickeiser
Astrophysics of the Diffuse Universe
By M. A. Dopita and R. S. Sutherland
The Sun An Introduction. 2nd Edition
By M. Stix
Order and Chaos in Dynamical Astronomy
By G. J. Contopoulos
Astronomical Image and Data Analysis
By J.-L. Starck and F. Murtagh

ASTRONOMY AND
ASTROPHYSICS LIBRARY
Series Editors:

G. Börner · A.Burkert · W. B. Burton · M. A. Dopita
A. Eckart · T. Encrenaz · B. Leibundgut · J. Lequeux
A. Maeder · V. Trimble

The Early Universe Facts and Fiction
4th Edition By G. Börner
The Design and Construction of Large
Optical Telescopes By P. Y. Bely
The Solar System 4th Edition
By T. Encrenaz, J.-P. Bibring, M. Blanc,
M. A. Barucci, F. Roques, Ph. Zarka
General Relativity, Astrophysics,
and Cosmology By A. K. Raychaudhuri,
S. Banerji, and A. Banerjee
Stellar Interiors Physical Principles,
Structure, and Evolution 2nd Edition
By C. J. Hansen, S. D. Kawaler, and V. Trimble
Asymptotic Giant Branch Stars
By H. J. Habing and H. Olofsson
The Interstellar Medium
By J. Lequeux
Methods of Celestial Mechanics (2 volumes)
Volume I: Physical, Mathematical, and
Numerical Principles
Volume II: Application to Planetary System,
Geodynamics and Satellite Geodesy
By G. Beutler
Solar-Type Activity in Main-Sequence Stars
By R. E. Gershberg
Relativistic Astrophysics and Cosmology
A Primer By P. Hoyng

Quantization in Astrophysics,
Brownian Motion,
and Supersymmetry
F. Smarandache & V. Christianto
(editors)
Including articles never before published

MathTiger, 2007
Chennai, Tamil Nadu
ISBN: 81-902190-9-X
Printed in India

This book can be ordered in a paper bound reprint from:
Books on Demand
ProQuest Information & Learning
(University of Microfilm International)
300 N. Zeeb Road
P.O. Box 1346, Ann Arbor
MI 48106-1346, USA
Tel.: 1-800-521-0600 (Customer Service)
http://wwwlib.umi.com/bod/basic

Copyright 2007 by MathTiger and Authors for their own articles.
Many books can be downloaded from the following
Digital Library of Science:
http://www.gallup.unm.edu/~smarandache/eBooks-otherformats.htm
Covers art (digital painting) by F. Smarandache.
Peer Reviewers:
1. Prof. E. Scholz, Dept. of Mathematics, Wuppertal University, Germany
2. Prof. T.R. Love, Dept. of Mathematics & Dept. of Physics,
California State University at Dominguez Hills
3. Dr. S. Trihandaru, Dept. of Physics, UKSW

(ISBN-10) 81-902190-9-X
(ISBN-13) 978-81-902190-9-9
(EAN) 9788190219099
Printed in India

ii

Quantization in Astrophysics, Brownian Motion, and Supersymmetry
The present book discusses, among other things, various quantization phenomena found in
Astrophysics and some related issues including Brownian Motion. With recent discoveries of
exoplanets in our galaxy and beyond, this Astrophysics quantization issue has attracted
numerous discussions in the past few years.
Most chapters in this book come from published papers in various peer-reviewed journals,
and they cover different methods to describe quantization, including Weyl geometry,
Supersymmetry, generalized Schrödinger, and Cartan torsion method. In some chapters
Navier-Stokes equations are also discussed, because it is likely that this theory will remain
relevant in Astrophysics and Cosmology
While much of the arguments presented in this book are theoretical, nonetheless we
recommend further observation in order to verify or refute the propositions described herein.
It is of our hope that this volume could open a new chapter in our knowledge on the
formation and structure of Astrophysical systems.
The present book is also intended for young physicist and math fellows who perhaps will
find the arguments described here are at least worth pondering.

ISBN 81-902190-9-X

90000>

9 788190 219099

iii

Preface
This book, titled Quantization in Astrophysics, Brownian Motion, and Supersymmetry,
is a collection of articles to large extent inspired by some less-understood empirical
findings of Astrophysics and Cosmology. Examples in relation to these findings are
small but non-vanishing cosmological constant and accelerating cosmological
expansion, indication of dark matter and dark energy, the evidence for approximate
Bohr quantization of radii of planetary orbits involving gigantic value of effective (or
real) Planck constant, Pioneer anomaly and flyby anomalies, and the Tifft’s redshift
quantization.
There is recently no generally accepted theoretical approach to these anomalies and
the book is intended to provide a representative collection of competing theories and
models. The general theoretical backgrounds indeed cover a wide spectrum: mention
only Nottale’s Scale Relativity and Schrödinger equation assigned with Brownian
motion and its modification proposed by Carlos Castro (#6), Castro’s Extended
Relativity in Clifford algebra and Weyl geometry based cosmology (#8), Diego
Rapoport’s work with Cartan-Weyl space-time geometry and representation of
random structures via torsion fields (#16,#17), and Pitkanen’s Topological
Geometrodynamics (#3).
In the case of dark matter and energy the proposals include Castro’s proposal for
cosmology based on Weyl geometry (#7). The approach of Pitkanen relies of the
identification of dark matter as a hierarchy of macroscopic quantum phases with
arbitrarily large values of ordinary Planck constant.
In the case of planetary Bohr orbitology one plausible method is based on Nottale’s
Scale Relativity inspired proposal that fractal hydrodynamics is equivalent with
Schrödinger equation with effective Planck constant which depends on the properties
of system and by Equivalence Principle is proportional to the product of interacting
gravitational masses in the recent case. Note that Nottale predicted Bohr
quantization already 1993, much before exoplanets provided further evidence for it.
Other early papers describing exoplanets are 1998-1999 Fizika paper of A. Rubcic
and J. Rubcic on Bohr quantization for planets and exoplanets, which are included
here for clarity (#1 & #2).
Several models for the planetary quantization are discussed: Fu Yuhua’s approach
relies on Hausdorff fractal dimension (#5); F. Smarandache & V. Christianto discuss
a plausible extension of Nottale’s generalized Schrödinger equation to GinzburgLandau (Gross-Pitaevskii) equation based on phion condensate model (#11, #14,
#26), which can also be considered as superfluid vortex in Cantorian spacetime; M.

iv

Pitkanen’s approach explains planetary Bohr orbitology as being a reflection of the
quantal character of dark matter in astrophysical length and time scales.
The present book also discuss solutions to a number of known problems with respect
to Relativity, Quantum Mechanics, Astrophysics, i.e. Bell’s theorem (F. Smarandache
& V. Christianto, #14), holographic dark energy (Gao Shan, #18), unified
thermostatistics (F. Smarandache & V. Christianto, #25), hypergeometrical universe
and supersymmetry (M. Pereira, #19), rotational aspects of relativity (A. Yefremov,
#21, #22), and also Pioneer anomaly (#20, #23, #24).
In the case of Pioneer anomaly the explanations include modifications of Newton’s
gravitational potential and the notion of metric in general relativity, dark matter
induced acceleration, the acceleration anomaly induced by the compensation of
cosmic expansion in planetary length scale, and mechanism inducing anomalous
Doppler frequency shift as Q-relativity effect. (Perhaps this Doppler frequency shift is
comparable with a daily idiom: “The grass always looks greener on the other side of
the fence.”)*
We would like to express our special thanks to journal editors for their kind
permission to us to include these published papers in this volume, and for all peerreviewers for their patience in reading our submitted drafts, and suggesting
improvement.
It is our hope that the present book could open a new chapter in our knowledge on
the formation and structure of Astrophysical systems.
November 26th, 2006
M. Pitkanen

______________
* German: Kirschen in Nachbars Garten sind immer süßer. Ref:
http://www.proz.com/kudoz/1020232. Also http://www.usingenglish.com/reference/idioms/t.html

v

Foreword
"The first principles of things will never be adequately known. Science is an open ended
endeavor, it can never be closed. We do science without knowing the first principles.
It does in fact not start from first principles, nor from the end principles, but
from the middle. We not only change theories, but also the concepts and entities
themselves, and what questions to ask. The foundations of science must be continuously
examined and modified; it will always be full of mysteries and surprises."
(A.O. Barut, Foundation of Physics 24(11), Nov. 1994, p.1571)

The present book is dedicated in particular for various quantization phenomena
found in Astrophysics. It includes various published (and unpublished) papers
discussing how ‘macroquantization’ could be described through different
frameworks, like Weyl geometry, or Cartan torsion field, or generalizing Schrödinger
equation.
To our present knowledge, quantization in various Astrophysics phenomena has not
been studied extensively yet, except by a number of physicists. Mostly, it is because
of scarcity of theoretical guidance to describe such ‘macroquantization phenomena’.
For decades, it becomes too ‘obvious’ for some physicists that quantum physics will
reduce to (semi)-classical mechanics as the scale grows. But as numerous recent
Astrophysical findings have shown, ‘quantization’ is also observed in macro-physics
phenomena, which indicates that quantum physics also seem to play significant role
to describe those celestial objects.
Nonetheless, it is worth noting here that the wavefunction description of the Universe
has been known since 1970s, for instance using Wheeler-DeWitt (EinsteinSchrödinger) equation, or Hartle’s, Vilenkin’s method in 1980s, albeit it is also known
that these approaches lack sufficient vindication in terms of Astrophysics data.
Therefore, from this viewpoint, the quantization description of Astrophysical systems
is merely a retro and improved version of those earlier ideas. Or, if we are allowed to
paraphrasing John Wheeler in this context, perhaps we could say: “Time is Nature’s
way to avoid all things from happening at once, and Quantization is Nature’s way to
bring arrangement and to avoid all things from colliding because of n-bodies
interaction,” (as shown by Poincare in early 20th century).
We would like to express our sincere gratitude not only to a number of journal editors
for their kind permission to enable us include these published papers in this volume
(including Fizika editor, AFLB editor, EJTP editor, PiP editor, Gravitation and
Cosmology editor and Apeiron editor); but also to Profs. E. Scholz, T. Love and S.
Trihandaru for their patience in reading the draft version of this book. And to
numerous colleagues and friends who share insightful discussions and with whom
we have been working with.

vi

As concluding remark to this foreword, we would like to note that after pre-release of
this book (at http://www.gallup.unm.edu/~smarandache/Quantization.pdf), it has
attracted not less than 1325 hits (downloads) to this book in the first three days
(January 21st, 2007), and 3708 hits within the first five days (January 24th, 2007).
Perhaps the printed version of this volume will be appreciated in similar way.

January 26th, 2007.
F.S. & V.C.

vii

Quantization in Astrophysics, Brownian Motion,
and Supersymmetry
F. Smarandache & V. Christianto
(editors)

Preface
Foreword
Content

iv
vi
viii

Quantization in Astrophysics, Tifft redshift, and generalized Schrödinger equation
1. Square laws for orbits in Extra-Solar Planetary Systems – A. Rubcic & J. Rubcic
(Fizika A 8, 1999, 2, 45-50)
1
2. The quantization of the solar-like gravitational systems – A. Rubcic & J. Rubcic
(Fizika B 7, 1998, 1, 1-13)
7
3. Quantization of Planck Constants and Dark Matter Hierarchy in Biology and
20
Astrophysics – M. Pitkanen (Oct. 28th, 2006)
4. Could q-Laguerre equation explain the chained fractionation of the principal
quantum number for hydrogen atom? – M. Pitkanen (Oct. 2006)
58
5. On quantization in Astrophysics – Fu Yuhua (Oct, 4th 2006)
68
Weyl Geometry, Extended Relativity, Supersymmetry, and application in Astrophysics
6. On Nonlinear Quantum Mechanics, Brownian Motion, Weyl Geometry, and
Fisher Information – C. Castro & J. Mahecha (Progress in Physics, Vol. 2 No. 1,
2006)
72
7. Does Weyl’s Geometry solve the Riddle of Dark Energy? – C. Castro (a
condensed version of forthcoming paper in Foundation of Physics, 2006) 88
8. The Extended Relativity Theory in Clifford spaces – C. Castro & M. Pavsic
(Progress in Physics, Vol. 1 No. 2, 2005)
97
9. On Area Coordinates and Quantum Mechanics in Yang’s Noncommutative
Spacetime with a Lower and Upper Scale – C. Castro (Progress in Physics, Vol.
2 No. 2, 2006)
162
10. Running Newtonian Coupling and Horizonless Solutions in Quantum Einstein
178
Gravity – C Castro, J.A. Nieto, J.F. Gonzalez (8th Dec. 2006)

viii

Quantization in Astrophysics and phion condensate: Five Easy Pieces
11. On the origin of macroquantization in Astrophysics and Celestial Motion – V.
Christianto (Annales de la Fondation Louis De Broglie, Vol. 31, 2006)
202
12. The Cantorian superfluid vortex hypothesis – V. Christianto (Apeiron Vol. 10,
No. 3, 2003)
214
13. A note on geometric and information fusion interpretation of Bell’s theorem
and quantum measurement – F. Smarandache & V. Christianto (Progress in
Physics Vol. 2, 2006)
232
14. Plausible explanation of Quantization of Intrinsic Redshift from Hall effect and
Weyl Quantization - F. Smarandache & V. Christianto (Progress in Physics Vol.
2, 2006)
241
15. A new wave quantum relativistic equation from Quaternionic Representation
of Maxwell-Dirac isomorphism – V. Christianto (EJTP, Vol. 3(12), 2006, 117144)
248
Cartan-Weyl space time, Torsion fields, and Navier-Stokes
16. Torsion Fields, the Quantum Potential, Cartan-Weyl Space-Time and StateSpace Geometries and their Brownian Motions – D.L. Rapoport (Dec 8th,
2006)
276
17. Viscous and Magnet -Fluid-Dynamics, Torsion Fields, and Brownian Motions
Representations on Compact Manifolds and the Random Symplectic
Invariants - D.L. Rapoport (Dec. 16th, 2006)
329
18. A note on Holographic Dark Energy – Gao Shan (Nov., 2006)
386
Hypergeometrical Universe, Pioneer Anomaly, Quaternion Relativity
391
19. Hypergeometrical Universe - M. Pereira (8th January, 2007)
20. Three Solar System anomalies indicating the presence of Macroscopically
Coherent Dark Matter in Solar System - M. Pitkanen (Nov., 2006)
433
21. Relativistic Oscillator in Quaternion Relativity – A. Yefremov (Nov. 2006) 440
22. Quaternionic Relativity. II Non-inertial motions – A. Yefremov (Grav. &
Cosmology, Vol. 2, 1996)
458
23. Relativistic Doppler effect and Thomas Precession in Arbitrary Trajectories
469
(and comment on Pioneer anomaly) – A. Yefremov (8th Jan. 2007)
24. Less mundane explanation of Pioneer anomaly from Q-relativity – F.
Smarandache and V. Christianto (Progress in Physics Vol. 3 No.1 2007) 473
25. A note on unified statistics including Fermi-Dirac, Bose-Einstein, and Tsallis
statistics, and plausible extension to anisotropic effect -- V. Christianto & F.
Smarandache (Jan. 8th, 2007)
481
26. Numerical solution of Time-dependent gravitational Schrödinger equation -–
V. Christianto, D. Rapoport, F. Smarandache (Jan. 2nd, 2007)
487
Abstract

505

ix

ISSN1330–0008
CODEN FIZAE4

LETTER TO THE EDITOR

SQUARE LAW FOR ORBITS IN EXTRA-SOLAR PLANETARY SYSTEMS
ANTUN RUBČIĆ and JASNA RUBČIĆ
Department of Physics, University of Zagreb, Bijenička 32,
10000 Zagreb, Croatia
E-mail: rubcic@sirius.phy.hr
Received 27 April 1999; Accepted 1 September 1999
Reprinted with kind permission from editor of Fizika A

The square law rn = r1 n2 for orbital sizes rn (r1 is a constant dependent on the
particular system, and n are consecutive integer numbers) is applied to the recently
discovered planets of υ Andromedae and to pulsars PSR B1257+12 and PSR 182811. A comparison with the solar planetary system is made. The product nvn of the
orbital velocity vn with the corresponding orbital number n for planets of υ Andromedae is in good agreement with those for terrestrial planets, demonstrating the
generality of the square law in dynamics of diverse planetary systems. ”Quantized
velocity” of nvn is very close to 24 kms−1 , i.e. to the step found in the quantized
redshifts of galaxies. A definite conclusion for planetary systems of pulsars requires
additional observations.
PACS numbers: 95.10.Ce, 95.10.Fh, 95.30.-t

UDC 523.2, 531.35

Keywords: planets of υ Andromedae and of pulsars PSR B1257+12 and PSR 1828-11,
square law for orbital sizes, ”quantized velocity” nvn

In our previous papers [1,2], the orbital distribution of planets and satellites in
the solar system has been described by the simple square law
r n = r 1 n2 .

(1)

Semimajor axes rn of planetary and satellite orbits are proportional to the square
of consecutive integer numbers n, where r1 is a constant dependent on the system.
We have also applied the square law to the planetary system of the pulsar PSR
B1257+12 [3].
Very recently, the planetary system of the nearby star υ Andromedae (from
hereafter: υ And) has been discovered using the Doppler radial velocity method
[4]. It is the first system of multiple companions with a parent star similar to the
Sun. Therefore, it is important to check whether the planets of υ And obey also the
square law. Moreover, the planets of the pulsar PSR 1828-11 will be considered,
FIZIKA A (Zagreb) 8 (1999) 2, 45–50

Quantization in Astrophysics ...

45

1

rubčić and rubčić: square law for orbits of extra-solar . . .

too, although the present findings are not yet confirmed. So far, only three extrasolar planetary systems with more than one observed planet per system have been
discovered.
The observational data for υ And and two pulsars are given in Table 1. Note
that masses (M ) of planets of υ And, and those of the pulsars are of the order of the
Jupiter mass (MJ ) and Earth mass (ME ), respectively. Question mark added to
the planet A of PSR B1257+12 means that original results [5] have been questioned
[6] with the suggestion that planet A might be an artefact in the calculations.
TABLE 1. Semimajor axes rn , masses (M ) sin(i), deduced orbital numbers n, products of n with the corresponding orbital velocity vn , and the mean values of nvn
for extra-solar planetary systems.
System

rn /(1011m)

(M ) sin(i)

n

nvn /(km s−1 )

υ Andromedae
υ And b
υ And c
υ And d

0.0883
1.242
3.740

0.71 (MJ )
2.11 ”
4.61 ”

1
4
7

138.52
147.7
148.99
145.08

PSRB1257 + 12
A(?)
B
C

0.285
0.540
0.705

0.015 (ME )
3.4
”
2.8
”

5
7
8

410.49
417.50
417.59
415.20

PSR 1828-11
A
B
C

1.391
1.975
3.142

3 (ME )
12 ”
18 ”

6
7
9

208.57
204.21
208.16
206.98

Data are taken from: Jean Schneider, Extra-solar Planets Encyclopaedia, update 15
April 1999.
http://www.obspm.fr/planets

In order to determine the orbital numbers n for the particular system, the square
roots of orbital semimajor axes have been plotted vs. integer numbers in such a
way that all observational points are close to a straight line without an intercept.
Deviations of the observational points from the straight line for pulsar planetary
systems are found to be less than 2%, while those of υ And less than 6.2% on the
average.
The results of the fit to the data in Table 1 are shown in Fig. 1. The square
law satisfactorily describes orbital sizes in extra-solar planetary systems, in spite
of the fact that only few planets per system have been found. It is evident that
FIZIKA A (Zagreb) 8 (1999) 2, 45–50

46

Quantization in Astrophysics ...

2

rubčić and rubčić: square law for orbits of extra-solar . . .

some orbits predicted by the square law are not occupied. For the planetary system
of υ And, the orbits at n equal to 2, 3, 5, and 6 are vacant. It may be that at
these orbits small planets exist, but undetectable by the present methods. Future
observations should confirm or disprove these assumptions.
11

rn1/2 / 105 m1/2

(semimajor axis)1/2

10
9

EXTRA-SOLAR PLANETS
8
7
Ce

6

υ

d

$1'520('$(

C

5

Ma

PSR 1828-11

4

B

E

c

A

3

V

C
B

Me

2

A

1

PSR B1257+12

b

0
0

1

2

3

4

n

5

6

7

8

9

orbital number

Fig. 1. Correlation of the square root of the semimajor axes rn with the orbital numbers n for extra-solar planetary systems. Terrestrial planets (open circles, dashed
line) are added for comparison.
We have shown [1] that the radius and velocity at the n-th orbit (within the approximation of circular orbits) is proportional to n2 and 1/n, respectively. Further
investigation [2] has shown that along with the orbital number n, an additional
number k may be introduced, resulting in the following relationships
rn =

G
n2
M
,
v02
k2

vn = v0

k
,
n

(2)

(3)

where G is the gravitational constant, M the mass of the central body, and v0
a fundamental velocity, which may be considered as an important quantity of all
considered systems.
The integer number n determines the quadratic increase of orbital radii, while
k defines the extension or spacing of orbits. By increasing k, orbits are more closely
FIZIKA A (Zagreb) 8 (1999) 2, 45–50

Quantization in Astrophysics ...

47

3

rubčić and rubčić: square law for orbits of extra-solar . . .

packed. Thus k may be named the ”spacing number” to differ from the main
”orbital number” n. Equation (3) states that nvn is a constant for a given system,
and for some other systems it is a multiple of the fundamental velocity v0 . Indeed,
this has been demonstrated for the solar system [2], i.e. for its five subsystems:
the terrestrial planets and the largest asteroid Ceres (k = 6), the Jovian planets
(k = 1), and satellites of Jupiter (k = 2), Saturn (k = 4) and Uranus (k = 1).
For all these subsystems, the value of nvn = kv0 is given by (25.0 ± 0.7)k km s−1
[2], confirming thus Eq. (3). It has to be pointed out that more accurate value of
nvn = [(23.5 ± 0.3)k + (4.0 ± 1.0)] km s−1 was obtained (Eq.(12) in Ref. [2]). A
similar situation for orbital velocities may be expected in extra-solar systems.

Planets of PSR B1257 + 12 (confirmed, except A)
B

400

C

A (?)

k spacing number

n vn / kms-1

300

Planets of PSR 1828 -11 (unconfirmed)

200
Terrestrial planets and Ceres
b, c, d: Planets of υ Andromedae
c
d
b

6
Me

V

E

Ma

Ce

Satellites of Saturn

100

4
Jan

Am

Io

J

S

Eu

Ga

Cal

N

Pl

Mim

Enc Teth

1

2

U

Puck

Mir

Ar

Um

Tit

Ob

3

4

5

6

7

8

n

Rhea

Satellites of Jupiter

2

Jovian planets
Satellites
of Uranus

0
0

Dio

9

0
10 11 12

orbital number

Fig. 2. Correlation of the products of orbital numbers n and orbital velocities vn
with n and the spacing number k, for the solar subsystems and three extra-solar
planetary systems.
FIZIKA A (Zagreb) 8 (1999) 2, 45–50

48

Quantization in Astrophysics ...

4

rubčić and rubčić: square law for orbits of extra-solar . . .

The correlation of nvn with n and k is shown in Fig. 2. This figure is based
on Fig. 3. of Ref. [2], where only data for the solar system have been taken into
account. Here, it is supplemented by the extra-solar system data of υ And and
pulsars PSR 1828-11 and PSR B1257+12. Figure 2 demonstrates that new data of
the planetary system of υ And, with the mean value of nvn equal to 145.1 km s−1
(see Table 1), are compatible with the data for terrestrial planets of the solar
system, for which nvn has almost the same value of 145.0 km s−1 [2]. A similarity
among the two planetary systems can be seen also in Fig. 1. Although the number
of planets for pulsar planetary systems are small, one may notice the well defined
”velocity levels” with the step of nearly 207 km s−1 . However, one should not take
this as a final result because only two nvn are known. Future discoveries of other
pulsar planetary systems will probably change the number of levels defined by k in
Fig. 2. Indeed, one may even expect that the step of 207 km s−1 might be decreased
to 207/8 = 25.9 km s−1 , which is nearly equal to that of the solar system. This
would lead to the similarity in dynamical properties of diverse systems. However,
only future observations should give a definite answer to these expectations.
The velocity about 24 km s−1 is deduced from the quantized redshifts of galaxies
[7-11] as one of the possible ”quantized periods”. Some other values like 36, 72 and
144 km s−1 are also found. It is a great puzzle why the orbital velocities should be
related to the velocities derived from redshifts. However, one suspects that some
fundamental link exists among the systems.
Some authors prefer the fundamental velocity of about 144 km s−1 [12–14]. This
was found for planets in the solar system if one takes all planets as a single system.
In the present model, the terrestrial planets are located at the level k = 6, and
Jovian planets at k = 1, because v0 is addopted to be 24 km s−1 . In that case,
Jovian planets are considered as a subsystem with n = 2 for Jupiter, n = 3 for
Saturn, etc., as can be seen in Fig. 2 (see also Refs. [1–3]). The terrestrial planets
could be considered as the remnants of mass of a Jupiter-like planet, which failed
to be formed at n = 1 [1,14]. However, terrestrial planets may be taken as an
independent subsystem, with Mercury at n = 3, Venus at n = 4, etc., as can be
seen in Figs. 1 and 2.
The assumption v0 ≈ 144 km s−1 will introduce many vacant orbits between
Jupiter and Pluto, if the square law for orbital radii is taken into account. Thus,
Jupiter will be at n = 11, Saturn at n = 15, Uranus at n = 21, Neptune at n = 26
and finally Pluto at n = 30. An analysis of the solar-system data suggests that
planets of υ And are located at the velocity level k = 6, with v0 ≈ 24 km s−1 . If v0
is taken to be 144 km s−1 , then k will be equal to one. Consequently, the value of
k, e.g., for the Jovian planets would be then 1/6. According to the present model,
that does not seem likely, because the ”spacing number” k is defined as an integer
number and determines the packing of orbits.
There is a hope that the same value v0 can be attributed to the systems around
alike stars. For pulsars, one may suppose that v0 could be equal to about 26
km s−1 and consequently k should be equal to 8 and 16 for PSR 1828-11 and
PSR B1257+12, respectively. Although this assumption seems very attractive, it
cannot be confirmed without further observations.
FIZIKA A (Zagreb) 8 (1999) 2, 45–50

Quantization in Astrophysics ...

49

5

rubčić and rubčić: square law for orbits of extra-solar . . .

In conclusion, one may claim that the square law is adequate for the description of the orbital distribution for diverse systems: solar subsystems, extra-solar
planetary systems with stars similar to the Sun and even to planetary systems of
pulsars.
Acknowledgements
The authors are very gratefull to Profs. H. Arp and K. Pavlovski for their
permanent interest, suggestions and support throughout the work.
References
1)
2)
3)
4)

5)
6)
7)
8)
9)
10)
11)
12)
13)
14)

A. Rubčić and J. Rubčić, Fizika B 4 (1995) 11;
A. Rubčić and J. Rubčić, Fizika B 7 (1998) 1;
A. Rubčić and J. Rubčić, Fizika B 5 (1996) 85;
P. Butler, G. Marcy, D. Fischer, T. Brown, A. Contos, S. Korzennik, P. Nisenson and
R. Noyes, Evidence for multiple companions to upsilon Andromedae, ApJ (1999) (in
press);
A. Wolszczan, in Compact Stars and Binaries, J. van Paradijs et al. (Eds), Proc. IAU
Symp. 165 (1996) 187;
K. Scherer, H. Fichtner, J.D. Anderson and E.L. Lau, Science 278 no. 5345 (1997)
1919;
W. G. Tift, ApJ 206 (1976) 38;
H. Arp and J. W. Sulentic, ApJ 291 (1985) 88;
W. J. Cocke and W. G. Tifft, BAAS 26 (1994) 1409;
B. N. G. Guthrie and W. M. Napier, Astron. Astrophys. 310 (1996) 353;
H. Arp, Seeing Red: Redshifts, Cosmology and Academic Science, Apeiron, Montreal,
Canada (1998) pp.195-223;
A. G. Agnese and R. Festa, Hadronic Journal 21 (1998) 237;
G. Reinisch, Astron. Astrophys. 337 (1998) 299;
L. Nottale, Chaos, Solitons and Fractals 9 (no. 7) (1998) 1043.

KVADRATNI ZAKON ZA STAZE IZVAN-SUNČEVIH PLANETARNIH
SUSTAVA
Kvadratni zakon rn = r1 n2 za polumjere staza rn (r1 stalnica ovisna o sustavu a n
uzastopni cijeli brojevi) primjenjujemo na nedavno otkrivene planete υ Andromede
i pulzara PSR B1257+12 i PSR 1828-11. Načinili smo usporedbu sa sunčevim sustavom. Umnožak nvn za stazne brzine vn i staznog broja n za planete υ Andromede
u dobrom je skladu s vrijednostima za terestrijalne planete. To pokazuje općenitost
kvadratnog zakona u dinamici različitih sustava. “Kvantizirana brzina” nvn vrlo je
blizu 24 kms−1 , tj. koraku koji se opaža u crvenim pomacima galaksija. Konačan
zaključak za planetarne sustave pulzara zahtijeva nove podatke.
FIZIKA A (Zagreb) 8 (1999) 2, 45–50

50

Quantization in Astrophysics ...

6

ISSN 1330–0016
CODEN FIZBE7

THE QUANTIZATION OF THE SOLAR-LIKE GRAVITATIONAL SYSTEMS
ANTUN RUBČIĆ and JASNA RUBČIĆ
Department of Physics, Faculty of Science, University of Zagreb, Bijenička cesta 32, HR-10001
Zagreb, Croatia; e-mail rubcic sirius.phy.hr
Received 24 January 1998; Accepted 1 June 1998
Reprinted with kind permission from editor of Fizika B



Mean orbital distances
of planets from the Sun and of major satellites from the parent
planets Jupiter, Saturn and Uranus are described by the square law
, where the
values of are consecutive integers, and is the mean orbital distance expected at
for a particular system. Terrestrial planets and Jovian planets are analysed as separate systems. Thus, five independent solar-like systems are considered. The basic assumption is
that specific orbital angular momentum is ”quantized”. Consequently, all orbital parameters are also discrete. The number relates to the law of orbital spacing. An additional
discretization, related to , i.e. to the scale of orbits, accounts for the detailed structure of
planar gravitational systems. Consequently, it is also found that orbital velocity
multiplied by is equal to the multiple of a fundamental velocity
km s , valid for all
subsystems in the Solar System. This velocity is equal to one of the “velocity” increments
of quantized redshifts of galaxies.







    





 

PACS numbers: 95.10.Ce, 95.10.Fh, 96.30.-t







UDC 523.2, 531.35

Keywords: planetary and satellite orbits, law of squares of integer numbers, discrete values of orbital
velocities

1. Introduction
Recently, Agnese and Festa [1] published their approach in explaining discrete orbital
spacing of planets in the Solar System. They used Bohr-Sommerfeld quantization rules and
obtained the square law for orbital radii of planets in the form
All planets have been treated as one group. That assumption leads to many vacant orbits.

            

FIZIKA B 7 (1998) 1, 1–13

Quantization in Astrophysics ...

1

7

RUB ČI Ć AND RUB ČI Ć : THE QUANTIZATION OF THE SOLAR - LIKE

 



For example, Jupiter and Saturn occupy the orbits at
and
, respectively,
leaving three vacant orbits in between. Likewise, there are five vacant orbits between Saturn and Uranus. However, according to the current views [2], the planets are about as
closely spaced as they could possibly be. Less massive planets are expected to be in more
tightly packed orbits than the larger ones.



! " # $%! & '  (*),+-( . / 0
+

Recently, Oliveira Neto [3] used the square law in the form
,
where and
are integers. Only for Venus, Earth, Mars and Vesta
is not equal to
, while
for all other planets, asteroid Camilla, Chiron and an unknown planet
between Uranus and Neptune. Moreover, an average mass of all planets and asteroids equal
to about 35 Earth masses is assumed in the calculation, which is not physically justified.



+
1+

In our earlier work [4,5], we have shown that a square law could be applied to planetary
orbital mean distances, as well as to those of major satellites of Jupiter, Saturn and Uranus.
The leading assumption was that vacant orbits should be avoided. A radical change in
treating the planetary orbits has been made by the separation of terrestrial planets from
the Jovian ones. It means that terrestrial planets are considered as an independent system,
enjoying the same status as the Jovian group of planets as well as the satellite system of
Jupiter, Saturn and Uranus. The division of planets into two groups is justified by their
different physical, chemical and dynamical properties [4,6,7]. From a cosmogonical point
of view, an explanation could be the following: the centres of aggregation of future planets
have been governed by the simple square law. After the accretion process, Jupiter has been
formed in the orbit at
, Saturn at
, ending with Pluto at
. The first
Jovian protoplanet close to the Sun at
, has never been formed due to the Sun’s
thermonuclear reactions. The high-melting-point materials have survived and accreted as
the system of terrestrial planets, while the gaseous components have been dispersed due to
the solar wind. Only beyond the ”temperature limit” of about 200 K, which corresponds to
about
m, could the giant Jovian planets exist [4].

%0

%2
%5

%43

76  8 9 9

The division of planets into two groups appeared also in solving the modified
Schrödinger radial equation of the hydrogen-like atom introducing, of course, the gravitational potential [8] and coefficient of diffusion of Brownian motion which characterizes
the effect of chaos on large time scales [9a,10]. From a dynamical point of view, the five
systems: terrestrial planets, Jovian planets, and satellites of Jupiter, Saturn and Uranus are
to a considerable degree adiabatic. Therefore, the relevant equations in the present model
include characteristic parameters of the particular system, but they also have a necessary
physical generality and consistency. However, many authors [7,11,12] have prefered to
treat the spacing of all planets with a single formula, like the Titius-Bode law or its numerous modifications. The authors of this work consider the square law, like that discovered
by Bohr in his planetary model of the hydrogen atom, more favourable for an analysis of
the planar gravitational systems. Moreover, it has been proposed [13] that the square law
of orbital spacing, could be termed the fourth Kepler’s law, in the honour of Kepler who
searched for a rule of planetary spacing about four centuries ago.
An application of the square law to the extra-solar planetary systems will certainly be
examined in the near future. Recently, first attempts [5,10] were made for the three planets
of pulsar PSR B 1257+12.
2

Quantization in Astrophysics ...

FIZIKA B 7 (1998) 1, 1–13

8

RUB ČI Ć AND RUB ČI Ć : THE QUANTIZATION OF THE SOLAR - LIKE

2. The model
A discrete distribution of planetary orbits may be obtained by the ”quantization” of angular momentum . Let an orbiting mass be denoted by
, and mass of the central body
by . Then, using Newton’s equation of motion for circular orbits, angular momentum
(supposing that
) is given by

:;
<;>>,=

=

<-;

: ;?,<; @ ; A ;B?C<-;ED F=A ;EG
where F is the gravitational constant, A ; is the radius of the K -th orbit and @ ;

HIJ
is the orbital

velocity. We assume that angular momentum is ”quantized”,

<-;ED F7=A ;B?CKBM L N G

HM J

L

where
may be treated as an effective ” Planck’s gravitational constant”, depending on
the particular system and even on the particular orbiting body. Equation (2) is not very
by the mass of the orbiting body to obtain the
useful. What one can do is to divide
”specific Planck’s constant”
which yields for the orbital radius

MN
M
N
L
O
< ;J
LP ? LO H 
A ;B? K FQ L=SP Q R
HT J
is also system dependent, but the quantity
of magnitude for
LP systems
LP O = isofof the =sameis order
all
(see Table 1, and also Ref. 4). Variability
described
by a dimenMN
sionless factor U multiplied by a universal constant V , i.e.,LP O H <; = JW?
LO
LP O =X?CUEV .
Then, Eq. (3) takes the form
A ;B? F I H UEV*J Q =K Q R
HY J

We have shown [4] that by comparing electrostatic and gravitational forces, as one possible
approach, the constant may be defined by the fundamental physical constants as follows:

V

V? M NZ F [ ?I R \ I ] ^_ I ` a b cWd QWe fga bih a b G
H] J
M N j Q H Y N k l m [ J is the fine-structure constant, j the charge of an electron, k l
where Z ?
m
the permitivity of O vacuum, the Planck constant and [ the velocity of light. The dimension
of the constant V is that of angular momentum per square mass, and, in accordance with
Eq. (5), the simple proportionality between V and the Planck constant per square Planck’s
m MN
M I ^ ^_ I ` aEq kg [9b] is given by
mass <-no? H [ H FJ J b p Q7?
O
R m
m
V? Z < Qn Gsr tuV? < lQ G
Hv J
l
where <Q ? Z <Qn . A constant analogous to V has been defined as w,?%`
R x _ I ` a b c mQ
kg a b s a b by Wesson [14] in searching for a clue to a unification of gravitation and particle
FIZIKA B 7 (1998) 1, 1–13

Quantization in Astrophysics ...

3

9

RUB ČI Ć AND RUB ČI Ć : THE QUANTIZATION OF THE SOLAR - LIKE

y z { |} ~         



physics. Such a constant appeared also in Ref. 1 with the value
m kg s .
Slightly different values of the same constant are due to different initial assumptions.
TABLE 1. Mean values of constants ,
and , with the assigned values of integers
, for planetary and satellite systems.

    

System
Terrestrial
planets
Jovian
planets
Jupiter’s
satellites
Saturn’s
satellites
Uranus’
satellites


(m)
  z  { o z  ~   ~   
 ~ z  | ~Wo z     ~   
 gz |  *o z ~    ~  
  z {  *o z  ~ y  ~  
  z   {*o z  ~   ~  

 o   





  
   kg  )
  z   yo z     ~  
 y z g~ o z  {   ~  
 ~ z y  o z  y |  ~  
  z   o z  ~   ~  
 y z |  yo z  y   ~  
(m s

3,4,5,6,
8
2,3,4,5,
6
2,3,4,5,
6
6,7,8,9,
10,11
3,4,5,6,
7,8

 y
 ~y
 
 {
 ~{


z g~W,
z  ~W,
z  y,
z | {,
z y ,

z {
z~ 
z~ {
z |
zy 

Using Eqs. (4) and (5), some important parameters of the solar subsystems, the orbital
radii
, specific angular momenta
, orbital periods
and velocities
are given by

  
 ¥

 g -
 B4 y i B¡ E¢    £


y ¢ 
-    B¡ 
EBy   y i B¡ 7¤ ¢  
¤

 B y  ~   z

E



 
 £
 £
~ 


In Eq. (10),
is the orbital velocity of an electron at the -th orbit in the
Bohr’s model of the hydrogen atom, and the term
is a gravitational correction
factor. This term is system dependent and it demonstrates that gravitational systems are
less regular than analogous electrodynamical systems.

~   y  E

3. Results and discussion
Distributions of specific angular momenta of planets and major satellites according to
the linear relationship (Eq. (8)) are illustrated in Fig. 1.
4

Quantization in Astrophysics ...

FIZIKA B 7 (1998) 1, 1–13

10

RUB ČI Ć AND RUB ČI Ć : THE QUANTIZATION OF THE SOLAR - LIKE

©¦

ºÇ

ð Ûì ¨ «
å
êÛ
Ûï
í éîì ¨ ¦
ççè æéêë
åæ § «

Å ° Ê É· Ë º Í · Ë Â ² Ì

Ë
µ
Æ ¿Ç
Ì

Þ ãâ ä

Å

àá §¦
Þß Û
ÚÜÝ «
Ù
ÚÛ
Ø Ù
¦
¦

§

È ¿Ã
Â»

ÉÁ

·¶

Ê

¹¾
¨

Å µ º É² Â ¼

Â

¹¿
Å¿Ã

º » ¹ ³¸ · ¸ µ ¶
© ª « ¬
ñ òñ óô õ ô ö

² Â ¼ ¼ Â Ì ² ¼ É· Í º Í · Ë Â ² Ì
Æ¾
Ì·²µ¼Ë
² ¾ ´ À ³Á ¼ ½ ¾ ¿
Â
Ã
Ä
¹ ³¶
µ¼·ËµÌ
² ³´ ° ±
­ ® ¯ §¦ §§
ñ÷ø ùôö

ú û ü ýûþ1ÿ

Ô×
ÔÕ

Õ ÖÑ

ÐÒÓ Ï
ÐÎ Ñ Ï

û

Fig. 1 Specific angular momentum
 versus the integer number  for
Jovian and terrestrial planets (left scale) and for the major satellites of Jupiter, Saturn and
Uranus (right scale).
Discrete values of
are obtained from Eq. (1) using the observed values of semimajor axes as the mean distances of planets from the Sun, or of satellites from the parent
planet, which are taken as the orbital radii  of approximate circular orbits. This introduces small errors of  [4], and of
for Mercury and Pluto, due to the eccentricities
of their orbits of 0.206 and 0.255, respectively [15]. The approximation of circular orbits is
very good for other planets and all major satellites. The integer numbers  are unambiguously determined by the requirement of Eq. (8) that angular momenta are zero at   ,
resulting in the straight lines shown in Fig. 1, with no intercepts, as the best fits to the
deduced values of
. The left scale corresponds to Jovian and terrestrial planets,
while the right scale is valid for major satellites of Jupiter, Saturn and Uranus. We have
also included in our calculations the satellites Amalthea, Janus and Puck (the largest of
the small ones), which are near the Roche limit of the parent planets Jupiter, Saturn and
Uranus, respectively, and also the largest asteroid Ceres. Therefore, the values of   in the

ú ûgü ýû
û

ú ûgü ý-û

û

Cþ

ú ûgü ý-û

FIZIKA B 7 (1998) 1, 1–13

Quantization in Astrophysics ...

5

11

RUB ČI Ć AND RUB ČI Ć : THE QUANTIZATION OF THE SOLAR - LIKE

square law
  for spacing of planetary orbits in accordance with Eq. (7), and also
of    and  , which are listed in Table 1, differ slightly from the values given in our
earlier work [4]. Note that the orbit of the asteroid Ceres is at  , which is nearly the
center of the Main Belt, whose extention is from  to   .
There is one exception in treating the spacing of major satellites. Titan, the largest
satellite of Saturn, is not included in the system of smaller satellites from Janus to Rhea.
Titan would have the orbit at   if it were a member of that system. Seven vacant orbits
between Rhea and Titan suggest that Titan could be a member of a more extensive system,
similarly to Jupiter in the Jovian group of planets in relation to the terrestrial planets. Titan
and small satellites Hyperion and Japetus do not form a complete system.
Note that asteroids (except for the largest, Ceres), comets, planetary rings and outer
small satellites of planets can not be treated by Eqs. (7-10) because, due to their small
masses, a variety of other physical processes (scattering, capture, impacts, planetary perturbations) prevail over the simple law. Moreover, it was recently shown in modeling the
massive extrasolar planets, that orbital evolution and significant migration of planets could
take place, due to the interaction of a planet with circumstellar disk, with the parent spinning star and also due to the Roche lobe overflow [16]. A planet may move very far from
its initial position of formation accompanied also with the loss of mass. However, under
certain conditions, planets maintain their position of formation. One may suppose that initial positions are governed by the square law according to the ”quantum-mechanical laws”,
but possible later evolution might be subjected to numerous “effects of classical physics”.
We have tried to correlate the factor  with the ratio of the total mass ( of orbiting
%&.
bodies to the mass  of the central body [5], more precisely, of  with ( !"# $
The values of  for terrestrial planets, Jovian planets and satellites of Jupiter fit very well
a straight line, but there are strong deviations of  for satellites of Saturn, and particularly
for those of Uranus. Note that the planes of planetary orbits are close to the ecliptic (except
those of Mercury and Pluto) which is also valid for satellites of Jupiter, due to the small
inclination of Jupiter’s spin axis. However, the satellites of Saturn have an inclination of
27' and those of Uranus 98 ' . Their satellites have supposedly been formed in the equatorial
planes after the protoplanets, within the planetary envelopes, and obtained an additional
angular momentum of yet unknown origin. We believe that the deviation of the factor 
from the introduced correlation [5] has the same cause as the change of inclination.
In our later investigation, we have found that reciprocal values of the factor  take
discrete values that may be described by another integer number ( , i.e.,

)  * + , + -  . /10+ , + + +  . $ (32 * + , +   .10+ , + + 4  $ 5

* $

as may be seen in Fig. 2. Therefore, Eq. (10) may be written in the form

76 6 98: * 4 / , .90+ , / $ (2 * ;#, +90< , + $ = > ?@ )  ,

*4 $

The product of 76 , i.e. the orbital speed 6  at A for a particular system, vs.  is
shown in Fig. 3. The values of  are taken from Table 1, and the mean velocities from
%
observed semimajor axes as 6 B* C  $  (see
6

Quantization in Astrophysics ...

FIZIKA B 7 (1998) 1, 1–13

12

RUB ČI Ć AND RUB ČI Ć : THE QUANTIZATION OF THE SOLAR - LIKE

D KH
uk
vp

L M N N M O P N QR S T SR U M P O

uu qm D K G
p
r ltq
r sq D K F
kpm
no
klm
D KE

\]^_ ` D KD J a I G b D KD D D c I d e f `D KD E E I b D KD D F g d
V R PW NU

X W T QP M N

j hi

D KD

X Y Z QR U T SR U M P O
[ NR U W O
D

E

F
w

G

H

xy z { | { } y ~   { }

I

J

Fig. 2. Correlation of the reciprocal value of the factor  with integer number  .

Horizontal lines represent ”velocity levels” with spacing defined by      km s 
(Eq. (12)). The integer number  is related to the scale of orbits in a system. It means that
a given system can have a series of discrete possible orbital distributions. That is hardly
understandable from the standpoints of classical physics, because one can only expect a
continuous change of orbital spacing. For example, Uranian satellites are characterized by
  . Neglecting the value of  at B  in Eq. (11), the orbital radii are approximately
described by  B         . If B  , the orbits would be contracted by the factor
four, i.e. contraction of orbits occurs in jumps. Consequently, reduced orbital radii    
become




  
 
         1   ¡    ¢  £

 ¡



where     may be called a characteristic length with a dimension mkg  . The value of
  in Eq. (13), equal to     3   ¡ km s was obtained from the fit of   vs.  with
zero intercept at  , and neglecting a constant term of velocity   at  (i.e., 4.0
km s  in Eq. (12)). That causes a larger error in the calculation of   ,   and of other
quantities, but the formulae are simpler in illustrating the main
FIZIKA B 7 (1998) 1, 1–13

Quantization in Astrophysics ...

7

13

RUB ČI Ć AND RUB ČI Ć : THE QUANTIZATION OF THE SOLAR - LIKE

©§ ¤
©¦ ¤

³Ä
Ç

©¥ ¤

îï

±°

ÀÁ

¿µ ¾½ ¼½¸

»
º

¯

¥¤

¹

Ý Ö à ÜÙ Ú Ó Ø × × Ô Ù Ú Û Û ÜÙ Ú ×

´¸

Ý Þ ß ÜÔ Õ à ÛÔ Õ Ú Ù ×
Ò
Ó
¬ ­® ª « Ô Õ Ö × Ø × Ô Ù Ú Û Û ÜÙ Ú ×

´ µ ¶ · ³ ­² ± ² ¯ °
¤
¤

¥
ô

¦

ç Ñ ÌÎ
ÐÏ
è ÌÎÊ
Ë ÌÍ
é ÈÉ Ê

» ½ Æ ³ ­ ° ¿ Æ ¶ ¬ Ä ® Ã Å ­Á Â Ã Ä ½

ðñ ¨ ¤
ëì í § ¤
¦¤

å
æ

á Ô Ù Ö Ó Õ Ø × × Ô Ù Ú Û Û ÜÙ Ú ×

©¤¤
óò

¿

â Ú Ó Ó Ú × Ù Ó ÜÔ Û à ÛÔ Õ Ú Ù × Ô Õ ã ä Ú Ó Ú ×
³½
¼Ä

§
¨
õô ö ÷ ø ÷ ù ô ú û ü ÷ ù

©¤

ê
©¥

¤

   

ÿ and integer number
Fig. 3. The product ý7þ ÿ of mean orbital velocity þ ÿ
ý versus ý for Jovian and terrestrial planets and for the major satellites of Jupiter, Saturn
and Uranus. Integer number (right scale) is related to the scaling of orbits. The ”velocity
levels” are given by Eq. (12).
The orbital integers ý and determine the details of possible discrete gravitational structures.
The value of þ
km s has been found as one of increments of the intrinsic
galactic redshifts derived from their ”quantized” values [17-21]. One may suspect that þ
is important not only for the Solar System, but that it has a deeper physical meaning to be
revealed.
Equation (13) may be rewritten in another important, symmetrical form









ÿ



 þ        ý 
 
   is equal to the ratio of the Planck’s length      !    "  and
The term
     "  , i.e.,      #' . Hence, Eq. (14) takes a form
Planck’s mass #$%&   

 ÿ   þ   #$   ý    
( 
8

Quantization in Astrophysics ...

FIZIKA B 7 (1998) 1, 1–13

14

RUB ČI Ć AND RUB ČI Ć : THE QUANTIZATION OF THE SOLAR - LIKE

Equation (15) gives a remarkable connection between macroscopic and microscopic parameters of gravitational systems.
Consider again the initial assumption in our model. The discretization of angular momenta, using the approximation of circular orbits, is given by Eq. (2), i.e.,
. The present model permits to write a proper ”quantum condition” in accordance
with Bohr as

)$* + * , *.-

/0'1 2 3

)'* + * , *'465 )$)$87 * 2 39;:=<> - @/ 2 ? 3A

BCD E

An approach to prove Eq. (16), using the theory of similarity, is given in Appendix. Equation (16) can be interpreted as follows: angular momentum of an orbiting body in a planar
gravitational system is proportional to the mass
of the central body and to the mass
of the orbiting body. Therefore, angular momentum per square mass is of special
scales a gravitational macroscopic
importance. Further multiplication by
system to the microscopic (atomic) one. However, the ratio
must be multiplied by the factor
, which has to be determined from observational data. Dynamic
properties of gravitational systems reach, in the limit, the electrodynamical ones. If the
and
are introduced in Eq. (16), one easily obtains
quantities
for the velocity at the -th orbit, in accordance with Eq. (10). For
, the orbital velocity distribution of the electron in Bohr’s hydrogen atom is obtained. It has already been shown that
(see Fig. 3). For
, one obtains
km s , and consequently, from
follows
that the maximum value of
is
. Orbital radii are then simply given
by
, which is just Eq. (13). From Eq. (16), an
effective ”Planck’s gravitational constant” appears to be
. Then,
the Schrödinger’s radial wave equation for a gravitational system generates the first orbital
radius in agreement with Eq. (7), as it is shown in Appendix.
The present model describes the structures of planar gravitational systems. It includes
three parameters: two integer numbers, and , and a factor or velocity . Eqs. (7-10)
may be written in an approximate form as

)$*

5

) 87 -GF) 7H

1
2 39
5 )$* ) 87
, *@-JI 1 5 / 1 + *7 ) 87 - ? 1 K /
2

3
9
+2 *@39 - B E <> FML
- C
/ + *N-O+ 
1 B 2 39 E -JP + 8
M
F
L
2
Q
>2Q
P - C
+ 8 - 2 A3R 9 2 3 <9 > 8
+ 8 -SFML 1 B 2 39 8 E
-T U
, * -OI 5 1 + *7 - B 2 39 8 1 B FML E E 7 I 5 / 7 1 P 7 A DWV A 0 2 39
- B 5 )$* 1 ) 87 E ?
,>
/ P
98
+8
/
, *X- + C 8 7 I 5 P 7M7 Y
BCUE
Z* C /
BCT E
)$* - + 8 I 5 P Y
[ *- 2 3C I / \
BC] E
+ 8\ 5 P \ Y
+ *^-@+ 8 /P A
B2 R E
According to Eq. (12), / + * _ B 2 ` A Q PaNb A R E km s <> . Therefore, Eq. (20) deviates from
the best fit (Eq. (12)) by the factor B CMc 2 Q P 1 B 2 ` A Q P6a'b A R E E , i.e. by about 9% if P^- C , and
FIZIKA B 7 (1998) 1, 1–13

Quantization in Astrophysics ...

9

15

RUB ČI Ć AND RUB ČI Ć : THE QUANTIZATION OF THE SOLAR - LIKE

dXe.f

gih jXe@g6k lmn o j p q r s

by about -3% if
, while observational mean values of
deviate
from the best fit (Eq. (12)) less than 2% on the average.
One may criticize the use of many parameters in the model. However, they seem to
be necessary, because is related to the principal spacing of orbits, takes care of the
packing of orbits, while (or ) characterizes several subsystems within a given system
(like our own Solar System). One should not be surprised if in another extra-solar system,
the quantity
would take a different value compared with the Solar System. It could
possibly be 72, 36, 24, or 18 km s , as obtained in an analysis of the quantized redshifts
of the galaxies [17–20]. For example, the pulsar PSR B 1257+12 has three planets in orbits
for equal to 5, 7 and 8 [5,10]. From the observational data, one obtains
km
s , which gives
for
km s . However, if one assumes
km
s (in accordance with Ref. 21, where the interval for redshift periodicity is 37.2 to 37.7
km s , then will be equal to 11. Hopefully, the future investigation of other planetary
systems will confirm the ideas proposed in the present model.

g

ht

ht

vq
v q

vq

g

vq ~

d

ut

d

d%e x z

h t'e{ w

gih j$e.w;x y
h t'e| z } |

vq

4. Conclusion
The basis of the square law for the spacing of orbits of planets and of major satellites is
the discretization of angular momenta, similarly as in the old Bohr’s theory of the hydrogen
atom. However, the angular momentum of an orbiting body has to be reduced by the
mass of orbiting body and also by the mass of the central body. Moreover, the product
of these two masses must also be reduced by square of Planck’s mass multiplied by the
fine-structure constant , in order to scale the macroscopic gravitational system to the
microscopic level, where the Planck’s reduced constant
represents a quantum of
angular momentum. As a result of such an approach, two ”quantum numbers” appear, the
first one for describing the law of orbital spacing and the second one for the ”packing”
of the orbits. One further parameter is necessary, that is equal for all systems within the
Solar System. It is the characteristic length
m kg . But
equally well, the third parameter may be a universal velocity
km s . The three
parameters and the mass of the central body (see Eqs. (17-20)) define possible the discrete
structures of a planar gravitational system within the approximation of the circular orbits.
Velocity is equal to the velocity increments of the quantized redshifts of galaxies. A
great puzzle is how the planetary orbital velocities can obey the same quantization periods
as the intrinsic redshifts of the galaxies.
It is known that some researches do not believe that ”quantum phenomena” play any
role, both in the formation and in the evolution of the Solar System. They rather suppose
that many macroscopic effects have had a predominant influence on planetary spacing.
However, in our opinion, the derived results shown in Figs. 1 to 3 strongly suggest the
necessity for a certain ”quantum mechanical” treatment. As the first approach, the model
analogous to the simplest one of the ”old quantum mechanics” has been elaborated in the
present work. Of course, further observational and theoretical investigations are necessary
for the development of more sophisticated models.



g

 ein { 

d

ln h t s ek x } y z.y } y f p x y vq  vq
h t^S{ w vq

ht

Acknowledgements
10

Quantization in Astrophysics ...

FIZIKA B 7 (1998) 1, 1–13

16

RUB ČI Ć AND RUB ČI Ć : THE QUANTIZATION OF THE SOLAR - LIKE

The authors are gratefull to Prof. A. Bjeliš and Prof. K. Pavlovski for their interest
and valuable discussions throughout the work and to Prof. H. Arp for his kind comments,
suggestions and support.

Appendix

'

'
X         =    =   i        
X

The similarity between the gravitational and Coulomb force between two particles of
mass
and charge is well known. Moreover, one can imagine that these two forces become identical for adequately chosen mass
. From
, it follows
that
, independently of the mutual distance of
particles. The mass
is related to the Planck’s mass
by
kg. It is reasonable to assume that for such a micro-gravitational system, a quantization of
angular momentum of the orbiting body should be the same as the one postulated by Bohr
for the electrodynamical system, i.e.,

= '   .         
' '=     '%&       i

X ¡ ¢   ¢ =@£    

¤  

For a real macro-gravitational system an analogous discretization could be

$¢ ¡ ¢  ¢X@£ ¥  

 ¤ 

To reach a complete similarity between the reference micro-model and a real planetary or
satellite system, analogous quantities must be in a constant ratio. These ratios, the so-called
,
and
, must be in
similarity constants, such as
definite mutual relationships, which can be generally determined from analogous equations
[22]. Thus, Eq. (A1) will transform into Eq. (A2) only with the correlation

¦§.@$¢  ' ¦¨=@¡ ¢  ¡ £i

¦©  ¢   £i

¦§¦¨ ¦©W¦ªO¥ $«
£

 ¤¬ 

which is an indicator of similarity, satisfied for every orbit and for any value of . To
determine , an additional indicator of similarity must be taken into account, which follows from analogous correlations for the forces corresponding to the micro-model and to
) orbiting the central one (of mass ):
a system of a body (of mass

¥

$¢

¡ ¢   ¢ ==X «
¡ ¢  ¢X@­ «¯® °;±
¦ ¨  ¦© X­ 

¥   ­.'¢  ' ¦¨

­

 ¤W 
¤  
 ¤² 

X'     '

Introducing the second indicator of similarity (A6) into the first one (A3), one obtains
. Further, from Eqs. (A1) and (A4) for
, it follows
FIZIKA B 7 (1998) 1, 1–13

Quantization in Astrophysics ...

11

17

RUB ČI Ć AND RUB ČI Ć : THE QUANTIZATION OF THE SOLAR - LIKE

³ ´ µ'¶·M¸ ¹ º , and according to Eq. (10), »¼ ¶O³ ´;¹ ³ ´ µ$¶ ½ ¾ ¿ÀiÁ ÂÃ . Thus, the effective
”Planck’s gravitational constant” Ä is given by
´
½ Ê=Ë Á
Ä ¶Å½ ¾ ¿ÀWÆ.Ç$Ç µÈ Á É
where the factor À (see Table 1), determined from astronomical data, is included.
Finally, by introducing Eq. (A7) into Eq. (A2), the scaled ”quantum condition” presented by Eq. (16) is proved.
Consequently, Eq. (A7) should be used, e.g., to define a macroscopic ”de Broglie wavelength”
. Introducing
from Eq. (10), one obtains
, where
is given by Eq. (7). This is an expected result in the present model.
may be transformed into a form dependent on and as
by using Eq. (17).
One may also write
, which is an equivalent simple form of the square law
.
Equation (A7) allows the use of the Schrödinger’s radial wave equation [8] to obtain the
orbital spacing. If the gravitational potential
and effective ”Planck’s
gravitational constant” are introduced into the radial equation, it takes the form

Í´

Ì ´'¶ Ä ¹ Ç ´ ³ ´

Í ´^¶@Í Ã º È

Ì ´.¶ Ì Ã º

º

³´

Ì ´'¶&´ ¾ ¿iÍ ´;¹ º
Ì
Î Ì ´ ¶J½ ¾ ¿M¹ ³ µÈ Á Ï Æ ºM¹ Î È

Ð ½ Í Á¶JÑÏ Æ.Ç ¹ Í
Ä
Ò Ó ¾ Ò Ó ¿ È Ç'È Ö× Ó ¾ ¿ È Ï Æ.Ç'È Ó Ñ.Ù ½ Ù Á Ó ¶ Û ÉÜ½ Ê Á
Ò Í ^È Ô Í Ò Í Ô&Õ Ä È Ô ÍWØ Ä È
Í ÔÈ Ú
¶ Ö ¹ Ç is the energy per unit mass of the orbiting body, Ó ½ Í Á is the radialÕ
where 
Ö
×
wave function and Ù is the angular quantum number. From the fourth term, ”the first Bohr’s
radius” is
Í Ã ¶ ¿ ÏÄ È
½ ÊÞ Á
È Æ.Ç ÈÝ
Ø
^
´
¶ Ç É one obtains
Introducing Ä defined by Eq. (A7), with Ç
Í Ã ¶ ß ¾ ·M¿¸^À à È Ï Æ É
½Ê Û Á
Ú
@
º
¶
which is in agreement with Eq. (7) for
. If the angular quantum number is limited
Ú
only to the values Ù ¶º'Ñ , then the probability
maxima of the mass distribution will be
at positions given by Í ´^¶@Í Ú Ã º È . Such an approximation has been recently used by Nottale
Û , with all possible values of Ù are used [8],
et al. [23]. If all wave functions up to º@¶
Ú deviate from the square law. However,
then the positions of probability maxima slightly
it was already pointed out that the simple approach, related to the old quantum theory is
more appropriate for an understanding of gravitational phenomena [24]. Therefore, the
complete understanding of the rather formal application of the Schrödinger’s equation to
the Solar System needs further research.
References

1) A. G. Agnese and R. Festa, Phys. Lett. A 227 (1997) 165;
12

Quantization in Astrophysics ...

FIZIKA B 7 (1998) 1, 1–13

18

RUB ČI Ć AND RUB ČI Ć : THE QUANTIZATION OF THE SOLAR - LIKE

2) J. J. Lissauer, Ann. Rev. Astron. Astrophys. 31 (1993) 129;
3) M. de Oliveira Neto, Ciencia e Cultura, Journal of the Brazilian Association for the Advancement
of Science, 48(3) (1996) 166;
4) A. Rubčić and J. Rubčić, Fizika B 4 (1995) 11;
5) A. Rubčić and J. Rubčić, Fizika B 5 (1996) 85;
6) H. Alfvén, Icarus 3 (1964) 52;
7) J. Laskar, Astron. Astrophys. 317 (1997) L75;
8) N. Qing-xiang, Acta. Astron. Sinica 34 (1993) 333;
9) L. Nottale, Fractal Space-Time and Microphysics, World Scientific, Singapore, 1993, a) p. 311,
b) p. 187;
10) L. Nottale, Astron. Astrophys. 315 (1996) L9;
11) R. Neuhauser and J. Feitzinger. Astron. Astrophys. 170 (1986) 174;
12) F. Graner and B.Dubrulle, Astron. Astrophys. 282 (1994) 262 and references therein;
13) A. Rubčić and J. Rubčić, EPS 10 Trends in Physics, September 9-13, 1996, Sevilla, Spain;
14) P. S. Wesson, Phys. Rev. D 23 (1981) 1730;
15) H. Karttunen, P. Kröger, H. Oja, M. Poutanen and K. J. Donner (Eds), Fundamental Astronomy,
Springer-Verlag, Berlin, 1996, p.487;
16) D.Trilling, W.Benz, T.Guilot, J.I.Lunine, W.B.Hubbard and A.Burrows, Orbital evolution and
migration of giant planets: modeling extrasolar planets, 1998, Astrophys. J. (in press);
17) W. G. Tifft, Astrophys. J. 206 (1976) 38;
18) W. G. Tifft and W. J. Cocke, Astrophys. J. 287 (1984) 492;
19) W. J. Cocke and W. G. Tifft, BAAS 26 (1994) 1409;
20) H. Arp and J. W. Sulentic, Astrophys. J. 291(1985) 88;
21) B. N. G. Guthrie and W. M. Napier, Astron. Astrophys. 310 (1996) 353;
22) J. Baturić-Rubčić, J. Phys. E (J. Sci. Instrum.) 1 (1968) 1090 and J. Hydr. Res. 7 (1) (1969) 31;
23) L. Nottale, G. Schumacher and J. Gay, Astron. Astrophys. 322 (1997) 1018;
24) J. Buitrago, Astrophys. Letters & Comm. 27 (1988) 1.

KVANTIZACIJA GRAVITACIJSKIH SUSTAVA SLIČNIH PLANETARNOM
SUSTAVU SUNCA

á â^ã@á ä åæ
å S
ã ç

å

Srednje orbitalne udaljenosti planeta od Sunca i glavnih satelita od planeta Jupitra, Saturna
i Urana opisane su kvadratnim zakonom
, gdje je sukcesivno rastući cijeli broj,
a je srednja orbitalna udaljenost za
. Terestrički i jovijanski planeti razmatrani su
kao nezavisni sustavi, pa zajedno sa satelitima triju spomenutih planeta daju pet sličnih
planarnih gravitacijskih sustava. Polazna pretpostavka je ”kvantiziranost” specifičnog orbitalnog momenta impulsa. Sukladno tome i svi ostali dinamički parametri sustava poprimaju diskretne vrijednosti. Broj odreduje zakonitost porasta orbitalnih udaljenosti planeta ili satelita, no pored toga uočeno je da i veličina , nezavisno od , poprima diskretne
vrijednosti. To znači da pojedini sustav može imati niz struktura različite gustoće orbita.
Jedna od posljedica toga je da produkt orbitalnih brzina
i pripadnog broja postaje
konstantan za dani sustav i ujedno je višekratnik osnovne brzine
km s . Ova
brzina jednaka je jednoj od ”brzina” izvedenih iz kvantiziranih crvenih pomaka galaksija,
pa ona možda ima i neko dublje fizičko kozmološko značenje.

áä

å

áä

FIZIKA B 7 (1998) 1, 1–13

Quantization in Astrophysics ...

èâ

å

èé ê ëì

å ä
í

13

19

Quantization of Planck Constants and Dark
Matter Hierarchy in Biology and Astrophysics
M. Pitkänen
Dept. of Physics, University of Helsinki, Helsinki, Finland.
Email: matpitka@rock.helsinki.fi.
http://www.helsinki.fi/∼matpitka/.
Abstract
The work with von Neumann algebras known as hyper-finite factors
of type II1 associated naturally with quantum TGD, led to a proposal
for the quantization of the Planck constants associated with the symmetry algebras in M 4 and CP2 degrees of freedom as h̄(M 4 ) = na h̄0
and h̄(CP2 ) = nb h̄0 . A generalization of the notion of imbedding
space emerged as a geometric realization of the quantization in terms
of Jones inclusions. As a consequence, also a quantization of the Planck
constant appearing in Schrödinger equation emerges and is given by
h̄/h̄0 = h̄(M 4 )/h̄(CP2 ). ”Ruler and compass” integers correspond to
a very restricted set of number theoretically preferred values of na and
nb . In this article the quantization of Planck constant and some of its
astrophysical and biological implications are briefly discussed.

Contents
1 Introduction
1.1 The model of Nottale and DaRocha . . . . . . . . . . . . . .
1.2 Quantization of Planck constant . . . . . . . . . . . . . . . .
1.2.1 Dark matter as macroscopic quantum phase with a
gigantic value of Planck constant . . . . . . . . . . . .
1.2.2 Quantization of Planck constants and hyper-finite factors of type II1 . . . . . . . . . . . . . . . . . . . . . .
1.3 The evolution of the model for planetary system . . . . . . .
1.3.1 Understanding the value of the parameter v0 . . . . .
1.3.2 View about evolution of planetary system . . . . . . .
1

Quantization in Astrophysics ...

20

3
3
3
4
4
5
5
6

1.3.3

Improved predictions for planetary radii and predictions for ratios of planetary masses . . . . . . . . . . .

7

2 Dark matter hierarchy and quantization of Planck constants 8
2.1 Generalization of the p-adic length scale hypothesis and preferred values of Planck constants . . . . . . . . . . . . . . . .
9
2.2 How Planck constants are visible in Kähler action? . . . . . . 10
2.3 Phase transitions changing the level in dark matter hierarchy 10
3 Some astrophysical applications
11
3.1 Bohr quantization of planetary orbits and preferred values of
Planck constant . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.2 Orbital radii of exoplanets . . . . . . . . . . . . . . . . . . . . 13
3.3 A more detailed model for planetary system . . . . . . . . . . 14
3.3.1 The interpretation of h̄gr and pre-planetary period . . 15
3.3.2 Inclinations for the planetary orbits and the quantum
evolution of the planetary system . . . . . . . . . . . . 16
3.3.3 Eccentricities and comets . . . . . . . . . . . . . . . . 18
3.4 About the interpretation of the parameter v0 . . . . . . . . . 19
3.5 How do the magnetic flux tube structures and quantum gravitational bound states relate? . . . . . . . . . . . . . . . . . . 22
3.5.1 The notion of field body . . . . . . . . . . . . . . . . . 22
3.5.2 Ga as a symmetry group of field body . . . . . . . . . 23
3.5.3 Could gravitational Schrödinger equation relate to a
quantum control at magnetic flux tubes? . . . . . . . 23
3.6 p-Adic length scale hypothesis and v0 → v0 /5 transition at
inner-outer border for planetary system . . . . . . . . . . . . 26
4 Some applications to condensed matter and biology
27
4.1 Exceptional groups and structure of water . . . . . . . . . . . 28
4.2 Aromatic rings and large h̄ phases . . . . . . . . . . . . . . . 29
4.3 Model for a hierarchy of EEGs . . . . . . . . . . . . . . . . . 29
5 Summary and outlook

1

30

Introduction

D. Da Rocha and Laurent Nottale, the developer of Scale Relativity, have
ended up with an highly interesting quantum theory like model for the
evolution of astrophysical systems [2]. In particular, this model applies to
2

Quantization in Astrophysics ...

21

planetary orbits. Nottale predicted Bohr model like quantization for radii of
planetary orbits in his book Fractal Spacetime and Microphysics published
1993. The quantization was later discovered for exoplanets [1].

1.1

The model of Nottale and DaRocha

The model is simply Schrödinger equation with Planck constant h̄ replaced
with what might be called gravitational Planck constant
h̄ → h̄gr =

GmM
.
v0

Here I have used units h̄ = c = 1. v0 is a velocity parameter having the value
v0 = 144.7 ± .7 km/s giving v0 /c = 4.6 × 10−4 . The peak orbital velocity of
stars in galactic halos is 142 ± 2 km/s whereas the average velocity is 156 ± 2
km/s. Also sub-harmonics and harmonics of v0 seem to appear.
The model makes fascinating predictions which seem to hold true. For
instance, the radii of planetary orbits fit nicely with the prediction of the
hydrogen atom like model. The inner solar system (Mercury, Venus, Earth,
Mars) corresponds to v0 and outer solar system to v0 /5.
The predictions for the distribution of major axis and eccentrities have
been tested successfully also for exoplanets. Also the periods of 3 planets around pulsar PSR B1257+12 fit with the predictions with a relative
accuracy of few hours/per several months. Also predictions for the distribution of stars in the regions where morphogenesis occurs follow from the
gravitational Schödinger equation.
What is important is that there are no free parameters besides v0 . In [2]
a wide variety of astrophysical data is discussed and it seem that the model
works and has already now made predictions which have been later verified.

1.2

Quantization of Planck constant

In TGD framework [TGDview] the idea about quantized Planck constant
emerged originally from a TGD inspired model of topological quantum computation [E9]. Large values of Planck constant would scale up quantal time
and length scales and make possible macroscopic quantum phases and thus
provide the new physics crucial for quantum models of living matter and
conscious brain.

3

Quantization in Astrophysics ...

22

1.2.1

Dark matter as macroscopic quantum phase with a gigantic
value of Planck constant

Learning about evidence for Bohr quantization of planetary orbits based
on a gigantic value of gravitational constant [2, 3] led to the idea that the
Bohr orbitology for visible matter might reflect the presence of dark matter
characterized by gigantic values of Planck constant and thus in ”astroscopic”
quantum phase. In a strong contrast with the top-down approach of Mtheory, the road to quantum gravity might mimic the much more modest
approach leading from hydrogen atom to QED. Just as the Bohr model
for hydrogen atom resolved the infrared catastrophe (electron falling into
nucleus by emission of radiation), the Bohr model for planetary system
could prevent collapse of matter to black hole.
1.2.2

Quantization of Planck constants and hyper-finite factors
of type II1

The infinite-dimensional Clifford algebra of the configuration space of 3surfaces (”world of classical worlds”) corresponds to von Neumann algebra
known as hyperfinite factor of type II1 . The so called Jones inclusions for
these algebras led via a sequence of educated guess to the recent proposal
for the quantization of Planck constants associated with symmetry algebras
of M 4 and CP2 as integer multiples h̄(M 4 ) = na h̄0 and h̄(CP2 ) = nb h̄0 of
the minimal value h̄0 of Planck constant. na and nb correspond to orders of
maximal cyclic subgroups for the discrete subgroups of SU(2) characterizing
these inclusions and the formula follows using anyonic arguments.
A considerable generalization of the notion of imbedding space emerged
and a concrete geometric and topological interpretation for how quantum
groups characterized by phases qi = exp(iπ/ni ), i = 1, b are realized in
physics. This implies also a model for phase transitions changing the values
of Planck constants as a complete or partial leakage of particle 3-surfaces
between different sectors of generalized imbedding spaces obtained by gluing
together various copies of imbedding space together along common M 4 or
CP2 factor. One can say that two levels of hierarchy are dark relative to
each other if they correspond to a different sector of imbedding space.
The basic prediction is that ordinary Planck constant h̄ appearing in the
Schrödinger equation can be expressed as h̄/h̄0 = h̄(M 4 )/h̄(CP2 ) = na /nb
and can in principle have all rational values. Number theoretic considerations however favor what might be called ruler and compass rationals for
which na and nb define n-polygons constructible using only ruler and com-

4

Quantization in Astrophysics ...

23

pass (the corresponding quantum phases are obtained by iterated square
root operation from rationals).
Quantization of Planck constants is equivalent with the scaling of covariant metrics of M 4 resp. CP2 by factor n2b resp. n2a followed by over-all
scaling by factor 1/n2a leaving Kähler action invariant. Hence CP2 metric remains invariant, and one avoids mathematical difficulties in gluing of various
copies of the imbedding space together isometrically. M 4 covariant metric
is scaled by (nb /na )2 meaning that effective Planck constant appearing in
Schrödinger equation is (na /nb )h̄0 . In this interpretation scaling of Planck
constants has a purely geometric meaning.

1.3

The evolution of the model for planetary system

A brief summary about the evolution of the model for planetary system is
in order.
1.3.1

Understanding the value of the parameter v0

The first observation was that TGD allows to understand the value of the
parameter v0 /c assuming that cosmic strings and their decay remnants are
responsible for the dark matter. The number theoretically preferred prediction would be v0 = 2−11 and expressible in terms of fundamental constants of
quantum TGD (Planck length, CP2 radius, and Kähler coupling strength).
The harmonics of v0 could be understood as corresponding to perturbations replacing cosmic strings with their n-branched coverings so that tension
becomes n2 -fold: much like the replacement of a closed orbit with an orbit
closing only after n turns. 1/n-sub-harmonic would result when a magnetic
flux tube split into n disjoint magnetic flux tubes. Also rational multiples
of v0 are possible if both mechanisms operate.
The general formula for h̄gr /h̄0 as ruler and compass rational allowed a
more precise prediction for v0 and led also to a prediction for the ratios of
planetary masses as ratios of ruler and compass rationals.
Later a possible interpretation of v0 as a reduced light velocity emerged.
The reduction would be due to the warping of dark space-time sheets meaning that the time component of the induced metric is reduced and one can
identify a possible mechanism leading to the warping in the phase transition increasing Planck constant. This effect implies also time dilatation and
distinguishes between TGD and General Relativity. These two explanations
need not be mutually exclusive.

5

Quantization in Astrophysics ...

24

1.3.2

View about evolution of planetary system

The study of inclinations (tilt angles with respect to the Earth’s orbital
plane) leads to a concrete model for the quantum evolution of the planetary
system. Only a stepwise breaking of the rotational symmetry and angular
momentum Bohr rules plus Newton’s equation (or geodesic equation) are
needed, and gravitational Shrödinger equation holds true only inside flux
quanta for the dark matter.
a) During pre-planetary period dark matter formed a quantum coherent state on the (Z 0 ) magnetic flux quanta (spherical cells or flux tubes).
This made the flux quantum effectively a single rigid body with rotational
degrees of freedom corresponding to a sphere or circle (full SO(3) or SO(2)
symmetry).
b) In the case of spherical shells associated with inner planets the SO(3) →
SO(2) symmetry breaking led to the generation of a flux tube with the inclination determined by m and j and a further symmetry breaking, kind
of an astral traffic jam inside the flux tube, generated a planet moving inside flux tube. The semiclassical interpretation of the angular momentum
algebra predicts the inclinations of the inner planets. The predicted (real)
inclinations are 6 (7) resp. 2.6 (3.4) degrees for Mercury resp. Venus). The
predicted (real) inclination of the Earth’s spin axis is 24 (23.5) degrees.
c) The v0 → v0 /5 transition allowing to understand the radii of the
outer planets in the model of Da Rocha and Nottale could be understood
as resulting from the splitting of (Z 0 and gravi-) magnetic flux tube to five
flux tubes representing Earth and outer planets except Pluto, whose orbital
parameters indeed differ dramatically from those of other planets. The flux
tube has a shape of a disk with a hole glued to the Earth’s spherical flux
shell.
It is important to notice that effectively a multiplication n → 5n of
the principal quantum number is in question. This allows to consider also
alternative explanations. Perhaps external gravitational perturbations have
kicked dark matter from the orbit or Earth to n = 5k, k = 2, 3, ..., 7 orbits:
the fact that the tilt angles for Earth and all outer planets except Pluto
(not a planet anymore!) are nearly the same, supports this explanation. Or
perhaps there exist at least small amounts of dark matter at all orbits but
visible matter is concentrated only around orbits containing some critical
amount of dark matter and these orbits satisfy n mod 5 = 0 for some
reason. TGD based explanation for so called flyby anomaly [6] is based on
this assumption [D6].
The rather amazing coincidences between basic bio-rhythms and the pe6

Quantization in Astrophysics ...

25

riods associated with the states of orbits in solar system [D6] suggest that
the frequencies defined by the energy levels of the gravitational Schrödinger
equation might entrain with various biological frequencies such as the cyclotron frequencies associated with the magnetic flux tubes. For instance,
the period associated with n = 1 orbit in the case of Sun is 24 hours within
experimental accuracy for v0 .
1.3.3

Improved predictions for planetary radii and predictions
for ratios of planetary masses

The general prediction for the spectrum of h̄ as ruler and compass rational
gives strong additional constraints but also flexibility since h̄gr = GM m/v0
can correspond to ruler and compass integer. The planetary mass ratios can
be produced with an accuracy better than 2 per cent assuming that h̄gr /h̄0
is ruler and compass rational.
Ruler and compass hypothesis for allows to improve the fit for the planetary radii in solar system. Also the radii of exoplanets can be fitted with
few per cent accuracy (see the section ”Orbital radii of exoplanets” and the
tables of the Appendix). One cannot hope much more since star masses
are deduced theoretically. Moreover the ratios of planetary masses are predicted to be expressible as ratios of ruler and compass rationals and this
turns out to be true with 2 per cent accuracy (Table 2). Hence it seems that
the hypothesis deserves to be taken seriously. One can even consider the
possibility of deducing masses of stars from the orbital radii of exoplanets
so that stars models could be tested.
To sum up, it would be too early to say that the proposed model has
reached its final form but already at this stage a rich spectrum of predictions
follows. It is probably needless to add that the existence of the proposed
dark matter hierarchy means that a new period of voyages of discovery to
the levels of existence responsible for the special properties of living systems
would be waiting for us.

2

Dark matter hierarchy and quantization of Planck
constants

In this section the quantization of Planck constants in TGD framework is
briefly discussed. The detailed discussion can be found in [A9].
The recent geometric interpretation for the quantization of Planck constants is based on Jones inclusions of hyper-finite factors of type II1 [A9].
7

Quantization in Astrophysics ...

26

a) One can argue that different values of Planck constant correspond to
imbedding space metrics involving scalings of M 4 resp. CP2 parts of the
metric deduced from the requirement that distances scale as h̄(CP2 ) resp.
h̄(M 4 ). Denoting the Planck constants by h̄(M 4 ) = na h̄0 and h̄(CP2 ) =
nb h̄0 , one has that covariant metric of M 4 is proportional to n2b and covariant
metric of CP2 to n2a .
This however leads to difficulties with the isometric gluing of CP2 factors
of different copies of H together. Kähler action is however invariant under
over-all scaling of H metric so that one can scale it down by 1/n2a meaning
that M 4 covariant metric is scaled by (nb /na )2 and CP2 metric remains
invariant and the difficulties in isometric gluing are avoided. This means
that if one regards Planck constant as a mere conversion factor, the effective
Planck constant scales as na /nb and Planck constant has a purely geometric
meaning as scaling factor of M 4 metric.
In Kähler action only the effective Planck constant h̄ef f /h̄0 = h̄(M 4 )/h̄(CP2 )
appears and by quantum classical correspondence same is true for Schödinger
equation. Elementary particle mass spectrum is also invariant. Same applies to gravitational constant. The alternative assumption that M 4 Planck
constant is proportional to nb would imply invariance of Schrödinger equation but would not allow to explain Bohr quantization of planetary orbits
and would to certain degree trivialize the theory.
b) M 4 and CP2 Planck constants do not fully characterize a given sector M±4 × CP2 . Rather, the scaling factors of Planck constant given by the
integer n characterizing the quantum phase q = exp(iπ/n) corresponds to
the order of the maximal cyclic subgroup for the group G ⊂ SU (2) characterizing the Jones inclusion N ⊂ M of hyper-finite factors realized as
subalgebras of the Clifford algebra of the ”world of the classical worlds”.
This means that subfactor N gives rise to G-invariant configuration space
spinors having interpretation as G-invariant fermionic states.
c) Gb ⊂ SU (2) ⊂ SU (3) defines a covering of M+4 by CP2 points and
Ga ⊂ SU (2) ⊂ SL(2, C) covering of CP2 by M+4 points with fixed points
defining orbifold singularities. Different sectors are glued isometrically together along CP2 if Gb is same for them and along M+4 if Ga is same for
them. The degrees of freedom lost by G-invariance in fermionic degrees of
freedom are gained back since the discrete degrees of freedom provided by
covering allow many-particle states formed from single particle states realized in G group algebra. Among other things these many-particle states
make possible the notion of N-atom.
d) Phases with different values of scalings of M 4 and CP2 Planck constants behave like dark matter with respect to each other in the sense that
8

Quantization in Astrophysics ...

27

they do not have direct interactions except at criticality corresponding to a
leakage between different sectors of imbedding space glued together along
M 4 or CP2 factors. In large h̄(M 4 ) phases various quantum time and length
scales are scaled up which means macroscopic and macro-temporal quantum coherence. In particular, quantum energies associated with classical
frequencies are scaled up by a factor na /nb which is of special relevance
for cyclotron energies and phonon energies (superconductivity). For large
h̄(CP2 ) the value of h̄ef f is small: this leads to interesting physics: in particular the binding energy scale of hydrogen atom increases by the factor
(nb /na )2 .

2.1

Generalization of the p-adic length scale hypothesis and
preferred values of Planck constants

The evolution in phase resolution in p-adic degrees of freedom corresponds
to emergence of algebraic extensions allowing increasing variety of phases
exp(iπ/n) expressible p-adically. This evolution can be assigned to the emergence of increasingly complex quantum phases and the increase of Planck
constant.
One expects that quantum phases q = exp(iπ/n) which are expressible
using only iterated square root operation are number theoretically very special since they correspond to algebraic extensions of p-adic numbers obtained
by an iterated square root operation, which should emerge first. Therefore
systems involving these values of q should be especially abundant in Nature.
These polygons are obtained by ruler and compass construction and
Gauss showed that these polygons, which could be called Fermat polygons,
Q
have nF = 2k s Fns sides/vertices: all Fermat primes Fns in this expression
must be different. The analog of the p-adic length scale hypothesis emerges
since larger Fermat primes are near a power of 2. The known Fermat primes
n
Fn = 22 + 1 correspond to n = 0, 1, 2, 3, 4 with F0 = 3, F1 = 5, F2 = 17,
F3 = 257, F4 = 65537. It is not known whether there are higher Fermat
primes. n = 3, 5, 15-multiples of p-adic length scales clearly distinguishable
from them are also predicted and this prediction is testable in living matter. I have already earlier considered the possibility that Fermat polygons
could be of special importance for cognition and for biological information
processing [H8].
This condition could be interpreted as a kind of resonance condition
guaranteing that scaled up sizes for space-time sheets have sizes given by padic length scales. The numbers nF could take the same role in the evolution
of Planck constant assignable with the phase resolution as Mersenne primes
9

Quantization in Astrophysics ...

28

have in the evolution assignable to the p-adic length scale resolution.

2.2

How Planck constants are visible in Kähler action?

h̄(M 4 ) and h̄(CP2 ) appear in the commutation and anticommutation relations of various superconformal algebras. Only the ratio na /nb of M 4 and
CP2 Planck constants appears in Kähler action. This implies that Kähler
function codes for radiative corrections to the classical action, which makes
possible to consider the possibility that higher order radiative corrections
to functional integral vanish as one might expect at quantum criticality.
For a given p-adic length scale space-time sheets with all allowed values
of Planck constants are possible. Hence the spectrum of quantum critical
fluctuations could in the ideal case correspond to the spectrum of h̄ coding
for the scaled up values of Compton lengths and other quantal lengths and
times. If so, large h̄ phases could be crucial for understanding of quantum
critical superconductors, in particular high Tc superconductors.

2.3

Phase transitions changing the level in dark matter hierarchy

The identification of the precise criterion characterizing dark matter phase
is far from obvious. TGD actually suggests an infinite number of phases
which are dark relative to each other in some sense and can transform to
each other only via a phase transition which might be called de-coherence
or its reversal and which should be also characterized precisely.
A possible solution of the problem comes from the general construction
recipe for S-matrix. Fundamental vertices correspond to partonic 2-surfaces
representing intersections of incoming and outgoing light-like partonic 3surfaces.
a) If the characterization of the interaction vertices involves all points
of partonic 2-surfaces, they must correspond to definite value of Planck
constant and more precisely, definite groups Ga and Gb characterizing dark
matter hierarchy. Particles of different phases could not appear in the same
vertex and a phase transition changing the particles to each other analogous
to a de-coherence would be necessary.
b) If transition amplitudes involve only a discrete set of common orbifold
points of 2-surface belonging to different sectors then the phase transition
between relatively dark matters can be described in terms of S-matrix. It
seems that this option is the correct one. In fact, also propagators are
essential for the interactions of visible and dark matter and since virtual
10

Quantization in Astrophysics ...

29

elementary particles correspond at space-time level CP2 type extremals with
4-dimensional CP2 projection, they cannot leak between different sectors of
imbedding space and therefore cannot mediate interactions between different
levels of the dark matter hierarchy. This would suggest that the direct
interactions between dark and ordinary matter are very weak.
If the matrix elements for real-real partonic transitions involve all or at
least a circle of the partonic 2-surface as stringy considerations suggest [C2],
then one would have clear distinction between quantum phase transitions
and ordinary quantum transitions. Of course, the fact that the points which
correspond to zero of Riemann Zeta form only a small subset of points common to real partonic 2-surface and corresponding p-adic 2-surface, implies
that the rate for phase transition is in general small. On the other hand, for
the non-diagonal S-matrix elements for ordinary transitions would become
very small by almost randomness caused by strong fluctuations and the rate
for phase transition could begin to dominate.

3

Some astrophysical applications

There is considerable support for the Bohr quantization of planetary orbits
both in solar system and from exoplanets. The needed gigantic values of
gravitational Planck constant can be understood in TGD framework and
assigned to dark matter. Theory also predicts preferred ratios for planetary
masses and provides a possible interpretation for the velocity parameter
characterizing h̄gr . The interpretation of the symmetry group Zn associated
with dark matter can be assigned as broken rotational symmetries of the
gravi-magnetic and electric bodies mediating interaction between star and
planet. Tifft’s quantization of cosmic redshifts can be also understand in
this framework. A thorough discussion of this subject can be found at [D6].
Here only a brief summary is given.

3.1

Bohr quantization of planetary orbits and preferred values of Planck constant

The predictions of the generalization of the p-adic length scale hypothesis are
consistent with the TGD based model for the Bohr quantization of planetary
orbits and some new non-trivial predictions follow.
Since the macroscopic quantum phases with minimum dimension of algebraic extension should be especially abundant in the universe, the natural
guess is that the values of the gravitational Planck constant correspond to
nF -multiples of ordinary Planck constant.
11

Quantization in Astrophysics ...

30

a) The model can explain the enormous values of gravitational Planck
constant h̄gr /h̄0 =' GM m/v0 ) = na /nb . The favored values of this parameter should correspond to nFa /nFb so that the mass ratios m1 /m2 =
nFa,1 nFb,2 /nFb,1 nFa,2 for planetary masses should be preferred. The general
prediction GM m/v0 = na /nb is of course not testable.
b) Nottale [2] has suggested that also the harmonics and subharmonics
of λ are possible and in fact required by the model for planetary Bohr orbits
(in TGD framework this is not absolutely necessary). The prediction is that
Q
favored values of n should be of form nF = 2k Fi such that Fi appears at
most once. In Nottale’s model for planetary orbits as Bohr orbits in solar
system n = 5 harmonics appear and are consistent with either nF,a → F1 nFa
or with nF,b → nFb /F1 if possible.

Planet
Mercury
Venus
Earth
Mars
Jupiter
Saturn
Uranus
Neptune
Pluto

T-B
Rpr /R
1
.93
.96
1.03
.95
1.00
.95
1.23
.92

Bohra
[n, Rpr /R]
[3, 1]
[4, .95]
[5, 1.08]
[6, 1.03]
[11, .98]
[3 × 5, 1.00]
[22, 1.04]
[27 , 1.03]
[31, 1.01]

Bohrb
[n, Rpr /R]
[3, 1]
[4, .95]
[5, 1.08]
[6, 1.03]
[2 × 5,.81]
[3 × 5, 1.00]
[4 × 5,.86]
[5 × 5, .88]
[6 × 5,.95]

Bohrc
[r/s, Rpr /R]
[1,1]
[1,.95]
[1,1.08]
[1,1.03]
[17/15,1.04]
[1,1.00]
[16/15,.98]
[17/16,.99]
[1,.95]

Table 1. The table represents the ratios Rpr /R of predictions Rpr of various models for orbital radii to their experimental average values R. The first
column represents Titius-Bode law (T-B in table). The remaining columns
represent variants of Bohr orbit model assuming a) that the principal quantum number n corresponds to the best possible fit and v0 has single value,
b) assuming the scaling v0 → v0 /5 for outer planets, c) assuming besides
v0 → v0 /5 the modification v0 → (r/s)v0 , where r/s is ruler and compass
rational. The scaling of v0 is chosen to give complete fit for Mercury.
Table 1 gives the radii of planet for Titius-Bode law and various Bohr
orbit models. Not surprisingly, option a) gives the best fit with errors being considerably smaller than the maximal error |∆R|/R ' 1/n except for
Uranus. The fit given by option b) is poor for Jupiter, Uranus and Saturnus
but improves for option c).

12

Quantization in Astrophysics ...

31

The prediction for the ratios of planetary masses can be tested. In the
table below are the experimental mass ratios rexp = m(pl)/m(E), the best
choice of rR = [nF,a /nF,b ] ∗ X, X common factor for all planets, and the
ratios rpred /rexp = nF,a (planet)nF,b (Earth)/nF,a (Earth)nF,b (planet). The
deviations are at most 2 per cent.
planet
y
y/x
planet
y
y/x

Me
213 ×5
17

1.01
S
14
2 × 3 × 5 × 17
1.01

V
211 × 17
.98
U

E
29 × 5 × 17
1.00
N

M
28 × 17
.98
P

221 ×5
17

217 ×17
3

24 ×17
3

.98

.99

J
223 ×5
7

1.01

.99

Table 2. The table compares the ratios x = m(pl)/(m(E) of planetary
mass to the mass of Earth to prediction for these ratios in terms of integers
nF associated with Fermat polygons. y gives the best fit for the allowed
factors of the known part y of the rational nF,a /nF,b = yX characterizing
planet, and the ratios y/x. Errors are at most 2 per cent.

3.2

Orbital radii of exoplanets

Orbital radii of exoplanets serve as a test for the quantization hypothesis.
Hundreds of them are already known and in [4] tables listing basic data for
for more than one hundred exoplanets can be found. Tables of Appendix
provide also references and links to sources giving data about stars, in particular star mass M using solar mass MS as a unit. Hence one can test the
formula for the orbital radii given by the expression
r
rE

=

n2 M
X .
52 MS

(1)

Here the correction factor X depends on the model.
a) X = 1 corresponds to the prediction of the simplest model allowing
only single value of v0 . It turns out that the simplest option assuming X = 1
fails badly for some planets: the resulting deviations of order 20 per cent
typically but in the worst cases the predicted radius is by factor of ∼ .5 too
small.
b) Nottale [2] has proposed that it is possible to improve the situation
by allowing harmonics and sub-harmonics of v0 which would mean X = n2
13

Quantization in Astrophysics ...

32

or 1/n2 .
c) In TGD framework general quantization of Planck constant allows X
to be any rational but number theoretical arguments prefer the values of X
which are squares of ”ruler and compass” rationals:
n1 2
) ,
n2
Y
= 2ki ×
Fsi , Fsi ∈ {3, 5, 17, 257, 216 + 1} .

X = (
ni

(2)

si

Here a given Fermat prime Fsi can appear only once.
The values of X used in the fit correspond to X ∈ {(2/3)2 , (3/4)2 ,
(4/5)2 , (5/6)2 , (15/17)2 , (15/16)2 , (16/17)2 } ' {.44, .56, .64, .69, .78, .88, .89}
and their inverses. The tables summarizing the resulting fit using both
X = 1 and value giving optimal fit are given in the Appendix. The deviations are typically few per cent and one must also take into account the fact
that the masses of stars are deduced theoretically using the spectral data
from star models. I am not able to form an opinion about the real error
bars related to the masses.

3.3

A more detailed model for planetary system

The Bohr orbit model for planetary system leads to the idea that the evolution of planetary system could be understood in terms of dark matter. One
can also ask whether the inclinations and eccentricities of planetary orbits
could be deduced from Bohr orbitology.
3.3.1

The interpretation of h̄gr and pre-planetary period

h̄gr could corresponds to a unit of angular momentum for quantum coherent states at magnetic flux tubes or walls containing macroscopic quantum
states. Quantitative estimate demonstrates that h̄gr for astrophysical objects cannot correspond to spin angular momentum. For Sun-Earth system
one would have h̄gr ' 1077 h̄. This amount of angular momentum realized as
a mere spin would require 1077 particles! Hence the only possible interpretation is as a unit of orbital angular momentum. The linear dependence of
h̄gr on m is consistent with the additivity of angular momenta in the fusion
of magnetic flux tubes to larger units if the angular momentum associated
with the tubes is proportional to both m and M .
Just as the gravitational acceleration is a more natural concept than
gravitational force, also h̄gr /m = GM/v0 could be more natural unit than
14

Quantization in Astrophysics ...

33

H

h̄gr . It would define a universal unit for the circulation
v · dl, which is
H
apart from 1/m-factor equal to the phase integral pφ dφ appearing in Bohr
rules for angular momentum. The circulation could be associated with the
flow associated with outer boundaries of magnetic flux tubes surrounding
the orbit of mass m around the central mass M À m and defining light like
3-D CDs analogous to black hole horizons.
The expression of h̄gr depends on masses M and m and can apply only
in space-time regions carrying information about the space-time sheets of
M and and the orbit of m. Quantum gravitational holography suggests that
the formula applies at 3-D light like causal determinant (CD) Xl3 defined by
the wormhole contacts gluing the space-time sheet Xl3 of the planet to that
of Sun. More generally, Xl3 could be the space-time sheet containing the
planet, most naturally the magnetic flux tube surrounding the orbit of the
planet and possibly containing dark matter in super-conducting state. This
would give a precise meaning for h̄gr and explain why h̄gr does not depend
on the masses of other planets.
The simplest option consistent with the quantization rules and with the
explanatory role of magnetic flux structures is perhaps the following one.
a) Xl3 is a torus like surface around the orbit of the planet containing delocalized dark matter. The key role of magnetic flux quantization in
understanding the values of v0 suggests the interpretation of the torus as
a magnetic or Z 0 magnetic flux tube. At pre-planetary period the dark
matter formed a torus like quantum object. The conditions defining the
radii of Bohr orbits follow from the requirement that the torus-like object
is in an eigen state of angular momentum in the center of mass rotational
degrees of freedom. The requirement that rotations do not leave the toruslike object invariant is obviously satisfied. Newton’s law required by the
quantum-classical correspondence stating that the orbit corresponds to a
geodesic line in general relativistic framework gives the additional condition
implying Bohr quantization.
b) A simple mechanism leading to the localization of the matter would
have been the pinching of the torus causing kind of a traffic jam leading to
the formation of the planet. This process could quite well have involved a
flow of matter to a smaller planet space-time sheet Yl3 topologically condensed at Xl3 . Most of the angular momentum associated with torus like
object would have transformed to that of planet and situation would have
become effectively classical.
c) The conservation of magnetic flux means that the splitting of the
orbital torus would generate a pair of Kähler magnetic charges. It is not
clear whether this is possible dynamically and hence the torus could still
15

Quantization in Astrophysics ...

34

be there. In fact, TGD explanation for the tritium beta decay anomaly
citeTroitsk,Mainz in terms of classical Z 0 force [F8] requires the existence
of this kind of torus containing neutrino cloud whose density varies along
the torus. This picture suggests that the lacking n = 1 and n = 2 orbits
in the region between Sun and Mercury are still in magnetic flux tube state
containing mostly dark matter.
d) The fact that h̄gr is proportional to m means that it could have varied
continuously during the accumulation of the planetary mass without any
effect in the planetary motion: this is of course nothing but a manifestation
of Equivalence Principle.
interesting to look for the scaled up versions of Planck
mass mP l =
q
q e) It isp
p
p
h̄gr /h̄× h̄/G = M1 M2 /v0 and Planck length LP l = h̄gr /h̄× h̄/G =
p
√
G M1 M2 /v0 . For M1 = M2 = M this gives mP l = M/ v0 ' 45.6 × M
√
and LP l = rS /2 v0 ' 22.8 × rS , where rS is Schwartshild radius. For Sun
rS is about 2.9 km so that one has LP l ' 66 km. For a few years ago it was
found that Sun contains ”inner-inner” core of radius about R = 300 km [11]
which is about 4.5 × LP l .
3.3.2

Inclinations for the planetary orbits and the quantum evolution of the planetary system

The inclinations of planetary orbits provide a test bed for the theory. The
semiclassical quantization of angular momentum gives the directions of angular momentum from the formula
cos(θ) =

p

m
,
j(j + 1)

|m| ≤ j .

(3)

where θ is the angle between angular momentum and quantization axis and
thus also that between orbital plane and (x,y)-plane. This angle defines the
angle of tilt between the orbital plane and (x,y)-plane.
m = j = n gives minimal value of angle of tilt for a given value of n of
the principal quantum number as
cos(θ) =

n
.
n(n + 1)

p

(4)

For n = 3, 4, 5 (Mercury, Venus, Earth) this gives θ = 30.0, 26.6, and 24.0
degrees respectively.

16

Quantization in Astrophysics ...

35

Only the relative tilt angles can be compared with the experimental
data. Taking as usual the Earth’s orbital plane as the reference the relative
tilt angles give what are known as inclinations. The predicted inclinations
are 6 degrees for Mercury and 2.6 degrees for Venus. The observed values
[12] are 7.0 and 3.4 degrees so that the agreement is satisfactory. If one
allows half-odd integer spin the fit is improved. For j = m = n − 1/2 the
predictions are 7.1 and 2.9 degrees for Mercury and Venus respectively. For
Mars, Jupiter, Saturn, Uranus, Neptune, and Pluto the inclinations are 1.9,
1.3, 2.5, 0.8, 1.8, 17.1 degrees. For Mars and outer planets the tilt angles
are predicted to have wrong sign for m = j. In a good approximation the
inclinations vanish forpouter planets except Pluto and this would allow to
determine m as m ' 5n(n + 1)/6: the fit is not good.
The assumption that matter has condensed from a matter rotating in
(x,y)-plane orthogonal to the quantization axis suggests that the directions
of the planetary rotation axes are more or less the same and by angular
momentum conservation have not changed appreciably. The prediction for
the tilt of the rotation axis of the Earth is 24 degrees of freedom in the
limit that the Earth’s spin can be treated completely classically, that is for
m = j >> 1 in the units used for the quantization of the Earth’s angular
momentum. What is the value of h̄gr for Earth is not obvious (using the unit
h̄gr = GM 2 /v0 the Earth’s angular momentum would be much smaller than
one). The tilt of the rotation axis of Earth with respect to the orbit plane
is 23.5 degrees so that the agreement is again satisfactory. This prediction
is essentially quantal: in purely classical theory the most natural guess for
the tilt angle for planetary spins is 0 degrees.
The observation that the inner planets Mercury, Venus, and Earth have
in a reasonable approximation the predicted inclinations suggest that they
originate from a primordial period during which they formed spherical cells
of dark matter and had thus full rotational degrees of freedom and were in
eigen states of angular momentum corresponding to a full rotational symmetry. The subsequent SO(3) → SO(2) symmetry breaking leading to the
formation of torus like configurations did not destroy the information about
this period since the information about the value of j and m was coded by
the inclination of the planetary orbit.
In contrast to this, the dark matter associated with Earth and outer
planets up to Neptune formed a flattened magnetic or Z 0 magnetic flux tube
resembling a disk with a hole and the subsequent symmetry breaking broke
it to separate flux tubes. Earth’s spherical disk was joined to the disk formed
by the outer planets. The spherical disk could be still present and contain
super-conducting dark matter. The presence of this ”heavenly sphere” might
17

Quantization in Astrophysics ...

36

closely relate to the fact that Earth is a living planet. The time scale T =
2πR/c is very nearly equal to 5 minutes and defines a candidate for a biorhythm.
If this flux tube carried the same magnetic flux as the flux tubes associated with the inner planets, the decomposition of the disk with a hole to
5 flux tubes corresponding to Earth and to the outer planets Mars, Jupiter,
Saturn and Neptune, would explain the value of v0 correctly and also the
small inclinations of outer planets. That Pluto would not originate from
this structure, is consistent with its anomalously large values of inclination
i = 17.1 degrees, small value of eccentricity e = .248, and anomalously large
value of inclination of equator to orbit about 122 degrees as compared to
23.5 degrees in the case of Earth [12].
3.3.3

Eccentricities and comets

Bohr-Sommerfeld quantization allows also to deduce the eccentricities of the
planetary and comet orbits. One can write the quantization of energy as
p2φ
p2θ
k
E1
p2r
+
−
= − 2 ,
+
2m1 2m1 r2 2m1 r2 sin2 (θ) r
n
2
2
v0
k
E 1 = 2 × m1 =
× m1 .
2
2h̄gr

(5)

Here one has k = GM m1 . E1 is the binding energy of n = 1 state. In
the orbital plane (θ = π/2, pθ = 0) the conditions are simplified. Bohr
quantization gives pφ = mh̄gr implying
k 2 h̄2gr
p2r
k
+
−
2
2m1 2m1 r
r

= −

E1
.
n2

(6)

For pr = 0 the formula gives maximum and minimum radii r± and eccentricity is given by
q

e

2

=

2

2 1− m
2
r+ − r−
q n
=
.
2
r+
1+ 1− m
n2

(7)

For small values of n the eccentricities are very large except for m = n.
For instance, for (m = n − 1, n) for n = 3, 4, 5 gives e = (.93, .89, .86)
18

Quantization in Astrophysics ...

37

to be compared with the experimental values (.206, .007, .0167). Thus
the planetary eccentricities with Pluto included (e = .248) must vanish in
the lowest order approximation and must result as a perturbation of the
magnetic flux tube.
The large eccentricities of comet orbits might however have an interpretation in terms of m < n states. The prediction is that comets with small
eccentricities have very large orbital radius. Oort’s cloud is a system weakly
bound to a solar system extending up to 3 light years. This gives the upper
bound n ≤ 700 if the comets of the cloud belong to the same family as
Mercury, otherwise the bound is smaller. This gives a lower bound to the
eccentricity of not nearly circular orbits in the Oort cloud as e > .32.

3.4

About the interpretation of the parameter v0

The formula for the gravitational Planck constant contains the parameter
v0 /c = 2−11 . This velocity defines the rotation velocities of distant stars
around galaxies. The presence of a parameter with dimensions of velocity
should carry some important information about the geometry of dark matter space-time sheets. The interpretation in terms of cosmic strings and
magnetic flux tubes has been already discussed but also alternative interpretations can be considered.
Velocity like parameters appear also in other contexts. There is evidence
for the Tifft’s quantization of cosmic red-shifts in multiples of v0 /c = 2.68 ×
10−5 /3: also other units of quantization have been proposed but they are
multiples of v0 [5].
The strange behavior of graphene includes high conductivity with conduction electrons behaving like massless particles with light velocity replaced
with v0 /c = 1/300. The TGD inspired model [J1] explains the high conductivity as being due to the Planck constant h̄(M 4 ) = 6h̄0 increasing the
delocalization length scale of electron pairs associated with hexagonal rings
of mono-atomic graphene layer by a factor 6 and thus making possible overlap of electron orbitals. This explains also the anomalous conductivity of
DNA containing 5- and 6-cycles [J1].
1. Is dark matter warped?
The reduced light velocity could be due to the warping of the space-time
sheet associated with dark electrons. TGD predicts besides gravitational
red-shift a non-gravitational red-shift due to the warping of space-time sheets
possible because space-time is 4-surface rather than abstract 4-manifold. A
simple example of everyday life is the warping of a paper sheet: it bends
19

Quantization in Astrophysics ...

38

but is not stretched, which means that the induced metric remains flat although one of its component scales (distance becomes longer along direction
of bending). For instance, empty Minkowski space represented canonically
as a surface of M 4 × CP2 with constant CP2 coordinates can become periodically warped in time direction because of the bending in CP2 direction.
As a consequence, the distance in time direction shortens and effective lightvelocity decreases when determined from the comparison of the time taken
for signal to propagate from A to B along warped space-time sheet with
propagation time along a non-warped space-time sheet.
The simplest warped imbedding defined by the map M 4 → S 1 , S 1 a
geodesic circle of CP2 . Let the angle coordinate of S 1 depend linearly on
time: Φ = ωt. gtt component √
of metric becomes 1 − R2 ω 2 so that the light
velocity is reduced to v0 /c = 1 − R2 ω 2 . No gravitational field is present.
The fact that M 4 Planck constant na h̄0 defines the scaling factor n2a of
CP2 metric could explain why dark matter resides around strongly warped
imbeddings of M 4 . The quantization of the scaling factor of CP2 by R2 →
n2a R2 implies that the initial small warping in the time direction given by
gtt = 1 − ², ² = R2 ω 2 , will be amplified to gtt = 1 − n2a ² if ω is not affected in
the transition to dark matter phase. na = 6 in the case of graphene would
give 1 − x ' 1 − 1/36 so that only a one per cent reduction of light velocity
is enough to explain the strong reduction of light velocity for dark matter.
2. Is c/v0 quantized in terms of ruler and compass rationals?
The known cases suggests that c/v0 is always a rational number expressible as a ratio of integers associated with n-polygons constructible using only
ruler and compass.
a) c/v0 = 300 would explain graphene. The nearest rational satisfying
the ruler and compass constraint would be q = 5 × 210 /17 ' 301.18.
b) If dark matter space-time sheets are warped with c0 /v = 211 one can
understand Nottale’s quantization for the radii of the inner planets. For
dark matter space-time sheets associated with outer planets one would have
c/v0 = 5 × 211 .
c) If Tifft’s red-shifts relate to the warping of dark matter space-time
sheets, warping would correspond to v0 /c = 2.68 × 10−5 /3. c/v0 = 25 × 17 ×
257/5 holds true with an error smaller than .1 per cent.
3. Tifft’s quantization and cosmic quantum coherence
An explanation for Tifft’s quantization in terms of Jones inclusions could
be that the subgroup G of Lorentz group defining the inclusion consists of
boosts defined by multiples η = nη0 of the hyperbolic angle η0 ' v0 /c. This
20

Quantization in Astrophysics ...

39

would give v/c = sinh(nη0 ) ' nv0 /c. Thus the dark matter systems around
which visible matter is condensed would be exact copies of each other in
cosmic length scales since G would be an exact symmetry. The property
of being an exact copy applies of course only in single level in the dark
matter hierarchy. This would mean a delocalization of elementary particles
in cosmological length scales made possible by the huge values of Planck
constant. A precise cosmic analog for the delocalization of electron pairs in
benzene ring would be in question.
Why then η0 should be quantized as ruler and compass rationals? In the
case of Planck constants the quantum phases q = exp(imπ/nF ) are number
theoretically simple for nF a ruler and compass integer. If the boost exp(η) is
represented as a unitary phase exp(imη) at the level of discretely delocalized
dark matter wave functions, the quantization η0 = n/nF would give rise to
number theoretically simple phases. Note that this quantization is more
general than η0 = nF,1 /nF,2 .
The interpretation in terms of warping would suggest that the dark matter associated with distant stars in the galactic halos moves with a reduced
light velocity in a state similar to that of conduction electrons in graphene.
The consistency with the interpretation based on magnetic flux quanta remains open.

3.5

How do the magnetic flux tube structures and quantum
gravitational bound states relate?

In the case of stars in galactic halo the appearance of the parameter v0
characterizing cosmic strings as orbital rotation velocity can be understood
classically. That v0 appears also in the gravitational dynamics of planetary
orbits could relate to the dark matter at magnetic flux tubes. The argument
explaining the harmonics and sub-harmonics of v0 in terms of properties
of cosmic strings and magnetic flux tubes identifiable as their descendants
strengthens this expectation. As a matter fact, magnetic body corresponds
also to gravi-magnetic body since classical gauge fields and gravitational
field are very closely related since CP2 coordinates are primary dynamical
variables.
3.5.1

The notion of field body

Topological field quantization implies that one can assign to a material system also field identity, field body. Field body contains both electric and
magnetic part and consists of flux quanta of these fields identifiable as space21

Quantization in Astrophysics ...

40

time sheets. The notion of magnetic body plays a key role in TGD inspired
theory of consciousness being the ultimate intentional agent, experiencer,
and performer of bio-control and can have astrophysical size. This does not
sound so counter-intuitive if one takes seriously the idea that cognition has
p-adic space-time sheets as space-time correlates and that rational points
are common to real and p-adic number fields. The point is that infinitesimal in p-adic topology corresponds to infinite in real sense so that cognitive
and intentional structures would have literally infinite size.
The magnetic flux tubes carrying various supra phases can be interpreted as special instance of dark energy and dark matter. This suggests
a correlation between gravitational self-organization and quantum phases
at the magnetic flux tubes and that the gravitational Schrödinger equation
somehow relates to the ordinary Schrödinger equation satisfied by the macroscopic quantum phases at magnetic flux tubes. In [A9] I have proposed that
the transition increasing Planck constant occurs when perturbation theory
fails and thus reduces the higher order radiative corrections. Interestingly,
the transition to large Planck constant phase should occur when the masses
of interacting is above Planck mass since gravitational self-interaction energy is V ∼ GM 2 /R. For the density of water about 103 kg/m3 the volume
carrying a Planck mass correspond to a cube with side 2.8 × 10−4 meters.
This corresponds to a volume of a large neuron, which suggests that this
phase transition might play an important role in neuronal dynamics.
3.5.2

Ga as a symmetry group of field body

The group Ga ⊂ SU (2) ⊂ SL(2, C) appearing in the quantization of Planck
constant, means exact rotational symmetry realized in terms of M±4 coverings of CP2 . The 5- and 6-cycles in biochemistry (sugars, DNA,....) are
excellent candidates for these symmetries. For very large values of Planck
constant, say for the values h̄(M±4 )/h̄(CP2 ) = GM m/v0 = (na /nb )h̄0 ,
v0 = 2−11 , required by the model for planetary orbits as Bohr orbits [D6],
Ga is huge and corresponds to either Zna or in the case of even value of na
to the group generated by Zn and reflection acting on plane and containing
2na elements.
The notion of field body, in particular magnetic body, seems to provide
the only conceivable candidate for a geometric object possessing Ga as symmetries. In the first approximation the magnetic field associated with a dark
matter system is expected to be modellable as a dipole field having rotational
symmetry around the dipole axis. Topological quantization means that this
field decomposes into flux tube like structures related by the rotations of Zn
22

Quantization in Astrophysics ...

41

or D2n . Dark particles would have wave functions delocalized to this set of
these flux quanta and span group algebra of Ga . Note that electric body as
a structure consisting of radial electric flux tubes makes also sense and can
possess Ga as a symmetry.
Magnetic and electric flux quanta would naturally mediate gravi-magnetic
and -electric interactions in the TGD based model for the quantization of
radii of planetary orbits and this explains the dependence of h̄gr on the
masses of planet and central object [D6].
3.5.3

Could gravitational Schrödinger equation relate to a quantum control at magnetic flux tubes?

An infinite self hierarchy is the basic prediction of TGD inspired theory
of consciousness (”everything is conscious and consciousness can be only
lost”). Topological quantization allows to assign to any material system a
field body as the topologically quantized field pattern created by the system
[L4, K1]. This field body can have an astrophysical size and would utilize
the material body as a sensory receptor and motor instrument.
Magnetic flux tube and flux wall structures are natural candidates for
the field bodies. Various empirical inputs have led to the hypothesis that
the magnetic flux tube structures define a hierarchy of magnetic bodies, and
that even Earth and larger astrophysical systems possess magnetic body
which makes them conscious self-organizing living systems. In particular,
life at Earth would have developed first as a self-organization of the superconducting dark matter at magnetic flux tubes [L4].
For instance, EEG frequencies corresponds to wavelengths of order Earth
size scale and the strange findings of Libet about time delays of conscious
experience [13, 14] find an elegant explanation in terms of time taken for signals propagate from brain to the magnetic body [K1]. Cyclotron frequencies,
various cavity frequencies, and the frequencies associated with various p-adic
frequency scales are in a key role in the model of bio-control performed by
the magnetic body. The cyclotron frequency scale is given by f = eB/m and
rather low as are also cavity frequencies such as Schumann frequencies: the
lowest Schumann frequency is in a good approximation given by f = 1/2πR
for Earth and equals to 7.8 Hz.
1. Quantum time scales as ”bio-rhythms” in solar system?
To get some idea about the possible connection of the quantum control possibly performed by the dark matter with gravitational Schrödinger
equation, it is useful to look for the values of the periods defined by the
23

Quantization in Astrophysics ...

42

gravitational binding energies of test particles in the fields of Sun and Earth
and look whether they correspond to some natural time scales. For instance,
the period T = 2GMS n2 /v03 defined by the energy of nth planetary orbit depends only on the mass of Sun and defines thus an ideal candidate for a
universal ”bio-rhytm”.
For Sun black hole radius is about 2.9 km. The period defined by the
binding energy of lowest state in the gravitational field of Sun is given TS =
2GMS /v03 and equals to 23.979 hours for v0 /c = 4.8233 × 10−4 . Within
experimental limits for v0 /c the prediction is consistent with 24 hours! The
value of v0 corresponding to exactly 24 hours would be v0 = 144.6578 km/s
(as a matter fact, the rotational period of Earth is 23.9345 hours). As if as
the frequency defined by the lowest energy state would define a ”biological”
clock at Earth! Mars is now a strong candidate for a seat of life and the day
in Mars lasts 24hr 37m 23s! n = 1 and n = 2 are orbitals are not realized in
solar system as planets but there is evidence for the n = 1 orbital as being
realized as a peak in the density of IR-dust [2]. One can of course consider
the possibility that these levels are populated by small dark matter planets
with matter at larger space-time sheets. Bet as it may, the result supports
the notion of quantum gravitational entrainment in the solar system.
The slower rhythms would become as n2 sub-harmonics of this time
scale. Earth itself corresponds to n = 5 state and to a rhythm of .96 hours:
perhaps the choice of 1 hour to serve as a fundamental time unit is not
merely accidental. The magnetic field with a typical ionic cyclotron frequency around 24 hours would be very weak: for 10 Hz cyclotron frequency
in Earth’s magnetic field the field strength would about 10−11 T. However,
T = 24 hours corresponds with 6 per cent accuracy to the p-adic time scale
T (k = 280) = 213 T (2, 127), where T (2, 127) corresponds to the secondary
p-adic time scale of .1 s associated with the Mersenne prime M127 = 2127 − 1
characterizing electron and defining a fundamental bio-rhytm and the duration of memetic codon [TGDgeme].
Comorosan effect [15, J5] demonstrates rather peculiar looking facts
about the interaction of organic molecules with visible laser light at wavelength λ = 546 nm. As a result of irradiation molecules seem to undergo
a transition S → S ∗ . S ∗ state has anomalously long lifetime and stability
in solution. S → S ∗ transition has been detected through the interaction
of S ∗ molecules with different biological macromolecules, like enzymes and
cellular receptors. Later Comorosan found that the effect occurs also in nonliving matter. The basic time scale is τ = 5 seconds. p-Adic length scale
hypothesis does not explain τ , and it does not correspond to any obvious
astrophysical time scale and has remained a mystery.
24

Quantization in Astrophysics ...

43

The idea about astro-quantal dark matter as a fundamental bio-controller
inspires the guess that τ could correspond to some Bohr radius R for a solar
system via the correspondence τ = R/c. As observed by Nottale, n = 1
orbit for v0 → 3v0 corresponds in a good approximation to the solar radius and to τ = 2.18 seconds. For v0 → 2v0 n = 1 orbit corresponds to
τ = AU/(4 × 25) = 4.992 seconds: here R = AU is the astronomical unit
equal to the average distance of Earth from Sun. The deviation from τC is
only one per cent and of the same order of magnitude as the variation of the
radius for the orbit due to orbital eccentricity (a − b)/a = .0167 [12].
2. Earth-Moon system
For Earth serving as the central mass the Bohr radius is about 18.7
km, much smaller than Earth radius so that Moon would correspond to
n = 147.47 for v0 and n = 1.02 for the sub-harmonic v0 /12 of v0 . For an
afficionado of cosmic jokes or a numerologist the presence of the number of
months in this formula might be of some interest. Those knowing that the
Mayan calendar had 11 months and that Moon is receding from Earth might
rush to check whether a transition from v/11 to v/12 state has occurred
after the Mayan culture ceased to exist: the increase of the orbital radius
by about 3 per cent would be required! Returning to a more serious mode,
an interesting question is whether light satellites of Earth consisting of dark
matter at larger space-time sheets could be present. For instance, in [L4]
I have discussed the possibility that the larger space-time sheets of Earth
could carry some kind of intelligent life crucial for the bio-control in the
Earth’s length scale.
The period corresponding to the lowest energy state is from the ratio
of the masses of Earth and Sun given by ME /MS = (5.974/1.989) × 10−6
given by TE = (ME /MS ) × TS = .2595 s. The corresponding frequency
fE = 3.8535 Hz frequency is at the lower end of the theta band in EEG
and is by 10 per cent higher than the p-adic frequency f (251) = 3.5355
Hz associated with the p-adic prime p ' 2k , k = 251. The corresponding
wavelength is 2.02 times Earth’s circumference. Note that the cyclotron
frequencies of Nn, Fe, Co, Ni, and Cu are 5.5, 5.0, 5.2, 4.8 Hz in the magnetic
field of .5 × 10−4 Tesla, which is the nominal value of the Earth’s magnetic
field. In [M4] I have proposed that the cyclotron frequencies of Fe and Co
could define biological rhythms important for brain functioning. For v0 /12
associated with Moon orbit the period would be 7.47 s: I do not know
whether this corresponds to some bio-rhytm.
It is better to leave for the reader to decide whether these findings support the idea that the super conducting cold dark matter at the magnetic
25

Quantization in Astrophysics ...

44

flux tubes could perform bio-control and whether the gravitational quantum
states and ordinary quantum states associated with the magnetic flux tubes
couple to each other and are synchronized.

3.6

p-Adic length scale hypothesis and v0 → v0 /5 transition
at inner-outer border for planetary system

v0 → v0 /5 transition would allow to interpret the orbits of outer planets
as n ≥ 1 orbits. The obvious question is whether inner to outer zone as
v0 → v0 /5 transition could be interpreted in terms of the p-adic length scale
hierarchy [E5, TGDpad].
a) The most important p-adic length scale are given by primary p-adic
length scales L(k) = 2(k−151)/2 × 10 nm and secondary p-adic length scales
L(2, k) = 2k−151 × 10 nm, k prime.
b) The p-adic scale L(2, 139) = 114 Mkm is slightly above the orbital
radius 109.4 Mkm of Venus. The p-adic length scale L(2, 137) ' 28.5 Mkm
is roughly one half of Mercury’s orbital radius 57.9 Mkm. Thus strong form
of p-adic length scale hypothesis could explain why the transition v0 → v0 /5
occurs in the region between Venus and Earth (n = 5 orbit for v0 layer and
n = 1 orbit for v0 /5 layer).
c) Interestingly, the primary p-adic length scales L(137) and L(139) correspond to fundamental atomic length scales which suggests that solar system be seen as a fractally scaled up ”secondary” version of atomic system.
d) Planetary radii have been fitted also using Titius-Bode law predicting r(n) = r0 + r1 × 2n . Hence on can ask whether planets are in one-one
correspondence with primary and secondary p-adic length scales L(k). For
the orbital radii 58, 110, 150, 228 Mkm of Mercury, Venus, Earth, and Mars
indeed correspond approximately to k= 276, 278, 279, 281: note the special position of Earth with respect to its predecessor. For Jupiter, Saturn,
Uranus, Neptune, and Pluto the radii are 52,95,191,301,395 Mkm and would
correspond to p-adic length scales L(280 + 2n)), n = 0, ..., 3. Obviously the
transition v0 → v0 /5 could occur in order to make the planet–p-adic length
scale one-one correspondence possible.
e) It is interesting to look whether the p-adic length scale hierarchy
applies also to the solar structure. In a good approximation solar radius .696
Mkm corresponds to L(270), the lower radius .496 Mkm of the convective
zone corresponds to L(269), and the lower radius .174 Mkm of the radiative
zone (radius of the solar core) corresponds to L(266). This encourages the
hypothesis that solar core has an onion like sub-structure corresponding to
various p-adic length scales. In particular, L(2, 127) (L(127) corresponds
26

Quantization in Astrophysics ...

45

to electron) would correspond to 28 Mm. The core is believed to contain
a structure with radius of about 10 km: this would correspond to L(231).
This picture would suggest universality of star structure in the sense that
stars would differ basically by the number of the onion like shells having
standard sizes.
Quite generally, in TGD Universe the formation of join along boundaries
bonds is the space-time correlate for the formation of bound states. This
encourages to think that (Z 0 ) magnetic flux tubes are involved with the
formation of gravitational bound states and that for v0 → v0 /k corresponds
either to a splitting of a flux tube resembling a disk with a whole to k pieces,
or to the scaling down B → B/k 2 so that the magnetic energy for the flux
tube thickened and stretched by the same factor k 2 would not change.

4

Some applications to condensed matter and biology

Dark matter hierarchy has a wide spectrum of biological applications. Examples are a model for high Tc super-conductivity as a quantum critical phenomenon involving phases with different values of Planck constant
[J1, J2, J3], a model for a hierarchy of EEGs based on the model of superconductivity and on the notion of dark magnetic body [M3, F9, J6], the
notion of dark ”N-atoms” (N corresponds to number of sheets in multiple covering of CP2 by M 4 points suggesting how symbolic representations
and language like structures emerge already at the level of bio-molecules
[F9, L2, J6].
Planck constant can have also values smaller than ordinary Planck constant given in terms of ruler and compass rationals. Hydrinos (hydrogen
atoms with fractional principal quantum number) reported by Mills [10]
could be understood in this framework [A9]. In this model the states with
fractional principal quantum number predicted by q-Laquerre equation [A9]
would serve as intermediate states for transitions to dark matter phase. Here
only two examples are briefly discussed.

4.1

Exceptional groups and structure of water

By McKay correspondence finite subgroups of SU(2) correspond to subset
of ADE groups which has led to a proposal that TGD could be able to
mimic corresponding gauge theories using the states of group algebras of finite sub-groups. The Dynkin diagrams of exceptional Lie groups E6 and E8
27

Quantization in Astrophysics ...

46

correspond to exceptional subgroups of rotation group in the sense that they
cannot be reduced to symmetry transformations of plane. They correspond
to the symmetry group S4 × Z2 of tedrahedron and A5 × Z2 of dodecahedron
or its dual polytope icosahedron (A5 is 60-element subgroup of S5 consisting of even permutations). Maximal cyclic subgroups are Z4 and Z5 and
and thus their orders correspond to Fermat polygons. Interestingly, n = 5
corresponds to minimum value of n making possible topological quantum
computation using braids and also to Golden Mean.
There is evidence for an icosahedral clustering in water [7]. Synaptic
contacts contain clathrin molecules which are truncated icosahedrons and
form lattice structures and are speculated to be involved with quantum computation like activities possibly performed by microtubules. Many viruses
have the shape of icosahedron. One can ask whether these structures could
be formed around templates formed by dark matter corresponding to 120fold covering of CP2 points by M±4 points and having h̄(CP2 ) = 5h̄0 perhaps
corresponding color confined light dark quarks. Of course, a similar covering
of M±4 points by CP2 could be involved.

4.2

Aromatic rings and large h̄ phases

Aromatic rings contain odd number of π delocalized electron pairs with
atoms in the same plane. The delocalization of π electrons in the ring
is used to explain the stability of these compounds [8]. Benzene is the
classical example of this kind of structure. Delocalization and anomalous
DNA conductivity [9] suggest interpretation in terms na = 5 or na = 6 phase
(note that these integers correspond to ruler and compass polygons). DNA
conductivity would result from overlap of electrons between rings along DNA
strand. Delocalization might give also rise to Cooper pairs [J1].
Aromatic rings consisting of 5 or 6 carbons are very common in biology. DNA basis have been already mentioned. Carbohydrates consist of
monosaccharide sugars of which most contain aromatic ring (glucose used
as metabolic fuel are exception). Monoamine neurotransmitters are neurotransmitters and neuromodulators that contain one amino group that is connected to an aromatic ring by a two-carbon chain (-CH2-CH2-). The neurotransmitters known a monoamines are derived from the four aromatic amino
acids phenylalanine, tyrosine, histidine, tryptophan. Also norepinephrine,
dopamine, and serotonin involve aromatic rings As a rule psychoactive drugs
involve aromatic rings: for instance, LSD contains four rings.
These observations inspire the question whether the compounds containing aromatic rings serve as junctions connecting pre- and postsynaptic
28

Quantization in Astrophysics ...

47

neurons and induce Josephson currents between them. If Josephson radiation codes for the mental images communicated to the magnetic body,
the psychoactive character of these compounds could be understood. One
can also ask whether these compounds induce quantum criticality making
possible generation of large h̄ phases?

4.3

Model for a hierarchy of EEGs

For the model of dark matter hierarchy appearing in the model of living
matter one has na = 211k , k = 1, 2, 3, .., 7 for cyclotron time scales below life
cycle for a magnetic field Bd = .2 Gauss at k = 4 level of hierarchy (the field
strength is fixed by the model for the effects of ELF em fields on vertebrate
brain at harmonics of cyclotron frequencies of biologically important ions
[M3]). Note that Bd scales as 2−11k from the requirement that cyclotron
energy is constant.
A successful model of EEG emerges explaining its band structure and
narrow resonances inside bands. EEG can be interpreted in terms of communications from cell membrane to magnetic body using dark Josephson
radiation and the control of genome by magnetic body using dark cyclotron
radiation. DNA strands would be organized at magnetic flux sheets like
lines of text on a page of book. Super-genome would code coherent gene expression at the level of organs and hyper-genome containing super-genomes
of different organisms as text lines would be responsible for coherent gene
expression at the level of populations.
The hierarchical structure of magnetic body implies a hierarchy of EEGs
and ordinary EEG corresponds to a magnetic body with size of order Earth
from Compton length of EEG photons. The large value of h̄ guarantees that
dark EEG photons are above thermal threshold and therefore not masked
by the thermal noise. Great leaps in evolution would naturally correspond
to an emergence of a new level in dark matter hierarchy at the level of
individual organism.
The not easily acceptable general prediction is that the field bodies associated with living matter would have sizes up to light life. On the other
hand, Libet’s findings about strange time delays of consciousness can be
understood in terms of magnetic body of size of order Earth.

5

Summary and outlook

The predicted dark matter hierarchy means giving up the reductionistic
world view. Fractality and possibility to used simple scaling arguments
29

Quantization in Astrophysics ...

48

makes this vision highly predictive and testable. Of course, a lot remains to
be understood. For instance, it is not yet clear whether the two interpretations of the parameter v0 appearing in the model of planetary orbits are
mutually consistent.
The new view has also implications for elementary particle -, nuclear
-, and condensed matter physics [F8, F9, J6, J1, J2, J3]. Darkness of valence quarks could allow improved understanding of color confinement. Dark
variants of electro-weak gauge bosons and gluons with zoomed up Compton
wave length might be directly relevant to the understanding of even ordinary
condensed matter [F9]. High Tc super-conductivity represents one particular condensed matter application in which zoomed up electrons play a role
[J1].
Perhaps the most fascinating applications of the theory would be to living
systems and to quantum model of brain. For instance, I have proposed that
charge entanglement over macroscopic distances made possible by dark W
bosons might be a fundamental mechanism in quantum control in living
matter.
Acknowldegements
I am grateful for Victor Christianto for informing me about the evidence for
the quantization of radii of planetary orbits.

References
[TGDview] M. Pitkänen (2006), Topological Geometrodynamics: Overview.
http://www.helsinki.fi/∼matpitka/tgdview/tgdview.html.
[TGDgeom] M. Pitkänen (2006), Quantum Physics as Infinite-Dimensional
Geometry.
http://www.helsinki.fi/∼matpitka/tgdgeom/tgdgeom.html.
[TGDquant] M. Pitkänen (2006), Quantum TGD.
http://www.helsinki.fi/∼matpitka/tgdquant/tgdquant.html.
[TGDclass] M. Pitkänen (2006), Physics in Many-Sheeted Space-Time.
http://www.helsinki.fi/∼matpitka/tgdclass/tgdclass.html.
[TGDnumber] M. Pitkänen (2006), TGD as a Generalized Number Theory.
http://www.helsinki.fi/∼matpitka/tgdnumber/tgdnumber.html.
30

Quantization in Astrophysics ...

49

[TGDpad] M. Pitkänen (2006), p-Adic length Scale Hypothesis and Dark
Matter Hierarchy.
http://www.helsinki.fi/∼matpitka/paddark/paddark.html.
[TGDfree] M. Pitkänen (2006), TGD and Fringe Physics.
http://www.helsinki.fi/∼matpitka/freenergy/freenergy.html.
[TGDconsc] M. Pitkänen (2006), TGD Inspired Theory of Consciousness.
http://www.helsinki.fi/∼matpitka/tgdconsc/tgdconsc.html.
[TGDselforg] M. Pitkänen (2006), Bio-Systems as Self-Organizing Quantum
Systems.
http://www.helsinki.fi/∼matpitka/bioselforg/bioselforg.html.
[TGDware] M. Pitkänen (2006), Quantum Hardware of Living Matter.
http://www.helsinki.fi/∼matpitka/bioware/bioware.html.
[TGDholo] M. Pitkänen (2006), Bio-Systems as Conscious Holograms.
http://www.helsinki.fi/∼matpitka/hologram/hologram.html.
[TGDgeme] M. Pitkänen (2006), Mathematical Aspects of Consciousness
Theory.
http://www.helsinki.fi/∼matpitka/genememe/genememe.html.
[TGDeeg] M. Pitkänen (2006), TGD and EEG.
http://www.helsinki.fi/∼matpitka/tgdeeg/tgdeeg/tgdeeg.html.
[TGDmagn] M. Pitkänen (2006), Magnetospheric Consciousness.
http://www.helsinki.fi/∼matpitka/magnconsc/magnconsc.html.
[TGDmath] M. Pitkänen (2006), Mathematical Aspects of Consciousness
Theory.
http://www.helsinki.fi/∼matpitka/mathconsc/mathconsc.html.
[1] L. Nottale (1993), Fractal Space-Time and Microphysics, World Scientific.
[2] D. Da Roacha and L. Nottale (2003), Gravitational Structure Formation
in Scale Relativity, astro-ph/0310036.
[3] A. Rubric and J. Rubric (1998), The Quantization of the Solar-like
Gravitational Systems, Fizika B7, 1, 1-13.
Idid (1999), Square Law for Orbits in Extra-solar Planetary Systems,
Fizika A 8, 2, 45-50.
31

Quantization in Astrophysics ...

50

[4] Masses and Orbital Characteristics of Extrasolar Planets using stellar
masses derived from Hipparcos, metalicity, and stellar evolution,
http://exoplanets.org/almanacframe.html.
[5] William G. Tifft (1976), Discrete States Of Redshift And Galaxy Dynamics I, Astrophysical Journal, Vol. 206:38-56, 15 May, 1976.
Ibid (1982), Quantum Effects In The Redshift Intervals For Double
Galaxies, Astrophysical Journal, Vol. 257:442-499, 15 June.
Ibid (1982), Double Galaxy Investigations II, Astrophysical Journal,
Vol. 262:44-47, 1 November.
[6] J. D. Anderson et al(2006), The energy Transfer Process in Planetary
Flybys, astro-ph/0608087.
[7] M. Chaplin (2006), Water as a Network of Icosahedral Water Clusters,
http://www.lsbu.ac.uk/water/clusters.html.
[8] Aromaticity.
http://en.wikipedia.org/wiki/Aromatic− rings.
[9] Science (1997), vol. 275, 7. March 1997. An article about the work of
Barton et al giving support for the ability of DNA to act as a conductor.
[10] R. Mills et al(2003), Spectroscopic and NMR identification of novel hybrid ions in fractional quantum energy states formed by an exothermic
reaction of atomic hydrogen with certain catalysts.
http://www.blacklightpower.com/techpapers.html .
[11] BBC NEWS Science/Nature (2002), Quakes reveal ’core within
a core’, Wednesday, 2 October,
http://news.bbc.co.uk/1/hi/sci/tech/2290551.stm .
[12] http://hyperphysics.phy-astr.gsu.edu/hbase/solar/soldata2.html.
[13] S. Klein (2002), Libet’s Research on Timing of Conscious Intention to
Act: A Commentary of Stanley Klein, Consciousness and Cognition
11, 273-279.
http://cornea.berkeley.edu/pubs/ccog− 2002− 0580-KleinCommentary.pdf.
[14] B. Libet, E. W. Wright Jr., B. Feinstein, and D. K. Pearl (1979), Subjective referral of the timing for a conscious sensory experience Brain,
102, 193-224.
32

Quantization in Astrophysics ...

51

[15] S. Comorosan(1975), On a possible biological spectroscopy, Bull. of
Math. Biol., Vol 37, p. 419.
S. Comorosan, M.Hristea, P. Murogoki (1980), On a new symmetry in
biological systems, Bull. of Math. Biol., Vol 42, p. 107.
[A9] The chapter Does TGD Predict the Spectrum of Planck Constants? of
[TGDview].
http://www.helsinki.fi/∼matpitka/tgdview/tgdview.html#Planck.
[C2] The chapter Construction of Quantum Theory:
S-matrix of
[TGDquant].
http://www.helsinki.fi/∼matpitka/tgdquant/tgdquant.html#towards.
[C4] The chapter Is it Possible to Understand Coupling Constant Evolution
at Space-Time Level? of [TGDquant].
http://www.helsinki.fi/∼matpitka/tgdquant/tgdquant.html#rgflow.
[D6] The chapter TGD and Astrophysics of [TGDclass].
http://www.helsinki.fi/∼matpitka/tgdclass/tgdclass.html#astro.
[E5] The chapter p-Adic Physics: Physical Ideas of [TGDnumber].
http://www.helsinki.fi/∼matpitka/tgdnumber/tgdnumber.html#phblocks.
[E9] The chapter Topological Quantum Computation in TGD Universe of
[TGDnumber].
http://www.helsinki.fi/∼matpitka/tgdnumber/tgdnumber.html#tqc.
[F8] The chapter TGD and Nuclear Physics of [TGDpad].
http://www.helsinki.fi/∼matpitka/paddark/paddark.html#padnucl.
[F9] The chapter Dark Nuclear Physics and Living Matter of [TGDpad].
http://www.helsinki.fi/∼matpitka/paddark/paddark.html#exonuclear.
[H8] The chapter p-Adic Physics as Physics of Cognition and Intention of
[TGDconsc].
http://www.helsinki.fi/∼matpitka/tgdconsc/tgdconsc.html#cognic.
[J1] The chapter Bio-Systems as Super-Conductors: part I of [TGDware].
http://www.helsinki.fi/∼matpitka/bioware/bioware.html#superc1.
[J2] The chapter Bio-Systems as Super-Conductors: part II of [TGDware].
http://www.helsinki.fi/∼matpitka/bioware/bioware.html#superc2.
[J3] The chapter Bio-Systems as Super-Conductors: part III of [TGDware].
http://www.helsinki.fi/∼matpitka/bioware/bioware.html#superc3.
33

Quantization in Astrophysics ...

52

[J5] The chapter Wormhole Magnetic Fields of [TGDware].
http://www.helsinki.fi/∼matpitka/bioware/bioware.html#wormc.
[J6] The chapter Coherent Dark Matter and Bio-Systems as Macroscopic
Quantum Systems of [TGDware].
http://www.helsinki.fi/∼matpitka/bioware/bioware.html#darkbio.
[K1] The chapter Time, Spacetime and Consciousness of [TGDholo].
http://www.helsinki.fi/∼matpitka/hologram/hologram.html#time.
[L2] The chapter Many-Sheeted DNA of [TGDgeme].
http://www.helsinki.fi/∼matpitka/genememe/genememe.html#genecodec.
[L4] The chapter Pre-Biotic Evolution in Many-Sheeted Space-Time of
[TGDgeme].
http://www.helsinki.fi/∼matpitka/genememe/genememe.html#prebio.
[M3] The chapter Dark Matter Hierarchy and Hierarchy of EEGs of
[TGDeeg].
http://www.helsinki.fi/∼matpitka/tgdeeg/tgdeeg/tgdeeg.html#eegdark.
[M4] The chapter Quantum Model for EEG: Part I of [TGDeeg].
http://www.helsinki.fi/∼matpitka/tgdeeg/tgdeeg/tgdeeg.html#eegI.

Appendix: Tables comparing predicted and observed
radii of exoplanets
The tables below represent the comparison of predictions of TGD based
model for the orbital radii with known radii in the case of exoplanets (the
model is discussed in section ”Orbital radii of exoplanets). In the tables
R denotes the value of minor semiaxis of the planetary orbit using AU
as a unit and M the mass of star using solar mass MS as a unit. n is
the value of the principal quantum number and R1 the radius assuming
X = (r/s)2 = 1 and R2 the value for the best choice of X as ratio of
”ruler and compass integers”. The data about radii of planets are from
tables at http://exoplanets.org/almanacframe.html and star masses from
the references contained by the tables.

34

Quantization in Astrophysics ...

53

Star Name
HD73256
HD83443
HD46375
HD179949
HD187123b
HD120136
HD330075
BD-103166
HD209458
HD76700
HD217014
HD9826b
HD49674
HD68988
HD168746
HD217107
HD162020
HD130322
HD108147
HD38529b
HD75732b
HD195019
HD6434
HD192263
GJ876c
HD37124b
HD143761
HD75732c
HD74156b
HD168443b
GJ876b
HD3651
HD121504
HD178911
HD16141
HD114762
HD80606
HD117176
HD216770

R
0.037
0.040
0.040
0.040
0.040
0.050
0.046
0.050
0.050
0.050
0.050
0.059
0.060
0.070
0.065
0.070
0.074
0.088
0.102
0.129
0.115
0.140
0.150
0.150
0.130
0.181
0.220
0.240
0.280
0.295
0.210
0.284
0.320
0.326
0.350
0.350
0.469
0.480
0.460

M
1.05
0.79
1.00
1.24
1.06
1.30
0.70
1.10
1.05
1.00
1.06
1.30
1.00
1.20
0.88
0.98
0.75
0.79
1.27
1.39
0.95
1.02
0.79
0.79
0.32
0.91
0.95
0.95
1.27
1.01
0.32
0.79
1.18
0.87
1.00
0.82
1.10
1.10
0.90

n
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
3
2
2
2
2
2
4
3
2
3
3
3
3
3
3

R1
0.042
0.032
0.040
0.050
0.042
0.052
0.028
0.044
0.042
0.040
0.042
0.052
0.040
0.048
0.035
0.039
0.030
0.032
0.051
0.056
0.038
0.163
0.126
0.126
0.115
0.146
0.152
0.152
0.203
0.162
0.205
0.284
0.189
0.313
0.360
0.295
0.396
0.396
0.324

R1/R
1.14
0.79
1.00
1.24
1.06
1.04
0.61
0.88
0.84
0.8
0.85
0.88
0.67
0.69
0.54
0.56
0.41
0.36
0.50
0.43
0.33
1.17
0.84
0.84
0.89
0.80
0.69
0.63
0.73
0.55
0.98
1.00
0.59
0.96
1.03
0.84
0.84
0.83
0.70

35

Quantization in Astrophysics ...

54

r
16
15
1
17
1
1
4
15
16
15
15
15
5
5
3
3
2
3
3
2
3
16
15
15
15
15
5
4
5
3
1
1
3
1
1
15
15
15
5

s
15
17
1
15
1
1
5
16
17
17
16
16
6
6
4
4
3
5
4
3
5
15
16
16
16
17
6
5
6
4
1
1
4
1
1
16
16
16
6

R2/R
1.00
1.01
1.00
0.97
1.06
1.04
0.95
1
0.95
1.03
0.96
1.00
0.96
0.99
0.96
1
0.91
1
0.89
0.97
0.92
1.02
0.96
0.96
1.01
1.03
0.99
0.99
1.05
0.97
0.98
1
1.05
0.96
1.03
0.96
0.96
0.94
1.01

Star Name
HD52265
HD73526
HD82943c
HD8574
HD169830
HD9826c
HD202206
HD89744
HD134987
HD12661b
HD150706
HD40979
HD92788
HD142
HD28185
HD142415
HD108874b
HD4203
HD177830
HD128311b
HD27442
HD210277
HD82943b
HD20367
HD114783
HD137759
HD19994
HD147513
HD222582
HD65216
HD141937
HD41004A
HD160691b

R
0.49
0.65
0.73
0.77
0.82
0.83
0.83
0.89
0.81
0.82
0.82
0.81
0.95
0.97
1.03
1.07
1.06
1.09
1.14
1.02
1.18
1.12
1.16
1.25
1.19
1.28
1.42
1.26
1.35
1.31
1.52
1.31
1.87

M
1.13
1.02
1.05
1.17
1.40
1.30
1.15
1.40
1.05
1.07
0.98
1.08
1.06
1.10
0.99
1.03
1.00
1.06
1.17
0.80
1.20
0.99
1.05
1.17
0.92
1.05
1.34
1.11
1.00
0.92
1.10
0.70
1.08

n
3
4
4
4
4
4
4
4
4
4
5
4
5
5
5
5
5
5
5
6
5
5
5
5
6
5
5
5
6
6
6
7
7

R1
0.41
0.65
0.67
0.75
0.90
0.83
0.74
0.9
0.67
0.68
0.98
0.69
1.06
1.1
0.99
1.03
1.00
1.06
1.17
1.15
1.20
0.99
1.05
1.17
1.32
1.05
1.34
1.11
1.44
1.32
1.58
1.37
2.12

R1/R
0.83
1
0.92
0.97
1.09
1.00
0.89
1.01
0.83
0.84
1.20
0.85
1.12
1.13
0.96
0.96
0.94
0.97
1.03
1.13
1.02
0.88
0.91
0.94
1.11
0.82
0.94
0.88
1.07
1.01
1.04
1.05
1.13

Star Name

R

M

n

R1

R1/R

0.49
0.65

1.13
1.02

3
4

0.41
0.65

HD52265
HD73526

0.83
1
36

Quantization in Astrophysics ...

55

r
15
1
16
1
17
1
15
1
15
15
16
15
16
16
1
1
1
1
1
1
1
15
15
1
1
15
1
15
1
1
1
1
16

r
15
1

s R2/R
16 0.94
1
1.00
17 1.04
1
0.97
16 0.97
1
1.00
16 1.01
1
1.01
16 0.94
16 0.95
15 1.05
16 0.97
15 0.98
15 1.00
1
0.96
1
0.96
1
0.94
1
0.97
1
1.03
1
1.13
1
1.02
16 1.01
16 1.03
1
0.94
1
1.11
17 1.05
1
0.94
16 1.00
1
1.07
1
1.01
1
1.04
1
1.05
15 0.99

s R2/R
16
1

0.94
1.00

HD82943c
HD8574
HD169830
HD9826c
HD202206
HD89744
HD134987
HD12661b
HD150706
HD40979
HD92788
HD142
HD28185
HD142415
HD108874b
HD4203
HD177830
HD128311b
HD27442
HD210277
HD82943b
HD20367
HD114783
HD137759
HD19994
HD147513
HD222582
HD65216
HD141937
HD41004A
HD160691b

0.73
0.77
0.82
0.83
0.83
0.89
0.81
0.82
0.82
0.81
0.95
0.97
1.03
1.07
1.06
1.09
1.14
1.02
1.18
1.12
1.16
1.25
1.19
1.28
1.42
1.26
1.35
1.31
1.52
1.31
1.87

1.05
1.17
1.40
1.30
1.15
1.40
1.05
1.07
0.98
1.08
1.06
1.10
0.99
1.03
1.00
1.06
1.17
0.80
1.20
0.99
1.05
1.17
0.92
1.05
1.34
1.11
1.00
0.92
1.10
0.70
1.08

4
4
4
4
4
4
4
4
5
4
5
5
5
5
5
5
5
6
5
5
5
5
6
5
5
5
6
6
6
7
7

0.67
0.75
0.90
0.83
0.74
0.9
0.67
0.68
0.98
0.69
1.06
1.1
0.99
1.03
1.00
1.06
1.17
1.15
1.20
0.99
1.05
1.17
1.32
1.05
1.34
1.11
1.44
1.32
1.58
1.37
2.12

0.92
0.97
1.09
1.00
0.89
1.01
0.83
0.84
1.20
0.85
1.12
1.13
0.96
0.96
0.94
0.97
1.03
1.13
1.02
0.88
0.91
0.94
1.11
0.82
0.94
0.88
1.07
1.01
1.04
1.05
1.13

37

Quantization in Astrophysics ...

56

16
1
17
1
15
1
15
15
16
15
16
16
1
1
1
1
1
1
1
15
15
1
1
15
1
15
1
1
1
1
16

17
1
16
1
16
1
16
16
15
16
15
15
1
1
1
1
1
1
1
16
16
1
1
17
1
16
1
1
1
1
15

1.04
0.97
0.97
1.00
1.01
1.01
0.94
0.95
1.05
0.97
0.98
1.00
0.96
0.96
0.94
0.97
1.03
1.13
1.02
1.01
1.03
0.94
1.11
1.05
0.94
1.00
1.07
1.01
1.04
1.05
0.99

Star Name
HD23079
HD186427
HD4208
HD114386
HD213240
HD10647
HD10697
HD95128b
HD190228
HD114729
HD111232
HD2039
HD136118
HD50554
HD9826d
HD196050
HD216437
HD216435
HD169830c
HD106252
HD12661c
HD23596
HD168443c
HD145675
HD11964b
HD39091
HD38529c
HD70642
HD33636
HD95128c
HD190360
HD74156c
HD22049
HD30177
HD89307
HD72659
HD75732d

R
1.65
1.67
1.67
1.62
2.03
2.10
2.13
2.09
2.00
2.08
1.97
2.19
2.40
2.32
2.53
2.43
2.43
2.70
2.75
2.54
2.60
2.86
2.87
2.85
3.10
3.29
3.71
3.30
3.56
3.73
3.65
3.82
3.54
3.86
4.15
4.50
5.90

M
1.10
1.01
0.93
0.68
1.22
1.07
1.10
1.03
0.83
0.93
0.78
0.98
1.24
1.07
1.30
1.10
1.07
1.25
1.40
0.96
1.07
1.30
1.01
1.00
1.10
1.10
1.39
1.00
0.99
1.03
0.96
1.27
0.80
0.95
0.95
0.95
0.95

n
6
6
7
8
6
7
7
7
8
7
8
7
7
7
7
7
8
7
7
8
8
7
8
8
8
9
8
9
9
10
10
9
11
10
10
11
13

R1
1.58
1.45
1.82
1.74
1.76
2.10
2.16
2.02
2.12
1.82
2.00
1.92
2.43
2.09
2.55
2.16
2.74
2.45
2.74
2.46
2.74
2.55
2.59
2.56
2.82
3.56
3.56
3.24
3.21
4.12
3.84
4.11
3.87
3.80
3.80
4.60
6.42

R1/R
0.96
0.87
1.09
1.07
0.87
1.00
1.01
0.97
1.06
0.88
1.01
0.88
1.01
0.9
1.01
0.89
1.13
0.91
1
0.97
1.05
0.89
0.9
0.9
0.91
1.08
0.96
0.98
0.9
1.1
1.05
1.08
1.09
0.98
0.92
1.02
1.09

38

Quantization in Astrophysics ...

57

r
1
15
16
17
15
1
1
1
1
15
1
15
1
15
1
15
17
1
1
1
1
15
15
15
16
17
1
1
15
16
1
1
16
1
1
1
16

s R2/R
1
0.96
16 0.99
15 0.96
16 0.95
16 0.98
1
1
1
1.01
1
0.97
1
1.06
16 1
1
1.01
16 1
1
1.01
16 1.02
1
1.01
16 1.01
15 0.88
1
0.91
1
1
1
0.97
1
1.05
16 1.01
16 1.03
16 1.02
17 1.03
16 0.96
1
0.96
1
0.98
16 1.03
15 0.97
1
1.05
1
1.08
15 0.96
1
0.98
1
0.92
1
1.02
15 0.96

Could q-Laguerre equation explain the claimed
fractionation of the principal quantum number for
hydrogen atom?
Matti Pitkänen
Puutarhurinkatu 10 10960, Hanko, Finland, E-mail matpitka@rock.helsinki.fi

September 25, 2006

Contents
1 Introduction

1

2 q-Laquerre equation for q = exp(iπ/m)

2

3 Polynomial solutions of q-Laquerre equation

3

4 Non-polynomial solutions of q-Laquerre equation

4

5 Results of numerical calculations

5

6 How to obtain nq) = 1/2 state?

6

7 Some comments

9

1

Introduction

In [G2] a semiclassical model based on dark matter and hierarchy of Planck
constants is developed for the fractionized principal quantum number n
claimed by Mills [1] to have at least the values n = 1/k, k = 2, 3, 4, 5, 6, 7, 10.
This model could explain the claimed fractionization of the principal quantum number n for hydrogen atom [1] in terms of single electron transitions
for all cases except n = 1/2: the basis reason is that Jones inclusions are
characterized by quantum phases q = exp(iπ/n), n > 2. Since quantum
deformation of the standard quantum mechanism is involved, this motivates
1

Quantization in Astrophysics ...

58

an attempt to understand the claimed fractionization in terms of q-analog
of hydrogen atom.
The Laguerre polynomials appearing in the solution of Schrödinger equation for hydrogen atom possess quantum variant, so called q-Laguerre polynomials [2], and one might hope that they would allow to realize this semiclassical picture at the level of solutions of appropriately modified Schrödinger
equation and perhaps also resolve the difficulty associated with n = 1/2. Unfortunately, the polynomials discussed in [2] correspond to 0 < q ≤ 1 rather
than complex values of q = exp(iπ/m) on circle and the extrapolation of the
formulas for energy eigenvalues gives complex energies. It is however easy
to modify the definition of q-derivative and it turns out that it is possible to
reproduce n = 1/2 state exactly and n = 1/m, m > 2 states in a reasonable
approximation as solutions of q-Laquerre equation for s-wave states. Also
the generalization to associated q-Laquerre equation is straightforward.

2

q-Laquerre equation for q = exp(iπ/m)

The most obvious modification of the Laguerre equation for S-wave sates
(which are the most interesting by semiclassical argument) in the complex
case is based on the replacement

∂x →
∂xq) f

=

q

=

1 q)
(∂ + ∂xq) )
2 x
f (qx) − f (x)
,
q−1
exp(iπ/m)

(1)

to guarantee hermiticity. When applied to the Laguerre equation

x

d2 Ln
dLn
+ (1 − x)
= nLn ,
2
dx
dx

(2)

and expanding Ln into Taylor series
X

Ln (x) =

ln xn ,

n≥0

one obtains difference equation

2

Quantization in Astrophysics ...

59

(3)

an+1 ln+1 + bn ln = 0 ,
1
1
an+1 =
[R2n+1 − R2n + 2Rn+1 R1 + 3R1 )] +
[Rn+1 + R1 ]
2
2R1
4R1
Rn
1
bn =
− nq) + ,
2R1
2
Rn = 2cos [(n − 1)π/m] − 2cos [nπ/m] .
(4)
Here nq) is the fractionized principal quantum number determining the energy of the q-hydrogen atom. One cannot pose the difference equation on
l0 since this together with the absence of negative powers of x would imply
the vanishing of the entire solution. This is natural since for first order
difference equations lowest term in the series should be chosen freely.

3

Polynomial solutions of q-Laquerre equation

The condition that the solution reduces to a polynomial reads as

bn = 0

(5)

and gives

nq) =

1
Rn
+
,
2 2R1

(6)

For n = 1 one has nq) = 1 so that the ground state energy is not affected.
At the limit N → ∞ one obtains nq) → n so that spectrum reduces to that
for hydrogen atom. The periodicity Rn+2N k = Rn reflects the corresponding
periodicity of the difference equation which suggests that only the values n ≤
2m−1 belong to the spectrum. Spectrum is actually symmetric with respect
to the middle point [N/2] which suggests that only n < [m/2] corresponds to
the physical spectrum. An analogous phenomenon occurs for representations
of quantum groups. When m increases the spectrum approaches integer
valued spectrum and one has n > 1 so that no fractionization in the desired
sense occurs for polynomial solutions.

3

Quantization in Astrophysics ...

60

4

Non-polynomial solutions of q-Laquerre equation

One might hope that non-polynomial solutions associated with some fractional values of nq) near to those claimed by Mills might be possible. Since
the coefficients an and bn are periodic, one can express the solution ansatz
as

Ln (x) = Pa2m) (x)

X

ak x2mk = Pa2m) (x)

k

Pa2m) (x) =

2m−1
X

1
,
1 − ax2m

lk xk ,

k=0

a =

l2m
,
l0

(7)

This solution behaves as 1/x asymptotically but has pole at x∞ = (1/a)1/2m
for a > 0.
The expression for l2m /l0 = a is

a =

2m
Y

b2m−k

k=1

a2m−k+1

.

(8)

This can be written more explicitly as

a = (2R1 )2m

2m
Y

Xk ,

k=1

Xk =

R2m−k + (−2nq) + 1)R1
,
R4m−2k+1 − R4m−2k + 4R2m−k+1 R1 + 2R12 + 3R1

Rn = 2cos [(n − 1)π/m] − 2cos [nπ/m] .

(9)

This formula is a specialization of a more general formula for n = 2m
2m)
and resulting ratios ln /l0 can be used to construct Pa
with normaliza2m)
tion Pa (0) = 1.

4

Quantization in Astrophysics ...

61

5

Results of numerical calculations

Numerical calculations demonstrate following.
a) For odd values of m one has a < 0 so that a a continuous spectrum
of energies seems to result without any further conditions.
b) For even values of m a has a positive sign so that a pole results.
2m)
For even value of m it could happen that the polynomial Pa (x) has
a compensating zero at x∞ so that the solution would become square integrable. The condition for reads explicitly
1 1
Pa2m) ( ) 2m
a




= 0 .

(10)

2m)

If Pa (x) has zeros there are hopes of finding energy eigen values satisfying the required conditions. Laguerre polynomials and also q-Laguerre
polynomials must posses maximal number of real zeros by their orthogonality implied by the hermiticity of the difference equation defining them.
2m)
This suggests that also Pa (x) possesses them if a does not deviate too
much from zero. Numerical calculations demonstrate that this is the case
for nq) < 1.
For ordinary Laguerre polynomials the naive estimate for the position of
the most distant zero in the units used is larger than n but not too much
so. The naive expectation is that L2m has largest zero somewhat above
x = 2m and that same holds true a small deformation of L2m considered
now since the value of the parameter a is indeed very small for nq) < 1. The
ratio x∞ /2m is below .2 for m ≤ 10 so that this argument gives good hopes
about zeros of desired kind.
One can check directly whether x∞ is near to zero for the experimentally
suggested candidates for nq) . The table below summarizes the results of
numerical calculations.
a) The table gives the exact eigenvalues 1/nq) with a 4-decimal accuracy
q)

and corresponding approximations 1/n' = k for k = 3, ..., 10. For a given
value of m only single eigenvalue nq) < 1 exists. If the observed anomalous
spectral lines correspond to single electron transitions, the values of m for
them must be different. The value of m for which nq) ' 1/k approximation
is optimal is given with boldface. The value of k increases as m increases.
The lowest value of m allowing the desired kind of zero of P 2m) is m = 18
and for k ∈ {3, 10} the allowed values are in range 18, .., 38.
b) nq) = 1/2 does not appear as an approximate eigenvalue so that for
even values of m quantum calculation produces same disappointing result
5

Quantization in Astrophysics ...

62

as the classical argument. Below it will be however found that nq) = 1/2 is
a universal eigenvalue for odd values of m.
q)

m
18
20
22
24
26
28

1/n'
3
4
5
5
6
7

1/nq)
2.7568
3.6748
4.5103
5.3062
6.0781
6.8330

m
30
32
34
36
38

q)

1/n'
8
8
9
10
10

1/nq)
7.5762
8.3086
9.0342
9.7529
10.4668

Table 1. The table gives the approximations 1/nq )' = 1/k and corre2m)
sponding exact values 1/nq ) in the range k = 3, ..., 10 for which Pa (x∞ )
is nearest to zero. The corresponding values of m = 2k vary in the range,
k = 18, ..., 38. For odd values of m the value of the parameter a is negative so that there is no pole. Boldface marks for the best approximation by
q)
1/n' = k.

6

How to obtain nq) = 1/2 state?

For odd values of m the quantization recipe fails and physical intuition tells
that there must be some manner to carry out quantization also now. The
following observations give a hunch about be the desired condition.
a) For the representations of quantum groups only the first m spins are
realized. This suggests that there should exist a symmetry relating the
coefficients ln and ln+m and implying nq) = 1/2 for odd values of m. This
symmetry would remove also the double degeneracy associated with the
almost integer eigenvalues of nq) . Also other fractional states are expected
on basis of physical intuition.
b) For nq) = 1/2 the recursion formula for the coefficients ln involves
only the coefficients Rm .
c) The coefficients Rk have symmetries Rk = Rk+2m and Rk+m = −Rm .
There is indeed this kind of symmetry. From the formula
ln
l0

n

= (2R1 )

Xk =

n
Y

Xk ,

k=1

Rn−k + (−2nq) + 1)R1
[R2n−2k+1 − Rn−2k + 4Rn−k+1 R1 + 2R12 + 3R1
6

Quantization in Astrophysics ...

63

(11)

one finds that for nq) = 1/2 the formula giving ln+m in terms of ln changes
sign when n increases by one unit
An+1 = (−1)m An ,
An =

m
Y

bn+m−k

k=1

an+m−k+1

=

m
Y

(2R1 )m

k=1

m
Y

Xk+n .

k=1

(12)
The change of sign is essentially due to the symmetries an+m = −an and
bn+m = bn . This means that the action of translations on An in the space
of indices n are represented by group Z2 .
This symmetry implies a = l2m /l0 = −(lm )(l0 )2 so that for nq) = 1/2
the polynomial in question has a special form
Pa2m) = Pam) (1 − Axm ) ,
A = A0 .

(13)

The relationship a = −A2 implies that the solution reduces to a form containing the product of mth (rather than (2m)th ) order polynomial with a
geometric series in xm (rather than x2m ):
m)

Pa (x)
.
1 + Axm

L1/2 (x) =

(14)

Hence the n first terms indeed determine the solution completely. For even
values of m one obtains similar result for nq) = 1/2 but now A is negative so
that the solution is excluded. This result also motivates the hypothesis that
for the counterparts of ordinary solutions of Laguerre equation sum (even
m) or difference (odd m) of solutions corresponding to n and 2m − n must
be formed to remove the non-physical degeneracy.
This argument does not exclude the possibility that there are also other
fractional values of n allowing this kind of symmetry. The condition for
symmetry would read as
m
Y

(Rk + R1 ) =

k=1

m
Y

(Rk − R1 ) ,

k=1

 = (2nq) − 1 .
7

Quantization in Astrophysics ...

64

(15)

The condition states that the odd part of the polynomial in question vanishes. Both  and − solutions so that nq) and 1 − nq) are solutions. If one
requires that the condition holds true for all values of m then the comparison of constant terms in these polynomials allows to conclude that  = 0
is the only universal solution. Since  is free parameter, it is clear that the
m:th order polynomial in question has at most m solutions which could correspond to other fractionized eigenvalues expected to be present on basis of
physical intuition.
This picture generalizes also to the case of even n so that also now
solutions of the form of Eq. 14 are possible. In this case the condition is
m
Y

m
Y

(Rk + R1 ) = −

(Rk − R1 ) .

(16)

k=1

k=1

Obviously  = 0 and thus n = 1/2 fails to be a solution to the eigenvalue
equation in this case. Also now one has the spectral symmetry n± = 1/2±.
The symmetry Rn = (−1)m Rn+m−1 = (−1)m Rn−m−1 = (−1)m Rm−n+1
can be applied to show that the polynomials associated with  and − contain
both the terms Rn −  and Rn +  as factors except for odd m for n =
(m + 1)/2. Hence the values of n can be written for even values of m as
nq) (n) =

1
Rn
m
±
, n = 1, ...,
,
2 2R1
2

(17)

and for odd values of m as
q)

1
Rn
m+1
±
, n = 1, ...,
−1 ,
2 2R1
2
= 1/2 .

n± (n) =
nq)

(18)

Plus sign obviously corresponds to the solutions which reduce to polynomials
and to nq) ' n for large m. The explicit expression for nq) reads as
q)

n± (n) =

1 (sin2 (π(n − 1)/2m) − sin2 (πn/2m))
±
.
2
2sin2 (π/2m)

(19)

At the limit of large m one has
q)

q)

n+ (n) ' n , n− (n) ' 1 − n .
8

Quantization in Astrophysics ...

65

(20)

so that the fractionization n ' 1/k claimed by Mills is not obtained at
this limit. The minimum for |nq) | satisfies |nq) | < 1 and its smallest value
|nq) | = .7071 corresponds to m = 4. Thus these zeros cannot correspond to
nq) ' 1/k yielded by the numerical computation for even values of m based
on the requirement that the zero of P 2m) cancels the pole of the geometric
series.

7

Some comments

Some closing comments are in order.
a) An open question is whether there are also zeros |nq) | > 1 satisfying
2m)
Pa ((1/a)1/2m ) = 0 for even values of m.
b) The treatment above is not completely general since only s-waves
are discussed. The generalization is however a rather trivial replacement
(1 − x)d/dx → (l + 1 − x)d/dx in the Laguerre equation to get associated
Laguerre equation. This modifies only the formula for an+1 in the recursion
for ln so that expression for nq) , which depends on bn :s only, is not affected.
Also the product of numerators in the formula for the parameter a = l2m /l0
remains invariant so that the general spectrum has the spectral symmetry
nq) → 1 − nq) . The only change to the spectrum occurs for even values of m
and is due to the dependence of x∞ = (1/a)1/2m on l and can be understood
in the semiclassical picture. It might happen that the value of l is modified
to its q counterpart corresponding to q-Legendre functions.
c) The model could explain the findings of Mills and nq) ' 1/k for k > 2
also fixes the value of corresponding m to a very high degree so that one
would have direct experimental contact with generalized imbedding space,
spectrum of Planck constants, and dark matter.
d) The obvious question is whether q-counterparts of angular momentum
eigenstates (idfm /dφ = mfm ) are needed and whether they make sense. The
basic idea of construction is that the phase transition changing h̄ does not
involve any other modifications except fractionization of angular momentum
eigenvalues and momentum eigenvalues having purely geometric origin. One
can however ask whether it is possible to identify q-plane waves as ordinary
plane waves. Using the definition Lz = 1/2(∂uq + ∂uq ), u = exp(iφ), one
obtains fn = exp(inφ) and eigenvalues as nq) = Rn /R1 → n for m → ∞.
Similar construction applies in the case of momentum components.
d) The obvious question is whether q-counterparts of angular momentum
eigenstates (idfm /dφ = mfm ) are needed and whether they make sense. The
basic idea of construction is that the phase transition changing h̄ does not

9

Quantization in Astrophysics ...

66

involve any other modifications except fractionization of angular momentum
eigenvalues and momentum eigenvalues having purely geometric origin. One
can however ask whether it is possible to identify q-plane waves as ordinary
plane waves. Using the definition Lz = 1/2(∂uq + ∂uq ), u = exp(iφ), one
obtains fn = exp(inφ) and eigenvalues as nq) = Rn /R1 → n for m → ∞.
Similar construction applies in the case of momentum components.

References
[tgdfree] M. Pitkänen (2006), TGD and Fringe Physics.
http://www.helsinki.fi/∼matpitka/freenergy/freenergy.html.
[G2] The chapter The Notion of Free Energy and Many-Sheeted Space-Time
Concept of [tgdfree].
http://www.helsinki.fi/∼matpitka/freenergy/freenergy.html#freenergy.
[1] R. Mills et al(2003), Spectroscopic and NMR identification of novel hybrid ions in fractional quantum energy states formed by an exothermic
reaction of atomic hydrogen with certain catalysts.
http://www.blacklightpower.com/techpapers.html .
[2] D. S. Moak (1981), The q-analogue of the Laguerre polynomials, J.
Math. Anal. Appl. 81 20 - 47.

10

Quantization in Astrophysics ...

67

On quantization in astrophysics
Fu Yuhua
China Offshore Oil Research Center, Beijing, 100027, China
E-mail: fuyh@cnooc.com.cn

1. Some Problems Cannot Be Solved By Relativity
1.1. Special Twin Paradox That Two Brothers’ States of Motion Are Quite Same
According to theory of relativity: Supposing they are pair of twins, the younger brother
keeps on the Earth, the elder brother roams through the outer space as a astronaut. As
the elder brother returns to the Earth, he will be much younger than his younger brother.
The twin paradox means: Because the movement is relative, also may think the younger
brother is carrying on the space navigation, therefore the younger brother should be much
younger than the elder brother. Such two conclusions mutually conflict.
The explanation to this twin paradox given by the theory of relativity is as follows: Two
brothers' states of motion are different. Thereupon we may make another special twin
paradox that two brothers’ states of motion are quite same. If the younger brother doesn’t
keep on the Earth, but the elder brother and the younger brother all ride their respective
high speed airships, facing the completely opposite directions to navigate from the
identical time and the identical site with the same speed along a straight line, after a quite
long period they begin to decelerate simultaneously until static, then they turn around to
navigate again along the same straight line with the manner of front to front, finally
simultaneously return to the starting point. From the younger brother's viewpoint that,
according to the theory of relativity, the elder brother should be much younger than the
younger brother; Similarly, from the elder brother's viewpoint that, according to the theory
of relativity, the younger brother should be much younger than the elder brother. Who is
much younger to the end?
With the theory of relativity, how to explain this special twin paradox that two brothers’
states of motion are quite same?
2. Quantization in Astrophysics Realized by Fractal Method
As everybody knows, the energy formula proposed by Planck is as follows

En = nhν
where: n =0，1，2，…
The quantization concept was introduced from this example.
Similarly, the quantization in astrophysics realized by fractal method, will be allowed
to reach this goal through taking some variables in the fractal formula for the integer.
Recently, fractal method has been successful used in many fields, it is used for opening
out the deeply hidden organized structure in the complicated phenomenon. The quantity for
reflecting the character of organized structure is called the fractal dimension, expressed with
the value of

D.

Quantization in Astrophysics ...

68

The fractal distribution may be defined as follows [1]

N=

C
rD

1

（ ）

where: r is the characteristic scale, such as velocity, mass, time, length and so on, in

astrophysics r may be taken for the planetary orbital motion average velocity, mass,
equatorial radius, volume, average density, orbital semi-major axis and so on; N is the
quantity related with the value of r , such as price, temperature, height and so on, in order to
realize the quantization in astrophysics,

N may be taken for the planetary arrange

sequence number and so on, namely N =1，2，3，…; C is a constant to be determined,

D is the fractal dimension.
It should be noted that in order to realize the quantization in astrophysics, r also

may be taken for the integer, namely r =1，2，3，…. But we will not consider this case
here.
In the fractal methods for general application at present, the fractal dimension D is a
constant, this kind of fractal may be called constant dimension fractal. It is a straight line on the
double logarithmic coordinates. for example the values of fractal dimension D for different
coastlines may be taken as 1.02, 1.25 and so on. But for the non-straight line functional
relation in the double logarithmic coordinates, it is unable to process with the constant
dimension fractal. While many questions are belonging to this kind of situation. In order to
overcome this difficulty, we introduced the concept of variable dimension fractal in references
[ 2 ] ~ [ 4 ], namely the fractal dimension D is the function of characteristic scale r .

D = F (r )
Furthermore, we also introduced the concept of complex number dimension fractal (the
D was taken for complex number) in reference [ 5 ]; in reference [ 6, 7 ],
we introduced the concept of fractal series, in which the exponents of Taylor series and the
like were changed from integer to fraction.
Now we analyze the fractal quantization structure of the nine planets in solar system,
and forecast the related parameter of the tenth planet. This example is taken from
reference [ 8 ].
The fractal quantization structure of the nine planets in solar system, may be fitted
with two fractal straight lines on the double logarithmic coordinates.
Above all we discuss the orbital motion average velocities of the nine planets (unit:
km/s), taking the characteristic dimension r for some planet orbital motion average
velocity, taking the value of N for the serial number according to the size of the orbital
motion average velocity, firstly considering the case of Mercury r=47.89, then we have
N=1 (Mercury's orbital motion average velocity is the greatest), thereupon we have the
coordinates point (47.89, 1), according to analogizes other 8 planets coordinates points
are as follows: (35.03, 2), (29.79, 3), (24.13, 4), (13.06, 5), (9.64, 6), (6.81, 7), (5.43, 8),
(4.74, 9). The above 9 coordinates points may be plotted on the double logarithmic
coordinates (r, N), they are fitted well with two straight lines, carrying on the fitting of
these two straight lines with least squares method, we have the following results: the

fractal dimension

Quantization in Astrophysics ...

69

fractal parameters of the straight line corresponding to the coordinates points with N=1,
2, 3 are as follows: C=7609, D=2.31; the fractal parameters of the straight line
corresponding to the coordinates points with N=4, 5, 6, 7, 8, 9 are as follows:
C=17.56186, D=0.4647264. Therefore, when the tenth planet is located outside Pluto,
its orbital motion average velocity will be smaller than 4.74, namely its value of N should
be equal to 10, from Eq. (1) we have

r=(

C 1/ D
)
N

2

（ ）

Substituting N=10, C=17.56186 and D=0.4647264 into Eq. (2), it gives the value of r,
namely when the tenth planet is located outside Pluto, its orbital motion average velocity is
equal to 3.3594 km/s.
Some scientists believed that, the tenth planet also possibly is located inside Uranus,
here the known nine planets coordinates points should be as follows: (47.89, 1), (35.03, 2),
(29.79, 3), (24.13, 4), (13.06, 5), (9.64, 6), (6.81, 8), (5.43, 9), (4.74, 10), this time the
value of N corresponding to the tenth planet should be equal to 7, namely N=7, by using
the interpolation method to obtain its orbital motion average velocity is equal to 8.76 km/s.
With the similar method we also have: When the tenth planet is located outside Pluto,
to compare with Earth (taking the value of Earth as 1), its mass, equatorial radius, volume,
average density, orbital semi-major axis respectively are as follows: 0.00032, 0.145,
0.0028, 0.10, 64.36. If the tenth planet is located inside Uranus, its orbital semi-major axis
should be equal to 15.94 to compare with Earth.

References

[1] Turcotte D L. Fractals and chaos in geology and geophysics. Cambridge: Cambridge University Press,
1992. 1~22

[2] Fu Yuhua. Variable dimension fractal in fluid mechanics. Proc. 2nd national numeration hydraulics
conference. 1993:202~207 (in Chinese)

[3] Fu Yuhua. Improvement of fractal technique on the application in oil and gas prospecting: variable
dimension fractal technique. China Offshore Oil and Gas (Geology). 1994, ( 3): 210~214 (in Chinese)

[4] Fu Yuhua.

Analyzed

and

fractal

single point method for

solving hydraulic problems in ocean

engineering (i.e., SPE(Society of Petroleum Engineers) 29986). International meeting on petroleum
engineering, Beijing, Nov 1995. 347~356
[5]

Fu Yuhua. Fractal in Complex Domain and Its Application in Ocean Hydrology. China Offshore Oil and Gas
(Engineering). 2003, (1): 32~33 (in Chinese)

[6]

，

Fu Yuhua. Fractal Series Solution for Mechanics Problem. Journal of Southwest Jiaotong University 1999, (5)
(in Chinese)

[7]

Fu Yuhua. Application of Fractal Series Perturbation Method in Vibration Problem. Structure &

，2001, (1)：62~64 (in Chinese)

Environment Engineering
[8]

Fu Yuhua. Science Unsettled Question  Tenth Planet. Scientific Chinese, 1999, (3) (in Chinese)

Quantization in Astrophysics ...

70

On Nonlinear Quantum Mechanics,
Brownian Motion, Weyl Geometry and
Fisher Information
Carlos Castro1 , Jorge Mahecha2
1

Center for Theoretical Studies of Physical Systems,

Clark Atlanta University, Atlanta, Georgia, USA, castro@ctsps.cau.edu
2

Institute of Physics, University of Antioquia, Medellı́n, Colombia, mahecha@fisica.udea.edu.co

February 2005
Abstract
A new nonlinear Schrödinger equation is obtained explicitly from the
(fractal) Brownian motion of a massive particle with a complex-valued
diffusion constant. Real-valued energy plane-wave solutions and solitons
exist in the free particle case. One remarkable feature of this nonlinear
Schrödinger equation based on a ( fractal) Brownian motion model, over
all the other nonlinear QM models, is that the quantum-mechanical energy
functional coincides precisely with the field theory one. We finalize by
showing why a complex momentum is essential to fully understand the
physical implications of Weyl’s geometry in QM, along with the interplay
between Bohm’s Quantum potential and Fisher Information which has
been overlooked by several authors in the past.

PACS numbers: 03.65, 05.40.J, 47.53, 04.20.G

1

Introduction

Over the years there has been a considerable debate as to whether linear QM
can fully describe Quantum Chaos. Despite that the quantum counterparts of
classical chaotic systems have been studied via the techniques of linear QM, it is
our opinion that Quantum Chaos is truly a new paradigm in physics which is associated with non-unitary and nonlinear QM processes based on non-Hermitian
operators (implementing time symmetry breaking). This Quantum Chaotic behavior should be linked more directly to the Nonlinear Schrödinger equation
without any reference to the nonlinear behavior of the classical limit. For this
reason, we will analyze in detail the fractal geometrical features underlying our
Nonlinear Schrödinger equation obtained in [6].

1

Quantization in Astrophysics ...

72

Nonlinear QM has a practical importance in different fields, like condensed
matter, quantum optics and atomic and molecular physics; even quantum gravity may involve nonlinear QM. Another important example is in the modern
field of quantum computing. If quantum states exhibit small nonlinearities during their temporal evolution, then quantum computers can be used to solve
NP-complete (non polynomial) and #P problems in polynomial time. Abrams
and Lloyd [19] proposed logical gates based on non linear Schrödinger equations
and suggested that a further step in quantum computing consists in finding
physical systems whose evolution is amenable to be described by a NLSE.
On other hand, we consider that Nottale and Ord’s formulation of quantum
mechanics [1], [2] from first principles based on the combination of scale relativity and fractal space-time is a very promising field of future research. In this
work we extend Nottale and Ord’s ideas to derive the nonlinear Schrödinger
equation. This could shed some light on the physical systems which could be
appropriately described by the nonlinear Schrödinger equation derived in what
follows.
The contents of this work are the following : In section 2 we derive the
nonlinear Schrödinger equation by extending Nottale-Ord’s approach to the case
of a fractal Brownian motion with a complex diffusion constant. We present
a thorough analysis of such nonlinear Schrödinger equation and show why it
cannot linearized by a naive complex scaling of the wavefunction ψ → ψ λ .
Afterwards we will describe the explicit interplay between Fisher Information, Weyl geometry and the Bohm’s potential by introducing an action based
on a complex momentum. The connection between Fisher Information and
Bohm’s potential has been studied by several authors [24], however the importance of introducing a complex momentum Pk = pk + iAk (where Ak is the
Weyl gauge field of dilatations) in order to fully understand the physical implications of Weyl’s geometry in QM, along with the interplay between Bohm’s
quantum potential and Fisher Information, has been overlooked by several authors in the past [24], [25]. For this reason we shall review in section 3 the
relationship between Bohm’s Quantum Potential and the Weyl curvature scalar
of the Statistical ensemble of particle-paths ( an Abelian fluid ) associated to a
single particle that was initially developed by [22] . A Weyl geometric formulation of the Dirac equation and the nonlinear Klein-Gordon wave equation was
provided by one of us [23]. In the final section 4 , we summarize our conclusions
and include some additional comments.

2

Nonlinear QM as a fractal Brownian motion
with a complex diffusion constant

We will be following very closely Nottale’s derivation of the ordinary Scrödinger
equation [1]. The readers familiar with this work may omit this section. Recently Nottale and Celerier [1] following similar methods were able to derive the
Dirac equation using bi-quaternions and after breaking the parity symmetry

2

Quantization in Astrophysics ...

73

dxµ ↔ −dxµ , see references for details. Also see the Ord’s paper [2] and the
Adlers’s book on quaternionic QM [16]. For simplicity the one-particle case is
investigated, but the derivation can be extended to many-particle systems. In
this approach particles do not follow smooth trajectories but fractal ones, that
can be described by a continuous but non-differentiable fractal function ~r(t).
The time variable is divided into infinitesimal intervals dt which can be taken
as a given scale of the resolution.
Then, following the definitions given by Nelson in his stochastic QM approach (Lemos in [12] p. 615; see also [13, 14]), Nottale define mean backward
an forward derivatives as follows,


~r(t + ∆t) − ~r(t)
d±~r(t)
= lim
,
(1)
∆t→±0
dt
∆t
from which the forward and backward mean velocities are obtained,
d±~r(t) ~
= b± .
dt

(2)

For his deduction of Schrödinger equation from this fractal space-time classical mechanics, Nottale starts by defining the complex-time derivative operator




δ
1 d+
d−
1 d+
d−
=
+
−i
−
,
(3)
dt
2 dt
dt
2 dt
dt
which after some straightforward definitions and transformations takes the following form,
δ
∂
~ ·∇
~ − iD∇2 .
=
+V
(4)
dt
∂t
D is a real-valued diffusion constant to be related to the Planck constant. Now
we are changing the meaning of D, since no longer a symbol for the fractal
dimension is needed, it will have the value 2.
The D comes from considering that the scale dependent part of the velocity
is a Gaussian stochastic variable with zero mean, (see de la Peña at [12] p. 428)
hdξ±i dξ±j i = ±2Dδij dt.

(5)

~ proportional to the ζ,
~ amount
In other words, the fractal part of the velocity ξ,
to a Wiener process when the fractal dimension is 2.
Afterwards, Nottale defines a set of complex quantities which are generalization of well known classical quantities (Lagrange action, velocity, momentum,
etc), in order to be coherent with the introduction of the complex-time derivative operator. The complex time dependent wave function ψ is expressed in
terms of a Lagrange action S by ψ = eiS/(2mD) . S is a complex-valued action
but D is real-valued. The velocity is related to the momentum, which can be
~
expressed as the gradient of S, p~ = ∇S.
Then the following known relation is
found,
~ = −2iD∇
~ ln ψ.
V
(6)
3

Quantization in Astrophysics ...

74

The Schrödinger equation is obtained from the Newton’s equation (force =
~ in terms of the wave
mass times acceleration) by using the expression of V
function ψ,
~ =mδV
~ = −2imD δ ∇
~ ln ψ.
−∇U
(7)
dt
dt
Replacing the complex-time derivation (4) in the Newton’s equation gives
us




∂ ~
∇2 ψ
~
~
−∇U = −2im D ∇ ln ψ − 2D∇ D
.
(8)
∂t
ψ
~ operator were used by Nottale. Integrating
Simple identities involving the ∇
this equation with respect to the position variables finally yields
D2 ∇2 ψ + iD

U
∂ψ
−
ψ = 0,
∂t
2m

(9)

up to an arbitrary phase factor which may set to zero. Now replacing D by
h̄/(2m), we get the Schrödinger equation,
ih̄

∂ψ
h̄2 2
+
∇ ψ = U ψ.
∂t
2m

(10)

The Hamiltonian operator is Hermitian, this equation is linear and clearly is
homogeneous of degree one under the substitution ψ → λψ.
Having reviewed Nottale’s work [1] we can generalize it by relaxing the assumption that the diffusion constant is real; we will be working with a complexvalued diffusion constant; i.e. with a complex-valued h̄. This is our new contribution. The reader may be immediately biased against such approach because
the Hamiltonian ceases to be Hermitian and the energy becomes complex-valued.
However this is not always the case. We will explicitly find plane wave solutions
and soliton solutions to the nonlinear and non-Hermitian wave equations with
real energies and momenta. For a detailed discussion on complex-valued spectral
representations in the formulation of quantum chaos and time-symmetry breaking see [10]. Nottale’s derivation of the Schrödinger equation in the previous
section required a complex-valued action S stemming from the complex-valued
velocities due to the breakdown of symmetry between the forwards and backwards velocities in the fractal zigzagging. If the action S was complex then
it is not farfetched to have a complex diffusion constant and consequently a
complex-valued h̄ (with same units as the complex-valued action).
Complex energy is not alien in ordinary linear QM. They appear in optical
potentials (complex) usually invoked to model the absorption in scattering processes [8] and decay of unstable particles. Complex potentials have also been
used to describe decoherenc. The accepted way to describe resonant states in
atomic and molecular physics is based on the complex scaling approach, which
in a natural way deals with complex energies [17]. Before, Nottale wrote,
hdζ± dζ± i = ±2Ddt,

4

Quantization in Astrophysics ...

75

(11)

with D and 2mD = h̄ real. Now we set
hdζ± dζ± i = ±(D + D∗ )dt,

(12)

with D and 2mD = h̄ = α + iβ complex. The complex-time derivative operator
becomes now
δ
∂
~ ·∇
~ − i (D + D∗ )∇2 .
=
+V
(13)
dt
∂t
2
In the real case D = D∗ . It reduces to the complex-time-derivative operator
described previously by Nottale. Writing again the ψ in terms of the complex
action S,
ψ = eiS/(2mD) = eiS/h̄ ,
(14)
where S, D and h̄ are complex-valued, the complex velocity is obtained from
~ as
the complex momentum p~ = ∇S
~ = −2iD∇
~ ln ψ.
V

(15)

The NLSE is obtained after we use the generalized Newton’s equation (force
= mass times acceleration) in terms of the ψ variable,
~ =m
−∇U

δ ~
δ ~
V = −2imD ∇
ln ψ.
dt
dt

(16)

Replacing the complex-time derivation (13) in the generalized Newton’s
equation gives us


~ = 2im D ∂ ∇
~ ln ψ − 2iD2 (∇
~ ln ψ · ∇)(
~ ∇
~ ln ψ) − i (D + D∗ )D∇2 (∇
~ ln ψ) .
∇U
∂t
2
(17)
~ (ii): 2(∇
~ ln ψ · ∇)(
~ ∇
~ ln ψ) =
~ 2 = ∇2 ∇;
Now, using the three identities (i): ∇∇
~ ∇
~ ln ψ)2 ; and (iii): ∇2 ln ψ = ∇2 ψ/ψ − (∇
~ ln ψ)2 allows us to integrate such
∇(
equation above yielding, after some straightforward algebra, the NLSE
2
∂ψ
h̄2 α 2
h̄2 β  ~
=−
∇ ψ + Uψ − i
∇ ln ψ ψ.
(18)
∂t
2m h̄
2m h̄
Note the crucial minus sign in front of the kinematic pressure term and that
h̄ = α + iβ = 2mD is complex. When β = 0 we recover the linear Schrödinger
equation.
The nonlinear potential is now complex-valued in general. Defining
ih̄

W = W (ψ) = −

2
h̄2 β  ~
∇ ln ψ ,
2m h̄

and U the ordinary potential, then the NLSE can be rewritten as


∂ψ
h̄2 α 2
ih̄
= −
∇ + U + iW ψ.
∂t
2m h̄
5

Quantization in Astrophysics ...

76

(19)

(20)

This is the fundamental nonlinear wave equation of this work. It has the form
of the ordinary Schrödinger equation with the complex potential U + iW and
the complex h̄. The Hamiltonian is no longer Hermitian and the potential
V = U + iW (ψ) itself depends on ψ. Nevertheless one could have meaningful
physical solutions with real valued energies and momenta, like the plane-wave
and soliton solutions studied in the next section. Some important remarks are
now in order.
• Notice that the NLSE above cannot be obtained by a naive scaling of the
wavefunction
ψ = eiS/h̄o → ψ 0 = eiS/h̄ = e(iS/h̄o )(h̄o /h̄) = ψ λ = ψh̄o /h̄ . h̄ = real.

(21)

related to a scaling of the diffusion constant h̄o = 2mDo → h̄ = 2mD . Upon
performing such scaling, the ordinary linear Schrödinger equation in the variable ψ will appear to be nonlinear in the new scaled wavefunction ψ 0
2
h̄2 h̄o 2 0
h̄2
h̄o  ~
∂ψ 0
=−
∇ ψ + U ψ0 −
(1 − ) ∇
ln ψ 0 ψ 0 .
(22)
∂t
2m h̄
2m
h̄
but this apparent nonlinearity is only an artif act of the change of variables (
the scaling of ψ ).
Notice that the latter (apparent) nonlinear equation , despite having the
same form as the NLSE , obtained from a complex-diffusion constant, differs
crucially in the actual values of the coefficients multiplying each of the terms.
The NLSE has the complex coefficients α/h̄ (in the kinetic terms), and −iβ/h̄
(in the nonlinear logarithmic terms) with h̄ = α + iβ = complex. However, the
nonlinear equation obtained from a naive scaling involves real and dif f erent
numerical coefficients than those present in the NLSE . Therefore, the genuine
NLSE cannot be obtained by a naive scaling (redefinition) of the ψ and the
diffusion constant.
Notice also that even if one scaled ψ by a complex exponent ψ → ψ λ with λ =
h̄o /h̄ and h̄ = complex, the actual numerical values in the apparent nonlinear
equation, in general, would have still been different than those present in the
NLSE . However, there is an actual equivalence, if, and only if, the scaling
exponent λ = h̄o /h̄ obeyed the condition:
ih̄

α
h̄ − iβ
β
h̄o
=1− =1−
=i
(23)
h̄
h̄
h̄
h̄
in this very special case, the NLSE would be obtained from a linear Schroedinger
equation after scaling the wavefunction ψ → ψ λ with a complex exponent
λ = h̄o /h̄ = α/h̄. In this very special and restricted case, the NLSE could
be linearized by a scaling of the wavefunction with complex exponent.
From this analysis one
p infers, immediately, that if one defines the norm of
the complex h̄ : ||h̄|| = α2 + β 2 = h̄o to coincide precisely with the observed
value h̄o of Planck’s constant, then α 6= h̄o , iβ 6= h̄ − h̄o and, consequently, the
NLSE cannot be obtained from the ordinary (linear) Schroedinger equations
α = h̄o ⇒ 1 −

6

Quantization in Astrophysics ...

77

after a naive scaling, with a complex exponent, ψ → ψ λ = ψh̄o /h̄ . Therefore,
a complex
p diffusion constant 2mD = h̄ = α + iβ, with the condition 2m||D|| =
||h̄|| = α2 + β 2 = h̄o ( observed value of Planck’s constant ) ensures that the
NLSE is not a mere artifact of the scaling of the wavefunction ψ → ψ λ = ψh̄o /h̄
in the ordinary linear Schroedinger equation.
It is important to emphasize that the diffusion constant is always chosen to
be related to Planck constant as follows: 2m||D|| = ||h̄|| = h̄o which is just
the transition length from a fractal to a scale-independence non-fractal regime
discussed by Nottale in numerous occasions. In the relativistic scale it is the
Compton wavelength of the particle (say an electron): λc = h̄o /(mc). In the
nonrelativistic case it is the de Broglie wavelength of the electron.
Therefore, the NLSE based on a fractal Brownian motion with a complex
valued diffusion constant 2mD = h̄ = α + iβ represents truly a new physical
phenomenon and a hallmark of nonlinearity in QM. For other generalizations
of QM see experimental tests of quaternionic QM (in the book by Adler [16]).
Equation (18) is the fundamental NLSE of this work.
• A Fractal Scale Calculus description of our NLSE was developed later on
by Cresson [20] who obtained, on a rigorous mathematical footing, the same
functional form of our NLSE equation above ( although with different complex
numerical coefficients) by using Nottale’s fractal scale-calculus that obeyed a
quantum bialgebra. A review of our NLSE was also given later on by [25]. Our
nonlinear wave equation originated from a complex-valued diffusion constant
that is related to a complex-valued extension of Planck’s constant. Hence, a
fractal spacetime is deeply ingrained with nonlinear wave equations as we have
shown and it was later corroborated by Cresson [20].
• Complex-valued viscosity solutions to the Navier-Stokes equations were
also analyzed by Nottale leading to the Fokker-Planck equation. Clifford-valued
extensions of QM were studied in [21] C-spaces (Clifford-spaces whose enlarged
coordinates are polyvectors, i.e antisymmetric tensors) that involved a Cliffordvalued number extension of Planck’s constant; i.e. the Planck constant was
a hypercomplex number. Modified dispersion relations were derived from the
underlying QM in Clifford-spaces that lead to faster than light propagation in
ordinary spacetime but without violating causality in the more fundamental
Clifford spaces. Therefore, one should not exclude the possibility of having
complex-extensions of the Planck constant leading to nonlinear wave equations
associated with the Brownian motion of a particle in fractal spacetimes.
• Notice that the NLSE (34) obeys the homogeneity condition ψ → λψ for
any constant λ. All the terms in the NLSE are scaled respectively by a factor
λ. Moreover, our two parameters α, β are intrinsically
connected to a complex
p
Planck constant h̄ = α + iβ such that ||h̄|| = α2 + β 2 = h̄o (observed Planck’s
constant ) rather that being ah-hoc constants to be determined experimentally.
Thus, the nonlinear QM equation derived from the fractal Brownian motion with
complex-valued diffusion coefficient is intrinsically tied up with a non-Hermitian
Hamiltonian and with complex-valued energy spectra [10].
• Despite having a non-Hermitian Hamiltonian we still could have eigen-

7

Quantization in Astrophysics ...

78

functions with real valued energies and momenta. Non-Hermitian Hamiltonians
( pseudo-Hermitian) have captured a lot of interest lately in the so-called P T
symmetric complex extensions of QM and QFT [27]. Therefore these ideas
cannot be ruled out and they are the subject of active investigation nowadays.

3

Complex Momenta, Weyl Geometry, Bohm’s
Potential and Fisher Information

Despite that the interplay between Fisher Information and Bohm’s potential has
been studied by several authors [24] the importance of introducing a complex
momentum Pk = pk + iAk in order to fully understand the physical implications
of Weyl’s geometry in QM has been overlooked by several authors [24], [25].
We shall begin by reviewing the relationship between the Bohm’s Quantum
Potential and the Weyl curvature scalar of the Statistical ensemble of particlepaths ( a fluid ) associated to a single particle and that was developed by
[22] . A Weyl geometric formulation of the Dirac equation and the nonlinear
Klein-Gordon wave equation was provided by one of us [23]. Afterwards we will
describe the interplay between Fisher Information and the Bohm’s potential by
introducing an action based on a complex momentum Pk = pk + iAk
In the description of [22] one deals with a geometric derivation of the nonrelativistic Schroedinger Equation by relating the Bohm’s quantum potential Q to
the Ricci-Weyl scalar curvature of an ensemble of particle-paths associated to
one particle. A quantum mechanical description of many particles is far more
complex. This ensemble of particle paths resemble an Abelian f luid that permeates spacetime and whose ensemble density ρ affects the Weyl curvature of
spacetime, which in turn, determines the geodesics of spacetime in guiding the
particle trajectories. See [22], [23] for details).
Again a relation between the relativistic version of Bohm’s potential Q
and the Weyl-Ricci curvature exists but without the ordinary nonrelativistic
probabilistic connections. In relativistic QM one does not speak of probability
density to find a particle in a given spacetime point but instead one refers
to the particle number current J µ = ρdxµ /dτ . In [22], [23] one begins with
an ordinary Lagrangian associated with a point particle and whose statistical
ensemble average over all particle-paths is performed only over the random
initial data (configurations) . Once the initial data is specified the trajectories
( or rays ) are completely determined by the Hamilton-Jacobi equations. The
statistical average over the random initial Cauchy data is performed by means
of the ensemble density ρ. It is then shown that the Schroedinger equation
can be derived after using the Hamilton-Jacobi equation in conjunction with
the continuity equation and where the “quantum force” arising from Bohm’s
quantum potential Q can be related to (or described by) the Weyl geometric
properties of space. To achieve this one defines the Lagrangian
L(q, q̇, t) = LC (q, q̇, t) + γ(h̄2 /m)R(q, t).

8

Quantization in Astrophysics ...

79

(24)

where γ = (1/6)(d − 2)/(d − 1) is a dimension-dependent numerical coefficient
and R is the Weyl scalar curvature of the corresponding d-dimensional Weyl
spacetime M where the particle lives.
Covariant derivatives are defined for contravariant vectors V k : V,ık = ∂i V k −
Γkim V m where the Weyl connection coefficients are composed of the ordinary
Christoffel connection plus terms involving the Weyl gauge field of dilatations
i
Ai . The curvature tensor Rmkn
obeys the same symmetry relations as the
curvature tensor of Riemann geometry as well as the Bianchi identity. The
Ricci symmetric tensor Rik and the scalar curvature R are defined by the same
n
formulas also, viz. Rik = Rink
and R = g ik Rik .
√
√
RW eyl = RRiemann + (d − 1)[ (d − 2)Ai Ai − 2(1/ g)∂i ( gAi ) ].

(25)

where RRiemann is the ordinary Riemannian curvature defined in terms of the
Christoffel symbols without the Weyl-gauge field contribution.
In the special case that the space is flat from the Riemannian point of view,
after some algebra one can show that the Weyl scalar curvature contains only
the Weyl gauge field of dilatations
RW eyl = (d − 1)(d − 2)(Ak Ak ) − 2(d − 1)(∂k Ak ).

(26)

Now the Weyl geometrical properties are to be derived from physical principles so the Ai cannot be arbitrary but must be related to the distribution of
matter encoded by the ensemble density of particle-paths ρ and can be obtained
by the same (averaged) least action principle giving the motion of the particle.
The minimum is to be evaluated now with respect to the class of all Weyl
geometries having arbitrarily Weyl-gauge fields but with fixed metric tensor .
A variational procedure [22] yields a minimum for
Ai (q, t) = −

1
∂k (log ρ) ⇒ Fij = ∂i Aj − ∂j Ai = 0.
d−2

(27)

which means that the ensemble density ρ is Weyl-covariantly constant
1
∂i (log ρ).
(28)
d−2
where ω(ρ) is the Weyl weight of the density ρ. Since Ai is a total derivative the
length of a vector transported from A to B along dif f erent paths changes by
the same amount . Therefore, a vector after being transported along a closed
path does not change its overall length. This is of fundamental importance to be
able to solve in a satisfactory manner Einstein’s objections to Weyl’s geometry.
If the lengths were to change in a path-dependent manner as one transports
vectors from point A to point B, two atomic clocks which followed different
paths from A to B will tick at dif f erent rates upon arrival at point B .
The continuity equation is
Di ρ = 0 = ∂i ρ + ω(ρ) ρAi = 0 ⇒ Ai (q, t) = −

∂ρ
1
√
+ √ ∂i ( g ρv i ) = 0.
∂t
g
9

Quantization in Astrophysics ...

80

(29)

In this spirit one goes next to a geometrical derivation of the Schroedinger
equation . By inserting
1 ∂ log ρ
.
(30)
Ak = −
d − 2 ∂xk
into
RW eyl = (d − 1)(d − 2)(Ak Ak ) − 2(d − 1)∂k Ak .
(31)
one gets for the Weyl scalar curvature, in the special case that the space is flat
from the Riemannian point of view, the following expression
RW eyl =

1
√
√ (∂i ∂ i ρ).
2γ ρ

(32)

which is precisely equal to the Bohm’s Quantum potential up to numerical
factors.
The Hamilton-Jacobi equation can be written as
∂S
h̄2
+ HC (q, S, t) − γ(
)R = 0
∂t
2m
where the effective Hamiltonian is
HC −γ(h̄2 /m)R =

(33)

h̄2
1 jk ∂S ∂S
h̄2
1 jk
g pj pk +V −γ R =
g
+V
−γ
R (34)
2m
m
2m
∂xj ∂xk
m

When the above expression for the Weyl scalar curvature (Bohm’s quantum
potential given in terms of the ensemble density) is inserted into the HamiltonJacobi equation, in conjunction with the continuity equation , for a momentum
given by pk = ∂k S, one has then a set of two nonlinear coupled partial differential
equations. After some straightforward algebra, one can verify that these two
coupled differential equations equations will lead to the Schroedinger equation
√
after the substitution Ψ = ρ eiS/h̄ is made.
For example, when d = 3, γ = 1/12 and consequently, Bohm’s quantum
potential Q = −(h̄2 /12m)R ( when RRiemann = 0 ) becomes
R=

√
√
1
1 ∆ ρ
h̄2 ∆ ρ
√
√ ∂i g ik ∂k ρ ∼
√ ⇒Q=−
√ .
2γ ρ
2γ
ρ
2m ρ

(35)

as is should be and from the two coupled differential equations, the HamiltonJacobi and the continuity equation, they both reduce to the standard Schroedinger
equation in flat space
∂Ψ(~x, t)
= −(h̄2 /2m)∆Ψ(~x.t) + V Ψ(~x, t).
(36)
∂t
√
after, and only after, one defines Ψ = ρ eiS/h̄ .
If one had a curved spacetime with a nontrivial metric one would obtain the
Schroedinger equation in a curved spacetime manifold by replacing the Laplace
operator by the Laplace-Beltrami operator. This requires, of course, to write
ih̄

10

Quantization in Astrophysics ...

81

the continuity and Hamilton Jacobi equations in a explicit covariant manner by
using the covariant form of the divergence and Laplace operator [22] , [23]. In
this way, the geometric properties of space are indeed affected by the presence of
the particle and in turn the alteration of geometry acts on the particle through
the quantum force fi = γ(h̄2 /m)∂i R which depends on the Weyl gauge potential
Ai and its derivatives. It is this peculiar feedback between the Weyl geometry
of space and the motion of the particle which recapture the effects of Bohm’s
quantum potential.
The formulation above from [22] was also developed for a derivation of the
Klein-Gordon (KG) equation. The Dirac equation and Nonlinear Relativistic QM equations were found by [23] via an average action principle. The
relativistic version of the Bohm potential (for signature (−, +, +, +)) can be
written
√
1 (∂µ ∂ µ ρ)
(37)
Q∼ 2
√
m
ρ
in terms of the D’Alambertian operator.
To finalize this section we will explain why the Bohm-potential/Weyl scalar
curvature relationship in a flat spacetime


h̄2 1 ik
h̄2 g ik 2∂i ∂k ρ ∂i ρ∂k ρ
√
Q=−
−
.
(38)
√ g ∂i ∂k ρ =
2m ρ
8m
ρ
ρ2
encodes already the explicit connection between Fisher Information and the
Weyl-Ricci scalar curvature RW eyl (for Riemann flat spaces) after one realizes
the importance of the complex momentum Pk = pk + iAk . This is typical of
Electromagnetism after a minimal coupling of a charged particle (of charge e)
to the U (1) gauge field Ak is introduced as follows Πk = pk + ieAk . Weyl’s
initial goal was to unify Electromagnetism with Gravity. It was later realized
that the gauge field of Weyl’s dilatations A was not the same as the U (1) gauge
field of Electromagnetism A.
Since we have reviewed the relationship between the Weyl scalar curvature
and Bohm’s Quantum potential, it is not surprising to find automatically a
connection between Fisher information and Weyl Geometry after a complex
momentum Pk = pk + iAk is introduced. A complex momentum has already
been discussed in previous sections within the context of fractal trajectories
moving forwards and backwards in time by Nottale and Ord.
If ρ is defined over an d-dimensional manifold with metric g ik one obtains a
natural definition of the Fisher information associated with the ensemble density
ρ
Z
g ik
1 ∂ρ ∂ρ n
ik
I = g Iik =
d y.
(39)
2
ρ ∂y i ∂y k
In the Hamilton-Jacobi formulation of classical mechanics the equation of motion takes the form
∂S
1 jk ∂S ∂S
+
g
+ V = 0.
(40)
∂t
2m
∂xj ∂xk

11

Quantization in Astrophysics ...

82

The momentum field pj is given by pj = g jk (∂S/∂xk ). The ensembleR probability
density of particle-paths ρ(t, xµ ) obeys the normalization condition dn x ρ = 1
. The continuity equation is
(∂ρ/∂t) +

1 1
√
√ (∂/∂xj )( g ρg jk (∂S/∂xk )) = 0.
m g

(41)

These equations completely describe the motion and can be derived from
the action

Z 
1 jk
S = ρ (∂S/∂t) +
g (∂S/∂xj )(∂S/∂xk ) + V dtdn x.
(42)
2m
using fixed endpoint variation in S and ρ.
The Quantization via the Weyl geometry procedure is obtained by defining
the complex momentum in terms of the Weyl gauge field of dilatations Ak as
Pk = pk + ieAk and constructing the modified Hamiltonian in terms of the
norm-squared of the complex momentum P k Pk∗ as follows
HW eyl =

g jk
[(pj + ieAj )(pk − ieAk )] + V
2m

The modif ied action is now :


Z
∂S
g jk
SW eyl = dtdn x
+
(pj + ieAj )(pk − ieAk ) + V .
∂t
2m

(43)

(44)

The relationship between the Weyl gauge potential and the ensemble density
ρ was
∂log(ρ)
Ak ∼
.
(45)
∂xk
the proportionality factors can be re-absorbed into the coupling constant e as
follows Pk = pk + ieAk = pk + i ∂k (log ρ). Hence, when the spacetime metric is
flat ( diagonal ) g jk = δ jk , SW eyl becomes


∂S
g jk
∂S
∂log(ρ)
∂S
∂log(ρ)
SW eyl = dtd x
+
(
+i
) ( k −i
) +V =
∂t
2m ∂xj
∂xj
∂x
∂xk


Z
Z
∂S
g jk ∂S
1 ∂ρ 2
∂S
1
n
dtd x
+V +
(
dtdn x [
)(
) +
] .
(46)
∂t
2m ∂xj ∂xk
2m
ρ ∂xk
Z

n

The expectation value of SW eyl is

Z

dtdn x ρ



< SW eyl > = < SC > + SF isher =
(47)

Z
jk
∂S
g
∂S
∂S
1
1 ∂ρ 2
+
(
)(
)+V +
dtdn x ρ [
] .
∂t
2m ∂xj ∂xk
2m
ρ ∂xk

12

Quantization in Astrophysics ...

83

This is how we have reproduced the Fisher Information expression directly
from the last term of < SW eyl > :
Z
1
1 ∂ρ 2
SF isher ≡
]
(48)
dtdn x ρ [
2m
ρ ∂xk
An Euler variation of the expectation value of the action < SW eyl > with
respect to the ρ yields :
∂S
δ < SW eyl >
δ < SW eyl >
+
− ∂j (
) = 0 ⇒
(49)
∂t
δρ
δ (∂j ρ)



∂S
1 jk ∂S ∂S
1 ∂ρ ∂ρ
2 ∂2ρ
+V +
g
+
−
=0
∂t
2m
∂xj ∂xk
ρ2 ∂xj ∂xk
ρ ∂xj ∂xk
Notice that the last term of the Euler variation


1 jk
1 ∂ρ ∂ρ
2 ∂2ρ
g
−
2m
ρ2 ∂xj ∂xk
ρ ∂xj ∂xk

(50)

(51)

is precisely the same as the Bohm’s quantum potential , which in turn, is proportional to the Weyl scalar curvature. If the continuity equation is implemented
at this point one can verify once again that the last equation is equivalent to
√
the Schrödinger equation after the replacement Ψ = ρ eiS/h̄ is made.
Notice that in the Euler variation variation of < SW eyl > w.r.t the ρ one
must include those terms involving the derivatives of ρ as follows
−∂j (

δ [ ρ (∂k ρ/ρ)2 ]
1
δ (∂k ρ)2
2
) = − ∂j (
) = − ∂j ∂ j ρ.
δ (∂j ρ)
ρ
δ (∂j ρ)
ρ

(52)

This explains the origins of all the terms in the Euler variation that yield Bohm’s
quantum potential.
Hence, to conclude, we have shown how the last term of the Euler variation
of the averaged action < SW eyl > , that automatically incorporates the Fisher
Information expression after a complex momentum Pk = pk + i∂k (log ρ) is
introduced via the Weyl gauge field of dilations Ak ∼ −∂k log ρ, generates once
again Bohm’s potential :


1 ∂ρ ∂ρ
2 ∂2ρ
Q∼
−
.
(53)
ρ2 ∂xj ∂xk
ρ ∂xj ∂xk
To conclude, the Quantization of a particle whose Statistical ensemble of
particle-paths permeate a spacetime background endowed with a Weyl geometry allows to construct a complex momentum Pk = ∂k S + i∂k (log ρ) that
yields automatically the Fisher Information SF isher term. The latter Fisher
Information term is crucial in generating Bohm’s quantum potential Q after an
Euler variation of the expectation value of the < SW eyl > with respect to the
ρ is performed. Once the Bohm’s quantum potential is obtained one recovers
the Schroedinger equation after implementing the continuity equation and per√
forming the replacement Ψ = ρ eiS/h̄ . This completes the relationship among
Bohm’s potential, the Weyl scalar curvature and Fisher Information af ter introducing a complex momentum.
13

Quantization in Astrophysics ...

84

4

Concluding Remarks

Based on Nottale and Ord’s formulation of QM from first principles; i.e. from
the fractal Brownian motion of a massive particle we have derived explicitly a
nonlinear Schrödinger equation. Despite the fact that the Hamiltonian is not
Hermitian, real-valued energy solutions exist like the plane wave and soliton
solutions found in the free particle case. The remarkable feature of the fractal
approach versus all the Nonlinear QM equation considered so far is that the
Quantum Mechanical energy functional coincides precisely with the field theory
one.
It has been known for some time, see Puskarz [8], that the expression for
the energy functional in nonlinear QM does not coincide with the QM energy
functional, nor it is unique. The classic Gross-Pitaveskii NLSE (of the 1960’),
based on a quartic interaction potential energy, relevant to Bose-Einstein condensation, contains the nonlinear cubic terms in the Schrödinger equation, after
differentiation, (ψ ∗ ψ)ψ. This equation does not satisfy the Weinberg homogeneity condition [9] and also the energy functional differs from the EQM by factors
of two.
However, in the fractal-based NLSE there is no discrepancy between the
quantum-mechanical energy functional and the field theory energy functional.
Both are given by
LSE
HfNractal
=−

h̄2 β ∗ ~
h̄2 α ∗ 2
ψ ∇ ψ + U ψ∗ ψ − i
ψ (∇ ln ψ)2 ψ.
2m h̄
2m h̄

(54)

This is why we push forward the NLSE derived from the fractal Brownian
motion with a complex-valued diffusion coefficient. Such equation does admit
plane-wave solutions with the dispersion relation E = p~2 /(2m). It is not hard
to see that after inserting the plane wave solution into the fractal-based NLSE
we get (after setting U = 0),
E=

h̄2 α p~2
β p~2
p~2 α + iβ
p~2
=
=
,
+
i
2m h̄ h̄2
h̄ 2m
2m h̄
2m

(55)

since h̄ = α + iβ. Hence, the plane-wave is a solution to our fractal-based NLSE
(when U = 0) with a real-valued energy and has the correct energy-momentum
dispersion relation.
Soliton solutions, with real-valued energy (momentum) are of the form
ψ ∼ [F (x − vt) + iG(x − vt)]eipx/h̄−iEt/h̄ ,

(56)

with F , G two functions of the argument x − vt obeying a coupled set of two
nonlinear differential equations.
It is warranted to study solutions when one turns-on an external potential
U 6= 0 and to generalize this construction to the Quaternionic Schroedinger
equation [16] based on the Hydrodynamical Nonabelian-fluid Madelung’s formulation of QM proposed by [26]. And, in particular, to explore further the

14

Quantization in Astrophysics ...

85

consequences of the Non-Hermitian Hamiltonian ( pseudo-Hermitian) associated with our NLSE (34) within the context of the so-called P T symmetric
complex extensions of QM and QFT [27]. Arguments why a quantum theory of
gravity should be nonlinear have been presented by [28] where a dif f erent nonlinear Schroedinger equation, but with a similar logarithmic dependence, was
found. This equation [28] is also similar to the one proposed by Doebner and
Goldin [29] from considerations of unitary representations of the diffeomorphism
group.

Acknowledgements
We acknowledge to the Center for Theoretical Studies of Physical Systems,
Clark Atlanta University, Atlanta, Georgia, USA, and the Research Committee
of the University of Antioquia (CODI), Medellı́n, Colombia for support.

References
[1] L. Nottale: Journal of Chaos, Solitons and Fractals 4 (3) (1994) 361; M.
Celerier and L. Nottale: Dirac equation in scale relativity, hep-th/0112213.
[2] G. N. Ord: Journal of Physics A: Math. Gen. 16 (1983) 1869.
[3] I. Bialynicki-Birula, J. Mycielsky: Annal of Physics 100 (1976) 62.
[4] M. Pardy: To the nonlinear QM. quant-ph/0111105.
[5] D. Bohm and J. Vigier: Phys. Rev. 96 (1954) 208.
[6] C. Castro, J. Mahecha and B. Rodriguez, ”Nonlinear QM as a Fractal Brownian motion with complex diffusion constant” [ quant-ph/0202026] .
[7] E. Madelung: Z. Physik 40 (1926) 322.
[8] A. Staruszkiewicz: Acta Phus. Pol. 14 (1983) 907; W. Puszkarz: On the
Staruszkiewicz modification of the Schrödinger equation. quant-ph/9912006.
[9] S. Weinberg: Ann. Phys. 194 (1989) 336.
[10] T. Petrosky, I. Prigogine: Journal of Chaos, Solitons and Fractals 4 (3)
(1994) 311.
[11] C. Castro: Journal of Chaos, Solitons and Fractals, 12 (2001) 101.
[12] B. Gómez, S. Moore, A. Rodrı́guez and A. Rueda (eds): Stochatic processes applied to physics and other related fields. World Scientific, Singapore
(1983).
[13] N. A. Lemos: Phys. Lett. 78A (1980) 237; 78A (1980) 239.

15

Quantization in Astrophysics ...

86

[14] G. C. Ghirardi, C. Omero, A. Rimini and T. Weber: Rivista del Nuovo
Cimento 1 (1978) 1.
[15] J. Kamesberger, J. Zeilinger: Physica B 151 (1988) 193.
[16] S. L. Adler: Quaternionic quantum mechanics and quantum fields. Oxford
University Press, Oxford (1995).
[17] R. Yaris and P. Winkler: J. Phys. B: Atom. Molec Phys. 11 (1978) 1475
and 11 (1978) 1481; N. Moiseyev: Phys. Rep. 302 (1998) 211.
[18] M. B. Mensky: Phys. Lett. A 196 (1995) 159.
[19] D. S. Abrams and S. Lloyd: Phys. Rev. Lett. 81 (1998) 3992.
[20] J. Cresson, ” Scale Calculus and the Schrodinger Equation ” [ arXiv :
math.GM/0211071 ].
[21] C. Castro , ” Foundations of Physics. 8 ( 2000 ) 1301; Jour. Chaos, Solitons and Fractals 11 (11) (2000) 1663-1670 ; C. Castro, M. Pavsic , ”The
Extended Relativity Theory in Clifford-spaces”, Progress in Physics vol 1
(2005) 31. C. Castro, Foundations of Physics vol. 35, no. 6 (2005) 971.
[22] E. Santamato, Phys. Rev. D 29 ( 1984) 216. Phys. Rev. D 32 ( 1985)
2615. Jour. Math. Phys. 25 ( 1984 ) 2477.
[23] C. Castro, Foundations of Physics 22 (1992 ) 569. Foundations of Physics
Letters 4 ( 1991 ) 81. Jour. Math. Physics. 31 no. 11 (1990) 2633.
[24] B. Frieden, ” Physics from Fisher Information” ( Cambridge University
Press 1998 ) M. Hall, M. Reginatto, Jour. Phys. A 35 ( 2002 ) 3829. B.
Frieden, A. Plastino, A.R Plastino and B. Soffer, ” A Schrø”dinger link
between non-equilibrium theormodynamics and Fisher information” condmat/0206107.
[25] R. Carroll, ” Fisher, Kahler, Weyl and the Quantum Potential” quantph/0406203. ”Remarks on the Schroedinger Equation” quant-ph/0401082.
[26] P. Love, B. Boghosian, ” Quaternionic Madelung Transformation and Nonabelian Fluid Dynamics” hep-th/0210242.
[27] C.Bender, ” Introduction to PT-Symmetric Quantum Theory” [quantph/0501052] C. Bender, I. Cavero-Pelaez, K. A. Milton, K. V. Shajesh ”PTSymmetric Quantum Electrodynamics” [ hep-th/0501180 ]
[28] T. Singh, S. Gutti and R. Tibrewala, ” Why Quantum Gravity should be
Nonlinear” [ arXiv. org : gr-qc/0503116].
[29] H. Doebner and G. Goldin, Phys. Letts A 162 ( 1992 ) 397.

16

Quantization in Astrophysics ...

87

Does Weyl’s Geometry solves the Riddle of
Dark Energy ?
Carlos Castro
August, 2006
Center for Theoretical Studies of Physical Systems, Clark Atlanta University, Atlanta;
castro@ctsps.cau.edu
Abstract
We rigorously prove why the proper use of Weyl’s Geometry within the context of
Friedman-Lemaitre-Robertson-Walker cosmological models can account for both the
origins and the value of the observed vacuum energy density ( dark energy ). The
source of dark energy is just the dilaton-like Jordan-Brans-Dicke scalar field that is
required to implement Weyl invariance of the most simple of all possible actions. A
nonvanishing value of the vacuum energy density of the order of 10−123 MP4 lanck is
derived in agreement with the experimental observations. The full theory involving
the dynamics of Weyl’s gauge field Aµ is very rich and may explain the anomalous
Pioneer acceleration and the temporal variations ( over cosmological scales ) of
the fundamental constants resulting from the expansion of the Universe. This is
consistent with Dirac’s old idea of the plausible variation of the physical constants
but with the advantage that it is not necessary to invoke extra dimensions.

The problem of dark energy is one of the most challenging problems facing us today,
see [1], [3] for a review. In this letter we will show how Weyl’s geometry (and its scaling
symmetry) is instrumental to solve this dark energy riddle. Before starting we must
emphasize that our procedure is quite different than previous proposals [4] to explain
dark matter ( instead of dark energy ) in terms of Brans-Dicke gravity. It is not only
necessary to include the Jordan-Brans-Dicke scalar field φ but it is essential to have a
Weyl geometric extension and generalization of Riemannian geometry ( ordinary gravity
). It will be shown why the scalar φ has a nontrivial energy density despite having trivial
dynamics due entirely to its potential energy density V (φ = φo ) and which is precisely
equal to the observed vacuum energy density of the order of 10−123 MP4 lanck . For other
approaches to solve the riddle of dark energy and dark matter based on modifications of
gravity by starting with Lagrangians of the type f (R) see [12], [14], [11] and references
therein.
Weyl’s geometry main feature is that the norm of vectors under parallel infinitesimal
displacement going from xµ to xµ + dxµ change as follows δ||V || ∼ ||V ||Aµ dxµ where
1
Quantization in Astrophysics ...

88

Aµ is the Weyl gauge field of scale calibrations that behaves as a connection under Weyl
transformations :
A0µ = Aµ − ∂µ Ω(x).

gµν → e2Ω gµν .

(1)

involving the Weyl scaling parameter Ω(xµ ) .
The Weyl covariant derivative operator acting on a tensor T is defined by Dµ T =
( ∇µ + ω(T ) Aµ ) T; where ω(T) is the Weyl weight of the tensor T and the derivative
operator ∇µ = ∂µ + Γµ involves a connection Γµ which is comprised of the ordinary
Christoffel symbols plus extra Aµ terms in order for the metric to obey the condition
Dµ (gνρ ) = 0. The Weyl weight of the metric gνρ is 2. The meaning of Dµ (gνρ ) = 0 is that
the angle formed by two vectors remains the same under parallel transport despite that
their lengths may change. This also occurs in conformal mappings of the complex plane.
The Weyl covariant derivative acting on a scalar φ of Weyl weight ω(φ) = −1 is defined
by
Dµ φ = ∂µ φ + ω(φ)Aµ φ = ∂µ φ − Aµ φ.
(2)
The Weyl scalar curvature in D dimensions and signature (+, −, −, −....) is
RW eyl = RRiemann − (D − 1)(D − 2)Aµ Aµ + 2(D − 1)∇µ Aµ .

(3)

For a signature of (−, +, +, +, ....) there is a sign change in the second and third terms
due to a sign change of RRiemann .
The Jordan-Brans-Dicke action involving the scalar φ and RW eyl is
S=−

Z

4

dx

q

|g| [ φ2 RW eyl ].

(4)

Under Weyl scalings,
RW eyl → e−2Ω RW eyl ;

φ2 → e−2Ω φ2 .
q

(5)
q

to compensate for the Weyl scaling ( in 4D ) of the measure |g| → e4Ω |g| in order to
render the action (4) Weyl invariant.
When the Weyl integrability condition is imposed Fµν = ∂µ Aν − ∂ν Aµ = 0 ⇒ Aµ =
∂µ Ω, the Weyl gauge field Aµ does not have dynamical degrees of freedom; it is pure gauge
and barring global topological obstructions, one can choose the gauge in eq-(4)
Aµ = 0;

φ20 =

1
= constant.
16πGN

(6)

such that the action (4) reduces to the standard Einstein-Hilbert action of Riemannian
geometry
Z
q
1
S=−
d4 x |g| [RRiemann (g)].
(7)
16πGN
The Weyl integrability condition Fµν = 0 means physically that if we parallel transport
a vector under a closed loop, as we come back to the starting point, the norm of the vector
has not changed; i.e, the rate at which a clock ticks does not change after being transported
2
Quantization in Astrophysics ...

89

along a closed loop back to the initial point; and if we transport a clock from A to B
along different paths, the clocks will tick at the same rate upon arrival at the same point
B. This will ensure, for example, that the observed spectral lines of identical atoms will
not change when the atoms arrive at the laboratory after taking different paths ( histories
) from their coincident starting point. If Fµν 6= 0 Weyl geometry may be responsible for
the alleged variations of the physical constants in recent Cosmological observations. A
study of the Pioneer anomaly based on Weyl geometry was made by [9]. The literature
is quite extensive on this topic.
Our starting action is
S = SW eyl (gµν , Aµ ) + S(φ).
with
SW eyl (gµν , Aµ ) = −

Z

d4 x

q

|g| φ2 [ RW eyl (gµν , Aµ ) ].

(8)
(9)

where we define φ2 = (1/16πG). The Newtonian coupling G is spacetime dependent in
general and has a Weyl weight equal to 2. The term S(φ) involving the Jordan-BransDicke scalar φ is
1 µν
g (Dµ φ)(Dν φ) − V (φ) ].
2
where Dµ φ = ∂µ φ − Aµ φ. The FRW metric is
Sφ =

Z

4

dx

q

|g| [

ds2 = dt2 − a2 (t) (

dr2
+ r2 (dΩ)2 ).
1 − k(r/R0 )2

(10)

(11a)

where k = 0 for a 3-dim spatially flat region; k = ±1 for regions of positive and negative
constant spatial curvature, respectively. The de Sitter metric belongs to a special class
of FRW metrics and it admits different forms depending on the coordinates chosen. The
Friedman-Einstein-Weyl equations in the gauge Aµ = (0, 0, 0, 0) (in units of c = 1)
Gµν = 8πG Tµν ;

φ2 =

2 δSmatter
1
. Tµν = − q
.
µν
16π G
|g| δg

(11b)

read
3(

(da/dt) 2
3k
) + ( 2 2 ) = 8πG(t)ρ.
a
a R0

(12)

and
−2 (

(d2 a/dt2 )
(da/dt) 2
k
)−(
) − ( 2 2 ) = 8πG(t) p.
a
a
a R0

(13a)

From eqs-(12-13a) one can infer the important relation :
−(

(d2 a/dt2 )
4πG(t)
) =
(ρ + 3p).
a
3
3

Quantization in Astrophysics ...

90

(13b)

Eqs-(12-13) are the ones one must use instead of the erroneous equations posed by [9] in
the partial gauge At = H(t), Ai = 0, i = 1, 2, 3 :
q
k
1
(da/dt) 2
2
t
q
) = H (t) = − ( 2 2 ) − 3( At (x) A (x) −
∂t ( |g|At ) ) +
(
a
a R0
|g|

8πG(t)
ρ.
3

(14a)

and

(d2 a/dt2 )
dH
4πG(t)
) = − ( H 2 (t) +
) =
(ρ + 3p).
(14b)
a
dt
3
The density and pressure terms should be given in terms of Weyl covariant derivatives
of the scalar φ and the potential density V (φ). The scalar φ must be chosen to depend
solely on time , φ(t), because this is the relevant case suitable for the FRW cosmologies
due to the fact that the geometry is spatially homogeneous and isotropic . The gauge
choice condition imposed by [9] : At = H(t); Ai = 0, i = 1, 2, 3 is compatible with the
spatial isotropy and homogeneity of the FRW models. However, despite that a non-zero
value At was chosen by [9] there is a residual symmetry that is still available to gauge
At to zero. As mentioned earlier, Weyl’s integrability condition Fµν = 0 when Aµ is pure
gauge, a total derivative, means that Aµ does not have true dynamical degrees of freedom
and all of its components can be gauged to zero Aµ = (0, 0, 0, 0) barring global topological
obstructions.
However, if one partially fixes the gauge At = H(t); Ai = 0 like it was done in [9], one
arrives at a caveat that was overlooked by [9] . One would arrive at a deep contradiction
and inconsistency between the left hand side (l.h.s) and the right hand side (r.h.s) of
the Friedman-Einstein-Weyl equations ( for example in eq-(14b) ) in the partially fixed
gauge At = H(t) because the l.h.s does not transform homogeneously under Weyl scalings,
whereas the r.h.s does; if the quantities ρ and p were to transform properly under Weyl
scalings, homogeneously, this behaviour would be incompatible with the transformation
properties of the At = H(t) terms appearing in the l.h.s of eqs-(14b).
In order to reconcile this incompatibility between the inhomogeneous transformation
properties of the l.h.s of eq-(14b) with the homogeneous transformation properties of the
r.h.s of (14b), one must f ix the gauge Aµ = 0 fully in the Einstein-Friedman-Weyl
equations as shown in eqs-(12-13). The latter equations are the physically relevant and
not eqs-(14). One may be inclined to say : if one is going to fix the gauge Aµ = 0 anyway,
then what is the role of Weyl’s geometry and symmetry in all of this ? We will show
below why despite fixing the gauge Aµ = 0 one cannot forget the constraint which arises
from the variations of the action w.r.t the Weyl’s field Aµ ! This constraint holds the key
to see why the density and pressure associated with the scalar φ obey the sought after
relation ρ(φ) = −p(φ) ( which is the hallmark of dark energy ) as we intend to prove next.
The Jordan-Brans-Dicke scalar φ must obey the generalized Klein-Gordon equations
of motion
−(

( Dµ Dµ + 2RW eyl ) φ + (
4
Quantization in Astrophysics ...

91

dV
)=0
dφ

(15)

notice
that because the Weyl covariant derivative q
obeys the condition Dµ (gνρ ) = 0 ⇒
q
Dµ ( |g|) = 0 there are no terms of the form (Dµ |g|)(Dµ φ) in the generalized Kleinq

Gordon equation like it would occur in ordinary Riemannian geometry (∂µ |g|)(∂ µ φ) 6= 0.
In addition, we have the crucial constraint equation obtained from the variation of the
action w.r.t to the Aµ field :
δS
1
(Aµ φ2 − ∂µ (φ)2 ) = 0.
(16)
= 0 ⇒ 6 (Aµ φ2 + ∂µ (φ2 )) +
µ
δA
2
The last constraint equation in the gauge Aµ = 0, forces ∂µ φ = 0 ⇒ φ = φo = constant.
Consequently G ∼ φ−2 is also constrained to a constant GN and one may set 16π GN φ2o =
1, where GN is the observed Newtonian constant today.
Furthermore, in the gauge Aµ = 0, due to the constraint eq-(16), one can infer that
Dµ φ = 0, ⇒ Dµ Dµ φ = 0 because Dt φ(t) = ∂t φ − At φ = ∂t φ = 0, and Di φ(t) =
−Ai φ(t) = 0. These results will be used in the generalized Klein-Gordon equation.
Therefore, the stress energy tensor Tµµ = diag (ρ, −p, −p, −p) corresponding to the
constant scalar field configuration φ(t) = φo , in the Aµ = 0 gauge, becomes :
1
ρφ = (∂t φ−At φ)2 + V (φ) = V (φ);
2

1
pφ = (∂t φ−At φ)2 − V (φ) = −V (φ). (17)
2

ρ + 3p = 2 (∂t φ − At φ)2 − 2V (φ) = −2V (φ).

(18)

This completes the proof why the above ρ and p terms, in the gauge Aµ = 0, become
ρ(φ) = V (φ) = −p(φ) such that ρ + 3p = −2V (φ) ( that will be used in the EinsteinFriedman-Weyl equations (13b) ). This is the key reason why Weyl’s geometry and
symmetry is essential to explain the origins of a non − vanishing vacuum energy ( dark
energy ). The latter relation ρ(φ) = V (φ) = −p(φ) is the key to derive the vacuum energy
density in terms of V (φ = φo ), because such relation resembles the dark energy relation
pDE = −ρDE . Had one not had the constraint condition Dt φ(t) = (∂t − At )φ = ∂t φ = 0,
and Di φ(t) = −Ai φ(t) = 0, in the gauge Aµ = 0, enforcing φ = φo , one would not have
been able to deduce the crucial condition ρ(φ = φo ) = − p(φ = φo ) = V (φ = φo ) that
will furnish the observed vacuum energy density today.
We will find now solutions of the Einstein-Friedman-Weyl equations in the gauge
Aµ = (0, 0, 0, 0) after having explained why Aµ can (and must) be gauged to zero. The
most relevant case corresponding to de Sitter space :
a(t) = eHo t ; Aµ = (0, 0, 0, 0); k = 0; RW eyl = RRiemann = −12 H02 ;

.

(19)

where we will show that the potential is
V (φ) = 12H02 φ2 + Vo .

(20)

one learns in this case that V (φ = φo ) 6= 0 since this non-vanishing value is precisely the
one that shall furnish the observed vacuum energy density today ( as we will see below ) .
We shall begin by solving the Einstein-Friedman-Weyl equations eq-(12-13) in the gauge
5
Quantization in Astrophysics ...

92

Aµ = (0, 0, 0, 0) for a spatially flat universe k = 0 and a(t) = eH0 t , corresponding to de
Sitter metric :
ds2 = dt2 − e2Ho t (dr2 + r2 (dΩ)2 ).

(21)

the Riemannian scalar curvature when k = 0 is
RRiemann = − 6 [ (

(da/dt) 2
(d2 a/dt2 )
)+(
) ] = −12 H02
a
a

(22)

( the negative sign is due to the chosen signature +, −, −, − ).
To scalar Weyl curvature RW eyl in the gauge Aµ = (0, 0, 0, 0) is the same as the
Riemannian one RW eyl = RRiemann = −12 H02 . Inserting the condition Dµ φ = Dt φ(t) =
(∂t φ − At φ) = ∂t φ = 0, in the gauge Aµ = 0, the generalized Klein-Gordon equation
(3.20) will be satisfied if, and only if, the potential density V (φ) is chosen to satisfy
( 12 H02 ) φ =

1 dV
( ) ⇒ V (φ) = 12 H02 φ2 + Vo
2 dφ

(23)

One must firstly differentiate w.r.t the scalar φ , and only afterwards, one may set φ = φo .
V (φ) has a Weyl weight equal to −4 under Weyl scalings in order to ensure that the full
action is Weyl invariant. H02 and φ2o have both a Weyl weight of −2, despite being
constants, because as one performs a Weyl scaling of these quantities ( a change of a
scales) they will acquire then a spacetime dependence. H02 is a masslike parameter, one
may interpret H02 ( up to numerical factors ) as the ”mass” squared of the Jordan-BransDicke scalar. We will see soon why the integration constant Vo plays the role of the
”cosmological constant”.
An important remark is in order. Even if we included other forms of matter in the
Einstein-Fredmann-Weyl equations, in the very large t regime, their contributions will be
washed away due to their scaling behaviour. We know that ordinary matter ( p = 0 );
dark matter ( pDM = wρDM with −1 < w < 0 ) and radiation terms ( prad = 31 ρrad ) are
all washed away due to their scaling behaviour :
ρmatter ∼ R(t)−3 .

ρradiation ∼ R(t)−4 . ρDM ∼ R(t)−3(1+w) .

(24)

where R(t) = a(t)R0 . The dark energy density remains constant with scale since w = −1
and the scaling exponent is zero, ρDE ∼ R0 = costant. For this reason it is the only
contributing factor at very large times.
Now we are ready to show that eqs-(12-13) are indeed satisfied when a(t) = eH0 t ; k =
0; Aµ = 0; φ = φo 6= 0. Eq-(13b), due to the conditions ρ + 3p = −2V (φ) and φ(t) = φo
(resulting from the constraint eq-(16) in the Aµ = 0 gauge ) gives :
(d2 a/dt2 )
4πGN
) = − H02 =
(ρ + 3p) =
a
3
8π GN V (φ = φo )
8π GN 12 H02 φ2o
8πGN Vo
− (
)=−(
) −
.
3
3
3
−(

6
Quantization in Astrophysics ...

93

(25)

Eq-(12) ( with k = 0 ) is just the same as eq-(13b) but with an overall change of sign
because ρ(φ = φo ) = V (φ = φo ). Using the definition 16π GN φ2o = 1 in (25) one gets
− H02 = − (

8π GN Vo
8π GN Vo
8π GN 12 H02 φ2o
) −
= −2 H02 −
⇒
3
3
3

8π GN Vo
(26)
= H02 ⇒ − 8π GN Vo = 3 H02
3
Therefore, we may identify the term − Vo with the vacuum energy density so the quantity
3H02 = −8π GN Vo = Λ is nothing but the cosmological constant. It is not surprising
at all to obtain Λ = 3 H02 in de Sitter space . One knew it long ago. What is most
relevant about eq-(26) is that the observed vacuum energy density is minus the constant
of integration Vo corresponding to the potential density V (φ) = 12H 2 φ2 + Vo !. Hence one
has from the last term of eq-(26) :
−

−Vo = ρvacuum =

3H02
.
8π GN

(27)

2
) and GN = L2P lanck in the last term of
and finally, when we set H02 = (1/R02 ) = (1/RHubble
eq-(26), as announced, the vacuum density ρvacuum observed today is precisely given by :

3H02
3
−Vo = ρvacuum =
=
(LP lanck )−2 (RHubble )−2 =
8π GN
8π
LP lanck 2
1
3
)4 (
) ∼ 10−123 (MP lanck )4 .
(28)
(
8π LP lanck
RHubble
This completes our third derivation of the vacuum energy density given by the formula
(26-28). The first derivation was attained in [5], while the second derivation was attained
in [6] .
Concluding this analysis of the Einstein-Friedman-Weyl eqs-(12-13) : By invoking the
principle of Weyl scaling symmetry in the context of Weyl’s geometry; when k = 0 (
spatially flat Universe ), a(t) = eH0 t ( de Sitter inflationary phase ) ; Ho = Hubble
constant today; φ(t) = φo = constant, such 16πGN φ2o = 1, one finds that
V (φ = φo ) = 12 H02 φ2o + Vo = 2ρvacuum − ρvacuum = ρvacuum =
3H02
∼ 10−123 MP4 lanck .
(29)
8π GN
is precisely the observed vacuum energy density (28) . Therefore, the observed vacuum
energy density is intrinsically and inexorably linked to the potential density V (φ = φo )
corresponding to the Jordan-Brans-Dicke scalar φ required to build Weyl invariant actions
and evaluated at the special point φ2o = (1/16πGN ).
The case of an ever expanding accelerating universe ( consistent with observations)
is so promising because it incorporates the presence of the Hubble Scale and Planck
scale into the expression for the observed vacuum energy density via the Jordan-BransDicke scalar field φ needed to implement Weyl invariance of the action. Weyl’s scaling
6H02 φ2o =

7
Quantization in Astrophysics ...

94

symmetry principle permits us to explain why the observed value of the vacuum energy
density ρvacuum is precisely given by the expression (28-29).
In order to introduce true dynamics to the Weyl gauge field, one must add the kinetic
term for the Weyl gauge field Fµν F µν . In this case, the integrability condition Fµν =
∂µ Aν − ∂ν Aµ = 0 is no longer obeyed in general and the rate at which clocks tick may
depend on their worldline history. This could induce a variation of the physical constants
( even dimensionless constants like the fine structure constant α = 1/137 ). For instance,
as the size of the universe grows, ( a(t) = eH0 t increases with time) the variable speed
of light, Newtonian coupling and cosmological constant , may vary according to the law
[G(t)/c4 (t) Λ(t)] ∼ (1/ρvacuum ) if the vacuum energy density ρvacuum would remain
constant. Many authors have speculated about this last behaviour among c, G, Λ as
well as the possibility that an explanation of the Pioneer anomaly could be due to the
accelerated expansion of the universe that accounts for an acceleration of c2 /RHubble , if
one views our solar system as non-expanding ”pennies” in an expanding balloon.
The most general Lagrangian involving dynamics for Aµ is
1
1
L = −φ2 RW eyl (gµν , Aµ ) + Fµν F µν + g µν (Dµ φ)(Dν φ) − V (φ) + Lmatter + .....
4
2

(30)

The Lmatter must involve the full fledged Weyl gauge covariant derivatives acting on
scalar and spinor fields contrary to the Cheng-Weyl models of [10] where there is no Weyl
gauge field in the derivatives. Lradiation terms may be included involving the Maxwell
field Aµ which must not be confused with the Weyl gauge field Aµ . Once could also add
Yang-Mills fields Aaµ and kinetic and potential terms for the Higgs scalars as well. The
simplest scenario, of course, was the one given in this section.
There are many differences among our approach to explain the origins of dark energy
and that of [7], [2], [3], [1], [10], [13], to cite a few. The Cheng-Weyl approach [10] to
account for dark energy and matter ( including phantom ) does not use the Weyl scalar
curvature with a variable Newtonian coupling 16π G = φ−2 for the gravitational part of
the action, but the ordinary Riemannian scalar curvature with the standard Newtonian
gravitational constant . Conformal transformations in accelerated cosmologies have been
studied by [11] but their approach is different than the Weyl geometric one presented
here. Weyl invariance has been used in [8] to construct Weyl-Conformally Invariant LightLike p-Brane Theories with numerous applications in Astrophysics, Cosmology, Particle
Physics Model Building, String theory,.....
To end this work, we just point out the known fact that the electron neutrino mass
mν ∼ 10−3 eV is of the same order as (mν )4 ∼ 10−123 MP4 lanck and that the SUSY breaking
scale in many models is given by a geometric mean relation : m2SU SY = mν MP lanck ∼
(5 T eV )2 . For interesting remarks on the fundamental constants see [15]. We hope
that the contents of this work will help us elucidate further the connection between the
microscopic and macroscopic world.
Acknowledgments
We are indebted to M. Bowers for assistance .

8
Quantization in Astrophysics ...

95

References
[1] T. Padmanabhan, ” Dark Energy : Mystery of the Millenium” astro-ph/0603114.
[2] E. I. Guendelman and A. B. Kaganovich, ”k-Essence, Avoidance of the Weinberg’s
Cosmological Constant No-Go Theorem and Other Dark Energy Effects of Two
Measures Field Theory”, gr-qc/0606017.
[3] N. Mavromatos, ” The issue of Dark Energy in Strng Theory” hep-th/0607006.
[4] H. Kim, ” Can the Brans-Dicke gravity possibly with Λ be a theory of Dark matter
? ” astro-ph/0604055. M. Arik and M. Calik, ” Can Brans-Dicke scalar field account
for dark energy and dark matter ? gr-qc/0505035.
[5] C. Castro, Mod. Phys. Lett A17 (2002) 2095-2103
[6] C. Castro, ”On Novel Static Spherically Symmetric Solutions of Einstein equations
and the Cosmological Constant Problem ” CTSPS preprint, May 2006, submitted
to the IJMPD.
[7] L. Nottale, Fractal Spacetime and Micrphysics : Towards Scale Relativity ( World
Scientific, Singapore, 1992 )
[8] E. Guendelman, A. Kaganovich, Phys. Rev D 53 (1996) 7020; Phys. Rev D 60 ,
065004 ( 1999) ; Int. J. Mod. Phys A 17 (2002) 417; E. Guendelman, A. Kaganovich,
E. Nissimov and S. Pacheva, ” Weyl-Conformally Invariant Light-Like p-Brane Theories ” hep-th/0409078.
[9] E. Scholz, ” On the Geometry of Cosmological Model Building” gr-qc/0511113
[10] H. Wei and R-G Cai, ” Cheng-Weyl Vector Field and Cosmological Application”
astro-phy/0607064.
[11] J. Crooks and P. Frampton, ” Conformal Transformations and Accelerated Cosmologies ” astro-ph/0601051. M.Cardoni, ” Conformal Symmetry of Gravity and
the Cosmological Constant Problem” hep-th/0606274.
[12] S. Capozzielo, V. Cardone and A. Troisi, ” Jour. of Cosmology and Astroparticle
Physics 08 (2006) 001.
[13] H. Brandt, J. Modern Optics 51, 2753-2759 (2004).
[14] S. Capozziello, S. Nojiri and S. Odintsov, Phys. Letts B 634 (2006) 93.
[15] J. Nieto, ” S-duality for Linearized Gravity” hep-th/9910049. J. Nieto, L. Ruiz and
J. Silvas, ” Thoughts on Duality and the Fundamental Constants” hep-th/0512256.

9
Quantization in Astrophysics ...

96

THE EXTENDED RELATIVITY THEORY
IN CLIFFORD SPACES
C. Castroa and M. Pavšičb
January 2004, Revised January 2005

∗

Abstract
An introduction to some of the most important features of the Extended Relativity theory in Clifford-spaces (C-spaces) is presented whose ”point” coordinates
are non-commuting Clifford-valued quantities which incorporate lines, areas, volumes, hyper-volumes.... degrees of freedom associated with the collective particle,
string, membrane, p-brane,... dynamics of p-loops (closed p-branes) in target Ddimensional spacetime backgrounds. C-space Relativity naturally incorporates the
ideas of an invariant length (Planck scale), maximal acceleration, non-commuting
coordinates, supersymmetry, holography, higher derivative gravity with torsion and
variable dimensions/signatures. It permits to study the dynamics of all (closed)
p-branes, for all values of p, on a unified footing. It resolves the ordering ambiguities in QFT, the problem of time in Cosmology and admits superluminal propagation ( tachyons ) without violations of causality. A discussion of the maximalacceleration Relativity principle in phase-spaces follows and the study of the invariance group of symmetry transformations in phase-space allows to show why
Planck areas are invariant under acceleration-boosts transformations . This invariance feature suggests that a maximal-string tension principle may be operating in
Nature. We continue by pointing out how the relativity of signatures of the underlying n-dimensional spacetime results from taking different n-dimensional slices
through C-space. The conformal group in spacetime emerges as a natural subgroup
of the Clifford group and Relativity in C-spaces involves natural scale changes in
the sizes of physical objects without the introduction of forces nor Weyl’s gauge
field of dilations. We finalize by constructing the generalization of Maxwell theory
of Electrodynamics of point charges to a theory in C-spaces that involves extended
charges coupled to antisymmetric tensor fields of arbitrary rank. In the concluding remarks we outline briefly the current promising research programs and their
plausible connections with C-space Relativity.
∗a

Center for Theoretical Studies of Physical Systems, Clark Atlanta University, Atlanta b Jožef Stefan
Institute, Jamova 39, SI-1000 Ljubljana, Slovenia

1

Quantization in Astrophysics ...

97

1

Introduction

In recent years it was argued that the underlying fundamental physical principle behind
string theory, not unlike the principle of equivalence and general covariance in Einstein’s
general relativity, might well be related to the existence of an invariant minimal length
scale (Planck scale) attainable in nature [8]. A theory involving spacetime resolutions
was developed long ago by Nottale [23] where the Planck scale was postulated as the
minimum observer independent invariant resolution [23] in Nature. Since “points” cannot
be observed physically with an ultimate resolution, it is reasonable to postulate that they
are smeared out into fuzzy balls. In refs.[8] it was assumed that those balls have the Planck
radius and arbitrary dimension. For this reason it was argued in refs.[8] that one should
construct a theory which includes all dimensions (and signatures) on the equal footing.
In [8] this Extended Scale Relativity principle was applied to the quantum mechanics
of p-branes which led to the construction of Clifford-space (C-space) where all p-branes
were taken to be on the same footing, in the sense that the transformations in C-space
reshuffled a string history for a five-brane history, a membrane history for a string history,
for example.
Clifford algebras contained the appropriate algebraic-geometric features to implement
this principle of polydimensional transformations [14]–[17]. In [14]–[16] it was proposed
that every physical quantity is in fact a polyvector, that is, a Clifford number or a Clifford
aggregate. Also, spinors are the members of left or right minimal ideals of Clifford algebra,
which may provide the framework for a deeper understanding of sypersymmetries, i.e.,
the transformations relating bosons and fermions. The Fock-Stueckelberg theory of a
relativistic particle can be embedded in the Clifford algebra of spacetime [15, 16]. Many
important aspects of Clifford algebra are described in [1],[6], [7], [3], [15, 16, 17], [5], [48].
It is our belief that this may lead to the proper formulation of string and M theory.
A geometric approach to the physics of the Standard Model in terms of Clifford algebras was advanced by [4]. It was realized in [43] that the Cl(8) Clifford algebra contains the 4 fundamental nontrivial representations of Spin(8) that accomodate the chiral
fermions and gauge bosons of the Standard model and which also includes gravitons via
the McDowell-Mansouri-Chamseddine-West formulation of gravity, which permits to construct locally, in D = 8, a geometric Lagrangian for the Standard Model plus Gravity.
Furthermore, discrete Clifford-algebraic methods based on hyperdiamond-lattices have
been instrumental in constructing E8 lattices and deriving the values of the force-strengths
(coupling constants) and masses of the Standard model with remarkable precision by [43].
These results have recently been corroborated by [46] for Electromagnetism, and by [47],
where all the Standard model parameters were obtained from first principles, despite the
contrary orthodox belief that it is senseless to ”derive” the values of the fundamental
constants in Nature from first principles, from pure thought alone; i.e. one must invoke
the Cosmological anthropic principle to explain why the constants of Nature have they
values they have.
Using these methods the bosonic p-brane propagator, in the quenched mini superspace
approximation, was constructed in [18, 19]; the logarithmic corrections to the black hole
2

Quantization in Astrophysics ...

98

entropy based on the geometry of Clifford space (in short C-space) were obtained in [21];
The modified nonlinear de Broglie dispersion relations, the corresponding minimal-length
stringy [11] and p-brane uncertainty relations also admitted a C-space interpretation [10],
[19]. A generalization of Maxwell theory of electromagnetism in C-spaces comprised of
extended charges coupled to antisymmetric tensor fields of arbitrary rank was attained
recently in [75]. The resolution of the ordering ambiguities of QFT in curved spaces
was resolved by using polyvectors, or Clifford-algebra valued objects [26]. One of the
most remarkable features of the Extended Relativity in C-spaces is that a higher derivative Gravity with Torsion in ordinary spacetime follows naturally from the analog of the
Einstein-Hlbert action in curved C-space [20].
In this new physical theory the arena for physics is no longer the ordinary spacetime,
but a more general manifold of Clifford algebra valued objects, noncommuting polyvectors.
Such a manifold has been called a pan-dimensional continuum [14] or C-space [8]. The
latter describes on a unified basis the objects of various dimensionality: not only points,
but also closed lines, surfaces, volumes,.., called 0-loops (points), 1-loops (closed strings) 2loops (closed membranes), 3-loops, etc.. It is a sort of a dimension category, where the role
of functorial maps is played by C-space transformations which reshuffles a p-brane history
for a p0 -brane history or a mixture of all of them, for example. The above geometric objects
may be considered as to corresponding to the well-known physical objects, namely closed
p-branes. Technically those transformations in C-space that reshuffle objects of different
dimensions are generalizations of the ordinary Lorentz transformations to C-space.
C-space Relativity involves a generalization of Lorentz invariance (and not a deformation of such symmetry) involving superpositions of p-branes (p-loops) of all possible
dimensions. The Planck scale is introduced as a natural parameter that allows us to
bridge extended objects of different dimensionalities. Like the speed of light was need in
Einstein Relativity to fuse space and time together in the Minkwoski spacetime interval.
Another important point is that the Conformal Group of four-dimensional spacetime is
a consequence of the Clifford algebra in four-dimensions [25] and it emphasizes the fact
why the natural dilations/contractions of objects in C-space is not the same physical phenomenon than what occurs in Weyl’s geometry which requires introducing, by hand, a
gauge field of dilations. Objects move dilationally, in the absence of forces, for a different
physical reasoning than in Weyl’s geometry: they move dilationally because of inertia.
This was discussed long ago in refs.[27, 28].
This review is organized as follows: Section 2 is dedicated to extending ordinary Special Relativity theory, from Minkowski spacetime to C-spaces, where the introduction of
the invariant Planck scale is required to bridge objects, p-branes, of different dimensionality.
The generalized dynamics of particles, fields and branes in C-space is studied in section
3 . This formalism allows us to construct for the first time, to our knowledge, a unif ied
action which comprises the dynamics of all p-branes in C-spaces, for all values of p, in one
single footing (see also [15]). In particular, the polyparticle dynamics in C-space, when
reduced to 4-dimensional spacetime leads to the Stuckelberg formalism and the solution
to the problem of time in Cosmology [15].
In section 4 we begin by discussing the geometric Clifford calculus that allows us
3

Quantization in Astrophysics ...

99

to reproduce all the standard results in differential and projective geometry [41]. The
resolution of the ordering ambiguities of QFT in curved spaces follows next when we
review how it can be resolved by using polyvectors, or Clifford-algebra valued objects [26].
Afterwards we construct the Generalized Gravitational Theories in Curved C-spaces, in
particular it is shown how Higher derivative Gravity with Torsion in ordinary spacetime
follows naturaly from the Geometry of C-space [20].
In section 5 we discuss the Quantization program in C-spaces, and write the C-space
Klein-Gordon and Dirac equations [15]. The coresponding bosonic/fermionic p-brane
loop-wave equations were studied by [12], [13] without employing Clifford algebra and the
concept of C-space.
In section 6 we review the Maximal-Acceleration Relativity in Phase-Spaces [127],
starting with the construction of the submaximally-accelerated particle action of [53] using
Clifford algebras in phase-spaces; the U (1, 3) invariance transformations [74] associated
with an 8-dimensional phase space, and show why the minimal Planck-Scale areas are
invariant under pure acceleration boosts which suggests that there could be a principle of
maximal-tension (maximal acceleration) operating in string theory [68].
In section 7 we discuss the important point that the notion of spacetime signature is
relative to a chosen n-dimensional subspace of 2n -dimensional Clifford space. Different
subspaces Vn —different sections through C-space—have in general different signature [15]
We show afterwards how the Conformal agebra of spacetime emerges from the Clifford
algebra [25] and emphasize the physical differences between our model and the one based
on Weyl geometry. At the end we show how Clifford algebraic methods permits one to
generalize Maxwell theory of Electrodynamics (asociated with ordinary point-charges) to
a generalized Maxwell theory in Clifford spaces involving extended charges and p-forms
of arbitrary rank [75]
In the concluding remarks, we briefly discuss the possible avenues of future research
in the construction of QFT in C-spaces, Quantum Gravity, Noncommutative Geometry,
and other lines of current promising research in the literature.

2

Extending Relativity from Minkowski Spacetime
to C-space

We embark into the construction of the extended relativity theory in C-spaces by a natural
generalization of the notion of a spacetime interval in Minkwoski space to C-space [8, 14,
16, 15, 17]:
dX 2 = dσ 2 + dxµ dxµ + dxµν dxµν + ...
(1)
where µ1 < µ2 < ... The Clifford valued polyvector:1
X = X M EM = σ1 + xµ γµ + xµν γµ ∧ γν + ...xµ1 µ2 ....µD γµ1 ∧ γµ2 .... ∧ γµD .
1

(2)

If we do not restrict indices according to µ1 < µ2 < µ3 < ..., then the factors 1/2!, 1/3!, respectively,
have to be included in front of every term in the expansion (1).

4

Quantization in Astrophysics ...

100

denotes the position of a point in a manifold, called Clifford space or C-space. The series
of terms in (2) terminates at a finite grade depending on the dimension D. A Clifford
algebra Cl(r, q) with r + q = D has 2D basis elements. For simplicity, the gammas γ µ
correspond to a Clifford algebra associated with a flat spacetime:
1 µ ν
{γ , γ } = η µν .
(3)
2
but in general one could extend this formulation to curved spacetimes with metric g µν
(see section 4).
The connection to strings and p-branes can be seen as follows. In the case of a closed
string (a 1-loop) embedded in a target flat spacetime background of D-dimensions, one
represents the projections of the closed string (1-loop) onto the embedding spacetime
coordinate-planes by the variables xµν . These variables represent the respective areas
enclosed by the projections of the closed string (1-loop) onto the corresponding embedding
spacetime planes. Similary, one can embed a closed membrane (a 2-loop) onto a D-dim
flat spacetime, where the projections given by the antisymmetric variables xµνρ represent
the corresponding volumes enclosed by the projections of the 2-loop along the hyperplanes
of the flat target spacetime background.
This procedure can be carried to all closed p-branes (p-loops) where the values of p
are p = 0, 1, 2, 3, .... The p = 0 value represents the center of mass and the coordinates
xµν , xµνρ ... have been coined in the string-brane literature [24]. as the holographic areas,
volumes,...projections of the nested family of p-loops ( closed p-branes) onto the embedding spacetime coordinate planes/hyperplanes. In ref.[17] they were interpreted as the
generalized centre of mass coordinates of an extended object. Extended objects were thus
modeled in C-space.
The scalar coordinate σ entering a polyvector X is a measure associated with the
p-brane’s world manifold Vp+1 (e.g., the string’s 2-dimensional worldsheet V2 ): it is proportional to the (p + 1)-dimensional area/volume of Vp+1 . In other words, σ is proportional to the areal-time parameter of the Eguchi-Schild formulation of string dynamics
[126, 37, 24].
We see in this generalized scheme the objects as observed in spacetime (which is a
section through C-space) need not be infinitely extended along time-like directions. They
need not be infinitely long world lines, world tubes. They can be finite world lines, world
tubes. The σ coordinate measures how long are world lines, world tubes. During evolution
they can becomes longer and longer or shorter and shorter.
If we take the differential dX of X and compute the scalar product among two polyvectors < dX † dX >0 ≡ dX † ∗ dX ≡ |dX|2 we obtain the C-space extension of the particles
proper time in Minkwoski space. The symbol X † denotes the reversion operation and
involves reversing the order of all the basis γ µ elements in the expansion of X. It is the
analog of the transpose (Hermitian) conjugation. The C-space proper time associated
with a polyparticle motion is then the expression (1) which can be written more explicitly
as:
|dX|2 = GM N dX M dX N = dS 2
= dσ 2 + L−2 dxµ dxµ + L−4 dxµν dxµν + ... + L−2D dxµ1 ...µD dxµ1 ...µD
5

Quantization in Astrophysics ...

101

(4)

†
where GM N = EM
∗ EN is the C-space metric.
Here we have introduced the Planck scale L since a length parameter is needed in order
to tie objects of different dimensionality together: 0-loops, 1-loops,..., p-loops. Einstein
introduced the speed of light as a universal absolute invariant in order to “unite” space
with time (to match units) in the Minkwoski space interval:

ds2 = c2 dt2 + dxi dxi .
A similar unification is needed here to “unite” objects of different dimensions, such as xµ ,
xµν , etc... The Planck scale then emerges as another universal invariant in constructing
an extended relativity theory in C-spaces [8].
Since the D-dimensional Planck scale is given explicitly in terms of the Newton constant: LD = (GN )1/(D−2) , in natural units of h̄ = c = 1, one can see that when D = ∞
the value of LD is then L∞ = G0 = 1 (assuming a finite value of G). Hence in D = ∞ the
Planck scale has the natural value of unity. However, if one wishes to avoid any serious
algebraic divergence problems in the series of terms appearing in the expansion of the
analog of proper time in C-spaces, in the extreme case when D = ∞, from now on we
shall focus solely on a f inite value of D. In this fashion we avoid any serious algebraic
convergence problems. We shall not be concerned in this work with the representations
of Clifford algebras in different dimensions and with different signatures.
The line element dS as defined in (4) is dimensionless. Alternatively, one can define
[8, 9] the line element whose dimension is that of the D-volume so that:
dΣ2 = L2D dσ 2 + L2D−2 dxµ dµ + L2D−4 dxµν dxµν + ... + dxµ1 ...µD dxµ1 ...µD

(5)

Let us use the relation
γµ1 ∧ ... ∧ γµD = γµ1 ...µD

(6)

and write the volume element as
dxµ1 ...µD γµ1 ∧ ... ∧ γµD ≡ γdσ̃

(7)

dσ̃ ≡ dxµ1 ...µD µ1 ...µD

(8)

where
In all expressions we assume the ordering prescription µ1 < µ2 < ... < µr , r = 1, 2, ..., D.
The line element can then be written in the form
dΣ2 = L2D dσ 2 + L2D−2 dxµ dxµ + L2D−4 dxµν dxµν + ... + |γ|2 dσ̃ 2

(9)

|γ|2 ≡ γ † ∗ γ

(10)

where
Here γ is the pseudoscalar basis element and can be writted as γ0 ∧ γ1 ∧ ...γD−1 . In
flat spacetime MD we have that |γ|2 = +1 or −1, depending on dimension and signature.
In M4 with signature (+ − −−) we have γ † ∗ γ = γ † γ = γ 2 = −1 (γ ≡ γ5 = γ0 γ1 γ2 γ3 ),
whilst in M5 with signature (+ − − − −) it is γ † γ = 1.
6

Quantization in Astrophysics ...

102

The analog of Lorentz transformations in C-spaces which transform a polyvector X
into another poly-vector X 0 is given by
X 0 = RXR−1

(11)

with
R = eθ

AE
A

= exp [(θI + θµ γµ + θµ1 µ2 γµ1 ∧ γµ2 .....)].

(12)

and
R−1 = e−θ

AE
A

= exp [−(θI + θν γν + θν1 ν2 γν1 ∧ γν2 .....)].

(13)

where the theta parameters in (12)(13) are the components of the Clifford-value parameter
Θ = θM EM :
θ; θµ ; θµν ; ....
(14)
they are the C-space version of the Lorentz rotations/boosts parameters.
Since a Clifford algebra admits a matrix representation, one can write the norm of a
poly-vectors in terms of the trace operation as: ||X||2 = T race X 2 Hence under C-space
Lorentz transformation the norms of poly-vectors behave like follows:
2

T race X 0 = T race [RX 2 R−1 ] = T race [RR−1 X 2 ] = T race X 2 .

(15)

These norms are invariant under C-space Lorentz transformations due to the cyclic property of the trace operation and RR−1 = 1. If one writes the invariant norm in terms of
the reversal operation < X † X >s this will constrain the explicit form of the terms in the
exponential which define the rotor R so the rotor R obeys the analog condition of an orthogonal rotation matrix R† = R−1 . Hence the appropriate poly-rotations of poly-vectors
which preserve the norm must be :
||(X 0 )2 || =< X 0† X 0 >s =< (R−1 )† X † R† RXR−1 >s =< RX † XR−1 >s =< X † X >s = ||X 2 ||.
(16)
where once again, we made use of the analog of the cyclic property of the trace, <
RX † XR−1 >s =< X † X >s .
This way of rewriting the inner product of poly-vectors by means of the reversal
operation that reverses the order of the Clifford basis generators : (γ µ ∧ γ ν )† = γ ν ∧ γ µ ,
etc... has some subtleties. The analog of an orthogonal matrix in Clifford spaces is
R† = R−1 such that
< X 0† X 0 >s =< (R−1 )† X † R† RXR−1 >s =< RX † XR−1 >s =< X † X >s = invariant.
This condition R† = R−1 , of course, will restrict the type of terms allowed inside the
exponential defining the rotor R because the reversal of a p-vector obeys
(γµ1 ∧ γµ2 ..... ∧ γµp )† = γµp ∧ γµp−1 ..... ∧ γµ2 ∧ γµ1 = (−1)p(p−1)/2 γµ1 ∧ γµ2 ..... ∧ γµp
Hence only those terms that change sign ( under the reversal operation ) are permitted
in the exponential defining R = exp[θA EA ].
7

Quantization in Astrophysics ...

103

Another possibility is to complexif y the C-space polyvector valued coordinates =
Z = Z A EA = X A EA +iY A EA and the boosts/rotation parameters θ allowing the unitarity
condition Ū † = U −1 to hold in the generalized Clifford unitary transformations Z 0 =
U ZU † associated with the complexified polyvector Z = Z A EA such that the interval
< dZ̄ † dZ >s = dΩ̄dΩ + dz̄ µ dzµ + dz̄ µν dzµν + dz̄ µνρ dzµνρ + .....
remains invariant ( upon setting the Planck scale Λ = 1 ).
The unitary condition Ū † = U −1 under the combined reversal and complex-conjugate
operation will constrain the form of the complexified boosts/rotation parameters θA appearing in the rotor : U = exp[ θA EA ]. The theta parameters θA are either purely real or
purely imaginary depending if the reversal EA † = ±EA , to ensure that an overall change
of sign occurs in the terms θA EA inside the exponential defining U so that Ū † = U −1 holds
and the norm < Z̄ † Z >s remains invariant under the analog of unitary transformations
in complexif ied C-spaces. These techniques are not very different from Penrose Twistor
spaces. As far as we know a Clifford-Twistor space construction of C-spaces has not been
performed so far.
Another alternative is to define the polyrotations by R = exp (ΘAB [EA , EB ]) where
the commutator [EA , EB ] = FABC EC is the C-space analog of the i[γµ , γν ] commutator
which is the generator of the Lorentz algebra, and the theta parameters ΘAB are the
C-space analogs of the rotation/boots parameters θµν . The diverse parameters ΘAB are
purely real or purely imaginary depending whether the reversal [EA , EB ]† = ±[EA , EB ]
to ensure that R† = R−1 so that the scalar part < X † X >s remains invariant under the
transformations X 0 = RXR−1 . This last alternative seems to be more physical because
a poly-rotation should map the EA direction into the EB direction in C-spaces, hence
the meaning of the generator [EA , EB ] which extends the notion of the [γµ , γν ] Lorentz
generator.
The above transformations are active transformations since the transformed Clifford
number X 0 (polyvector) is different from the “original” Clifford number X. Considering
the transformations of components we have
X 0 = X 0M EM = LM N X N EM

(17)

If we compare (17) with (11) we find
LM N EN = REN R−1

(18)

0
.
LM N = hE M REN R−1 i0 ≡ E M ∗ (REN R−1 ) = E M ∗ EN

(19)

from which it follows that

0
where we have labelled EN
as new basis element since in the active interpretation one may
perform either a change of the polyvector components or a change of the basis elements.
The h i0 means the scalar part of the expression and “∗” the scalar product. Eq (19)
has been obtained after multiplying (18) from the left by E J , taking into account that
hE J EN i0 ≡ E J ∗ EN = δ J N , and renamiming the index J into M .

8

Quantization in Astrophysics ...

104

3

Generalized Dynamics of Particles, Fields and
Branes in C-space

An immediate application of this theory is that one may consider “strings” and “branes”
in C-spaces as a unifying description of all branes of different dimensionality. As we have
already indicated, since spinors are in left/right ideals of a Clifford algebra, a supersymmetry is then naturally incorporated into this approach as well. In particular, one can have
world manifold and target space supersymmetry simultaneously [15]. We hope that the
C-space “strings” and “branes” may lead us towards discovering the physical foundations
of string and M-theory. For other alternatives to supersymmetry see the work by [50]. In
particular, Z3 generalizations of supersymmetry based on ternary algebras and Clifford
algebras have been proposed by Kerner [128] in what has been called Hypersymmetry.

3.1

The Polyparticle Dynamics in C-space

We will now review the theory [15, 17] in which an extended object is modeled by the
components σ, xµ , xµν , ... of the Clifford valued polyvector (2). By assumption the extended objects, as observed from Minkowski spacetime, can in general be localized not
only along space-like, but also along time-like directions [15, 17]. In particular, they can
be “instantonic” p-loops with either space-like or time-like orientation. Or they may be
long, but finite, tube-like objetcs. The theory that we consider here goes beyond the
ordinary relativity in Minkowski spacetime, therefore such localized objects in Minkowski
spacetime pose no problems. They are postulated to satisfy the dynamical principle which
is formulated in C-space. All conservation laws hold in C-space where we have infinitely
long world “lines” or Clifford lines. In Minkowski spacetime M4 –which is a subspace of
C-space– we observe the intersections of Clifford lines with M4 . And those intersections
appear as localized extended objects, p-loops, described above.
Let the motion of such an extended object be determined by the action principle
I=κ

Z

†

1/2

dτ (Ẋ ∗ Ẋ)

=κ

Z

dτ (Ẋ A ẊA )1/2

(20)

where κ is a constant, playing the role of “mass” in C-space, and τ is an arbitrary
parameter. The C-space velocities Ẋ A = dX A /dτ = (σ̇, ẋµ , ẋµ nu , ...) are also called
“hollographic” velocities.
The equation of motion resulting from (20) is




d  Ẋ A 
q
=0
dτ
B
Ẋ ẊB

(21)

Taking Ẋ B ẊB = constant 6= 0 we have that Ẍ A = 0, so that xA (τ ) is a straight worldline
in C-space. The components xA then change linearly with the parameter τ . This means
that the extended object position xµ , effective area xµν , 3-volume xµνα , 4-volume xµναβ ,
etc., they all change with time. That is, such object experiences a sort of generalized
dilational motion [17].
9

Quantization in Astrophysics ...

105

We shall now review the procedure exposed in ref. [17] according to which in such a
generalized dynamics an object may be accelerated to faster than light speeds as viewed
from a 4-dimensional Minkowski space, which is a subspace of C-space. For a different explanation of superluminal propagation based on the modified nonlinear de Broglie
dispersion relations see [68].
The canonical momentum belonging to the action (20) is
PA =

κẊA
(Ẋ B ẊB )1/2

(22)

When the denominator in eq.(22) is zero the momentum becomes infinite. We shall
now calculate the speed at which this happens. This will be the maximum speed that
an object accelerating in C-space can reach. Although an initially slow object cannot
accelerate beyond that speed limit, this does not automatically exclude the possibility
that fast objects traveling at a speed above that limit may exist. Such objects are Cspace analog of tachyons [31, 32]. All the well known objections against tachyons should
be reconsidered for the case of C-space before we could say for sure that C-space tachyons
do not exist as freely propagating objects. We will leave aside this interesting possibility,
and assume as a working hypothesis that there is no tachyons in C-space.
Vanishing of Ẋ B ẊB is equivalent to vanishing of the C-space line element
dx0
dX dXA = dσ +
L
A

!2

2

dx1
−
L

!2

dx01
−
L2

!2

dx12
....+
L2

!2

dx123
−
L3

!2

dx0123
−
L4

!2

+... = 0

(23)
where by “...” we mean the terms with the remaining components such as x , x , x ,...,
x012 , etc.. The C-space line element is associated with a particular choice of C-space
†
∗ EN . If the basis EM , M = 1, 2, ..., 2D is generated by the
metric, namely GM N = EM
flat space γ µ satisfying (3), then the C-space has the diagonal metric of eq. (23) with +, −
signa. In general this is not necessarily so and the C-space metric is a more complicated
expression. We take now dimension of spacetime being 4, so that x0123 is the highest
grade coordinate. In eq. (23) we introduce a length parameter L. This is necessary, since
x0 = ct has dimension of length, x12 of length square, x123 of length to the third power,
and x0123 of length to the forth power. It is natural to assume that L is the Planck length,
that is L = 1.6 × 10−35 m.
Let us assume that the coordinate time t = x0 /c is the parameter with respect to
which we define the speed V in C-space.
So we have
2

dσ
V =− L
dt
2

!2

dx1
+
dt

!2

dx01
+
L2

!2

1 dx12
.....−
L dt

!2

1 dx123
+
L2 dt

!2

01

1 dx0123
+
L3 dt

23

!2

−...

(24)
From eqs. (23),(24) we find that the maximum speed is the maximum speed is given
by
V 2 = c2

10

Quantization in Astrophysics ...

106

(25)

First, we see that the maximum speed squared V 2 contains not only the components
of the 1-vector velocity dx1 /dt, as it is the case in the ordinary relativity, but also the
multivector components such as dx12 /dt, dx123 /dt, etc..
The following special cases when only certain components of the velocity in C-space
are different from zero, are of particular interest:
(i) Maximum 1-vector speed
dx1
= c = 3.0 × 108 m/s
dt
(ii) Maximum 3-vector speed
dx123
= L2 c = 7.7 × 10−62 m3 /s
dt
√
3
d x123
= 4.3 × 10−21 m/s (diameter speed)
dt
(iii) Maximum 4-vector speed
dx0123
= L3 c = 1.2 × 10−96 m4 /s
dt
√
4
d x0123
= 1.05 × 10−24 m/s (diameter speed)
dt
Above we have also calculated the corresponding diameter speeds for the illustration of
how fast the object expands or contracts.
We see that the maximum multivector speeds are very small. The diameters of objects
change very slowly. Therefore we normally do not observe the dilatational motion.
Because of the positive sign in front of the σ and x12 , x012 , etc., terms in the quadratic
form (23) there are no limits to correspondintg 0-vector, 2-vector and 3-vector speeds. But
if we calculate, for instance, the energy necessary to excite 2-vector motion we find that it
is very high. Or equivalently, to the relatively modest energies (available at the surface of
the Earth), the corresponding 2-vector speed is very small. This can be seen by calculating
the energy
κc2
(26)
p0 = q
2
1 − Vc2
(a) for the case of pure 1-vector motion by taking V = dx1 /dt, and
(b) for the case of pure 2-vector motion by taking V = dx12 /(Ldt).
By equating the energies belonging to the cases (a) and (b we have
κc2
s
=
2
2


1
1 dx12
1 − 1c ddxt
1 − Lc
dt

p0 = s

κc2

11

Quantization in Astrophysics ...

107

(27)

which gives
1

12

1 dx
1 dx
=
=
c dt
Lc dt

v
u
u
t

κc2
1−
p0

!2

(28)

Thus to the energy of an object moving translationally at dx1 /dt =1 m/s, there corresponds the 2-vector speed dx12 /dt = L dx1 /dt = 1.6 × 10−35 m2 /s (diameter speed
4 ×10−18 m/s). This would be a typical 2-vector speed of a macroscopic object. For a
microscopic object, such as the electron, which can be accelerated close to the speed of
light, the corresponding 2-vector speed could be of the order of 10−26 m2 /s (diameter
speed 10−13 m/s). In the examples above we have provided rough estimations of possible
2-vector speeds. Exact calculations should treat concrete situations of collisions of two
or more objects, assume that not only 1-vector, but also 2-vector, 3-vector and 4-vector
motions are possible, and take into account the conservation of the polyvector momentum
PA .
Maximum 1-vector speed, i.e., the usual speed, can exceed the speed of light when the
holographic components such as dσ/dt, dx12 /dt, dx012 /dt, etc., are different from zero
[17]. This can be immediately verified from eqs. (23),(24). The speed of light is no longer
such a strict barrier as it appears in the ordinary theory of relativity in M4 . In C-space
a particle has extra degrees of freedom, besides the translational degrees of freedom. The
scalar, σ, the bivector, x12 (in general, xrs , r, s = 1, 2, 3) and the three vector, x012
(in general, x0rs , r, s = 1, 2, 3), contributions to the C-space quadratic form (23) have
positive sign, which is just opposite to the contributions of other components, such as
xr , x0r , xrst , xµνρσ . Because some terms in the quadratic form have + and some − sign,
the absolute value of the 3-velocity dxr /dx0 can be greater than c.
It is known that when tachyons can induce a breakdown of causality. The simplest way
to see why causality is violated when tachyons are used to exchange signals is by writing
the temporal displacements δt = tB − tA between two events (in Minkowski space-time)
in two different frames of reference:
1 δx
δx
sinh(ξ) = (δt)[cosh(ξ) + (
)sinh(ξ)] =
c
c δt
(δt)[cosh(ξ) + (βtachyon )sinh(ξ)]

(δt)0 = (δt)cosh(ξ) +

(29)
(30)

the boost parameter ξ is defined in terms of the velocity as βf rame = vf rame /c = tanh(ξ),
where vf rame is is the relative velocity ( in the x-direction ) of the two reference frames
and can be written in terms of the Lorentz-boost rapidity parameter ξ by using hyperbolic
functions. The Lorentz dilation factor is cosh(ξ) = (1 − βf2rame )−1/2 ; whereas βtachyon =
vtachyon /c is the beta parameter associated with the tachyon velocity δx/δt . By emitting
a tachyon along the negative x -direction one has βtachyon < 0 and such that its velocity
exceeds the speed of light |βtachyon | > 1
A reversal in the sign of (δt)0 < 0 in the above boost transformations occurs when the
tachyon velocity |βtachyon | > 1 and the relative velocity of the reference frames |βf rame | < 1
obey the inequality condition :

12

Quantization in Astrophysics ...

108

(δt)0 = (δt)[cosh(ξ) − |βtachyon |sinh(ξ)] < 0 ⇒ 1 <

1
1
=
< |βtachyon |. (31)
tanh(ξ)
βf rame

thereby resulting in a causality violation in the primed reference frame since the effect (
event B ) occurs bef ore the cause ( event A ) in the primed reference frame.
In the case of subluminal propagation |βparticle | < 1 there is no causality violation since
one would have :
(δt)0 = (δt)[cosh(ξ) − |βparticle |sinh(ξ)] > 0

(32)

due to the hyperbolic trigonometric relation :
cosh2 (ξ) − sinh2 (ξ) = 1 ⇒ cosh(ξ) − sinh(ξ) ≥ 0

(33)

In the theory considered here, there are no tachyons in C-space, because physical
signals in C-space are constrained to live inside the C-space-light cone, defined by eq.
(23). However, certain worldlines in C-space, when projected onto the subspace M4 ,
can appear as worldlines of ordinary tachyons outside the lightcone in M4 . The physical
analog of photons in C-space corresponds to tensionless p-loops, i.e., tensionless closed
branes, since the analog of mass m in C-space is the maximal p-loop tension. By ‘maximal
p-loop’ we mean the loop with the maximum value of p associated with the hierarchy of
p-loops (closed p-branes): p = 0, 1, 2, .... living in the embedding target spacetime. One
must not confuse the Stueckelberg parameter σ with the C-space Proper-time Σ (eq.(5));
so one could have a world line in C-space such that
dΣ = 0 ↔ C-space photon ↔

tensionless branes with a monotonically increasing
Stueckelberg parameter σ

.
In C-space the dynamics refers to a larger space. Minkowski space is just a subspace
of C-space. ”Wordlines” now live in C-space that can be projected onto the Minkwoski
subspace M4 . Concerning tachyons and causality within the framework of the C-space
relativity, the authors of this review propose two different explanations, described below.
According to one author (C.C) one has to take into account the fact that one is
enlarging the ordinary Lorentz group to a larger group of C-space Lorentz transformations
which involve poly-rotations and generalizations of boosts transformations. In particular,
the C-space generalization of the ordinary boost transformations associated with the
boost rapidity parameter ξ such that tanh(ξ) = βf rame will involve now the family of
C-space boost rapidity parameters θt1 , θt12 , θt123 , ....θt123... , ... since boosts are just ( poly
) rotations along directions involving the time coordinate. Thus, one is replacing the
ordinary boost transformations in Minkowski spacetime for the more general C-space
boost transformations as we go from one frame of reference to another frame of reference.
Due to the linkage among the C-space coordinates (poly-dimensional covariance) when
we envision an ordinary boost along the x1 - direction, we must not forget that it is also
interconnected to the area-boosts in the x12 -direction as well, and, which in turn, is also
13

Quantization in Astrophysics ...

109

linked to the x2 direction. Because the latter direction is transverse to the original
tachyonic x1 -motion, the latter x2 -boosts won’t affect things and we may concentrate on
the area-boosts along the x12 direction involving the θt12 parameter that will appear in
the C-space boosts and which contribute to a crucial extra term in the transformations
such that no sign-change in δt0 will occur.
More precisely, let us set all the values of the theta parameters to zero except the
parameters θt1 and θt12 related to the ordinary boosts in the x1 direction and area-boosts
in the x12 directions of C-space. This requires, for example, that one has at least one
spatial-area component, and one temporal coordinate, which implies that the dimensions
must be at least D = 2 + 1 = 3 . Thus, we have in this case :
X 0 = RXR−1 = eθ

t1 γ

t ∧γ1 +θ

t12 γ

t ∧γ1 ∧γ2

X M EM e−θ

t1 γ

t ∧γ1 −θ

t12 γ

t ∧γ1 ∧γ2

M
⇒ X 0N = LN
M X . (34)

N
−1
>0 . When one concentrates on
where as we shown previously LN
M =< E REM R
the transformations of the time coordinate, we have now that the C-space boosts do not
coincide with ordinary boosts in the x1 direction :

t0 = LtM X M =< E t REM R−1 >0 X M 6= (Ltt )t + (Lt1 )x1 .

(35)

because of the extra non-vanishing θ parameter θt12 .
This is because the rotor R includes the extra generator θt12 γt ∧γ1 ∧γ2 which will bring
extra terms into the transformations ; i.e. it will rotate the E[12] bivector- basis , that
couples to the holographic coordinates x12 , into the Et direction which is being contracted
with the E t element in the definition of LtM . There are extra terms in the C-space boosts
because the poly-particle dynamics is taking place in C-space and all coordinates X M
which contain the t, x1 , x12 directions will contribute to the C-space boosts in D = 3,
since one is projecting down the dynamics from C-space onto the (t, x1 ) plane when one
studies the motion of the tachyon in M4 .
Concluding, in the case when one sets all the theta parameters to zero, except the θt1
and θt12 , the X 0 = RX M EM R−1 transformations will be :
(δt)0 = LtM (θt1 ; θt12 )(δX M ) 6= Ltt (δt) + Lt1 (δx1 ).

(36)

due to the presence of the extra term Lt12 (δX 12 ) in the transformations. In the more
general case, when there are more non-vanishing theta parameters , the indices M of
the X M coordinates must be restricted to those directions in C-space which involve the
t, x1 , x12 , x123 ..... directions as required by the C-space poly-particle dynamics. The generalized C-space boosts involve now the ordinary tachyon velocity component of the polyparticle as well as the generalized holographic areas, volumes, hyper-volumes...velocities
V M = (δX M /δt) associated with the poly-vector components of the Clifford-valued Cspace velocity.
Hence, at the expense of enlarging the ordinary Lorentz boosts to the C-space Lorentz
boosts, and the degrees of freedom of a point particle into an extended poly-particle
by including the holographic coordinates, in C-space one can still have ordinary pointparticle tachyons without changing the sign of δt, and without violating causality, due to
14

Quantization in Astrophysics ...

110

the presence of the extra terms in the C-space boosts transformations which ensure us
that the sign of δt > 0 is maintained as we go from one frame of reference to another one.
Naturally, if one were to f reeze all the θ parameters to zero except one θt1 one would end
up with the standard Lorentz boosts along the x1 -direction and a violation of causality
would occur for tachyons as a result of the sign-change in δt0 .
In future work we shall analyze in more detail if the condition δt0 = LtM (δX M ) > 0
is satisfied for any physical values of the theta C-space boosts parameters and for any
physical values of the holographic velocities consistent with the condition that the Cspace velocity VM V M ≥ 0 . What one cannot have is a C-space tachyon; i.e. the physical
signals in C-space must be constrained to live inside the C-space light-cone. The analog
of ” photons ” in C-space are tensionless branes . The corresponding analog of C-space
tachyons involve branes with imaginary tensions, not unlike ordinary tachyons m2 < 0 of
imaginary mass.
To sum up : Relativity in C-space demands enlarging the ordinary Lorentz group (
boosts ) to a larger symmetry group of C-space Lorentz group and enlarging the degrees
of freedom by including Clifford-valued coordinates X = X M EM . This is the only way
one can have a point-particle tachyonic speed in a Minkowski subspace without violating causality in C-space. Ordinary Lorentz boosts are incompatible with tachyons if one
wishes to preserve causality . In C-space one requires to have, at least, two theta parameters θt1 and θt12 with the inclusion, at least, of the t, x1 , x12 coordinates in a C-space
boost, to be able to enforce the condition δt0 > 0 under ( combined ) boosts along the
x1 direction accompanied by an area-boost along the x12 direction of C-space . It is beyond the scope of this review to analyze all the further details of the full-fledged C-boosts
transformations in order to check that the condition δt0 > 0 is obeyed for any physical
values of the theta parameters and holographic velocities.
According to the other author (M.P.), the problem of causality could be explained as
follows. In the usual theory of relativity the existence of tachyons is problematic because
one can arrange for situations such that tachyons are sent into the past. A tachyon T1
is emitted from an aparatus worldline C at x01 and a second tachyon T2 can arrive to the
same worldline C at an earlier time x00 < x01 and trigger destruction of the aparatus. The
spacetime event E 0 at which the aparatus is destroyed cooncides with the event E at
which the aparatus by initial assumtion kept on functioning normally and later emitted
T1 . So there is a paradox from the ordinary ( constrained ) relativistic particle dynamics.
There is no paradox if one invokes the unconstrained Stueckelberg description of superluminal propagation in M4 . It can be described as follows. A C-space worldline can be
described in terms of five functions xµ (τ ), σ(τ ) (all other C-space coordinates being kept
constant). In C-space we have the constrained action (20), whilst in Minkowski space we
have a reduced, unconstrained action. A reduction of variables can be done by choosing
a gauge in which σ(τ ) = τ . It was shown in ref.[16, 15, 17] that the latter unconstrained
action is equivalent to the well known Stueckelberg action [33, 34]. In other words, the
Stueckelberg relativistic dynamics is embedded in C-space. In Stueckelberg theory all
four spacetime coordinates xµ are independent dynamical degrees of freedom that evolve
in terms of an extra parameter σ which is invariant under Lorentz transformations in M4 .
From the C-space point of view, the evolution parameter σ is just one of the C-space
15

Quantization in Astrophysics ...

111

coordintes X M . By assumption, σ is monotonically increasing along particles’ worldlines.
Certain C-space worldlines may appear tachyonic from the point of view of M4 . If we now
repeat the above experiment with the emission of the first and absorption of the second
tachyon we find out that the second tachyon T2 cannot reach the aparatus worldline earlier
than it was emmitted from. Namely, T2 can arrive at a C-space event E 0 with x00 < x01 ,
but the latter event does not coincide with the event E on the aparatus worldline, since
although having the same coordinates x0µ = xµ , the events E and E 0 have different extra
coordinates σ 0 6= σ. In other words, E and E 0 are different points in C-space. Therefore
T2 cannot destroy the aparatus and there is no paradox.
If nature indeed obeys the dynamics in Clifford space, then a particle, as observed
from the 4-dimensional Minkowski space, can be accelerated beyond the speed of light
[17], provided that its extra degrees of freedom xµν , xµνα ,..., are changing simultaneously
with the ordinary position xµ . But such a particle, although moving faster than light
in the subspace M4 , is moving slower than light in C-space, since its speed V , defined
in eq.(24), is smaller than c. In this respect, our particle is not tachyon at all! In Cspace we thus retain all the nice features of relativity, but in the subspace M4 we have,
as a particular case, the unconstrained Stueckelberg theory in which faster-than-light
propagation is not paradoxical and is consistent with the quantum field theory as well
[15]. This is so, because the unconstrained Stueckelberg theory is quite different from
the ordinary (constrained) theory of relativity in M4 , and faster than light motion in the
former theory is of totally different nature from the faster that light motion in the latter
theory. The tachyonic “world lines” in M4 are just projections of trajectories in C-space
onto Minkowski space, however, the true world lines of M4 must be interpreted always as
being embedded onto a larger C-space, such that they cannot take part in the paradoxical
arrangement in which future could influence the past. The well known objections against
tachyons are not valid for our particle which moves according to the relativity in C-space.
We have described how one can obtain faster than light motion in M4 from the theory of
relativity in C-space. There are other possible ways to achieve superluminal propagation.
One such approach is described in refs. [84]
An alternative procedure In ref. [9] an alternative factorization of the C-space line
element has been undertaken. Starting from the line element dΣ of eq. (5), instead of
factoring out the (dx0 )2 element, one may factor out the (dΩ)2 ≡ L2D dσ 2 element, giving
rise to the generalized ”holographic ” velocities measured w.r.t the Ω parameter, for
example the areal-time parameter in the Eguchi-Schild formulation of string dynamics
[126], [37], [24], instead of the x0 parameter (coordinate clock). One then obtains


dΣ2 = dΩ2 1 + L2D−2

dxµν dxµν
dσ̃
dxµ dxµ
+ L2D−4
+ ... + |γ|2
dΩ dΩ
dΩ dΩ
dΩ

!2 


(37)

The idea of ref. [9] was to restrict the line element (37) to the non tachyonic values which
imposes un upper limit on the holographic velocities. The motivation was to find a lower
bound of length scale. This upper holographic-velocity bound does not necessarily translate into a lower bound on the values of lengths, areas, volumes....without the introduction
16

Quantization in Astrophysics ...

112

of quantum mechanical considerations. One possibility could be that the upper limiting
speed of light and the upper bound of the momentum mp c of a Planck-mass elementary
particle (the so-called Planckton in the literature) generalizes now to an upper-bound in
the p-loop holographic velocities and the p-loop holographic momenta associated with
elementary closed p-branes whose tensions are given by powers of the Planck mass. And
the latter upper bounds on the holographic p-loop momenta implies a lower-bound on the
holographic areas, volumes,..., resulting from the string/brane uncertainty relations [11],
[10],[19]. Thus, Quantum Mechanics is required to implement the postulated principle of
minimal lengths, areas, volumes...and which cannot be derived from the classical geometry
alone. The emergence of minimal Planck areas occurs also in the Loop Quantum Gravity
program [111] where the expecation values of the Area operator are given by multiples of
Planck area.
Recently in [134] an isomorphism between Yang’s Noncommutative space-time algebra
(involving two length scales) [136] and the holographic area coordinates algebra of Cspaces (Clifford spaces) was constructed via an AdS5 space-time which is instrumental
in explaining the origins of an extra (infrared) scale R in conjunction to the (ultraviolet)
Planck scale λ characteristic of C-spaces. Yang’s Noncommutative space-time algebra
allowed Tanaka [137] to explain the origins behind the discrete nature of the spectrum
for the spatial coordinates and spatial momenta which yields a minimum length-scale
λ (ultraviolet cutoff) and a minimum momentum p = h̄/R ( maximal length R, infrared
cutoff ) . In particular, the norm-squared A2 of the holographic Area operator XAB X AB
has a correspondence with the quadratic Casimir operator ΣAB ΣAB of the conformal
algebra SO(4, 2) ( SO(5, 1) in the Euclideanized AdS5 case ). This holographic areaCasimir relationship does not differ much from the area-spin relation in Loop Quantum
P
Gravity A2 ∼ λ4 ji (ji + 1) in terms of the SU (2) Casimir J 2 with eigenvalues j(j + 1)
and where the sum is taken over the spin network sites.

3.2

A Unified Theory of all p-Branes in C-Spaces

The generalization to C-spaces of string and p-brane actions as embeddings of worldmanifolds onto target spacetime backgrounds involves the embeddings of polyvectorvalued world-manifolds (of dimensions 2d ) onto polyvector-valued target spaces (of dimensions 2D ), given by the Clifford-valued maps X = X(Σ) (see [15]). These are maps
from the Clifford-valued world-manifold, parametrized by the polyvector-valued variables
Σ, onto the Clifford-valued target space parametrized by the polyvector-valued coordinates X. Physically one envisions these maps as taking an n-dimensional simplicial cell
(n-loop) of the world-manifold onto an m-dimensional simplicial cell (m-loop) of the target
C-space manifold ; i.e. maps from n-dim objects onto m-dim objects generalizing the old
maps of taking points onto points. One is basically dealing with a dimension-category of
objects. The size of the simplicial cells (p-loops), upon quantization of a generalized harmonic oscillator, for example, are given by multiples of the Planck scale, in area, volume,
hypervolume units or Clifford-bits.
In compact multi-index notation X = X M ΓM one denotes for each one of the compo-

17

Quantization in Astrophysics ...

113

nents of the target space polyvector X:
X M ≡ X µ1 µ2 ....µr , µ1 < µ2 < ... < µr .

(38)

and for the world-manifold polyvector Σ = ΣA EA :
ΣA ≡ ξ a1 a2 ....as , a1 < a2 < ... < as .

(39)

where ΓM = (1, γµ , γµν , ...) and EA = (1, ea , eab , ...) form the basis of the target manifold and world manifold Clifford algebra, respectively. It is very important to order the
indices within each multi-index M and A as shown above. The above Clifford-valued coordinates X M , ΣA correspond to antisymmetric tensors of ranks r, s in the target spacetime
background and in the world-manifold, respectively.
There are many different ways to construct C-space brane actions which are on-shell
equivalent to the analogs of the Dirac-Nambu-Goto action for extended objects and that
are given by the world-volume spanned by the branes in their motion through the target
spacetime background.
One of these actions is the Polyakov-Howe-Tucker action:
I=

q
T Z
[DΣ] |H|[H AB ∂A X M ∂B X N GM N + (2 − 2d )].
2

(40)

with the 2d -dim world-manifold measure:
[DΣ] = (dξ)(dξ a )(dξ a1 a2 )(dξ a1 a2 a3 ).....

(41)

Upon the algebraic elimination of the auxiliary world-manifold metric H AB from the
action (40), via the equations of motion, yields for its on-shell solution the pullback of
the target C-space metric onto the C-space world-manifold:
HAB (on − shell) = GAB = ∂A X M ∂B X N GM N

(42)

upon inserting back the on-shell solutions (42) into (40) gives the Dirac-Nambu-Goto
action for the C-space branes directly in terms of the C-space determinant, or measure,
of the induced C-space world-manifold metric GAB , as a result of the embedding:
I=T

Z

q

[DΣ] Det(∂A X M ∂B X N GM N ).

(43)

However in C-space, the Polyakov-Howe-Tucker action admits an even further generalization that is comprised of two terms S1 + S2 . The first term is [15] :
S1 =

Z

[DΣ]|E|E A E B ∂A X M ∂B X N ΓM ΓN .

(44)

Notice that this is a generalized action which is written in terms of the C-space coordinates X M (Σ) and the C-space analog of the target-spacetime vielbein/frame one-forms
em = em µ dxµ given by the ΓM variables. The auxiliary world-manifold vielbein variables
ea , are given now by the Clifford-valued frame E A variables.
18

Quantization in Astrophysics ...

114

In the conventional Polyakov-Howe-Tucker action, the auxiliary world-manifold metric
h associated with the standard p-brane actions is given by the usual scalar product of
the frame vectors ea .eb = eaµ ebν g µν = hab . Hence, the C-space world-manifold metric H AB
appearing in (42) is given by scalar product < (E A )† E B >0 = H AB , where (E A )† denotes
the reversal operation of E A which requires reversing the orderering of the vectors present
in the Clifford aggregate E A .
Notice, however, that the form of the action (44) is far more general than the action in (40). In particular, the S1 itself can be decomposed futher into two additional
pieces by rewriting the Clifford product of two basis elements into a symmetric plus an
antisymmetric piece, respectively:
ab

1
1
E A E B = {E A , E B } + [E A , E B ].
2
2

(45)

1
1
ΓM ΓN = {ΓM , ΓN } + [ΓM , ΓN ].
(46)
2
2
In this fashion, the S1 component has two kinds of terms. The first term containing
the symmetric combination is just the analog of the standard non-linear sigma model
action, and the second term is a Wess-Zumino-like term, containing the antisymmetric
combination . To extract the non-linear sigma model part of the generalized action above,
we may simply take the scalar product of the vielbein-variables as follows:
(S1 )sigma =

T Z
[DΣ]|E| < (E A ∂A X M ΓM )† (E B ∂B X N ΓN ) >0 .
2

(47)

where once again we have made use of the reversal operation (the analog of the hermitian
adjoint) before contracting multi-indices. In this fashion we recover again the Cliffordscalar valued action given by [15].
Actions like the ones presented here in terms of derivatives with respect to quantities
with multi-indices can be mapped to actions involving higher derivatives, in the same
fashion that the C-space scalar curvature, the analog of the Einstein-Hilbert action, could
be recast as a higher derivative gravity with torsion (reviewed in sec. 4). Higher derivatives
actions are also related to theories of Higher spin fields [117] and W -geometry, W -algebras
[116], [122]. For the role of Clifford algerbras to higher spin theories see [51].
The S2 (scalar) component of the C-space brane action is the usual cosmological
constant term given by the C-space determinant |E| = det(H AB ) based on the scalar part
of the geometric product < (E A )† E B >0 = H AB
S2 =

T Z
[DΣ]|E|(2 − 2d )
2

where the C-space determinant |E| =
manifold metric H AB is given by:
det(H AB ) =

q

|det(H AB )| of the 2d × 2d generalized world-

1
A A ....A B B ....B H A1 B1 H A2 B2 ....H A2d B2d .
(2d )! 1 2 2d 1 2 2d
19

Quantization in Astrophysics ...

(48)

115

(49)

The A1 A2 ....A2d is the totally antisymmetric tensor density in C-space.
There are many different forms of p-brane actions, with and without a cosmological
constant [123], and based on a new integration measure by recurring to auxiliary scalar
fields [115], that one could have used to construct their C-space generalizations. Since all
of them are on-shell equivalent to the Dirac-Nambu-Goto p-brane actions, we decided to
focus solely on those actions having the Polyakov-Howe-Tucker form.

4

Generalized Gravitational Theories in Curved Cspaces: Higher Derivative Gravity and Torsion
from the Geometry of C-Space

4.1
4.1.1

Ordinary space
Clifford algebra based geometric calculus in curved space(time)

Clifforfd algebra is a very useful tool for description of geometry, especially of curved
space Vn . Let us first review how it works in curved space(time). Later we will discuss a
generalization to curved Clifford space [20].
We would like to make those techniques accessible to a wide audience of physicists
who are not so familiar with the rigorous underlying mathematics, and demonstrate how
Clifford algebra can be straightforwardly employed in the theory of gravity and its generalization. So we will leave aside the sophisticated mathematical approach, and rather follow
as simple line of thought as possible, a praxis that is normally pursued by physicists. For
instance, physicists in their works on general relativity employ a mathematical formulation
and notation which is much simpler from that of purely mathematical or mathamatically
oriented works. For rigorous mathematical treatment the reader is adviced to study, refs.
[1, 76, 77, 78, 79].
Let the vector fields γµ , µ = 1, 2, ..., n be a coordinate basis in Vn satisfying the Clifford
algebra relation
1
(50)
γµ · γν ≡ (γµ γν + γν γµ ) = gµν
2
where gµν is the metric of Vn . In curved space γµ and gµν cannot be constant but necessarily depend on position xµ . An arbitrary vector is a linear superposition [1]
a = aµ γµ

(51)

where the components aµ are scalars from the geometric point of view, whilst γµ are
vectors.
Besides the basis {γµ } we can introduce the reciprocal basis2 {γ µ } satisfying
γ µ · γ ν ≡ 21 (γ µ γ ν + γ ν γ µ ) = g µν
2

(52)

In Appendix A of the Hesteness book [1] the frame {γ µ } is called dual frame because the duality
operation is used in constructing it.

20

Quantization in Astrophysics ...

116

where g µν is the covariant metric tensor such that g µα gαν = δ µ ν , γ µ γν + γν γ µ = 2δ µ ν and
γ µ = g µν γν .
Following ref.[1] (see also [15]) we consider the vector derivative or gradient defined
according to
∂ ≡ γ µ ∂µ
(53)
where ∂µ is an operator whose action depends on the quantity it acts on [26].
Applying the vector derivative ∂ on a scalar field φ we have
∂φ = γ µ ∂µ φ

(54)

where ∂µ φ ≡ (∂/∂xµ )φ coincides with the partial derivative of φ.
But if we apply it on a vector field a we have
∂a = γ µ ∂µ (aν γν ) = γ µ (∂µ aν γν + aν ∂µ γν )

(55)

In general γν is not constant; it satisfies the relation [1, 15]
∂µ γν = Γαµν γα

(56)

where Γαµν is the connection. Similarly, for γ ν = g να γα we have
∂µ γ ν = −Γνµα γ α

(57)

The non commuting operator ∂µ so defined determines the parallel transport of a basis
vector γ ν . Instead of the symbol ∂µ Hestenes uses 2µ , whilst Wheeler et. al. [36] use
∇µ and call it “covariant derivative”. In modern, mathematically opriented literature
more explicit notation such as Dγµ or ∇γµ is used. However, such a notation, although
mathematically very relevant, would not be very practical in long computations. We
find it very convenient to keep the symbol ∂µ for components of the geometric operator
∂ = γ µ ∂µ . When acting on a scalar field the derivative ∂µ happens to be commuting and
thus behaves as the ordinary partial derivative. When acting on a vector field, ∂µ is a non
commuting operator. In this respect, there can be no confusion with partial derivative,
because the latter normally acts on scalar fields, and in such a case partial derivative and
∂µ are one and the same thing. However, when acting on a vector field, the derivative
∂µ is non commuting. Our operator ∂µ when acting on γµ or γ µ should be distinguished
from the ordinary—commuting—partial derivative, let be denoted γ ν ,µ , usually used in
the literature on the Dirac equation in curved spacetime. The latter derivative is not used
in the present paper, so there should be no confusion.
Using (56), eq.(55) becomes
∂a = γ µ γν (∂µ aν + Γνµα aα ) ≡ γ µ γν Dµ aν = γ µ γ ν Dµ aν

(58)

where Dµ is the covariant derivative of tensor analysis..
Decomposing the Clifford product γ µ γ ν into its symmetric and antisymmetric part [1]
γ µγν = γµ · γν + γµ ∧ γν
21

Quantization in Astrophysics ...

117

(59)

where

1
γ µ · γ ν ≡ (γ µ γ ν + γ ν γ µ ) = g µν
2

(60)

is the inner product and
1
γ µ ∧ γ ν ≡ (γ µ γ ν − γ ν γ µ )
2
the outer product, we can write eq.(58) as
1
∂a = g µν Dµ aν + γ µ ∧ γ ν Dµ aν = Dµ aµ + γ µ ∧ γ ν (Dµ aν − Dν aµ )
2

(61)

(62)

Without employing the expansion in terms of γµ we have simply
∂a = ∂ · a + ∂ ∧ a

(63)

Acting twice on a vector by the operator ∂ we have3
∂∂a = γ µ ∂µ (γ ν ∂ν )(aα γα ) = γ µ γ ν γα Dµ Dν aα
1
= γα Dµ Dµ aα + (γ µ ∧ γ ν )γα [Dµ , Dν ]aα
2
µ α
= γα Dµ D a + γ µ (Rµρ aρ + Kµα ρ Dρ aα )
1
+ (γ µ ∧ γ ν ∧ γα )(Rµνρ α aρ + Kµν ρ Dρ aα )
2

(64)

We have used
[Dµ , Dν ]aα = Rµνρ α aρ + Kµν ρ Dρ aα

(65)

Kµν ρ = Γρµν − Γρνµ

(66)

where
is torsion and Rµνρ α the curvature tensor. Using eq.(56) we find
[∂α , ∂β ]γµ = Rαβµ ν γν

(67)

Rαβµ ν = ([[∂α , ∂β ]γµ ) · γ ν

(68)

from which we have
Thus in general the commutator of derivatives ∂µ acting on a vector does not give zero,
but is given by the curvature tensor.
In general, for an r-vector A = aα1 ...αr γα1 γα2 ...γαr we have
∂∂...∂A = (γ µ1 ∂µ1 )(γ µ2 ∂µ2 )...(γ µk ∂µk )(aα1 ...αr γα1 γα2 ...γαr )
= γ µ1 γ µ2 ...γ µk γα1 γα2 ...γαr Dµ1 Dµ2 ...Dµk aα1 ...αr
3

We use (a ∧ b)c = (a ∧ b) · c + a ∧ b ∧ c [1] and (a ∧ b) · c = (b · c)a − (a · c)b.

22

Quantization in Astrophysics ...

118

(69)

4.1.2

Clifford algebra based geometric calculus and resolution of the ordering
ambiguity for the product of momentum operators

Clifford algebra is a very useful tool for description of geometry of curved space. Moreover, as shown in ref.[26] it provides a resolution of the long standing problem of the
ordering ambiguity of quantum mechanics in curved space. Namely, eq.(53) for the vector
derivative suggests that the momentum operator is given by
p = −i ∂ = −iγ µ ∂µ

(70)

One can consider three distinct models:
(i) The non relativistic particle moving in ndimensional curved space. Then, µ =
1, 2, ..., n, and signature is (+ + + + ....).
(ii) The relativistic particle in curved spacetime, described by the Schild action [37].
Then, µ = 0, 1, 2, ..., n − 1 and signature is (+ − − − ...).
(iii) The Stueckelberg unconstrained particle. [33, 34, 35, 29].
In all three cases the classical action has the form
1 Z
dτ gµν (x)Ẋ µ Ẋ ν
I[X ] =
2Λ
µ

(71)

and the corresponding Hamiltonian is
H=

Λ
Λ µν
g (x)pµ pν = p2
2
2

(72)

If, upon quantization we take for the momentum operator pµ = −i ∂µ , then the ambiguity arises of how to write the quantum Hamilton operator. The problem occurs because
the expressions g µν pµ pν , pµ g µν pν and pµ pν g µν are not equivalent.
But, if we rewrite H as
Λ
(73)
H = p2
2
where p = γ µ pµ is the momentum vector which upon quantization becomes the momentum
vector operator (70), we find that there is no ambiguity in writing the square p2 . When
acting with H on a scalar wave function φ we obtain the unambiguous expression
Hφ =

Λ 2
Λ
Λ
p φ = (−i)2 (γ µ ∂µ )(γ ν ∂ν )φ = − Dµ Dµ φ
2
2
2

(74)

in which there is no curvature term R. We expect that a term with R will arise upon
acting with H on a spinor field ψ.

23

Quantization in Astrophysics ...

119

4.2

C-space

Let us now consider C-space and review the procedure of ref. [20]. . A basis in C-space
is given by
EA = {γ, γµ , γµ ∧ γν , γµ ∧ γν ∧ γρ , ...}
(75)
where in an r-vector γµ1 ∧ γµ2 ∧ ... ∧ γµr we take the indices so that µ1 < µ2 < ... < µr .
An element of C-space is a Clifford number, called also Polyvector or Clifford aggregate
which we now write in the form
X = X A EA = s γ + xµ γµ + xµν γµ ∧ γν + ...

(76)

A C-space is parametrized not only by 1-vector coordinates xµ but also by the 2-vector
coordinates xµν , 3-vector coordinates xµνα , etc., called also holographic coordinates, since
they describe the holographic projections of 1-loops, 2-loops, 3-loops, etc., onto the coordinate planes. By p-loop we mean a closed p-brane; in particular, a 1-loop is closed
string.
In order to avoid using the powers of the Planck scale length parameter L in the
expansion of the polyvector X we use the dilatationally invariant units [15] in which L is
set to 1. The dilation invariant physics was discussed from a different perspective also in
refs. [23, 21].
In a flat C-space the basis vectors E A are constants. In a curved C-space this is no
longer true. Each EA is a function of the C-space coordinates
X A = {s, xµ , xµν , ...}

(77)

which include scalar, vector, bivector,..., r-vector,..., coordinates.
Now we define the connection Γ̃C
AB in C-space according to
∂A EB = Γ̃C
AB EC

(78)

where ∂A ≡ ∂/∂X A is the derivative in C-space. This definition is analogous to the one
in ordinary space. Let us therefore define the C-space curvature as
RABC D = ([∂A , ∂B ]EC ) ∗ E D

(79)

which is a straightforward generalization of the relation (68). The ‘star’ means the scalar
product between two polyvectors A and B, defined as
A ∗ B = hA BiS

(80)

where ’S’ means ’the scalar part’ of the geometric product AB.
In the following we shall explore the above relation for curvature and see how it is
related to the curvature of the ordinary space. Before doing that we shall demonstrate
that the derivative with respect to the bivector coordinate xµν is equal to the commutator
of the derivatives with respect to the vector coordinates xµ .

24

Quantization in Astrophysics ...

120

Returning now to eq.(78), the differential of a C-space basis vector is given by
dEA =

∂EA
B
dX B = ΓC
AB EC dX
∂X B

(81)

In particular, for A = µ and EA = γµ we have
∂γµ ν
∂γµ
αβ
ν
A
+ ...
dx + αβ dxαβ + ... = Γ̃A
νµ EA dx + Γ̃[αβ]µ EA dx
ν
∂X
∂x
ν
= (Γ̃ανµ γα + Γ̃[ρσ]
νµ γρ ∧ γσ + ...)dx

dγµ =

[ρσ]

+(Γ̃ρ[αβ]µ γρ + Γ̃[αβ]µ γρ ∧ γσ + ...)dxαβ + ...

(82)

We see that the differential dγµ is in general a polyvector, i.e., a Clifford aggregate. In
eq.(82) we have used
∂γµ
[ρσ]
= Γ̃ανµ γα + Γ̃νµ
γρ ∧ γσ + ...
(83)
∂xν
∂γµ
[ρσ]
= Γ̃ρ[αβ]µ γρ + Γ̃[αβ]µ γρ ∧ γσ + ...
(84)
∂xαβ
Let us now consider a restricted space in which the derivatives of γµ with respect to
ν
x and xαβ do not contain higher rank multivectors. Then eqs. (83),(84) become
∂γµ
= Γ̃ανµ γα
∂xν

(85)

∂γµ
= Γ̃ρ[αβ]µ γρ
∂xαβ

(86)

Further we assume that
α
(i) the components Γ̃ανµ of the C-space connection Γ̃C
AB coincide with the connection Γνµ
of an ordinary space.

(ii) the components Γ̃ρ[αβ]µ of the C-space connection coincide with the curvature tensor
Rαβµ ρ of an ordinary space.
Hence, eqs.(85),(86) read
∂γµ
= Γανµ γα
∂xν
∂γµ
= Rαβµ ρ γρ
∂xαβ

(87)
(88)

and the differential (82) becomes
dγµ = (Γραµ dxα + 12 Rαβµ ρ dxαβ )γρ

(89)

The same relation was obtained by Pezzaglia [14] by using a different method, namely
by considering how polyvectors change with position. The above relation demonstrates
that a geodesic in C-space is not a geodesic in ordinary spacetime. Namely, in ordinary
25

Quantization in Astrophysics ...

121

spacetime we obtain Papapetrou’s equation. This was previously pointed out by Pezzaglia
[14].
Although a C-space connection does not transform like a C-space tensor, some of its
components, i.e., those of eq. (86), may have the transformation properties of a tensor in
an ordinary space.
Under a general coordinate transformation in C-space
X A → X 0A = X 0A (X B )

(90)

the connection transforms according to4
C

Γ̃0 AB =

∂X 0C ∂ 2 X J
∂X 0C ∂X J ∂X K E
+
Γ̃
∂X E ∂X 0A ∂X 0B JK
∂X J ∂X 0A ∂X 0B

(91)

In particular, the components which contain the bivector index A = [αβ] transform as
∂X 0ρ ∂X J ∂X K E
∂x0ρ ∂ 2 X J
Γ̃
+
∂X E ∂σ 0αβ ∂x0µ JK ∂X J ∂σ 0αβ ∂x0µ

ρ

Γ̃0 [αβ]µ =

(92)

Let us now consider a particular class of coordinate transformations in C-space such that
∂x0ρ
=0,
∂xµν

∂xµν
=0
∂x0α

(93)

Then the second term in eq.(92) vanishes and the transformation becomes
ρ

Γ̃0 [αβ]µ =

∂X 0ρ ∂xρσ ∂xγ 
Γ̃
∂x ∂σ 0αβ ∂x0µ [ρσ]γ

(94)

Now, for the bivector whose components are dxαβ we have
dσ 0αβ γα0 ∧ γβ0 = dxαβ γα ∧ γβ

(95)

Taking into account that in our particular case (93) γα transforms as a basis vector in an
ordinary space
∂xµ
γα0 =
γµ
(96)
∂x0α
we find that (95) and (96) imply
dσ 0αβ

∂xµ ∂xν
= dxµν
0α
0β
∂x ∂x

(97)

which means that
∂xµν
1
=
0αβ
∂σ
2
4

∂xµ ∂xν
∂xν ∂xµ
−
∂x0α ∂x0β ∂x0α ∂x0β

!

≡

∂x[µ ∂xν]
∂x0α ∂x0β

This can be derived from the relation
0
dEA
=

0
∂EA
dX 0B
∂X 0B

where

0
EA
=

∂X D
ED
∂X 0A

.

26

Quantization in Astrophysics ...

122

and dX 0B =

∂X 0B
dX C
∂X C

(98)

The transformation of the bivector coordinate xµν is thus determined by the transformation of the vector coordinates xµ . This is so because the basis bivectors are the wedge
products of basis vectors γµ .
¿From (94) and (98) we see that Γ̃[ρσ]γ transforms like a 4th-rank tensor in an ordinary
space.
Comparing eq.(88) with the relation (67) we find
∂γµ
= [∂α , ∂β ]γµ
(99)
∂xαβ
The derivative of a basis vector with respect to the bivector coordinates xαβ is equal to
the commutator of the derivatives with respect to the vector coordinates xα .
The above relation (99) holds for the basis vectors γµ . For an arbitrary polyvector
A = AA EA = sγ + aα γα + aαβ γα ∧ γβ + ...

(100)

we will assume the validity of the following relation

where D/Dxµν

DAA
= [Dµ , Dν ]AA
(101)
Dxµν
is the covariant derivative, defined in analogous way as in eqs. (58):
∂AA
DAA
C
=
+ Γ̃A
BC A
DX B
∂X B

(102)

Ds
= [Dµ , Dν ]s = Kµν ρ ∂ρ s
Dxµν

(103)

¿From eq.(101) we obtain

Daα
= [Dµ , Dν ]aα = Rµνρ α aρ + Kµν ρ Dρ aα
(104)
Dxµν
Using (102) we have that
∂s
Ds
=
(105)
Dxµν
∂xµν
and
Daα
∂aα
∂aα
α
ρ
=
+
Γ̃
a
=
+ Rµνρ α aρ
(106)
[µν]ρ
Dxµν
∂xµν
∂xµν
where, according to (ii), Γ̃α[µν]ρ has been identified with curvature. So we obtain, after
inserting (105),(106) into (103),(104) that
(a) the partial derivatives of the coefficients s and aα , which are Clifford scalars5 , with
respect to xµν are related to torsion:
∂s
= Kµν ρ ∂ρ s
∂xµν
∂aα
= Kµν ρ Dρ aα
∂xµν
5

(107)
(108)

In the geometric calculus based on Clifford algebra, the coefficients such as s, aα , aαβ , ..., are called
scalars (although in tensor calculus they are called scalars, vectors and tensors, respectively), whilst the
objects γα , γα ∧ γβ , ..., are called vectors, bivectors, etc. .

27

Quantization in Astrophysics ...

123

(b) whilst the derivative of the basis vectors with respect to xµν are related to curvature:
∂γα
= Rµνα β γβ
∂xµν

(109)

In other words, the dependence of coefficients s and aα on xµν indicates the presence
of torsion. On the contrary, when basis vectors γα depend on xµν this indicates that the
corresponding vector space has non vanishing curvature.

4.3

On the relation between the curvature of C-space and the
curvature of an ordinary space

Let us now consider the C-space curvature defined in eq.(79) The indices A,B, can be of
vector, bivector, etc., type. It is instructive to consider a particular example.
A = [µν], B = [αβ], C = γ, D = δ
"

#

!

∂
∂
, αβ γγ · γ δ = R[µν][αβ]γ δ
µν
∂x ∂x

(110)

Using (88) we have
∂
∂
∂
γγ =
(Rαβγ ρ γρ ) = Rαβγ ρ Rµνρ σ γσ
µν
αβ
∂x ∂x
∂xµν

(111)

where we have taken

∂
Rαβγ ρ = 0
(112)
µν
∂x
which is true in the case of vanishing torsion (see also an explanation that follows after
the next paragraph). Inserting (111) into (110) we find
R[µν][αβ]γ δ = Rµνγ ρ Rαβρ δ − Rαβγ ρ Rµνρ δ

(113)

which is the product of two usual curvature tensors. We can proceed in analogous way
to calculate the other components of RABC D such as R[αβγδ][ρσ] µ , R[αβγδ][ρστ κ] [µν] , etc. .
These contain higher powers of the curvature in an ordinary space. All this is true in
our restricted C-space given by eqs.(85),(86) and the assumptions (i),(ii) bellow those
equations. By releasing those restrictions we would have arrived at an even more involved
situation which is beyond the scope of the present paper.
After performing the contractions of (113) and the corresponding higher order relations
we obtain the expansion of the form
R = R + α1 R2 + α2 Rµν Rµν + ...

(114)

So we have shown that the C-space curvature can be expressed as the sum of the products
of the ordinary spacetime curvature. This bears a resemblance to the string effective
action in curved spacetimes given by sums of powers of the curvature tensors based on
the quantization of non-linear sigma models [118].
28

Quantization in Astrophysics ...

124

If one sets aside the algebraic convergence problems when working with Clifford algebras in infinite dimensions, one can consider the possibility of studying Quantum Gravity
in a very large number of dimensions which has been revisited recently [83] in connection
to a perturbative renormalizable quantum theory of gravity in infinite dimensions. Another interesting possibility is that an infinite series expansion of the powers of the scalar
curvature could yield the recently proposed modified Lagrangians R + 1/R of gravity to
accomodate the cosmological accelerated expansion of the Universe [131], after a judicious
choice of the algebraic coefficients is taken. One may notice also that having a vanishing
cosmological constant in C-space, R = Λ = 0 does not necessarily imply that one has a
vanishing cosmological constant in ordinary spacetime. For example, in the very special
case of homogeneous symmetric spacetimes, like spheres and hyperboloids, where all the
curvature tensors are proportional to suitable combinations of the metric tensor times the
scalar curvature, it is possible to envision that the net combination of the sum of all the
powers of the curvature tensors may cancel-out giving an overall zero value R = 0. This
possibility deserves investigation.
Let us now show that for vanishing torsion the curvature is independent of the bivector
coordinates xµν , as it was taken in eq.(112). Consider the basic relation
γµ · γν = gµν

(115)

Differentiating with respect to xαβ we have
∂γµ
∂γν
∂
(γµ · γν ) =
· γν + γµ · αβ = Rαβµν + Rαβνµ = 0
αβ
αβ
∂x
∂x
∂x

(116)

This implies that
∂gµν
= [∂α , ∂β ]gµν = 0
∂σαβ

(117)

Hence the metric, in this particular case, is independent of the holographic (bivector)
coordinates. Since the curvature tensor —when torsion is zero— can be written in terms
of the metric tensor and its derivatives, we conclude that not only the metric, but also
the curvature is independent of xµν . In general, when the metric has a dependence on the
holographic coordinates one expects further corrections to eq.(113) that would include
torsion.

5
5.1

On the Quantization in C-spaces
The momentum constraint in C-space

A detailed discussion of the physical properties of all the components of the polymomentum P in four dimensions and the emergence of the physical mass in Minkowski spacetime
has been provided in the book [15]. The polymomentum in D = 4, canonically conjugate
to the position polyvector
X = σ + xµ γµ + γ µν γµ ∧ γν + ξ µ γ5 γµ + sγ5
29

Quantization in Astrophysics ...

125

(118)

can be written as:
P = µ + pµ γµ + S µν γµ ∧ γν + π µ γ5 γµ + mγ5 .

(119)

where besides the vector components pµ we have the scalar component µ, the 2-vector
components S µν , that are connected to the spin as shown by [14]; the pseudovector components π µ and the pseudoscalar component m.
The most salient feature of the polyparticle dynamics in C-spaces [15] is that one can
start with a constrained action in C-space and arrive, nevertheless, at an unconstrained
Stuckelberg action in Minkowski space (a subspace of C-space) in which pµ pµ is a constant
of motion. The true constraint in C-space is:
PA P A = µ2 + pµ pµ − 2S µν Sµν + πµ π µ − m2 = M 2 .

(120)

where M is a fixed constant, the mass in C-space. The pseudoscalar component m is a
variable, like µ, pµ , S µν , and π µ , which altogether are constrained according to eq.(120).
It becomes the physical mass in Minkwoski spacetime in the special case when other
extra components vanish, i.e., when µ = 0, S µν = 0 and π µ = 0. This justifies using the
notation m for mass. This is basically the distinction between the mass in Minkowski space
which is a constant of motion pµ pµ and the fixed mass M in C-space. The variable m is
canonically conjugate to s which acquires the role of the Stuckelberg evolution parameter
s that allowed ref.[29, 15] to propose a natural solution of the problem of time in quantum
gravity. The polyparticle dynamics in C-space is a generalization of the relativistic Regge
top construction which has recently been studied in de Sitter spaces by [135].
A derivation of a charge, mass, and spin relationship of a polyparticle can be obtained
from the above polymomentum constraint in C-space if one relates the norm of the axialmomentum component π µ of the polymomentum P to the charge [80]. It agrees exactly
with the recent charge-mass-spin relationship obtained by [44] based on the Kerr-Newman
black hole metric solutions of the Einstein-Maxwell equations. The naked singularity KerrNewman solutions have been interpreted by [45] as Dirac particles. Further investigation is
needed to understand better these relationships, in particular, the deep reasons behind the
charge assignment to the norm of the axial-vector π µ component of the polymomentum
which suggests that mass has a gravitational, electromagnetic and rotational aspects to it.
In a Kaluza-Klein reduction from D = 5 to D = 4 it is well known that the electric charge
is related to the p5 component of the momentum. Hence, charge bears a connection to
an internal momentum.

5.2

C-space Klein-Gordon and Dirac Wave Equations

The ordinary Klein-Gordon equation can be easily obtained by implementing the on-shell
constraint p2 − m2 = 0 as an operator constraint on the physical states after replacing pµ
for −i∂/∂xµ (we use units in which h̄ = 1, c = 1):
∂2
+ m2 φ = 0.
∂xµ ∂xµ
!

30

Quantization in Astrophysics ...

126

(121)

The C-space generalization follows from the P 2 − M 2 = 0 condition by replacing
!

∂
∂
∂
∂
PA → −i
,
=
−i
,
, ...
∂X A
∂σ ∂xµ ∂xµν

(122)

∂2
∂2
∂2
+
+ ... + M 2 Φ = 0
(123)
+
∂σ 2 ∂xµ ∂xµ ∂xµν ∂xµν
where we have set L = h̄ = c = 1 for convenience purposes and the C-space scalar field
Φ(σ, xµ , xµν , ....) is a polyvector-valued scalar function of all the C-space variables. This
is the Klein-Gordon equation associated with a free scalar polyparticle in C-space.
A wave equation for a generalized C-space harmonic oscillator requires to introduce the
potential of the form V = κX 2 that admits straightforward solutions in terms of Gaussians
and Hermite polynomials similar to the ordinary point-particle oscillator. There are now
collective excitations of the Clifford-oscillator in terms of the number of Clifford-bits and
which represent the quanta of areas, volumes, hypervolumes,..., associated with the ploops oscillations in Planck scale units. The logarithm of the degeneracy of the first
collective state of the C-space oscillator, as a function of the number of bits, bears the
same functional form as the Bekenstein-Hawking black hole entropy, with the upshot
that one recovers, in a natural way, the logarithmic corrections to the black-hole entropy
as well, if one identifies the number of Clifford-bits with the number of area-quanta of
the black hole horizon. For further details about this derivation and the emergence of
the Schwarzschild horizon radius relation, the Hawking temperature, the maximal Planck
temperature condition, etc., we refer to [21]. Perhaps the most important consequence
of this latter view of black hole entropy is the possibility that there is a ground state of
quantum spacetime, resulting from of a Bose-Einstein condensate of the C-space harmonic
oscillator.
A C-space version of the Dirac Equation, representing the dynamics of spinningpolyparticles (theories of extended-spin, extended charges) is obtained via the square-root
procedure of the Klein-Gordon equation:
!

!

∂
∂
∂
+ γµ
+ γµ ∧ γν
+ ... Ψ = M Ψ
−i
∂σ
∂xµ
∂xµν

(124)

where Ψ(σ, xµ , xµν , ...) is a polyvector-valued function, a Clifford-number, Ψ = ΨA EA of
all the C-space variables. For simplicity we consider here a flat C-space in which the
metric GAB = EA† ∗ EB = ηAB is diagonal, ηAB being the C-space analog of Minkowski
tensor. In curved C-space the equation (124) should be properly generalized. This goes
beyond the scope of the present paper.
Ordinary spinors are nothing but elements of the left/right ideals of a Clifford algebra.
So they are automatically contained in the polyvector valued wave function Ψ. The
ordinary Dirac equation can be obtained when Ψ is independent of the extra variables
associated with a polyvector-valued coordinates X (i.e., of xµν , xµνρ , ...). For details see
[15].
Thus far we have written ordinary wave equations in C-space, that is, we considered
the wave equations for a “point particle” in C-space. From the perspective of the 4dimensional Minkowski spacetime the latter “point particle” has, of course, a much richer
31

Quantization in Astrophysics ...

127

structure then a mere point: it is an extended object, modeled by coordinates xµ , xµν , ....
But such modeling does not embrace all the details of an extended object. In order to
provide a description with more details, one can considere not the “point particles” in
C-space, but branes in C-space. They are described by the embeddings X = X(Σ), that
is X M = X M (ΣA ), considered in sec.3.2. Quantization of such branes can employ wave
functional equation, or other methods, including the second quantization formalism. For
a more detailed study detailed study of the second quantization of extended objects using
the tools of Clifford algebra see [15].
Without emplying Clifford algebra a lot of illuminating work has been done in relation
to description of branes in terms of p-loop coordinates [132]. A bosonic/fermionic p-brane
wave-functional equation was presented in [12], generalizing the closed-string(loop) results
in [13] and the the quantum bosonic p-brane propagator, in the quenched-reduced minisuperspace approximation, was attained by [18]. In the latter work branes are described
in terms of the collective coordinates which are just the highest grade components in the
expansion of a poplyvector X given in eq (2). This work thus paved the way for the next
logical step, that is, to consider other multivector components of X in a unified description
of all branes.
Notice that the approach based on eqs.(123),(124) is different from that by Hestenes
[1] who proposed an equation which is known as the Dirac-Hestenes equation. Dirac’s
equation using quaternions (related to Clifford algebras) was first derived by Lanczos [91].
Later on the Dirac-Lanczos equation was rediscovered by many people, in particular by
Hestenes and Gursey [92] in what became known as the Dirac-Hestenes equation. The
former Dirac-Lanczos equation is Lorentz covariant despite the fact that it singles out
an arbitrary but unique direction in ordinary space: the spin quantization axis. Lanczos,
without knowing, had anticipated the existence of isospin as well. The Dirac-Hestenes
equation ∂Ψe21 = mΨeo is covariant under a change of frame [133] , [93]. e0µ = U eµ U −1
and Ψ0 = ΨU −1 with U an element of the Spin+ (1, 3) yielding ∂Ψ0 e021 = mΨ0 e00 . As
Lanczos had anticipated, in a new frame of reference, the spin quantization axis is also
rotated appropriately , thus there is no breakdown of covariance by introducing bivectors
in the Dirac-Hestenes equation.
However, subtleties still remain. In the Dirac-Hestenes equation instead of the imaginary unit i there occurs the bivector γ1 γ − 2. Its square is −1 and commutes with all the
elements of the Dirac algebra which is just a desired property. But on the other hand, the
introduction of a bivector into an equation implies a selection of a preferred orientation
in spacetime; i.e. the choice of the spin quantization axis in the original Dirac-Lanczos
quaternionic equation. How is such preferred orientation (spin quantization axis) determined ? Is there some dynamical symmetry which determines the preferred orientation
(spin quantization axis) ? is there an action which encodes a hidden dynamical principle
that selects dynamically a preferred spacetime orientation ( spin quantization axis ) ?
Many subtleties of the Dirac-Hesteness equation and its relation to the ordinary Dirac
equation and the Seiberg-Witten equation are investigated from the rigorous mathematical
point of view in refs. [93]. The approach in refs. [16, 15, 17, 8], reviewed here, is different.
We start from the usual formulation of quantum theory and extend it to C-space. We
retain the imaginary unit i. Next step is to give a geometric interpretation to i. Instead
32

Quantization in Astrophysics ...

128

of trying to find a geometric origin of i in spacetime we adopt the interpretation proposed
in [15] according to which the i is the bivector of the 2-dimensional phase space (whose
direct product with the n-dimensional configuration space gives the 2n-dimensional phase
space). 6 This appears to be a natural assumption due to the fact that complex valued
quantum mechanical wave functions involve momenta pµ and coordinates xµ (e.g., a plane
wave is given by exp[ipµ xµ ], and arbitrary wave packet is a superposition of plane waves).

6

Maximal-Acceleration Relativity in Phase-Spaces

In this section we shall discuss the maximal acceleration Relativity principle [68] based on
Finsler geometry which does not destroy, nor deform, Lorentz invariance. Our discussion
differs from the pseudo-complex Lorentz group description by Schuller [61] related to
the effects of maximal acceleration in Born-Infeld models that also maintains Lorentz
invariance, in contrast to the approaches of Double Special Relativity (DSR). In addition
one does not need to modify the energy-momentum addition (conservation) laws in the
scattering of particles which break translational invariance. For a discussions on the
open problems of Double Special Relativity theories based on kappa-deformed Poincare
symmetries [63] and motivated by the anomalous Lorentz-violating dispersion relations in
the ultra high energy cosmic rays [71, 72, 73], we refer to [70].
Related to the minimal Planck scale, an upper limit on the maximal acceleration principle in Nature was proposed by long ago Cainello [52]. This idea is a direct consequence
of a suggestion made years earlier by Max Born on a Dual Relativity principle operating
in phase spaces [49], [74] wherethere is an upper bound on the four-force (maximal string
tension or tidal forces in the string case) acting on a particle as well as an upper bound
in the particle velocity. One can combine the maximum speed of light with a minimum
Planck scale into a maximal proper-accleration a = c2 /L = within the framework of
Finsler geometry [56]. For a recent status of the geometries behind maximal-acceleration
see [73]; its relation to the Double Special Relativity programs was studied by [55] and the
possibility that Moyal deformations of Poincare algebras could be related to the kappadeformed Poincare algebras was raised in [68]. A thorough study of Finsler geometry and
Clifford algebras has been undertaken by Vacaru [81] where Clifford/spinor structures
were defined with respect to Nonlinear connections associated with certain nonholonomic
modifications of Riemann–Cartan gravity.
Other several new physical implications of the maximal acceleration principle in Nature, like neutrino oscillations and other phenomena, have been studied by [54], [67], [42].
Recently, the variations of the fine structure constant α [64], with the cosmological accelerated expansion of the Universe, was recast as a renormalization group-like equation
governing the cosmological reshift (Universe scale) variations of α based on this maximal
acceleration principle in Nature [68]. The fine structure constant was smaller in the past.
Pushing the cuttof scale to the minimum Planck scale led to the intriguing result that
the fine structure constant could have been extremely small (zero) in the early Universe
6
Yet another interpretation of the imaginary unit i present in the Heisenberg uncertainty relations
has been undertaken by Finkelstein and collaborators [96].

33

Quantization in Astrophysics ...

129

and that all matter in the Universe could have emerged via the Unruh-Rindler-Hawking
effect (creation of radiation/matter) due to the acceleration w.r.t the vacuum frame of
reference. For reviews on the alledged variations of the fundamental constants in Nature
see [65] and for more astonishing variations of αdriven by quintessence see [66].

6.1

Clifford algebras in Phase space

We shall employ the procedure described in [15] to construct the Phase Space Clifford
algebra that allowed [127] to reproduce the sub-maximally accelerated particle action of
[53].
For simplicity we will focus on a two-dim phase space. Let ep , eq be the Clifford-algebra
basis elements in a two-dim phase space obeying the following relations [15]:
1
ep .eq ≡ (eq ep + ep eq ) = 0.
2

(125)

and ep .ep = eq .eq = 1.
The Clifford product of ep , eq is by definition the sum of the scalar and the wedge
product:
ep eq = ep .eq + ep ∧ eq = 0 + ep ∧ eq = i.
(126)
such that i2 = ep eq ep eq = −1. Hence, the imaginary unit i, i2 = −1 admits a very natural
interpretation in terms of Clifford algebras, i.e., it is represented by the wedge product
i = ep ∧ eq , a phase-space area element. Such imaginary unit allows us to express vectors
in a C-phase space in the form:
Q = Q = qeq + peq
Qeq = q + pep eq = q + ip = z
eq Q = q + peq ep = q − ip = z ∗

(127)

which reminds us of the creation/anihilation operators used in the harmonic oscillator.
We shall now review the steps in [127] to reproduce the sub-maximally accelerated
particle action [53]. The phase-space analog of the spacetime action is:
2

2

dQ.dQ = (dq) + (dp) ⇒ S = m

Z q

(dq)2 + (dp)2 .

(128)

Introducing the appropriate length/mass scale parameters in order to have consistent
units yields:
s
Z
L
S=m
(dq)2 + ( )2 (dp)2 .
(129)
m
where we have introduced the Planck scale L and have chosen the natural units h̄ = c = 1.
A detailed physical discussion of the dilational invariant system of units h̄ = c = G =
4πo = 1 was presented in ref. [15]. G is the Newton constant and o is the permitivity
of the vacuum.

34

Quantization in Astrophysics ...

130

Extending this two-dim result to a 2n-dim phase space result requires to have for
Clifford basis the elements epµ , eqµ , where µ = 1, 2, 3, ...n. The action in the 2n-dim phase
space is:
S=m

Z

s

s

Z
L
L
(dq µ dqµ ) + ( )2 (dpµ dpµ ) = m dτ 1 + ( )2 (dpµ /dτ )(dpµ /dτ ).
m
m

(130)

where we have factored-out of the square-root the infinitesimal proper-time displacement
(dτ )2 = dq µ dqµ .
One can reccognize the action (130), up to a numerical factor of m/a, where a is the
proper acceleration, as the same action for a sub-maximally accelerated particle given by
Nesterenko [53] by rewriting (dpµ /dτ ) = m(d2 xµ /dτ 2 ):
S=m

Z

q

dτ 1 + L2 (d2 xµ /dτ 2 )(d2 xµ /dτ 2 ).

(131)

Postulating that the maximal proper-acceleration is given in terms of the speed of light
and the minimal Planck scale by a = c2 /L = 1/L, the action above gives the Nesterenko
action, up to a numerical m/a factor:
S=m

Z

q

dτ 1 + a−2 (d2 xµ /dτ 2 )(d2 xµ /dτ 2 ).

(132)

The proper-acceleration is orthogonal to the proper-velocity and this can be easily
verified by differentiating the timelike proper-velocity squared:
V2 =

dV µ
d2 xµ
dxµ dxµ
= V µ Vµ = 1 > 0 ⇒
Vµ =
Vµ = 0.
dτ dτ
dτ
dτ 2

(133)

which implies that the proper-acceleration is spacelike:
s

Z
Z
d2 xµ d2 xµ
g2
g (τ ) = − 2
> 0 ⇒ S = m dτ 1 − 2 = m dω.
dτ dτ 2
a
2

(134)

where the analog of the Lorentz time-dilation factor for a sub-maximally accelerated
particle is given by
s
g 2 (τ )
dω = dτ 1 − 2 .
(135)
a
Therefore the dynamics of a sub-maximally accelerated particle can be reinterpreted
as that of a particle moving in the spacetime tangent bundle whose Finsler-like metric is
(dω)2 = gµν (xµ , dxµ )dxµ dxν = (dτ )2 (1 −

g 2 (τ )
).
a2

(136)

The invariant time now is no longer the standard proper-time τ but is given by the
quantity ω(τ ). The deep connection between the physics of maximal acceleration and
Finsler geometry has been analyzed by [56]. This sort of actions involving second derivatives have also been studied in the construction of actions associated with rigid particles
(strings) [57], [58], [59], [60] among others.
35

Quantization in Astrophysics ...

131

The action is real-valued if, and only if, g 2 < a2 in the same fashion that the action
in Minkowski spacetime is real-valued if, and only if, v 2 < c2 . This is the physical reason
why there is an upper bound in the proper-acceleration. In the special case of uniformlyaccelerated motion g(τ ) = go = constant, the trajectory of the particle in Minkowski
spacetime is a hyperbola.
Most recently, an Extended Relativity Theory in Born-Clifford-Phase spaces with an
upper and lower length scales (infrared/ultraviolet cutoff ) has been constructed [138].
The invariance symmetry associated with an 8D Phase Space leads naturally to the real
Clifford algebra Cl(2, 6, R) and complexified Clifford ClC (4) algebra related to Twistors.
The consequences of Mach’s principle of inertia within the context of Born’s Dual Phase
Space Relativity Principle were also studied in [138] and they were compatible with the
Eddington-Dirac large numbers coincidence and with the observed values of the anomalous
Galileo-Pioneer acceleration. The modified Newtonian dynamics due to the upper/lower
scales and modified Schwarzschild dynamics due the maximal acceleration were also provided.

6.2

Invariance under the U (1, 3) Group

In this section we will review in detail the principle of Maximal-acceleration Relativity
[68] from the perspective of 8D Phase Spaces and the U (1, 3) Group. The U (1, 3) =
SU (1, 3) ⊗ U (1) Group transformations, which leave invariant the phase-space intervals
under rotations, velocity and acceleration boosts, were found by Low [74] and can be
simplified drastically when the velocity/acceleration boosts are taken to lie in the zdirection, leaving the transverse directions x, y, px , py intact ; i.e., the U (1, 1) = SU (1, 1)⊗
U (1) subgroup transformations that leave invariant the phase-space interval are given by
(in units of h̄ = c = 1)
(dE)2 − (dP )2
=
(dσ) = (dT ) − (dX) +
b2
(dE/dτ )2 − (dP/dτ )2
m2 g 2 (τ )
2
(dτ )2 [1 +
]
=
(dτ
)
[1
−
].
b2
m2P A2max
2

2

2

(137)

where we have factored out the proper time infinitesimal (dτ )2 = dT 2 − dX 2 in eq.(137)
and the maximal proper-force is set to be b ≡ mP Amax . mP is the Planck mass 1/LP so
that b = (1/LP )2 , may also be interpreted as the maximal string tension when LP is the
Planck scale.
The quantity g(τ ) is the proper four-acceleration of a particle of mass m in the zdirection which we take to be X. Notice that the invariant interval (dσ)2 in eq.(137) is
not strictly the same as the interval (dω)2 of the Nesterenko action eq.(132), which was
invariant under a pseudo-complexification of the Lorentz group [61]. Only when m = mP ,
the two intervals agree. The interval (dσ)2 described by Low [74] is U (1, 3)-invariant
for the most general transformations in the 8D phase-space. These transformatiosn are
rather elaborate, so we refer to the references [74] for details. The analog of the Lorentz
relativistic factor in eq.(137) involves the ratios of two proper forces. One variable force
is given by ma and the maximal proper force sustained by an elementary particle of
36

Quantization in Astrophysics ...

132

mass mP (a Planckton) is assumed to be Fmax = mP lanck c2 /LP . When m = mP , the
ratio-squared of the forces appearing in the relativistic factor of eq.(137) becomes then
g 2 /A2max , and the phase space interval (137) coincides with the geometric interval of (132).
The transformations laws of the coordinates in that leave invariant the interval (137)
are [74]:
ξv X ξa P sinhξ
.
(138)
T 0 = T coshξ + ( 2 + 2 )
c
b
ξ
sinhξ
E 0 = Ecoshξ + (−ξa X + ξv P )
.
(139)
ξ
ξa E sinhξ
X 0 = Xcoshξ + (ξv T − 2 )
.
(140)
b
ξ
ξv E
sinhξ
P 0 = P coshξ + ( 2 + ξa T )
.
(141)
c
ξ
The ξv is velocity-boost rapidity parameter and the ξa is the force/acceleration-boost
rapidity parameter of the primed-reference frame. They are defined respectively (in the
special case when m = mP ):
v
ξv
tanh( ) =
c
c
ma
ξa
tanh =
.
(142)
b
mP Amax
The effective boost parameter ξ of the U (1, 1) subgroup transformations appearing
in eqs.(138)–(141) is defined in terms of the velocity and acceleration boosts parameters
ξv , ξa respectively as:
s
ξv2 ξa2
+ 2.
(143)
ξ≡
c2
b
Our definition of the rapidity parameters are different than those in [74].
Straightforward algebra allows us to verify that these transformations leave the interval
of eq.(137) in classical phase space invariant. They are are fully consistent with Born’s
duality Relativity symmetry principle [49] (Q, P ) → (P, −Q). By inspection we can see
that under Born duality, the transformations in eqs.(138)–(141) are rotated into each
other, up to numerical b factors in order to match units. When on sets ξa = 0 in (138)–
(141) one recovers automatically the standard Lorentz transformations for the X, T and
E, P variables separately, leaving invariant the intervals dT 2 − dX 2 = (dτ )2 and (dE 2 −
dP 2 )/b2 separately.
When one sets ξv = 0 we obtain the transformations rules of the events in Phase
space, from one reference-frame into another unif ormly-accelerated frame of reference,
a = constant, whose acceleration-rapidity parameter is in this particular case:
ξ≡

ma
ξa
. tanhξ =
.
b
mP Amax

(144)

The transformations for pure acceleration-boosts in are:
T 0 = T coshξ +

P
sinhξ.
b

37

Quantization in Astrophysics ...

133

(145)

E 0 = Ecoshξ − bXsinhξ.

(146)

E
sinhξ
b
P 0 = P coshξ + bT sinhξ.
X 0 = Xcoshξ −

(147)
(148)

It is straightforwad to verify that the transformations (145)–(147) leave invariant the
fully phase space interval (137) but does not leave invariant the proper time interval
(dτ )2 = dT 2 − dX 2 . Only the combination:
(dσ)2 = (dτ )2 (1 −

m2 g 2
)
m2P A2max

(149)

is truly left invariant under pure acceleration-boosts (145)–(147). One can verify as well
that these transformations satisfy Born’s duality symmetry principle:
(T, X) → (E, P ). (E, P ) → (−T, −X).

(150)

and b → 1b . The latter Born duality transformation is nothing but a manifestation of
the large/small tension duality principle reminiscent of the T -duality symmetry in string
theory; i.e. namely, a small/large radius duality, a winding modes/ Kaluza-Klein modes
duality symmetry in string compactifications and the Ultraviolet/Infrared entanglement
in Noncommutative Field Theories. Hence, Born’s duality principle in exchanging coordinates for momenta could be the underlying physical reason behind T -duality in string
theory.
The composition of two succesive pure acceleration-boosts is another pure accelerationboost with acceleration rapidity given by ξ 00 = ξ + ξ 0 . The addition of proper four-forces
( accelerations ) follows the usual relativistic composition rule:
ma0

ma

+ mP A
ma00
tanhξ + tanh ξ 0
mP A
.
⇒
=
tanhξ 00 = tanh(ξ + ξ 0 ) =
m2 aa0
1 + tanhξtanhξ 0
mP A
1+ m
2 A2

(151)

P

and in this fashion the upper limiting proper acceleration is never surpassed like it happens
with the ordinary Special Relativistic addition of velocities.
The group properties of the full combination of velocity and acceleration boosts (138)–
(141) requires much more algebra [68]. A careful study reveals that the composition rule
of two succesive full transformations is given by ξ 00 = ξ + ξ 0 and the transformation laws
are preserved if, and only if, the ξ; ξ 0 ; ξ 00 ...... parameters obeyed the suitable relations:
ξa
ξ0
ξ 00
ξa00
= a0 = a00 =
.
ξ
ξ
ξ
ξ + ξ0

(152)

ξv
ξ0
ξ 00
ξv00
= v0 = v00 =
.
ξ
ξ
ξ
ξ + ξ0

(153)

Finally we arrive at the compostion law for the effective, velocity and acceleration
boosts parameters ξ 00 ; ξv00 ; ξa00 respectively:
38

Quantization in Astrophysics ...

134

ξv00 = ξv + ξv0 .

(154)

ξa00 = ξa + ξa0 .

(155)

ξ 00 = ξ + ξ 0 .

(156)

The relations (152, 153, 154, 155, 156) are required in order to prove the group composition
law of the transformations of (138)–(141) and, consequently, in order to have a truly
Maximal-Acceleration Phase Space Relativity theory resulting from a phase-space change
of coordinates in the cotangent bundle of spacetime.

6.3

Planck-Scale Areas are Invariant under Acceleration Boosts

Having displayed explicity the Group transformations rules of the coordinates in Phase
space we will show why inf inite acceleration-boosts (which is not the same as infinite
proper acceleration) preserve Planck-Scale Areas [68] as a result of the fact that b =
(1/L2P ) equals the maximal invariant force, or string tension, if the units of h̄ = c = 1
are used.
At Planck-scale LP intervals/increments in one reference frame we have by definition
(in units of h̄ = c = 1): ∆X = ∆T = LP and ∆E = ∆P = L1P where b ≡ L12 is the
P
maximal tension. ¿From eqs.(138)–(141) we get for the transformation rules of the finite
intervals ∆X, ∆T, ∆E, ∆P , from one reference frame into another frame, in the inf inite
acceleration-boost limit ξ → ∞,
∆T 0 = LP (coshξ + sinhξ) → ∞

(157)

1
(coshξ − sinhξ) → 0
(158)
LP
by a simple use of L’Hopital’s rule or by noticing that both coshξ; sinhξ functions approach infinity at the same rate.
∆E 0 =

∆X 0 = LP (coshξ − sinhξ) → 0.

(159)

1
(coshξ + sinhξ) → ∞
(160)
LP
where the discrete displacements of two events in Phase Space are defined: ∆X = X2 −
X1 = LP , ∆E = E2 − E1 = L1P , ∆T = T2 − T1 = LP and ∆P = P2 − P1 = L1P .
Due to the identity:
∆P 0 =

(coshξ + sinhξ)(coshξ − sinhξ) = cosh2 ξ − sinh2 ξ = 1

(161)

one can see from eqs. (157)–(160) that the Planck-scale Areas are truly invariant under
infinite acceleration-boosts ξ = ∞:
∆X 0 ∆P 0 = 0 × ∞ = ∆X∆P (cosh2 ξ − sinh2 ξ) = ∆X∆P =
39

Quantization in Astrophysics ...

135

LP
= 1.
LP

(162)

LP
= 1.
LP

(163)

∆X 0 ∆T 0 = 0 × ∞ = ∆X∆T (cosh2 ξ − sinh2 ξ) = ∆X∆T = (LP )2 .

(164)

∆T 0 ∆E 0 = ∞ × 0 = ∆T ∆E(cosh2 ξ − sinh2 ξ) = ∆T ∆E =

∆P 0 ∆E 0 = ∞ × 0 = ∆P ∆E(cosh2 ξ − sinh2 ξ) = ∆P ∆E =

1
.
L2P

(165)

It is important to emphasize that the invariance property of the minimal Planck-scale
Areas (maximal Tension) is not an exclusive property of inf inite acceleration boosts
ξ = ∞, but, as a result of the identity cosh2 ξ − sinh2 ξ = 1, for all values of ξ, the minimal
Planck-scale Areas are always invariant under any acceleration-boosts transformations.
Meaning physically, in units of h̄ = c = 1, that the Maximal Tension (or maximal Force)
b = L12 is a true physical invariant universal quantity. Also we notice that the PhaseP
space areas, or cells, in units of h̄, are also invariant ! The pure-acceleration boosts
transformations are ” symplectic ”. It can be shown also that areas greater ( smaller
) than the Planck-area remain greater ( smaller ) than the invariant Planck-area under
acceleration-boosts transformations.
The infinite acceleration-boosts are closely related to the infinite red-shift effects when
light signals barely escape Black hole Horizons reaching an asymptotic observer with an
infinite redshift factor. The important fact is that the Planck-scale Areas are truly
maintained invariant under acceleration-boosts. This could reveal very important information about Black-holes Entropy and Holography. The logarithimic corrections to the
Black-Hole Area-Entropy relation were obtained directly from Clifford-algebraic methods in C-spaces [21], in addition to the derivation of the maximal Planck temperature
condition and the Schwarzchild radius in terms of the Thermodynamicsof a gas of p-looposcillatorsquanta represented by area-bits, volume-bits, ... hyper-volume-bits in Planck
scale units. Minimal loop-areas, in Planck units, is also one of the most important consequences found in Loop Quantum Gravity long ago [111].

7
7.1

Some Further Important Physical Applications
Related to the C-Space Physics
Relativity of signature

In previous sections we have seen how Clifford algebra can be used in the formulation of
the point particle classical and quantum theory. The metric of spacetime was assumed, as
usually, to have the Minkowski signature, and we have used the choice (+ − −−). There
were arguments in the literature of why the spacetime signature is of the Minkowski type
[113, 43]. But there are also studies in which signature changes are admitted [112]. It has
been found out [16, 15, 30] that within Clifford algebra the signature of the underlying
space is a matter of choice of basis vectors amongst available Clifford numbers. We are
now going to review those important topics.

40

Quantization in Astrophysics ...

136

Suppose we have a 4-dimensional space V4 with signature (+ + + +). Let eµ , µ =
0, 1, 2, 3, be basis vectors satisfying
eµ · eν ≡ 21 (eµ eν + eν eµ ) = δµν ,

(166)

where δµν is the Euclidean signature of V4 . The vectors eµ can be used as generators
of Clifford algebra C4 over V4 with a generic Clifford number (also called polyvector or
Clifford aggregate) expanded in term of eJ = (1, eµ , eµν , eµνα , eµναβ ), µ < ν < α < β,
A = aJ eJ = a + aµ eµ + aµν eµ eν + aµνα eµ eν eα + aµναβ eµ eν eα eβ .

(167)

Let us consider the set of four Clifford numbers (e0 , ei e0 ), i = 1, 2, 3, and denote them as
e0 ≡ γ0 ,
ei e0 ≡ γi .

(168)

The Clifford numbers γµ , µ = 0, 1, 2, 3, satisfy
1
(γ γ
2 µ ν

+ γν γµ ) = ηµν ,

(169)

where ηµν = diag(1, −1, −1, −1) is the Minkowski tensor. We see that the γµ behave
as basis vectors in a 4-dimensional space V1,3 with signature (+ − −−). We can form a
Clifford aggregate
α = αµ γµ
(170)
which has the properties of a vector in V1,3 . From the point of view of the space V4 the
same object α is a linear combination of a vector and bivector:
α = α0 e0 + αi ei e0 .

(171)

We may use γµ as generators of the Clifford algebra C1,3 defined over the pseudo-Euclidean
space V1,3 . The basis elements of C1,3 are γJ = (1, γµ , γµν , γµνα , γµναβ ), with µ < ν < α <
β. A generic Clifford aggregate in C1,3 is given by
B = bJ γJ = b + bµ γµ + bµν γµ γν + bµνα γµ γν γα + bµναβ γµ γν γα γβ .

(172)

With suitable choice of the coefficients bJ = (b, bµ , bµν , bµνα , bµναβ ) we have that B of
eq. (172) is equal to A of eq.(167). Thus the same number A can be described either with
eµ which generate C4 , or with γµ which generate C1,3 . The expansions (172) and (167)
exhaust all possible numbers of the Clifford algebras C1,3 and C4 . Those expansions are
just two different representations of the same set of Clifford numbers (also being called
polyvectors or Clifford aggregates).
As an alternative to (168) we can choose
e0 e3 ≡ γ̃0 ,
ei ≡ γ̃i ,
41

Quantization in Astrophysics ...

137

(173)

from which we have
1
(γ̃ γ̃
2 µ ν

+ γ̃ν γ̃µ ) = η̃µν

(174)

with η̃µν = diag(−1, 1, 1, 1). Obviously γ̃µ are basis vectors of a pseudo-Euclidean space
Ve1,3 and they generate the Clifford algebra over Ve1,3 which is yet another representation of
the same set of objects (i.e., polyvectors). The spaces V4 , V1,3 and Ve1,3 are different slices
through C-space, and they span different subsets of polyvectors. In a similar way we can
obtain spaces with signatures (+ − ++), (+ + −+), (+ + +−), (− + −−), (− − +−),
(−−−+) and corresponding higher dimensional analogs. But we cannot obtain signatures
of the type (+ + −−), (+ − +−), etc. In order to obtain such signatures we proceed as
follows.
4-space. First we observe that the bivector I¯ = e3 e4 satisfies I¯2 = −1, commutes
with e1 , e2 and anticommutes with e3 , e4 . So we obtain that the set of Clifford numbers
¯ e2 I,
¯ e3 , e3 ) satisfies
γµ = (e1 I,
γµ · γν = η̄µν ,
(175)
where η̄ = diag(−1, −1, 1, 1).
8-space. Let eA be basis vectors of 8-dimensional vector space with signature (+ +
+ + + + + +). Let us decompose
eA = (eµ , eµ̄ ) ,

µ = 0, 1, 2, 3,
µ̄ = 0̄, 1̄, 2̄, 3̄.

(176)

The inner product of two basis vectors
eA · eB = δAB ,

(177)

then splits into the following set of equations:
eµ · eν = δµν ,
eµ̄ · eν̄ = δµ̄ν̄ ,
eµ · eν̄ = 0.

(178)

The number I¯ = e0̄ e1̄ e2̄ e3̄ has the properties
I¯2 = 1,
¯ µ = eµ I,
¯
Ie
¯ µ̄ = −eµ̄ I.
¯
Ie

(179)

γµ = eµ ,
γµ̄ = eµ̄ I¯

(180)

γµ · γν = δµν ,
γµ̄ · γν̄ = −δµν ,
γµ · γµ̄ = 0.

(181)

The set of numbers

satisfies

42

Quantization in Astrophysics ...

138

The numbers (γµ , γµ̄ ) thus form a set of basis vectors of a vector space V4,4 with signature
(+ + + + − − −−).
10-space. Let eA = (eµ , eµ̄ ), µ = 1, 2, 3, 4, 5; µ̄ = 1̄, 2̄, 3̄, 4̄, 5̄ be basis vectors of a 10dimensional Euclidean space V10 with signature (+ + +....). We introduce I¯ = e1̄ e2̄ e3̄ e4̄ e5̄
which satisfies
I¯2 = 1 ,
¯µ,
eµ I¯ = −Ie
¯ µ̄ .
eµ̄ I¯ = Ie

(182)

γµ = eµ I¯ ,
γµ̄ = eµ

(183)

γµ · γν = −δµν ,
γµ̄ · γν̄ = δµ̄ν̄ ,
γµ · γµ̄ = 0.

(184)

Then the Clifford numbers

satisfy

The set γA = (γµ , γµ̄ ) therefore spans the vector space of signature (− − − − − + + + ++).
The examples above demonstrate how vector spaces of various signatures are obtained
within a given set of polyvectors. Namely, vector spaces of different signature are different
subsets of polyvectors within the same Clifford algebra. In other words, vector spaces of
different signature are different subspaces of C-space, i.e., different sections through Cspace7 .
This has important physical implications. We have argued that physical quantities are
polyvectors (Clifford numbers or Clifford aggregates). Physical space is then not simply
a vector space (e.g., Minkowski space), but a space of polyvectors, called C-space, a
pandimensional continuum of points, lines, planes, volumes, etc., altogether. Minkowski
space is then just a subspace with pseudo-Euclidean signature. Other subspaces with
other signatures also exist within the pandimensional continuum C and they all have
physical significance. If we describe a particle as moving in Minkowski spacetime V1,3 we
consider only certain physical aspects of the object considered. We have omitted its other
physical properties like spin, charge, magnetic moment, etc.. We can as well describe the
same object as moving in an Euclidean space V4 . Again such a description would reflect
only a part of the underlying physical situation described by Clifford algebra.
7

What we consider here should not be confused with the well known fact that Clifford algebras
associated with vector spaces of different signatures (p, q), with p + q = n, are not all isomorphic.

43

Quantization in Astrophysics ...

139

7.2
7.2.1

Clifford space and the conformal group
Line element in C-space of Minkowski spacetime

In 4-dimensional spacetime a polyvector and its square (1) can be written as
1
dX = dσ + dxµ γµ + dxµν γµ ∧ γν + dx̃µ Iγµ + dσ̃I
2

(185)

1
|dX|2 = dσ 2 + dxµ dxµ + dxµν dxµν − dx̃µ dx̃µ − dσ̃ 2
(186)
2
The minus sign in the last two terms of the above quadratic form occurs because in 4dimensional spacetime with signature (+ − −−) we have I 2 = (γ0 γ1 γ2 γ3 )(γ0 γ1 γ2 γ3 ) = −1,
and I † I = (γ3 γ2 γ1 γ0 )(γ0 γ1 γ2 γ3 ) = −1.
In eq.(186) the line element dxµ dxµ of the ordinary special or general relativity is
replaced by the line element in Clifford space. A “square root” of such a generalized line
element is dX of eq.(185). The latter object is a polyvector, a differential of the coordinate
polyvector field
1
(187)
X = σ + xµ γµ + xµν γµ ∧ γν + x̃µ Iγµ + σ̃I
2
whose square is
1
|X|2 = σ 2 + xµ xµ + xµν xµν − x̃µ x̃µ − σ̃ 2
(188)
2
The polyvector X contains not only the vector part xµ γµ , but also a scalar part σ, tensor
part xµν γµ ∧ γν , pseudovector part x̃µ Iγµ and pseudoscalar part σ̃I. Similarly for the
differential dX.
When calculating the quadratic forms |X|2 and |dX|2 one obtains in 4-dimensional
spacetime with pseudo euclidean signature (+ − −−) the minus sign in front of the
squares of the pseudovector and pseudoscalar terms. This is so, because in such a case
the pseudoscalar unit square in flat spacetime is I 2 = I † I = −1. In 4-dimensions I † = I
regardless of the signature.
Instead of Lorentz transformations—pseudo rotations in spacetime—which preserve
µ
x xµ and dxµ dxµ we have now more general rotations—rotations in C-space—which preserve |X|2 and |dX|2 .
7.2.2

C-space and conformal transformations

From (186) and (188) we see [25] that a subgroup of the Clifford Group, or rotations in
C-space is the group SO(4,2). The transformations of the latter group rotate xµ , σ, σ̃,
but leave xµν and x̃µ unchanged. Although according to our assumption physics takes
place in full C-space, it is very instructive to consider a subspace of C-space, that we shall
call conformal space whose isometry group is SO(4,2).
Coordinates can be given arbitrary symbols. Let us now use the symbol η µ instead of
xµ , and η 5 ,η 6 instead of σ̃, σ. In other words, instead of (xµ , σ̃, σ) we write (η µ , η 5 , η 6 ) ≡ η a ,
µ = 0, 1, 2, 3, a = 0, 1, 2, 3, 5, 6. The quadratic form reads
η a ηa = gab η a η b
44

Quantization in Astrophysics ...

140

(189)

with
gab = diag(1, −1, −1, −1, −1, 1)

(190)

being the diagonal metric of the flat 6-dimensional space, a subspace of C-space,
parametrized by coordinates η a . The transformations which preserve the quadratic form
(189) belong to the group SO(4,2). It is well known [38, 39] that the latter group, when
taken on the cone
η a ηa = 0
(191)
is isomorphic to the 15-parameter group of conformal transformations in 4-dimensional
spacetime [40].
Let us consider first the rotations of η 5 and η 6 which leave coordinates η µ unchanged.
The transformations that leave −(η 5 )2 + (η 6 )2 invariant are
η 05 = η 5 cosh α + η 6 sinh α
η 06 = η 5 sinh α + η 6 cosh α

(192)

where α is a parameter of such pseudo rotations.
Instead of the coordinates η 5 , η 6 we can introduce [38, 39] new coordinates κ, λ
according to
κ = η5 − η6
λ = η5 + η6

(193)
(194)

In the new coordinates the quadratic form (189) reads
η a ηa = η µ ηµ − (η 5 )2 − (η 6 )2 = η µ ηµ − κλ

(195)

The transformation (192) becomes
κ0 = ρ−1 κ

(196)

λ0 = ρλ

(197)

where ρ = eα . This is just a dilation of κ and the inverse dilation of λ.
Let us now introduce new coordinates xµ according xµ to8
η µ = κxµ

(198)

Under the transformation (198) we have
η 0µ = η µ

(199)

x0µ = ρxµ

(200)

but
The latter transformation is dilatation of coordinates xµ .
8

These new coordinates xµ should not be confused with coordinate xµ used in Sec.2.

45

Quantization in Astrophysics ...

141

Considering now a line element
dη a dηa = dη µ dηµ − dκdλ

(201)

we find that on the cone η a ηa = 0 it is
dη a dηa = κ2 dxµ dxµ

(202)

even if κ is not constant. Under the transformation (196) we have
dη 0a dηa0 = dη a dηa

(203)

dx0µ dx0µ = ρ2 dxµ dxµ

(204)

The last relation is a dilatation of the 4-dimensional line element related to coordinates xµ .
In a similar way also other transformations of the group SO(4,2) that preserve (191) and
(203) we can rewrite in terms of of the coordinates xµ . So we obtain—besides dilations—
translations, Lorentz transformations, and special conformal transformations; altogether
they are called conformal transformations. This is a well known old observation [38, 39]
and we shall not discuss it further. What we wanted to point out here is that conformal
group SO(4,2) is a subgroup of the Clifford group.
7.2.3

On the physical interpretation of the conformal group SO(4,2)

In order to understand the physical meaning of the transformations (198) from the coordinates η µ to the coordinates xµ let us consider the following transformation in 6-dimensional
space V6 :
xµ = κ−1 η µ
α = −κ−1
Λ = λ − κ−1 η µ ηµ

(205)

This is a transformation from the coordinates η a = (η µ , κ, λ) to the new coordinates
xa = (xµ , α, Λ). No extra condition on coordinates, such as (191), is assumed now. If
we calculate the line element in the coordinates η a and xa , respectively, we find the the
following relation [27]
dη µ dη ν gµν − dκ dλ = α−2 (dxµ dxν gµν − dαdΛ)

(206)

We can interpret a transformation of coordinates passively or actively. Geometric
calculus clarifies significantly the meaning of passive and active transformations. Under
a passive transformation a vector remains the same, but its components and basis vector
change. For a vector dη = dη a γa we have
dη 0 = dη 0a γa0 = dη a γa = dη
with
dη 0a =

∂η 0a b
dη
∂η b
46

Quantization in Astrophysics ...

142

(207)

(208)

and

∂η b
γb
∂η 0a
Since the vector is invariant, so it is its square:
γa0 =

(209)

0
dη 02 = dη 0a γa0 dη 0b γb0 = dη 0a dη 0b gab
= dη a dη b gab

(210)

¿From (209) we read that the well known relation between new and old coordinates:
0
gab
=

∂η c ∂η d
gcd
∂η 0a ∂η 0b

(211)

Under an active transformation a vector changes. This means that in a fixed basis the
components of a vector change:
dη 0 = dη 0a γa
(212)
with

∂η 0a b
dη
(213)
∂η b
The transformed vector dη 0 is different from the original vector dη = dη a γa . For the
square we find
∂η 0a ∂η 0b c d
dη dη gab
(214)
dη 02 = dη 0a dη 0b gab =
∂η c ∂η d
dη 0a =

i.e., the transformed line element dη 02 is different from the original line element.
Returning now to the coordinate transformation (205) with the identification η 0a = xa ,
we can interpret eq. (206) passively or actively.
In the passive interpretation the metric tensor and the components dη a change under
a transformation, so that in our particular case the relation (210) becomes
0
dxa dxb gab
= α−2 (dxµ dxν gµν − dα dΛ) = dη a dη b gab = dη µ dη ν gµν − dκ dλ

with



gµν
0
gab
= α−2 
 0
0

0
0
− 12



0

− 21 
0



gµν

gab =  0
0

,

0
0
− 12

(215)



0

− 21 
0

(216)

In the above equation the same infinitesimal distance squared is expressed in two different
coordinates η a or xa .
In active interpretation, only dη a change, whilst the metric remains the same, so that
the transformed element is
dxa dxb gab = dxµ dxν gµν − dα dΛ = κ−2 dη a dη b gab = κ−2 (dη µ dη ν gµν − dκ dλ)

(217)

The transformed line lelement dxa dxa is physically different from the original line element
dη a dηa by a factor α2 = κ−2
A rotation (192) in the plane (η 5 , η 6 ) (i.e., the transformation (196),(197) of (κ, λ))
manifests in the new coordinates xa as a dilatation of the line element dxa dxa = κ−2 dη a ηa :
dx0a dx0a = ρ2 dxa dxa
47

Quantization in Astrophysics ...

143

(218)

All this is true in the full space V6 . On the cone η a ηa = 0 we have Λ = λ − κη µ ηµ = 0,
dΛ = 0 so that dxa dxa = dxµ dxµ and we reproduce the relations (204) which is a dilatation
of the 4-dimensional line element. It can be interpreted either passively or actively. In
general, the pseudo rotations in V6 , that is, the transformations of the 15-parameter group
SO(4,2) when expressed in terms of coordinates xa , assume on the cone η a ηa = 0 the form
of the ordinary conformal transformations. They all can be given the active interpretation
[27, 28].
We started from the new paradigm that physical phenomena actually occur not in
spacetime, but in a larger space, the so called Clifford space or C-space which is a manifold
associated with the Clifford algebra generated by the basis vectors γµ of spacetime. An
arbitrary element of Cliffod algebra can be expanded in terms of the objects EA , A =
1, 2, ..., 2D , which include, when D = 4, the scalar unit 1, vectors γµ , bivectors γµ ∧ γν ,
pseudovectors Iγµ and the pseudoscalar unit I ≡ γ5 . C-space contains 6-dimensional
subspace V6 spanned9 by 1, γµ , and γ5 . The metric of V6 has the signature (+ − − − −+).
It is well known that the rotations in V6 , when taken on the conformal cone η a ηa = 0, are
isomorphic to the non linear transformations of the conformal group in spacetime. Thus
we have found out that C-space contains —as a subspace— the 6-dimensional space V6
in which the conformal group acts linearly. From the physical point of view this is an
important and, as far as we know, a novel finding, although it might look mathematically
trivial. So far it has not been clear what could be a physical interpretation of the 6
dimensional conformal space. Now we see that it is just a subspace of Clifford space. The
two extra dimensions, parametrized by κ and λ, are not the ordinary extra dimensions;
they are coordinates of Clifford space C4 of the 4-dimensional Minkowski spacetime V4 .
We take C-space seriously as an arena in which physics takes place. The theory is
a very natural, although not trivial, extension of the special relativity in spacetime. In
special relativity the transformations that preserve the quadratic form are given an active
interpretation: they relate the objects or the systems of reference in relative translational
motion. Analogously also the transformations that preserve the quadratic form (186) or
(188) in C-space should be given an active interpretation. We have found that among
such transformations (rotations in C-space) there exist the transformations of the group
SO(4,2). Those transformations also should be given an active interpretation as the
transformations that relate different physical objects or reference frames. Since in the
ordinary relativity we do not impose any constraint on the coordinates of a freely moving
object so we should not impose any constraint in C-space, or in the subspace V6 . However,
by using the projective coordinate transformation (205), without any constraint such as
η a ηa = 0, we arrived at the relation (217) for the line elements. If in the coordinates η a
the line element is constant, then in the coordinates xa the line element is changing by a
scale factor κ which, in general, depends on the evolution parameter τ . The line element
need not be one associated between two events along a point particle’s worldline: it can
9

It is a well known observation that the generators Lab of SO(4,2) can be realized in terms of 1, γµ , and
γ5 . Lorentz generators are Mµν = − 4i [γµ , γν ], dilatations are generated by D = L65 = − 21 γ5 , translations
by Pµ = L5µ +L6µ = 12 γµ (1−iγ5 ) and the special conformal transformations by L5µ −L6µ = 21 γµ (1+iγ5 ).
This essentially means that the generators are Lab = − 4i [ea , eb ] with ea = (γµ , γ5 , 1), where care must be
taken to replace commutators [1, γ5 ] and [1, γµ ] with 2γ5 and 2γµ

48

Quantization in Astrophysics ...

144

be between two arbitrary (space-like or time-like) events within an extended object. We
may consider the line element (≡ distance squared) between two infinitesimally separated
events within an extended object such that both events have the same coordinate label
Λ so that dΛ = 0. Then the 6-dimensional line element dxµ dxν gµν − dα dΛ becomes
the 4-dimensional line element dxµ dxν gµν and, because of (217) it changes with τ when
κ does change. This means that the object changes its size, it is moving dilatationally
[27, 28]. We have thus arrived at a very far reaching observation that the relativity in Cspace implies scale changes of physical objects as a result of free motion, without presence
of any forces or such fields as assumed in Weyl theory. This was advocated long time
ago [27, 28], but without recurse to C-space. However, if we consider the full Clifford
space C and not only the Minkowski spacetime section through C, then we arrive at a
more general dilatational motion [17] related to the polyvector coordinates xµν , xµνα and
x0123 ≡ σ̃ (also denoted s) as reviewed in section 3.

7.3

C-space Maxwell Electrodynamics

Finally, in this section we will review and complement the proposal of ref.[75] to generalize Maxwell Electrodynamics to C-spaces, namely, construct the Clifford algebra-valued
extension of the Abelian field strength F = dA associated with ordinary vectors Aµ . Using Clifford algebraic methods we shall describe how to generalize Maxwell’s theory of
Electrodynamics asociated with ordinary point-charges to a generalized Maxwell theory
in Clifford spaces involving extended charges and p-forms of arbitrary rank, not unlike
the couplings of p-branes to antisymmetric tensor fields.
Based on the standard definition of the Abelian field strength F = dA we shall use
the same definition in terms of polyvector-valued quantities and differential operators in
C-space
A = AN E N = φ1 + Aµ γ µ + Aµν γ µ ∧ γ ν + ......

(219)

The first component in the expansion φ is a scalar field; Aµ is the standard Maxwell
field Aµ , the third component Aµν is a rank two antisymmetric tensor field....and the last
component of the expansion is a pseudo-scalar. The fact that a scalar and pseudo-scalar
field appear very naturally in the expansion of the C-space polyvector valued field AN
suggests that one could attempt to identify the latter fields with a dilaton-like and axionlike field, respectively.Once again= , in order to match units in the expansion (219),
it requires the introduction of suitable powers of a length scale parameter, the Planck
scalewhich is conveniently set to unity.
The differential operator is the generalized Dirac operator
d = E M ∂M = 1∂σ + γ µ ∂xµ + γ µ ∧ γ ν ∂xµν + ...)

(220)

the polyvector-valued indices M, N.... range from 1, 2.....2D since a Clifford algebra in
D-dim has 2D basis elements. The generalized Maxwell field strength in C-space is
1
F = dA = E M ∂M (E N AN ) = E M E N ∂M AN = {E M , E N }∂M AN +
2
49

Quantization in Astrophysics ...

145

1
1
1 M N
[E , E ]∂M AN = F(M N ) {E M , E N } + F[M N ] [E M , E N ].
(221)
2
2
2
where one has decomposed the Field strength components into a symmetric plus antisymmetric piece by simply writing the Clifford geometric product of two polyvectors E M E N
as the sum of an anticommutator plus a commutator piece respectively,
1
F(M N ) = (∂M AN + ∂N AM ).
2

(222)

1
F[M N ] = (∂M AN − ∂N AM ).
(223)
2
Let the C-space Maxwell action (up to a numerical factor) be given in terms of the
antisymmetric part of the field strength:
I[A] =

Z

[DX]F[M N ] F [M N ] .

(224)

where [DX] is a C-space measure comprised of all the (holographic) coordinates degrees
of freedom
[DX] ≡ (dσ)(dx0 dx1 ...)(dx01 dx02 ...)....(dx012...D ).
(225)
Action (224) is invariant under the gauge transformations
A0M = AM + ∂M Λ

(226)

The matter-field minimal coupling (interaction term) is:
Z

AM dX M =

Z

[DX]JM AM ,

(227)

where one has reabsorbed the coupling constant, the C-space analog of the electric charge,
within the expression for the A field itself. Notice that this term (227) has the same form
as the coupling of p-branes (whose world volume is p + 1-dimensional) to antisymmetric
tensor fields of rank p + 1.
The open line integral in C-space of the matter-field interaction term in the action is
taken from the polyparticle’s proper time interval S ranging from −∞ to +∞ and can be
recast via the Stokes law solely in terms of the antisymmetric part of the field strength.
This requires closing off the integration countour by a semi-circle that starts at S = +∞,
goes all the way to C-space infinity, and comes back to the point S = −∞. The field
strength vanishes along the points of the semi-circle at infinity, and for this reason the
net contribution
to the contour integral is given by the open-line integral. Therefore, by
R
rewriting the AM dX M via the Stokes law relation, it yields
Z

AM dX

M

=
Z

Z

F[M N ] dS

[M N ]

=

F[M N ] X M dX N =

dSF[M N ] X M (dX N /dS).

50

Quantization in Astrophysics ...

Z

146

(228)

where in order to go from the second term to the third term in the above equation we have
integrated by parts and then used the Bianchi identity for the antisymmetric component
F[M N ] .
The integration by parts permits us to go from a C-space domain integral, represented
by the Clifford-value hypersurface S M N , to = a C-space boundary-line integral
1Z
(X M dX N − X N dX M ).
(229)
dS
=
2
The pure matter terms in the action are given by the analog of the proper time integral
spanned by the motion of a particle in spacetime:
Z

MN

s

dX M dXM
.
(230)
dS dS
where κ is a parameter whose dimensions are (mass)p+1 and S is the polyparticle proper
time in C-space.
The Lorentz force relation in C-space is directly obtained from a variation of
κ

Z

dS = κ

Z

and
κ

Z

dS

dSF[M N ] X M (dX N /dS).
Z

dS = κ

Z q

dX M dXM .

(231)

(232)

with respect tothe X M variables:
d2 XM
dX N
=
eF
.
(233)
[M N ]
dS 2
dS
where we have re-introduced the C-space charge e back into the Lorentz force equation in
C-space. A variation of the terms in the action w.r.t the AM field furnishes the following
equation of motion for the A fields:
κ

∂M F [M N ] = J N .

(234)

By taking derivatives on both sides of the last equation with respect to the X N coordinate,
one obtains due to the symmmetry condition of ∂M ∂N versus the antisymmetry of F [M N ]
that
∂N ∂M F [M N ] = 0 = ∂N J N = 0.
(235)
which is precisely the continuity equation for the current.
The continuity
equation
is essential to ensure that the matter-field coupling term of
R
R
M
the action AM dX = [DX]J M AM is also gauge invariant, which can be readily verified
after an integration by parts and setting the boundary terms to zero:
δ

Z

M

[DX]J AM =

Z

Z

M

[DX]J ∂M Λ = − [DX](∂M J M )Λ = 0.

(236)

Gauge invariance also ensures the conservation of the energy-momentum (via Noether’s
theorem) defined in tems of the Lagrangian density variation. We refer to [75] for further
details.
51

Quantization in Astrophysics ...

147

The gauge invariant C-space Maxwell action as given in eq. (224) is in fact only a
part of a more general action given by the expression
I[A] =

Z

†

[DX] F ∗ F =

Z

[DX] < F † F >scalar .

(237)

This action can also be written in terms of components, up to dimension-dependent
numerical coefficients, as [75] :
I[A] =

Z

[DX] (F(M N ) F (M N ) + F[M N ] F [M N ] )

(238)

For rigor, one should introduce the numerical coefficients in front of the F terms, noticing that the symmetric combination should have a different dimension-dependent coefficient than the anti-symmetric combination since the former involves contractions of
{E M , E N }∗ {EM , EN } and the latter contractions of [E M , E N ]∗ [EM , EN ] .
The latter action is strictly speaking not gauge invariant, since it contains not only the
antisymmetric but also the symmetric part of F . It is invariant under a restricted gauge
symmetry transformations. It is invariant ( up to total derivatives) under inf initesimal
gauge transformations provided the symmetric part of F is divergence-free ∂M F (M N ) = 0
[75] . This divergence-free condition has the same effects as if one were fixing a gauge
leaving a residual symmetry of restricted gauge transformations such that the gauge
symmetry parameter obeys the Laplace-like equation ∂M ∂ M Λ = 0. Such residual ( restricted ) symmetries are precisely those that leave invariant the divergence-free condition
on the symmetric part of F . Residual, restricted symmetries occur, for example, in the
light-cone gauge of p-brane actions leaving a residual symmetry of volume-preserving
diffs. They also occur in string theory when the conformal gauge is chosen leaving a
residual symmetry under conformal reparametrizations; i.e. the so-called Virasoro algebras whose symmetry transformations are given by holomorphic and anti-holomorphic
reparametrizations of the string world-sheet.
This Laplace-like condition on the gauge parameter is also the one required such that
the action in [75] is invariant under f inite (restricted) gauge transformations since under
such (restricted) finite transformations the Lagrangian changes by second-order terms of
the form (∂M ∂N Λ)2 , which are total derivatives if, and only if, the gauge parameter is
restricted to obey the analog of Laplace equation ∂M ∂ M Λ = 0
Therefore the action of eq- ( 233 ) is invariant under a restricted gauge transformation
which bears a resemblance to volume-preserving diffeomorphisms of the p-branes action in
the light-cone gauge. A lesson that we have from these considerations is that the C-space
Maxwell action written in the form (237) automatically contains a gauge fixing term.
Analogous result for ordinary Maxwell field is known from Hestenes work [1], although
formulated in a slightly different way, namely by direclty considering the field equations
without emplying the action.
It remains to be seen if this construction of C-space generalized Maxwell Electrodynamics of p-forms can be generalized to the Nonabelian case when we replace ordinary
derivatives by gauge-covariant ones:
F = dA → F = DA = (dA + A • A).
52

Quantization in Astrophysics ...

148

(239)

For example, one could define the graded-symmetric product EM • EN based on the
graded commutator of Superalgebras:
[A, B] = AB − (−1)sA sB BA.

(240)

sA , sB is the grade of A and B respectively. For bosons the grade is even and for fermions
is odd. In this fashion the graded commutator captures both the anti-commutator of two
fermions and the commutator of two bosons in one stroke. One may extend this graded
bracket definition to the graded structure present in Clifford algebras, and define
EM • EN = EM EN − (−1)sM sN EN EM .

(241)

sM , sN is the grade of EM and EN respectively. Even or odd depending on the grade of
the basis elements.
One may generalize Maxwell’s theory to Born-Infeld nonlinear Electrodynamics in Cspacesbased on this extension of Maxwell Electrodynamics in C-spaces and to couple a
C-space version of a Yang-Mills theory to C-space gravity, a higher derivative gravity with
torsion, this will be left for a future publication. Clifford algebras have been used in the
past [62] to study the Born-Infeld model in ordinary spacetime and to write a nonlinear
version of the Dirac equation. The natural incorporation of monopoles in Maxwell’s theory
was investigated by [89] and a recent critical analysis of ” unified ” theories of gravity
with electromagnetism has been presented by [90]. Most recently [22] has studied the
covariance of Maxwell’s theory from a Clifford algebraic point of view.

8

Concluding Remarks

We have presented a brief review of some of the most important features of the Extended
Relativity theory in Clifford-spaces (C-spaces). The ”coordinates” X are noncommuting
Clifford-valued quantities which incoporate the lines, areas, volumes,....degrees of freedom
associated with the collective particle, string, membrane,... dynamics underlying the
center-of-mass motion and holographic projections of the p-loops onto the embedding
target spacetime backgrounds. C-space Relativity incoporates the idea of an invariant
length, which upon quantization, should lead to the notion of minimal Planck scale [23].
Other relevant features are those of maximal acceleration [52], [49] ; the invariance of
Planck-areas under acceleration boosts; the resolution of ordering ambiguities in QFT;
supersymmetry ; holography [119]; the emergence of higher derivative gravity with torsion
;and the inclusion of variable dimensions/signatures that allows to study the dynamics of
all (closed) p-branes, for all values of p, in one single unified footing, by starting with the
C-space brane action constructed in this work.
The Conformal group construction presented in 7 , as a natural subgroup of the
Clifford group in four-dimenions, needs to be generalized to other dimensions, in particular
to two dimensions where the Conformal group is infinite-dimensional. Kinani [130] has
shown that the Virasoro algebra can be obtained from generalized Clifford algebras. The
construction of area-preserving diffs algebras, like w∞ and su(∞), from Clifford algebras
remains an open problem. Area-preserving diffs algebras are very important in the study
53

Quantization in Astrophysics ...

149

of membranes and gravity since Higher-dim Gravity in m + n-dim has been shown a
while ago to be equivalent to a lower m-dim Yang-Mills-like gauge theory of diffs of an
internal n-dim space [120] and that amounts to another explanation of the holographic
principle behind the AdS/CF T duality conjecture [121]. We have shown how C-space
Relativity involves scale changes in the sizes of physical objects, in the absence of forces
and Weyl’gauge field of dilations. The introduction of scale-motion degrees of freedom
has recently been implemented in the wavelet-based regularization procedure of QFT by
[87]. The connection to Penrose’s Twistors program is another interesting project worthy
of investigation.
The quantization and construction of QFTs in C-spaces remains a very daunting
task since it may involve the construction of QM in Noncommutative spacetimes [136],
braided Hopf quantum Clifford algebras [86], hypercomplex extensions of QM like quaternionic and octonionic QM [99], [97], [98], exceptional group extensions of the Standard
Model [85],hyper-matrices and hyper-determinants [88], multi-symplectic mechanics, the
de Donde-Weyl formulations of QFT [82], to cite a few, for example. The quantization
program in C-spaces should share similar results as those in Loop Quantum Gravity [111],
in particular the minimal Planck areas of the expectation values of the area-operator.
Spacetime at the Planck scale may be discrete, fractal, fuzzy, noncommutative... The
original Scale Relativity theory in fractal spacetime [23] needs to be extended futher
to incoporate the notion of fractal ”manifolds”. A scale-fractal calculus and a fractalanalysis construction that are esential in building the notion of a fractal ”manifold” has
been initiated in the past years by [129]. It remains yet to be proven that a scalefractal calculus in fractal spacetimes is another realization of a Connes Noncommutative
Geometry. Fractal strings/branes and their spectrum have been studied by [104] that
may require generalized Statistics beyond the Boltzmann-Gibbs, Bose-Einstein and FermiDirac, investigated by [105], [103], among others.
Non-Archimedean geometry has been recognized long ago as the natural one operating at the minimal Planck scale and requires the use p-adic numbers instead of ordinary
numbers [101]. By implementing the small/large scale, ultraviolet/infrared duality principle associated with QFTs in Noncommutative spaces, see [125] for a review, one would
expect an upper maximum scale [23] and a maximum temperature [21] to be operating
in Nature. Non-Archimedean Cosmologies based on an upper scale has been investigated
by [94].
An upper/lower scale can be accomodated simultaneously and very naturally in the
q-Gravity theory of [114], [69] based on bicovariant quantum group extensions of the
Poincare, Conformal group, where the q deformation parameter could be equated to the
quantity eΛ/L , such that both Λ = 0 and L = ∞, yield the same classical q = 1 limit. For
a review of q-deformations of Clifford algebras and their generalizations see [86], [128].
It was advocated long ago by Wheeler and others, that information theory [106], set
theory and number theory, may be the ultimate physical theory. The important role of
Clifford algebras in information theory have been known for some time [95]. Wheeler’s
spacetime foam at the Planck scale may be the background source generation of Noise in
the Parisi-Wu stochastic qunatization [47] that is very relevant in Number theory [100].
The pre-geometry cellular-networks approach of [107] and the quantum-topos views based
54

Quantization in Astrophysics ...

150

on gravitational quantum causal sets, noncommutative topology and category theory
[109], [110], [124] deserves a futher study within the C-space Relativity framework, since
the latter theory also invokes a Category point of view to the notion of dimensions.
C-space is a pandimensional continuum [14], [8]. Dimensions are topological invariants
and, since the dimensions of the extended objects change in C-space, topology-change is
another ingredient that needs to be addressed in C-space Relativity and which may shed
some light into the physical foundations of string/M theory [118]. It has been speculated
that the universal symmetries of string theory [108] may be linked to Borcherds Vertex
operator algebras (the Monstruous moonshine) that underline the deep interplay between
Conformal Field Theories and Number theory. A lot remains to be done to bridge together
these numerous branches of physics and mathematics. Many surprises may lie ahead of
us. For a most recent discussion on the path towards a Clifford-Geometric Unified Field
theory of all forces see [138], [140]. The notion of a Generalized Supersymmetry in Clifford
Superspaces as extensions of M, F theory algebras was recently advanced in [139]
Acknowledgements
We are indebted to M. Bowers and C. Handy at the CTSPS ( Atlanta ) for support .
The work of M. P. has been supported by the Ministry of Education, Science and Sport
of Slovenia.

References
[1] D. Hestenes, “Spacetime Algebra” Gordon and Breach, New York, 1996.
D. Hestenes and G. Sobcyk, “Clifford Algebra to Geometric Calculus” D. Reidel
Publishing Company, Dordrecht, 1984.
[2] I. R. Porteous, Clifford Algebras and the Classical Groups, Cambridge University
Press, 1995.
[3] W. Baylis, Electrodynamics, A Modern Geometric Approach, Boston, Birkhauser,
1999.
[4] G. Trayling and W. Baylis, J.Phys. A34 (2001) 3309-3324; Int.J.Mod.Phys. A16S1C
(2001) 909-912.
[5] B. Jancewicz, Multivectors and Clifford Algebra in Electrodynamics World Scientific, Singapore 1989.
[6] “Clifford Algebras and their applications in Mathematical PhysicsVol 1: Algebras
and Physics. eds by R. Ablamowicz, B. Fauser. Vol 2: Clifford analysis. eds by J.
Ryan, W. Sprosig Birkhauser, Boston 2000.
[7] P. Lounesto, “Clifford Algebras and Spinors”. Cambridge University Press. 1997.
[8] C. Castro, Chaos, Solitons and Fractals 10 (1999) 295. Chaos, Solitons and Fractals
12 (2001) 1585. ” The Search for the Origins of M Theory: Loop Quantum Mechanics, Loops/Strings and Bulk/Boundary Dualities ” [arXiv: hep-th/9809102].
55

Quantization in Astrophysics ...

151

[9] C. Castro The Programs of the Extended Relavity in C-spaces, towards physical
foundations of String Theory, Advance NATO Workshop on the Nature of Time,
Geometry and the Physics of Perception, Tatranksa Lomnica, Slovakia, May 2001.
Kluwer Publishers 2003.
[10] C. Castro, Chaos, Solitons and Fractals 11 (2000) 1663. Foundations of Physics 30
(2000) 1301.
[11] D. Amati, M. Ciafaloni and G. Veneziano, Phys. Letts B 197 (1987) 81. D. Gross,
P. Mende, Phys. Letts B 197 (1987) 129. M.Maggiore, Phys. Lett B 304 (1993)
65.
[12] S. Ansoldi, C.Castro and E. Spalluci, Class. Quant. Grav 18 (1999) 1833. C. Castro,
Chaos, Solitons and Fractals 11 (2000) 1721
[13] Y.Hosotani, Phys. Rev. Lett 55 (1985) 1719. L.Carson, C.H. Ho and Y. Hosotani,
Phys. Rev. D 37 (1988) 1519. C.H. Ho, Jour. Math. Phys 30 (1989) 2168.
[14] W. Pezzaglia, “Physical Applications of a Generalized Geometric Calculus” [arXiv:
gr-qc/9710027]. Dimensionally Democratic calculus and Principles of Polydimensional Physics [arXiv: gr-qc/9912025]. Classification of Multivector Theories and
Modifications of the Postulates of Physics [arXiv: gr-qc/9306006] ; Physical Applications of Generalized Clifford Calculus: Papatetrou equations and Metamorphic
Curvature [arXiv: gr-qc/9710027]. Classification of Multivector theories and modification of the postulates of Physics [arXiv: gr-qc/9306006].
[15] M.Pavšič: “The Landscape of Theoretical Physics: A Global View; From Point
Particle to the Brane World and Beyond, in Search of Unifying Principle”, Kluwer
Academic, Dordrecht 2001.
[16] M.Pavšič, Foundations of Physics 31 (2001) 1185.
[17] M.Pavšič, Foundations of Physics 33 (2003) 1277 gr-qc/0211085.
[18] S. Ansoldi, A. Aurilia, C. Castro, E. Spallucci, Phys. Rev. D 64 026003 (2001)
[19] A. Aurilia, S. Ansoldi and E. Spallucci, Class. Quant. Grav. 19 (2002) 3207
[arXiv:hep-th/0205028].
[20] C. Castro, M. Pavšič, Phys. Lets. B 539 (2002) 133.
[21] C. Castro, Jour. of Entropy 3 (2001) 12-26. C. Castro and Alex Granik, Foundations
of Physics 33 (3) (2003) 445.
[22] T. Ivezic, Foundations of Physics Lett 15 (2002) 27. Invariant Relativistic Electrodynamics, Clifford Algebra Approach arXiv: hep-th/0207250].

56

Quantization in Astrophysics ...

152

[23] L. Nottale: Fractal Spacetime and Microphysics, towards the Theory of Scale Relativity, World Scientific 1992. La Relativité dans tous ses états, Hachette Literature,
Paris,
[24] S. Ansoldi, A. Aurilia and E. Spallucci, Phys. Rev D 56 no. 4 (1997) 2352. Chaos,
Solitons and Fractals 10 (1999) 197.
[25] C. Castro and M. Pavšič, Int. J. Theor. Phys. 42 (2003) 1693.
[26] M. Pavšič, Class. Quant. Grav. 20 (2003) 2697 [arXiv:gr-qc/0111092].
[27] M. Pavšič, Nuovo Cim. B 41 (1977) 397; International Journal of Theoretical
Physics 14, 299 (1975)
[28] M. Pavšič, J. Phys. A 13 (1980) 1367.
[29] M. Pavšič, Found. Phys. 26 (1996) 159 [arXiv:gr-qc/9506057].
[30] M. Pavšič, arXiv:gr-qc/0210060.
[31] J. P. Terletsky, Doklady Akadm. Nauk SSSR, 133, 329 (1960); M. P. Bilaniuk, V.
K. Deshpande and E.C. G. Sudarshan, American Journal of Physics 30, 718 (1962);
E. Recami and R. Mignani, Rivista del Nuovo Cimento 4, 209 (1974); E. Recami,
Rivista del Nuovo Cimento 9, 1 (1986)
[32] M. Pavšič, J. Phys. A 14 (1981) 3217.
[33] E.C.G. Stueckelberg, Helv. Phys. Acta, 14, 322 (1941); 14, 588 (1941); 15, 23 (1942)
[34] L.P. Horwitz and C. Piron, Helv. Phys. Acta, 46, 316 (1973); L.P. Horwitz and F.
Rohrlich, Physical Review D 24, 1528 (1981); 26, 3452 (1982); L.P. Horwitz, R.I.
Arshansky and A.C. Elitzur Found. Phys 18, 1159 (1988); R.I. Arshansky, L.P.
Horwitz andY. Lavie, Foundations of Physics 13, 1167 (1983)
[35] M. Pavšič, Found. Phys. 21, 1005 (1991); Nuovo Cim. A104, 1337 (1991); Doga,
Turkish Journ. Phys. 17, 768 (1993) Found. Phys. 25, 819 (1995); Nuovo Cimento A
108, 221 (1995); Foundations of Physics 26, 159 (1996); M. Pavšič, Nuovo Cimento
A 110, 369 (1997); Found. Phys. 28 (1998); 1443.Found. Phys. 28 (1998) 1453.
[36] C.W. Misner, K. S. Thorne and J. A. Wheeler, Gravitation, Freeman, San Francisco,
1973), p. 1215
[37] A. Schild, Phys.Rev. D16, 1722 (1977)
[38] H.A. Kastrup, Annaled der Physik (Lpz.) 7, 388 (1962)
[39] A.O. Barut and R. B. Haugen, Annals of Physics 71, 519 (1972)

57

Quantization in Astrophysics ...

153

[40] E. Cunningham, Proc. London Math. Soc. 8, 77 (1909); H. Bateman, Proc. London
Math. Soc. 8, 223 (1910); T. Fulton, F. Rohrlich and L. Witten, Rev. Mod. Phys.
34, 442 (1962); J. Wess, Nuovo Cim., 18, 1086 (1960); G. Mack, Nucl. Phys. B5,
499 (1968); A.F. Grillo, Riv. Nuovo Cim. 3, 146 (1973); J. Niederle and J. Tolar,
Czech. J. Phys. B23, 871 (1973)
[41] D. Hestenes and R. Ziegler, Projective Geometry with Clifford Algebra, Acta Applicandae Mathematicae 23 (1991) 25-63.
[42] D. V Ahluwalia and C. Burgard, General Relativity and Gravitation 28 (10) (1996)
1163. D.V. Ahluwalia: General Relativity and Gravitation 29 (12) (1997) 1491.
D.V. Ahluwalia, Phys. Letts A 275 (2000) 31. G.Adunas, E. Rodriguez-Milla and
D.V. Ahluwalia, Phys. Letts B 485 (2000) 215.
[43] F.D. Smith, Int. Jour. of Theoretical Physics 24 (1985) 155. Int. Jour. of Theoretical Physics 25 (1985) 355. From Sets to Quarks [arXiv: hep-ph/9708379].
http://www.innerx.net/personal/tsmith/clfpq.html.
[44] F. I. Cooperstock, V. Faraoini, ” Extended Planck Scale ” [arXiv: hep-th/0302080].
[45] H. I. Arcos and J. G. Pereira, ” Kerr-Newman solutions as a Dirac particle” [arXiv:
hep-th/0210103].
[46] W. Smilga: Higher order terms in the contraction of S0(3, 2) [arXiv:
th/0304137]. A. Wyler, C. R. Acad. Sc. Paris 269 (1969) 743-745.

hep-

[47] C.Beck, Spatio-Temporal Vacuum fluctuations of Quantized Fields . World Scientific
series in Advances in Nonlinear Dynamics, vol. 21 (2002).
[48] G. Trayling and W. Baylis, J. Phys. A 34 (2001) 3309. J. Chisholm and R. Farwell:
J. Phys. A 32 (1999) 2805. J. Chisholm and R. Farwell: Foundations of Physics
25 (1995) 1511.
[49] M. Born: Proc. Royal Society A 165 (1938) 291. Rev. Mod. Physics 21 (1949)
463.
[50] Clifford (Geometric) Algebras, with applications to Physics, Mathematics and Engineering W. E. Baylis, editor; Birkhauser, Boston, 1997.
[51] S. Somaro: Higher Spin and the Spacetime Algebra V. Dietrich et al. (eds), Clifford
Algebras and their Applications in Mathematical Physics (1998) 347-368; Kluwer
Academic Publishers, the Netherlands.
[52] E. Caianiello, “Is there a maximal acceleration?”, Lett. Nuovo Cimento 32 (1981)
65.
[53] V. Nesterenko, Class. Quant. Grav. 9 (1992) 1101; Phys. Lett. B 327 (1994) 50;

58

Quantization in Astrophysics ...

154

[54] V. Bozza, A. Feoli, G. Lambiase, G. Papini and G. Scarpetta, Phys. Let A 283
(2001) 53. V. Nesterenko, A. Feoli, G. Lambiase and G. Scarpetta, Phys. Rev D
60, 065001 (1999).
[55] K. Rama, ”Classical velocity in kappa-deformed Poincare algebra and amaximal
acceleration” [arXiv: hep-th/0209129].
[56] H. Brandt: Contemporary Mathematics 196 (1996) 273. Chaos, Solitons and Fractals 10 (2-3) (1999) 267.
[57] M.Pavšič: Phys. Lett B 205 (1988) 231 ; Phys. Lett B 221 (1989) 264. H. Arodz,
A. Sitarz, P. Wegrzyn: Acta Physica Polonica B 20 (1989) 921
[58] M. Plyushchay, ” Comment on the relativistic particle with curvature and torsion of
world trajectory” [arXiv: hep-th/9810101]. Mod.Phys.Lett. A10 (1995) 1463-1469
[59] E. Ramos, J. Roca, Nucl.Phys. B477 (1996) 606-622
[60] H. Kleinert, A.M Chervyakov, ” Evidence for Negative Stiffness of QCD Strings ”
[arXiv: hep-th/9601030].
[61] F. Schuller, Annals of Phys. 299 (2002) 174.
[62] A. Chernitskii: Born-Infeld electrodynamics, Clifford numbers and spinor representations [arXiv: hep-th/0009121].
[63] J. Lukierski, A. Nowicki, H. Ruegg, V. Tolstoy, Phys. Lett 264 (1991) 331. J.
Lukierski, H. Ruegg, W. Zakrzewski: Ann. Phys 243 (1995) 90. J. Lukierski, A.
Nowicki: Double Special Relativity verus kappa-deformed dynamics. [arXiv: hepth/0203065].
[64] J. Webb, M. Murphy, V. Flambaum, V. Dzuba, J. Barrow, C. Churchill, J.
Prochaska, and A. Wolfe, ” Further evidence for Cosmological Evolution of the
Fine Structure Constant ” Monthly Notices of the Royal Astronomical Society 327
(2001) 1208.
[65] J.P. Uzan, ” The fundamental constants and their variations: observational status
and theoretical motivations” [arXiv: hep-ph/0205340].
[66] L. Anchordogui, H. Goldberg, Phys. Rev. D 68 (2003) 083513.
[67] G. Lambiase, G.Papini. G. Scarpetta: Maximal Acceleration Corrections to the
Lamb Shift of one Electron Atoms [ arXiv: hep-th/9702130]. G. Lambiase, G.Papini.
G. Scarpetta, Phys. Let A 224 (1998) 349. G. Papini, ” Shadows of a maximal
acceleration” [arXiv: gr-qc/0211011].
[68] C. Castro, Int. J. Mod. Phys. A 18 (2003) 5445 [arXiv: hep-th/0210061]
[69] L. Castellani: Phys. Lett B 327 (1994) 22. Comm. Math. Phys 171 (1995) 383.
59

Quantization in Astrophysics ...

155

[70] G.Amelino-Camelia, Phys. Lett B 510 (2001) 255. Int. J. Mod. Phys D 11 (2002)
35. Int. J. Mod. Phys D 11 (2002) 1643.
[71] K. Greisen, Phys. Rev. Lett 16 (1966) 748. G.T. Zatsepin, V. Kurmin, Sov. Phys.
JETP Lett 4 (1966) 78.
[72] J. Ellis, N. Mavromatos and D. V. Nanopolous, Chaos, Solitons and Fractals, 10
(1999) 345.
[73] M. Toller, “The Geometry of Maximal Acceleration” [ArXiv: hep-th/0312016].
[74] S. Low: Jour. Phys A Math. Gen 35 (2002) 5711.
[75] C.Castro, Mod. Phys. Letts. A 19 (2004) 19
[76] Y. Choquet-Bruhat, C. DeWitt-Morette and M. Dillard-Bleick, Analysis, Manifolds
and Physics (revised edition), North Holland Publ. Co., Amsterdam, 1982.
[77] A. Crumeyrole, Orthogonal and Sympletic Clifford Algebras, Kluwer Acad, Publ.,
Dordrecht, 1990.
[78] H. Blaine Lawson and M.L. Michelson, Spin Geometry, Princeton University Press,
Princeton, 1980.
[79] A.M.Moya, V.V Fernandez and W.A. Rodrigues, Int.J.Theor.Phys. 40 (2001) 23472378 [arXiv: math-ph/0302007]; Multivector Functions of a Multivector Variable
[arXiv: math.GM/0212223]; Multivector Functionals [arXiv: math.GM/0212224]
[80] C. Castro, “The charge-mass-spin relationship of a Clifford polyparticle, KerrNewman Black holes and the Fine structure Constant”, to appear in Foundations
of Physics.
[81] S. Vacaru and N Vicol, ” Nonlinear Connections and Clifford Structures” [ arXiv
: math.DG/0205190]. S. Vacaru, ” (Non) Commutative Finsler Geometry from
String/M Theory ” [arXiv: hep-th/0211068]. S. Vacaru, A. Nadejda, Int. J.
Math. Math. Sci. 23 (2004) 1189-1233. S. Vacaru, ”Clifford Structures and Spinors
on Spaces with Local Anisotropy”, Buletinul Academiei de Stiinte a Republicii
Moldova, Fizica si Tehnica [Izvestia Academii Nauk Respubliky Moldova, fizica i
tehnika], 3, 53-62 (1995), Matscinet: 98i:53022 S. Vacaru, ”Superstrings in Higher
Order Extensions of Finsler Superspaces”, Nucl. Phys. B, 434 (1997) 590 -656. S.
Vacaru and P. Stavrinos, ”Spinors and Space-Time Anisotropy” (Athens University
Press, Athens, Greece, 2002), 301 pages, [ arXiv : gr-qc/0112028].
[82] I.Kanatchikov, Rep. Math. Phys 43
ph/0306101].

(1999) 157. V. Kisil, [arXiv:

quant-

[83] N. Bjerrus-Bohr, ” Quantum Gravity at large number of dimensions” [arXiv: hepth/0310263]. J.F. Donoghue, Phys. Rev. D 50 (1994) 3874. A. Strominger, Phys.
Rev. D 24 (1981) 3082.
60

Quantization in Astrophysics ...

156

[84] W.A. Rodrigues, Jr, J. Vaz, Jr, Adv. Appl. Clifford Algebras 7 ( 1997 ) 457-466.
E.C de Oliveira and W.A. Rodrigues, Jr, Ann. der Physik 7 ( 1998 ) 654-659. Phys.
Lett bf A 291 ( 2001 ) 367-370. W.A. Rodrigues, Jr, J.Y.Lu, Foundations of Physics
27 ( 1997 ) 435-508.
[85] P. Ramond, ” Exceptional Groups and Physics ” [arXiv: hep-th/0301050]
[86] . B. Fauser, ” A treatise on Quantum Clifford Algebras” [ arXiv: math.QA/0202059.]
[87] M. Altaisky, ” Wavelet based regularization for Euclidean field theory and stochastic
quantization ” [arXiv: hep-th/0311048].
[88] V. Tapia, ” Polynomial identities for Hypermatrices” [ arXiv: math-ph/0208010].
[89] M. Defaria-Rosa, E. Recami and W. Rodrigues, Phys. Letts B 173 (1986) 233.
[90] E. Capelas de Oliveira and W.A Rodrigues, ” Clifford valued Differential Forms,
Algebraic spinor fields, Gravitation, Electromagnetism and ” unified ” theories ”
[arXiv: math-ph/0311001].
[91] C. Lanczos, Z. Physik 57 (1929) 447–473; ibid 474-483; ibid 484–493; C. Lanczos,
Physik. Zeitschr. 31 (1930) 120–130.
[92] F. Gursey, Applications of Quaternions to Field Equations, Ph.D thesis, University
of London (1950) 204 pp.
[93] R.A. Mosna and W.A. Rodrigues, Jr., J. Math. Phys. 45 (2004) 7 [arXiv: mathph/0212033]; W.A. Rodrigues, J. Math. Phys. 45 (2004) (to appear) [arXiv: mathph/0212030]
[94] K. Avinash and V. I. Rvachev, Foundations of Physics 30(2000) 139.
[95] D. Gottesman, ”The Heisenberg Representation of Quantum Computers” [ArXiv:
quant-ph9807006].
[96] J. Baugh, D, Finkelstein, A. Galiautdinov and M. Shiri-Garakaui, Found.Phys. 33
(2003) 1267-1275
[97] S.L. Adler, Quaternionic Quantum Mechanics and Quantum Fields Oxford Univ.
Press, New York, 1995.
[98] S. de Leo and K. Abdel-Khalek, ” Towards an Octonionic World ” [arXiv:hepth/9905123]. S. de Leo, ” Hypercomplex Group Theory ” [arXiv: physics/9703033].
[99] C.Castro, ” Noncommutative QM and Geometry from quantization in C-spaces ”
[arXiv: hep-th/0206181].
[100] M.
Watkins,
”
Number
theory
http://www.maths.ex.ac.uk/mwatkins
61

Quantization in Astrophysics ...

157

and

Physics

”

website,

[101] V. Vladimorov, I. Volovich and I. Zelenov, p-adic Numbers in Mathematical Physics World Scientific, 1994, Singapore. L. Brekke and P. Freund,
Phys. Reports 231
(1991) 1. M. Pitkanen, Topological Geometrodynamics
http://www.physics.helsinki.fi/matpitka/tgd.html
[102] C.N. Yang, Phys. Rev. 72 (1947) 874; H.S. Snyder, Phys. Rev. 71 (1947) 38. H.S.
Snyder, Phys. Rev 72 (1947) 68. S. Tanaka, Yang’s Quantized Spacetime Algebra
and Holographic Hypothesis [arXiv: hep-th/0303105].
[103] W. da Cruz: ”Fractal von Neumann Entropy” [arXiv: cond-mat/0201489]. ” Fractons and Fractal Statistics “ arXiv: hep-th/9905229.
[104] C.Castro, Chaos, Solitons and Fractals 14 (2002) 1341. ibid 15 (2003) 797. M.
Lapidus and M. Frankenhuysen, Fractal strings, complex dimensions and the zeros
of the zeta function Birkhauser, New York (2000).
[105] C. Castro, ” The Nonextensive Statistics of Fractal Strings and Branes ” to appear
in Physica A . J. Havrda and F. Charvat: Kybernatica 3 (1967) 30. C-Tsallis,
Jour. of Statistical Physics 52 (1988) 479. C.Tsallis, “ Non-extensive Statistical
mechanics: A brief review of its present status “ [arXiv: cond-mat/0205571].
[106] D. Frieden, Physics from Fisher Information Theory, Cambridge University Press
1998.
[107] T. Nowotny, M. Requardt, Chaos, Solitons and Fractals 10 (1999) 469.
[108] , F. Lizzi, R.J Szabo, Chaos, Solitons and Fractals 10 (1999) 445.
[109] I. Raptis, ” Quantum Space-Time as a Quantum Causal Set” [arXiv gr-qc/0201004].
” Presheaves, Sheaves and their Topoi in Quantum Gravity and Quantum Logic”
[arXiv: gr-qc/0110064]. ” Non-Commutative Topology for Curved Quantum Causality ” [arXiv: gr-qc/0101082].
[110] C. Isham and J. Butterfield, Found.Phys. 30 (2000) 1707-1735. A. K. Guts, E. B.
Grinkevich, ” Toposes in General Theory of Relativity ” [arXiv: gr-qc/9610073].
[111] A. Ashtekar, C. Rovelli and L. Smolin, Phys. Rev. Lett 69 (1992) 237. C. Rovelli,
” A dialog on quantum gravity ” [arXiv: hep-th/0310077] L. Freidel, E. Livine and
Carlo Rovelli, Class.Quant.Grav. 20 (2003) 1463-1478. L. Smolin, ” How far are we
from the quantum theory of gravity? ” [ arXiv:hep-th/0303185]
[112] M.Saniga, Chaos, Solitons and Fractals 19 (2004) 739-741
[113] Norma Mankoc Borstnik and H.B. Nielsen, J. Math. Phys. 44 (2003) 4817-4827
[114] L. Castellani, Class. Quant. Grav. 17 (2000) 3377-3402 Phys.Lett. B327 (1994) 22-28
[115] E. Guendelman, E. Nissimov and S. Pacheva, ” Strings, p-Branes and Dp-Branes
With Dynamical Tension ”. hep-th/0304269
62

Quantization in Astrophysics ...

158

[116] P. Bouwknegt, K. Schouetens, Phys. Reports 223 (1993) 183-276. E. Sezgin, ”
Aspects of W∞ Symmetry ” hep-th/9112025. X. Shen, Int.J.Mod.Phys. A7 (1992)
6953-6994.
[117] M. Vasiliev: ” Higher Spin Gauge Theories, Star Product and AdS spaces ” [arXiv:
hep-th/9910096]. M. Vasiliev, S. Prokushkin: “Higher-Spin Gauge Theories with
Matter”. [arXiv: hep-th/9812242, hep-th/9806236].
[118] Y. Ne’eman, E. Eizenberg, “Membranes and Other Extendons (p-branes) ”, World
Scientific Lecture Notes in Physics vol. 39 1995. J. Polchinski, Superstrings. Cambridge University Press (2000). M. Green, J. Schwarz and E. Witten, Superstring
Theory. Cambridge University Press (1986).
[119] J. Maldacena, Adv. Theor. Math. Phys 2 (1998) 231.
[120] Y.Cho, K, Soh, Q. Park, J. Yoon: Phys. Lets B 286 (1992) 251. J. Yoon, Phys.
Letts B 308 (1993) 240; J. Yoon, Phys. Lett A 292 (2001) 166. J. Yoon, Class.
Quan. Grav 16 (1999) 1863.
[121] C. Castro, Europhysics Letters 61 (4) (2003) 480. Class. Quant. Gravity 20 no. 16
(2003) 3577.
[122] A.B. Zamoldchikov, Teor. Fiz 65 (1985) 347 Pope et al C. Pope, Nucl.Phys. B413
(1994) 413-432 C. Pope, L. Romans, X. Shen, Nuc. Phys, B 339 (1990) 191. C.
Pope, L. Romans, X. Shen, Phys. Letts B 236 (1990) 173. C. Pope, L. Romans, X.
Shen, Phys. Letts B 242 (1990) 401.
[123] B. Dolan, Tchrakian: Phys. Letts B 202 (2) (1988) 211.
[124] E. Hawkins, F. Markopoulou and H. Sahlmann ” Evolution in Quantum Causal Histories ” [arXIv ; hep-the/0302111]. F. Markoupolu, L. Smolin, ” Quantum Theory
from Quantum Gravity ” [arXiv: gr-qc/0311059].
[125] M.Douglas, N. Nekrasov, Rev.Mod.Phys. 73 (2001) 977-1029
[126] T. Eguchi, Phys. Rev. Lett. 44, 126 (1980)
[127] C. Castro, ” Maximal-acceleration phase space relativity from Clifford algebras ”
[arXiv: hep-th/0208138].
[128] V. Abramov, R. Kerner and B. Le Roy, J.Math.Phys. 38 (1997) 1650-1669
[129] J. Cresson, ” Scale calculus and the Schrodinger equation ” [arXiv:
math.GM/0211071].
[130] E.H. Kinani, ” Between Quantum Virasoro Algebras and Generalized Clifford Algebras [arXiv: math-ph/0310044].

63

Quantization in Astrophysics ...

159

[131] S. Capozziello, S. Carloni and A. Troisi, ” Quintessence without scalar fields ”
[arXiv: astro-ph/0303041]. S. Carroll, V. Duvvuri, M. Trodden and M. Turner, ” Is
Cosmic Speed-Up Due to New Gravitational Physics? ” [arXiv: astro-ph/0306438].
A. Lue, R. Scoccimarro and G. Strakman, ” Differentiating between Modified Gravity and Dark Energy ” [arXiv: astro-ph/0307034].
[132] A. Aurilia, A. Smailagic and E. Spallucci, Physical Review D 47,2536 (1993); A.
Aurilia and E. Spallucci, Classical and Quantum Gravity 10, 1217 (1993); A. Aurilia,
E. Spallucci and I. Vanzetta, Physical Review D 50, 6490 (1994); S. Ansoldi, A.
Aurilia and E. Spallucci, Physical Review D 53, 870 (1996); S. Ansoldi, A. Aurilia
and E. Spallucci, Physical Review D 56, 2352 (1997)
[133] R. A. Mosna, D. Miralles and J. Vaz Jr, ” Z2 -gradings of Clifford algebras and
multivector structures ” [ arXiv : math-ph/0212020] .
[134] C.Castro, ”On Noncommutative Yang’s space-time algebra, Holography, Area
Quantization and C-space Relativity ” submitted to Eur. Physics Journal C, CERNEXT-2004-090 preprint. ” Noncommutative Branes in Clifford Space Backgrounds
and Moyal-Yang star products with UV-IR cutoffs ” submitted to the Jour. Math.
Phys ( 2005 ).
[135] J. Armenta, J. A. Nieto, ” The de Sitter Relativistic Top Theory” [ arXiv : 0405254
] . J.A.Nieto, ”Chirotope Concept in Various Scenarios of Physics” [arXiv : hepth/0407093]. J.A.Nieto, ”Matroids and p-branes”, Adv.Theor.Math.Phys. 8 (2004)
177-188
[136] C.N Yang, Phys. Rev 72 ( 1947 ) 874. Proceedings of the International Conference
on Elementary Particles, ( 1965 ) Kyoto, pp. 322-323.
[137] S.Tanaka, Nuovo Cimento 114 B ( 1999 ) 49. S. Tanaka, ” Noncommutative Field
Theory on Yang’s Space-Time Algebra, Covariant Moyal Star products and Matrix
Model ” [ arXiv : hep-th/0406166 ] . ” Space-Time Quantization and Nonlocal Field
Theory ...” [ arXiv : hep-th/0002001 ] . ”Yang’s Quantized Space-Time Algebra and
Holographic Hypothesis ” [ arXiv : hep-th/0303105] .
[138] C. Castro, ” On Dual Phase Space Relativity, the Machian Principle and Modified
Newtonian Dynamics” Progress in Theoretical and Experimental Physics, E-print
Journal, March 10 ( 2005 ) 1-15. ”The Extended Relativity Theory in Born-Clifford
Phase Spaces with a Lower and Upper Length Scales and Clifford Group Geometric
Unification” submitted to Foundations of Physics ( 2004 ) ; CDS-CERN-EXT-2004128.
[139] C. Castro, ” Polyvector Super-Poincare Algebras, M, F Theory Algebras and Generalized Supersymmetry in Clifford Spaces” submitted to the Int. Jour. Mod. Phys
A ( 2005).

64

Quantization in Astrophysics ...

160

[140] M. Pavsic, ”Kaluza Klein Theory without Extra Dimensions : Curved Clifford
Space” arXiv.org: hep-th/0412255. ” Clifford Spaces as a Generalization of Spacetime : Prospects for QFT of Point Particles and Strings” hep-th/0501222. ”An
alternative approcah to the relation bewteen bosons and fermions : employing Clifford space” hep-th/0502067.

65

Quantization in Astrophysics ...

161

On Area Coordinates and Quantum
Mechanics in Yang’s Noncommutative
Spacetime with a lower and upper scale
Carlos Castro
Center for Theoretical Studies of Physical Systems
Clark Atlanta University, Atlanta, GA. 30314, castro@ctsps.cau.edu
November 2005, Revised February 2006
Abstract
We explore Yang’s Noncommutative space-time algebra (involving two
length scales) within the context of QM defined in Noncommutative spacetimes and the holographic area-coordinates algebra in Clifford spaces.
Casimir invariant wave equations corresponding to Noncommutative coordinates and momenta in d-dimensions can be recast in terms of ordinary
QM wave equations in d + 2-dimensions. It is conjectured that QM over
Noncommutative spacetimes (Noncommutative QM) may be described by
ordinary QM in higher dimensions. Novel Moyal-Yang-Fedosov-Kontsevich
star products deformations of the Noncommutative Poisson Brackets are
employed to construct star product deformations of scalar field theories. Finally, generalizations of the Dirac-Konstant and Klein-Gordonlike equations relevant to the physics of D-branes and Matrix Models are
presented.

1

Introduction

Yang’s noncommutative space time algebra [?] is a generalization of the Snyder
algebra [?] (where now both coordinates and momenta are not commuting) that
has received more attention recently, see for example [?] and references therein.
In particular, Noncommutative p-brane actions, for even p+1 = 2n-dimensional
world-volumes, were written explicitly [?] in terms of the novel Moyal-Yang (
Fedosov-Kontsevich ) star product deformations [?], [?] of the Noncommutative
Nambu Poisson Brackets (NCNPB) that are associated with the noncommuting
world-volume coordinates q A , pA for A = 1, 2, 3, ...n. The latter noncommuting coordinates obey the noncommutative Yang algebra with an ultraviolet LP

1

Quantization in Astrophysics ...

162

(Planck) scale and infrared (R ) scale cutoff. It was shown why the novel pbrane actions in the ”classical” limit h̄ef f = h̄LP /R → 0 still acquire nontrivial
noncommutative corrections that differ from ordinary p-brane actions . Super
p-branes actions in the light-cone gauge are also amenable to Moyal-Yang star
product deformations as well due to the fact that p-branes moving in flat spacetime backgrounds, in the light-cone gauge, can be recast as gauge theories of
volume-preserving diffeomorphisms. The most general construction of noncommutative super p-branes actions based on non (anti) commuting superspaces
and quantum group methods remains an open problem.
The purpose of this work is to explore further the consequences of Yang’s
Noncommutative spacetime algebra within the context of QM in Noncommutative spacetimes and the holographic area-coordinates algebra in Clifford spaces
[?]. In section 2 we study the interplay among Yang’s Noncommutative spacetime algebra and the former area-coordinates algebra in Clifford spaces . In
section 3 we show how Casimir invariant wave equations corresponding to
Noncommutative coordinates and momenta in D-dimensions, can be recast in
terms of ordinary QM wave equations in D + 2-dimensions. In particular, we
shall present explicit solutions of the D’Alambertian operator in the bulk of AdS
spaces and explain its correspondence with the Casimir invariant wave equations
associated with the Yang’s Noncommutative spacetime algebra at the projective boundary of the conformally compactified AdS spacetime. We conjecture
that QM over Noncommutative spacetimes ( Noncommutative QM ) may be
described by ordinary QM in higher dimensions.
In section 4 we recur to the novel Moyal-Yang (Fedosov-Kontsevich) star
products [?], [?] deformations of the Noncommutative Poisson Brackets to construct Moyal-Yang star product deformations of scalar field theories. The role
of star products in the construction of p-branes actions from the large N limit
of SU (N ) Yang-Mills can be found in [?] and in the Self-Dual Gravity/ SU (∞)
Self Dual Yang-Mills relation in [?], [?], [?].[?]. Finally, in the conclusion 5 , we
present the generalizations of the Dirac-Konstant equations (and their ”square”
Klein-Gordon type equations ) that are relevant to the incorporation of fermions
and the physics of D-branes and Matrix Models .

2

Noncommutative Yang’s Spacetime Algebra
in terms of Area-Coordinates in Clifford Spaces

The main result of this section is that there is a subalgebra of the C-space
operator-valued coordinates [?] which is isomorphic to the Noncommutative
Yang’s spacetime algebra [?], [?] . This, in conjunction to the discrete spectrum of angular momentum, leads to the discrete area quantization in multiples
of Planck areas. Namely, the 4D Yang’s Noncommutative space-time algebra
[?] (written in terms of 8D phase-space coordinates) is isomorphic to the 15dimensional subalgebra of the C-space operator-valued coordinates associated
2

Quantization in Astrophysics ...

163

with the holographic areas of C-space. This connection between Yang’s algebra
and the 6D Clifford algebra is possible because the 8D phase-space coordinates
xµ , pµ ( associated to a 4D spacetime ) have a one-to-one correspondence to the
X̂ µ5 ; X̂ µ6 holographic area-coordinates of the C-space (corresponding to the 6D
Clifford algebra). Furhermore, Tanaka [?] has shown that the Yang’s algebra
[?] ( with 15 generators ) is related to the 4D conformal algebra (15 generators)
which in turn is isomorphic to a subalgebra of the 4D Clifford algebra because
it is known that the 15 generators of the 4D conformal algebra SO(4, 2) can be
explicitly realized in terms of the 4D Clifford algebra as shown in [?] .
The correspondence between the holographic area coordinates X AB ↔ λ2 ΣAB
and the angular momentum variables when A, B = 1, 2, 3, .....6 yields an isomorphism between the holographic area coordinates algebra in Clifford spaces [?]
and the noncommutative Yang’s spacetime algebra in D = 4 . The scale λ is the
ultraviolet lower Planck scale. We begin by writing the exchange algebra between the position and momentum coordinates encapsulated by the commutator
:

[X̂ µ6 , X̂ 56 ] = −iλ2 η 66 X̂ µ5 ↔ [

λ2 R µ 2 56
p̂ , λ Σ ] = −iλ2 η 66 λx̂µ .
h̄

(2.1)

from which we can deduce that :
[p̂µ , Σ56 ] = −iη 66

h̄ µ
x̂ .
λR

(2.2)

hence, after using the definition N = (λ/R)Σ56 , where R is the infrared upper
scale, one has the exchange algebra commutator of pµ and N of the Yang’s
spacetime algebra given by
[p̂µ , N ] = −iη 66

h̄ µ
x̂ .
R2

(2.3)

From the commutator

[X̂ µ5 , X̂ 56 ] = −[X̂ µ5 , X̂ 65 ] = iη 55 λ2 X̂ µ6 ↔ [λx̂µ , λ2 Σ56 ] = iη 55 λ2 λ2
we can deduce that
[x̂µ , Σ56 ] = iη 55

λR µ
p̂ .
h̄

R µ
p̂ .
h̄
(2.4)
(2.5)

and after using the definition N = (λ/R)Σ56 one has the exchange algebra
commutator of xµ and N of the Yang’s spacetime algebra
[x̂µ , N ] = iη 55

λ2 µ
p̂ .
h̄

(2.6)

The other relevant holographic area-coordinates commutators in C-space are

3

Quantization in Astrophysics ...

164

[X̂ µ5 , X̂ ν5 ] = −iη 55 λ2 X̂ µν ↔ [x̂µ , x̂ν ] = −iη 55 λ2 Σµν .

(2.7)

that yield the noncommuting coordinates algebra after having used the representation of the C-space operator holographic area-coordinates
1
iX̂ µν ↔ iλ2 Mµν = iλ2 Σµν iX̂ 56 ↔ iλ2 Σ56 .
(2.8)
h̄
where we appropriately introduced the Planck scale λ as one should to match
units. From the correspondence
h̄ µ6
h̄ 1 µ6
X̂ .
Σ ↔
R
R λ2
one can obtain nonvanishing momentum commutator
p̂µ =

(2.9)

h̄2 µν
Σ .
(2.10)
R2
The signatures for AdS5 space are η 55 = +1; η 66 = −1 and for the Euclideanized
AdS5 space are η 55 = +1 and η 66 = +1. Yang’s space-time algebra corresponds
to the latter case. Finally, the modif ied Heisenberg algebra can be read from
the following C-space commutators :
[X̂ µ6 , X̂ ν6 ] = −iη 66 λ2 X̂ µν ↔ [p̂µ , p̂ν ] = −iη 66

[X̂ µ5 , X̂ ν6 ] = iη µν λ2 X̂ 56 ↔
λ 56
Σ = ih̄η µν N .
(2.11)
R
Eqs-(2.1-2.11) are the defining relations of Yang’s Noncommutative 4D spacetime algebra [?] involving the 8D phase-space variables. These commutators
obey the Jacobi identities. There are other commutation relations like [Mµν , xρ ],
.... that we did not write down. These are just the well known rotations ( boosts
) of the coordinates and momenta.
When λ → 0 and R → ∞ one recovers the ordinary commutative spacetime
algebra. The Snyder algebra [?] is recovered by setting R → ∞ while leaving λ
intact. To recover the ordinary Weyl-Heisenberg algebra is more subtle. Tanaka
[?] has shown the the spectrum of the operator N = (λ/R)Σ56 is discrete given
by n(λ/R) . This is not suprising since the angular momentum generator M56
associated with the Euclideanized AdS5 space is a rotation in the now compact
x5 − x6 directions. This is not the case in AdS5 space since η 66 = −1 and this
timelike direction is no longer compact. Rotations involving timelike directions
are equivalent to noncompact boosts with a continuous spectrum.
In order to recover the standard Weyl-Heisenberg algebra from Yang’s Noncommutative spacetime algebra, and the standard uncertainty relations ∆x∆p ≥
h̄ with the ordinary h̄ term , rather than the nh̄ term, one needs to take the
λ
limit n → ∞ limit in such a way that the net combination of n R
→ 1. This can
be attained when one takes the double scaling limit of the quantities as follows
[x̂µ , p̂µ ] = ih̄η µν

λ → 0.

R → ∞.

λR → L2 .

4

Quantization in Astrophysics ...

165

λ
λ2
nλ2
=n
= 2 → 1.
R
λR
L
From eq-(2.12) one learns then that :
limn→∞ n

nλ2 = λR = L2 .

(2.12)

(2.13)

The spectrum n corresponds to the quantization of the angular momentum
operator in the x5 − x6 direction (after embedding the 5D hyperboloid of throat
size R onto 6D ) . Tanaka [?] has shown why there is a discrete spectra for
the spatial coordinates and spatial momenta in Yang’s spacetime algebra that
yields a minimum length λ ( ultraviolet cutoff in energy ) and a minimum
momentum p = h̄/R ( maximal length R , infrared cutoff ) . The energy and
temporal coordinates had a continous spectrum.
The physical interpretation of the double-scaling limit of eq-( 2.12 ) is that
the the area L2 = λR becomes now quantized in units of the Planck area λ2
as L2 = nλ2 . Thus the quantization of the area ( via the double scaling limit
) L2 = λR = nλ2 is a result of the discrete angular momentum spectrum in
the x5 − x6 directions of the Yang’s Noncommutative spacetime algebra when
it is realized by ( angular momentum ) differential operators acting on the
Euclideanized AdS5 space ( two branches of a 5D hyperboloid embedded in
6D ). A general interplay between quantum of areas and quantum of angular
momentum, for p
arbitrary values of spin, in terms of the square root of the
Casimir A ∼ λ2 j(j + 1), has been obtained a while ago in Loop Quantum
Gravity by using spin-networks techniques and highly technical area-operator
regularization procedures [?] .
The advantage of this work is that we have arrived at similar ( not identical
) area-quantization conclusions in terms of minimal Planck areas and a discrete
angular momentum spectrum n via the double scaling limit based on Clifford
algebraic methods (C-space holographic area-coordinates). This is not surprising since the norm-squared of the holographic Area operator has a correspondence with the quadratic Casimir ΣAB ΣAB of the conformal algebra SO(4, 2)
( SO(5, 1) in the Euclideanized AdS5 case ). This quadratic Casimir must not
be confused with the SU (2) Casimir J 2 with eigenvalues j(j + 1) . Hence, the
correspondence given by eqs-(2.3-2.8) gives A2 ↔ λ4 ΣAB ΣAB .
In [?] we have shown why AdS4 gravity with a topological term; i.e. an
Einstein-Hilbert action with a cosmological constant plus Gauss-Bonnet terms
can be obtained from the vacuum state of a BF-Chern-Simons-Higgs theory
without introducing by hand the zero torsion condition imposed in the McDowellMansouri-Chamsedine-West construction. One of the most salient features of
[?] was that a geometric mean relationship was found among the cosmological
constant Λc , the Planck area λ2 and the AdS4 throat size squared R2 given by
(Λc )−1 = (λ)2 (R2 ). Upon setting the throat size to be of the order of the Hubble
scale RH and λ = LP (Planck scale), one recovers the observed value of the cos−2
−4
2
−120
mological constant L−2
MP4 . A similar geometric
P RH = LP (LP /RH ) ∼ 10
mean relation is also obeyed by the condition λR = L2 (= nλ2 ) in the double
scaling limit of Yang’s algebra which suggests to identify the cosmological con5

Quantization in Astrophysics ...

166

stant as Λc = L−4 . This geometric mean condition remains to be investigated
further. In particular, we presented the preliminary steps how to construct a
Noncommutative Gravity via the Vasiliev-Moyal star products deformations of
the SO(4, 2) algebra used in the study of higher conformal massless spin theories
in AdS spaces by taking the inverse-throat size 1/R as a deformation parameter
of the SO(4, 2) algebra. A Moyal deformation of ordinary Gravity via SU (∞)
gauge theories was advanced in [?] .

3

Noncommutative QM in Yang’s Spacetime from
ordinary QM in Higher Dimensions

In order to write wave equations in non-commuting spacetimes we start with
a Hamiltonian written in dimensionless variables involving the terms of the
relativistic oscillator ( let us say oscillations of the center of mass ) and the rigid
rotor/top terms ( rotations about the center of mass ) :
H=(

xµ 2
pµ 2
) +(
) + (Σµν )2 .
(h̄/R)
LP

(3.1)

with the fundamental difference that the coordinates xµ and momenta pµ obey
the non-commutative Yang’s space time algebra. For this reason one cannot
naively replace pµ any longer by the differential operator −ih̄∂/∂xµ nor write the
Σµν generators as (1/h̄)(xµ ∂xν − xν ∂xµ ). The correct coordinate realization of
Yang’s noncommutative spacetime algebra requires, for example, embedding the
4-dim space into 6-dim and expressing the coordinates and momenta operators
as follows :
pµ
1
↔ Σµ6 = i (X µ ∂X6 − X 6 ∂Xµ ).
(h̄/R)
h̄

xµ
1
↔ Σµ5 = i (X µ ∂X5 − X 5 ∂Xµ ).
LP
h̄

1
1
(3.2)
Σµν ↔ i (X µ ∂Xν − X ν ∂Xµ ). N = Σ56 ↔ i (X 5 ∂X6 − X 6 ∂X5 ).
h̄
h̄
this allows to express H in terms of the standard angular momentum operators in 6-dim. The X A = X µ , X 5 , X 6 coordinates (µ = 1, 2, 3, 4) and
P A = P µ , P 5 , P 6 momentum variables obey the standard commutation relations of ordinary QM in 6-dim
[X A , X B ] = 0. [P A , P B ] = 0.

[X A , P B ] = ih̄η AB .
A

(3.3)

so that the momentum admits the standard realization as P = −ih̄∂/∂XA
Therefore, concluding, the Hamiltonian H in eq-( 3-1) associated with the
non-commuting coordinates xµ and momenta pµ in d − 1-dimensions can be
written in terms of the standard angular momentum operators in (d − 1) + 2 =

6

Quantization in Astrophysics ...

167

d + 1-dim as H = C2 − N 2 , where C2 agrees precisely with the quadratic Casimir
operator of the SO(d − 1, 2) algebra in the spin s = 0 case,
C2 = ΣAB ΣAB = (XA ∂B − XB ∂A )(X A ∂ B − X B ∂ A ).

(3.4)

One remarkable feature is that C2 also agrees with the D’Alambertian operator
for the Anti de Sitter Space AdSd of unit radius ( throat size ) (Dµ Dµ )AdSd as
it was shown by [?].
The proof requires to show that the D’Alambertian operator for the d + 1dim embedding space ( expressed in terms of the X A coordinates ) is related to
the D’Alambertian operator in AdSd space of unit radius expressed in terms of
the z 1 , z 2 , ....., z d bulk intrinsic coordinates as :
(Dµ Dµ )Rd+1 = −

d ∂
1
∂2
−
+ (Dµ Dµ )AdS ⇒
∂ρ2
ρ ∂ρ ρ2

C2 = ρ2 (Dµ Dµ )Rd+1 + [ (d − 1) + ρ

∂
∂
]ρ
= (Dµ Dµ )AdSd .
∂ρ
∂ρ

(3.5)

This result is just the hyperbolic-space generalization of the standard decomposition of the Laplace operator in spherical coordinates in terms of the radial derivatives plus a term containing the square of the orbital angular momentum operator L2 /r2 . In the case of nontrivial spin, the Casimir C2 =
ΣAB ΣAB + SAB S AB has additional terms stemming from the spin operator.
The quantity Φ(z 1 , z 2 , ....., z d )|boundary restricted to the d − 1-dim projective boundary of the conformally compactified AdSd space ( of unit throat size,
whose topology is S d−2 × S 1 ) is the sought-after solution to the Casimir invariant wave equation associated with the non-commutative xµ coordinates and
momenta pµ of the Yang’s algebra ( µ = 1, 2, ...., d − 1 ). Pertaining to the
boundary of the conformally compactified AdSd space, there are two radii R1 , R2
associated with S d−2 and S 1 , respectively, and which must not be confused with
the two scales R, LP appearing in eq-(3-1). One can choose the units such that
the present value of the Hubble scale ( taking the Hubble scale as the infrared
cutoff ) is R = 1. In these units the Planck scale LP will be of the order of
LP ∼ 10−60 . In essence, there has been a trade-off of two scales LP , R with the
two radii R1 , R2 .
Once can parametrize the coordinates of AdSd = AdSp+2 by writing [?]
X0 = R cosh(ρ)cos(τ ). Xp+1 = R cosh(ρ)sin(τ ). Xi = R sinh(ρ)Ωi . (3.6a)
The metric of AdSd = AdSp+2 space in these coordinates is :
ds2 = R2 [−(cosh2 ρ)dτ 2 + dρ2 + (sinh2 ρ)dΩ2 ].

(3.6b)

where 0 ≤ ρ and 0 ≤ τ < 2π are the global coordinates. The topology of
this hyperboloid is S 1 × Rp+1 . To study the causal structure of AdS it is
convenient to unwrap the circle S 1 ( closed-timelike coordinate τ ) to obtain
the universal covering of the hyperboloid without closed-timelike curves and

7

Quantization in Astrophysics ...

168

take −∞ ≤ τ ≤ +∞. Upon introducing the new coordinate 0 ≤ θ < π/2
related to ρ by tan(θ) = sinh(ρ), the metric in (3-6b) becomes
ds2 =

R2
[−dτ 2 + dθ2 + (sinh2 ρ)dΩ2 ].
cos2 θ

(3.7)

It is a conformally-rescaled version of the metric of the Einstein static universe.
Namely, AdSd = AdSp+2 can be conformally mapped into one-half of the Einstein static universe, since the coordinate θ takes values 0 ≤ θ < π/2 rather
than 0 ≤ θ < π. The boundary of the conformally compactified AdSp+2 space
has the topology of S p × S 1 ( identical to the conformal compactification of the
p + 1-dim Minkowski space ). Therefore, the equator at θ = π/2 is a boundary
of the space with the topology of S p . Ωp is the solid angle coordinates corresponding to S p and τ is the coordinate which parametrizes S 1 . For a detailed
discussion of AdS spaces and the AdS/CF T duality see [?] .
The D’Alambertian in AdSd space ( of radius R, later we shall set R = 1 )
is :
1
√
Dµ Dµ = √ ∂µ ( g g µν ∂ν ) =
g
1
1
cos2 θ
[ − ∂τ2 +
∂θ ( (R tanθ)p ∂θ ) ] + 2
L2
2
p
R
(R tanθ)
R tan2 θ

(3.8)

where L2 is the Laplacian operator in the p-dim sphere S p whose eigenvalues
are l(l + p − 1).
The scalar field can be decomposed as Φ = eωRτ Yl (Ωp ) G(θ) and the wave
equation
(Dµ Dµ − m2 )Φ = 0.

(3.9)

leads to :

[ cos2 θ ( ω 2 + ∂θ2 +

p
l(l + p − 1)
∂θ ) +
− m2 R2 ] G(θ) = 0. (3.10)
tanθ cos2 θ
tan2 θ

whose solution is
G(θ) = (sinθ)l (cosθ)λ± 2 F1 (a, b, c; sinθ).

(3.11)

The hypergeometric function is defined
2 F1 (a, b, c, z)

(λ)o = 1. (λ)k =

=

X (a)k (b)k
z n . |z| < 1.
(c)k k!

Γ(λ + k)
= λ(λ + 1)(λ + 2).......(λ + k − 1).
Γ(λ)

(3.12)

k = 1, 2, ....
(3.13)

8

Quantization in Astrophysics ...

169

where
1
(l + λ± − ωR).
2

a=

b=

1
(l + λ± + ωR).
2

1
c = l + (p + 1) > 0.
2

(3.14a)

1
1p
(p + 1) ±
(p + 1)2 + 4(mR)2 .
(3.14b)
2
2
The analytical continuation of the hypergeometric function for |z| ≥ 1 is :
λ± =

2 F1 (a, b, c, z)

=

Γ(c)
Γ(b)Γ(c − b)

Z

1

tb−1 (1 − t)c−b−1 (1 − tz)−a dt. .

(3.15)

0

with Real(c) > 0 and Real(b) > 0. The boundary value when θ = π/2 gives
limz→1− F (a, b, c; z) =

Γ(c)Γ(c − a − b)
.
Γ(c − a)Γ(c − b)

(3.16)

Let us study the behaviour of the solution G(θ) in the massless case
m = 0.

λ− = 0.

λ+ = p + 1.

(3.17)

Solutions with λ+ = p + 1 yield a trivial value of G(θ) = 0 at the boundary θ =
π/2 since cos(π/2)p+1 = 0. Solutions with λ− = 0 lead to cos(θ)λ− = cos(θ)0 =
1 prior to taking the limit θ = π/2. The expression cos(π/2)λ− = 00 = is ill
defined. Upon using L’ Hopital rule it yields 0. Thus, the limit θ = π/2 must
be taken afterwards the limit λ− = 0 :
limθ→π/2 [ cos(θ)λ− ] = limθ→π/2 [cos(θ)0 ] = limθ→π/2 [1] = 1.

(3.18)

In this fashion the value of G(θ) is well defined and nonzero at the boundary
when λ− = 0 and leads to the value of the wavefunction at the boundary of the
conformally compactified AdSd ( for d = p + 2 with radius R )
Γ(l + (p + 1)/2)Γ((p + 1)/2)
Γ(ωR + (l + p + 1)/2)Γ(−ωR + (l + p + 1)/2)
(3.19a)
upon setting the radius of AdSd space to unity it gives
Φboundary = eiωRτ Yl (Ωp )

Φboundary = eiωτ Yl (Ωp )

Γ(l + (p + 1)/2)Γ((p + 1)/2)
. (3.19b)
Γ(ω + (l + p + 1)/2)Γ(−ω + (l + p + 1)/2)

Hence, Φboundary in eq-(3-19b) is the solution to the Casimir invariant wave
equation in the massless m = 0 case :
C2 Φ = [ (

pµ 2
xµ 2
) +(
) + (Σµν )2 + N 2 ] Φ = 0.
(h̄/R)
LP

And :

9

Quantization in Astrophysics ...

170

(3.20)

pµ 2
xµ 2
) +(
) + (Σµν )2 ] Φ = [ C2 − N 2 ] Φ = −ω 2 Φ. (when R = 1)
(h̄/R)
LP
(3.21)
since N = Σ56 is the rotation generator along the S 1 component of AdS space. It
acts as ∂/∂τ only on the eiωRτ piece of Φ. Concluding : Φ(z 1 , z 2 , ....., z d )|boundary ,
restricted to the d − 1-dim projective boundary of the conformally compactified
AdSd space ( of unit radius and topology S d−2 × S 1 ) given by eq-(3-19), is the
sought-after solution to the wave equations (3-20, 3-21) associated with the noncommutative xµ coordinates and momenta pµ of the Yang’s algebra and where
the indices µ range over the dimensions of the boundary µ = 1, 2, ...., d − 1 .
This suggests that QM over Yang’s Noncommutative Spacetimes could be well
defined in terms of ordinary QM in higher dimensions ! This idea deserves
further investigations. For example, it was argued by [?] that the quantized
Nonabelian gauge theory in d dimensions can be obtained as the infrared limit
of the corresponding classical gauge theory in d + 1-dim.
[(

4

Star Products and Noncommutative QM

The ordinary Moyal star-product of two functions in phase space f (x, p), g(x, p)
is :

(f ∗ g)(x, p) =

s
X h̄s X

s!

s

(−1)t C(s, t)(∂xs−t ∂pt f (x, p))(∂xt ∂ps−t g(x, p))

(4.1)

t=0

where C(s, t) is the binomial coefficient s!/t!(s − t)!. In the h̄ → 0 limit the star
product f ∗ g reduces to the ordinary pointwise product f g of functions. The
Moyal product of two functions of the 2n-dim phase space coordinates (qi , pi )
with i = 1, 2...n is:

(f ∗ g)(x, p) =

n X s X
s
X
h̄
i

s

s!

(−1)t C(s, t)(∂xs−t
∂pt i f (x, p))(∂xt i ∂ps−t
g(x, p)) (4.2)
i
i

t=0

The noncommutative, associative Moyal bracket is defined:
1
(f ∗ g − g ∗ f ).
(4.3)
ih̄
The task now is to construct novel Moyal-Yang star products based on the
noncommutative spacetime Yang’s algebra. A novel star product deformations
of (super) p-brane actions based on the noncommutative spacetime Yang’s algebra where the deformation parameter is h̄ef f = h̄LP /R , for nonzero values of
h̄, was obtained in [?] The modified (noncommutative) Poisson bracket is now
given by
{f, g}M B =

10

Quantization in Astrophysics ...

171

{ F (q m , pm ) , G (q m , pm ) }Ω = (∂qm F) {q m , q n } (∂qn G) +
(∂pm F) {pm , pn } (∂pn G) + (∂qm F) {q m , pn } (∂pn G) + (∂pm F) {pm , q n } (∂qn G).
(4.4)
where the entries {q m , q n } 6= 0, {pm , pn } 6= 0, and {pm , q n } = −{q n , pm } can
be read from the commutators described in section 2 by simply defining the
deformation parameter h̄ef f ≡ h̄(LP /R). One can generalize Yang’s original
4-dim algebra to noncommutative 2n-dim world-volumes and/or spacetimes by
working with the 2n + 2-dim angular-momentum algebra SO(d, 2) = SO(p +
1, 2) = SO(2n, 2).
The Noncommutative Poisson brackets ( NCPB ) are defined by
Ω(q m , q n ) = {q m , q n }N CP B = limh̄ef f →0

L2
1
[q m , q n ] = − Σmn .
ih̄ef f
h̄

(4.5a)

Ω(pm , pn ) = {pm , pn }N CP B = limh̄ef f →0

1
h̄
[pm , pn ] = − 2 Σmn
ih̄ef f
L

(4.5b)

1
[q m , pn ] = −η mn .
ih̄ef f
(4.5c)
where Σmn above is the ”classical ” h̄ef f = (h̄LP /R) → 0 limit ( R → ∞, LP →
0, RLP = L2 , h̄ 6= 0 ) of the quantity Σmn = h̄1 (X m P n − X n P m ), after embedding the d − 1 dimensional spacetime ( boundary of AdSd ) into an ordinary
(d − 1) + 2-dimensional one. In the R → ∞, ...... limit, the AdSd space ( the
hyperboloid ) degenerates into a f lat Minkowski spacetime and the coordinates
q m , pn , in that infrared limit, coincide with the coordinates X m , P n . Concluding, in the ”classical” limit ( R → ∞, ....., flat limit ) one has
Ω(q m , pn ) = −Ω(pn , q m ) = {q m , pn }N CP B = limh̄ef f →0

1 m n
1
(X P − X n P m ) → (q m pn − q n pm ).
(4.5d)
h̄
h̄
and then one recovers in that limit the ordinary definition of the angular momentum in terms of commuting coordinates q’s and commuting momenta p’s.
Denoting the coordinates (q m , pm ) by Z m and when the Poisson structure
mn
Ω
is given in terms of constant numerical coefficients, the Moyal star product
is defined in terms of the deformation parameter h̄ef f = h̄LP /R as
Σmn ≡

(z1 ) (z2 )
( F ∗ G )(z) ≡ exp [ (ih̄ef f ) Ωmn ∂m
∂n ] F(z1 ) G(z2 )|z1 =z2 =z .
(z )

(z )

(4.6)

where the derivatives ∂m 1 act only on the F(z1 ) term and ∂n 2 act only on
the G(z2 ) term. In our case the generalized Poisson structure Ωmn is given in
terms of variable coefficients, it is a function of the coordinates, then ∂Ωmn 6= 0,
since the Yang’s algebra is basically an angular momentum algebra, therefore
the suitable Moyal-Yang star product given by Kontsevich [?] will contain the
appropriate corrections ∂Ωmn to the ordinary Moyal star product
11

Quantization in Astrophysics ...

172

Denoting by ∂m = ∂/∂z m = (∂/∂q m ; /∂/∂pm ) the Moyal-Yang-Kontsevich
star product, let us say, of the Hamiltonian H(q, p) with the density distribution
in phase space ρ(q, p) (not necessarily positive definite) , H(q, p) ∗ ρ(q, p) is
Hρ + ih̄ef f Ωmn (∂m H∂n ρ) +

(ih̄ef f )2 m1 n1 m2 n2 2
(∂m1 m2 H) (∂n2 1 n2 ρ) +
Ω
Ω
2

(ih̄ef f )2 m1 n1
[Ω
(∂n1 Ωm2 n2 )(∂m1 ∂n2 H∂n2 ρ − ∂m2 H∂m1 ∂n2 ρ) ] + O(h̄3ef f ). (4.7)
3
where the explicit components of Ωmn are given by eqs-(4-5a-4-5d). The Kontsevich star product is associative up to second order [?] (f ∗ g) ∗ h = f ∗ (g ∗
h) + O(h̄3ef f ).
The most general expression of the Kontsevich star product in Poisson manifolds is quite elaborate and shall not be given here. Star products in curved
phase spaces have been constructed by Fedosov [?] . Despite these technical
subtlelties it did not affect the final expressions for the ”classical” Noncommutative p-brane actions as shown in [?] when one takes the h̄ef f → 0 ”classical”
limit. In that limit there are still nontrivial noncommutative corrections to
the ordinary p-brane actions.
In the Weyl-Wigner-Gronewold-Moyal quantization scheme in phase spaces
one writes
H(x, p) ∗ ρ(x, p) = ρ(x, p) ∗ H(x, p) = Eρ(x, p).
(4.8)
where the Wigner density function in phase space associated with the Hilbert
space state |Ψ > is
Z
h̄y
h̄y ipy/h̄
1
dy Ψ∗ (x −
) Ψ(x +
)e
(4.9)
ρ(x, p, h̄) =
2π
2
2
plus their higher dimensional generalizations. It remains to be studied if this
Weyl-Wigner-Gronewold-Moyal quantization scheme is appropriate to study
QM over Noncommutative Yang’s spacetimes when we use the above MoyalYang-Kontsevich star products. A recent study of the Yang’s Noncommutative
algebra and discrete Hilbert (Buniy-Hsu-Zee) spaces was undertaken by Tanaka
[?].
Let us write down the Moyal-Yang-Konstevich star deformations of the Field
theory Lagrangian corresponding to the scalar field Φ = Φ(X AB ) which depends on the holographic-area coordinates X AB [?]. The reason one should not
try to construct the star product of Φ(xm ) ∗ Φ(xn ) based on the Moyal-YangKontsevich product, is because the latter star product given by eq-(4-7) will
introduce explicit momentum terms in the r.h.s of Φ(xm ) ∗ Φ(xm ), stemming
from the expression Σmn = xm pn − xn pm of eq-(4-5d), and thus it invalidates
writing φ = φ(x) in the first place. If the Σmn were numerical constants, like
Θmn , then one could write the Φ(xm ) ∗ Φ(xm ) in a straightforward fashion as
it is done in the literature.
The reason behind choosing Φ = Φ(X AB ) is more clear after one invokes the
area-coordinates and angular momentum correspondence discussed in detail in
12

Quantization in Astrophysics ...

173

section 2 . It allows to properly define the star products. A typical Lagrangian
is of the form
m2
2
AB
L = − Φ ∗ ∂X
) +
Φ(X AB ) ∗ Φ(X AB ) +
AB Φ(X
2
gn
Φ(X AB ) ∗ Φ(X AB ) ∗ .... ∗n Φ(X AB ).
(4.10)
n
and leads to the equations of motion
−( ∂/∂X AB ) (∂/∂X AB ) Φ(X AB ) + m2 Φ(X AB ) +
g n Φ(X AB ) ∗ Φ(X AB ) ∗ .... ∗n−1 Φ(X AB ) = 0.
when the multi-symplectic Ω
product is

ABCD

(4.11)

form is coordinate-independent, the star

(Φ ∗ Φ) (Z AB ) ≡ exp [ ( iλ ΩABCD ∂X AB ∂Y AB ) ] Φ(X AB ) Φ(Y AB )|X=Y =Z
= exp [ ( ΣABCD ∂X AB ∂Y AB ) ] Φ(X AB ) Φ(Y AB )|X=Y =Z

(4.12)

where ΣABCD is derived from the structure constants of the holographic areacoordinate algebra in C-spaces [?]
[X AB , X CD ] = ΣABCD ≡ iL2P (η AD X BC − η AC X BD + η BC X AD − η BD X AC ).
(4.13)
there are nontrivial derivative terms acting on ΣABCD in the definition of the
star product ( Φ ∗ Φ ) (Z M N ) as we have seen in the definition of the Kontsevich
star product H(x, p) ∗ ρ(x, p) in eq-(4-7) . The expansion parameter in the star
product is the Planck scale squared λ = L2P . The star product has the same
functional form as (4-7) with the only difference that now we are taking derivatives w.r.t the area-coordinates X AB instead of derivatives w.r.t the variables
x, p, hence to order O(L4P ), the star product is
Φ ∗ Φ = Φ2 + ΣABCD (∂AB Φ∂CD Φ)+
1 A1 B 1 C 1 D1 A2 B 2 C 2 D2 2
2
Σ
(∂A1 B1 A2 B2 Φ) (∂C
Φ) +
Σ
1 D1 C 2 D2
2
1 A 1 B 1 C 1 D1
[Σ
( ∂C1 D1 ΣA2 B2 C2 D2 )( ∂A1 B1 ∂A2 B2 Φ ∂C2 D2 Φ − B1 ↔ B2 ) ]. (4.14)
3
Notice that the powers of iL2P are encoded in the definition of ΣABCD . The star
product is noncommutative but is also nonassociative at the order O(L6P ) and
beyond. The Jacobi identities would be anomalous at that order and beyond.
The derivatives acting on ΣABCD are
B2 C 2
B 2 D2
(∂C1 D1 ΣA2 B2 C2 D2 ) = iL2P (η A2 D2 δC
− η A2 C 2 δ C
) +
1 D1
1 D1
A2 D2
A2 C 2
iL2P (η B2 C2 δC
− η B 2 D2 δ C
).
1 D1
1 D1
AB
δCD

where
=
will be zero.

A B
δC
δD

A B
−δD
δC

and the higher derivatives like

13

Quantization in Astrophysics ...

174

(4.15)
2
∂A
Σ A2 B 2 C 2 D2
1 B 1 C 1 D1

5

On the Generalized Dirac-Konstant Equation
in Clifford Spaces

To conclude this work we will discuss the wave equations relevant to fermions.
The ”square” of the Dirac-Konstant equation
(γ [µν] Σµν )Ψ = λΨ.

(5.1)

yields
(γ [µν] γ [ρτ ] Σµν Σρτ )Ψ = λ2 Ψ ⇒
[γ [µνρτ ] +(η µρ γ [ντ ] −η µτ γ [νρ] +....... ) +(η µρ η ντ 1−η µτ η νρ 1)] Σµν Σρτ Ψ = λ2 Ψ.
(5.2)
where we omitted numerical factors. The generalized Dirac equation in Clifford
spaces is given by [?]
∂
∂
∂
∂
+γ µ µ +γ [µν] µν +............. +γ [µ1 µ2 ....µd ] µ1 µ2 ....µ ) Ψ = λΨ. (5.3)
d
∂σ
∂x
∂x
∂x
µ
µν
where σ, x , x , ..... are the generalized coordinates associated with the Clifford
polyvector in C-space
−i(

X = σ1 + γ µ xµ + γ µ1 µ2 xµ1 µ2 + ..... γ µ1 µ2 .....µd xµ1 µ2 .....µd .

(5.4)

after the length scale expansion parameter is set to unity. The generalized
Dirac-Konstant equations in Clifford-spaces are obtained after introducing the
generalized angular momentum operators [?]
Σ[
X

[ [µ1 µ2 ....µn ]

P

[ν1 ν2 ......νn ] ]

X
by writing
X
γ[

[µ1 µ2 ....µn ] [ν1 ν2 ....νn ] ]

=X

[ν1 ν2 ....νn ]

[µ1 µ2 .....µn ] [ν1 ν2 .....νn ] ]

γ

[µ1 µ2 ....µn ]

=

i(∂/∂X[ν1 ν2 ......νn ] ) −

i(∂/∂X[µ1 µ2 ......µn ] ).
Σ[

[µ1 µ2 ....µn ] [ν1 ν2 ....νn ] ]

(5.5)

Ψ = λΨ.

(5.6)

n

and where we sum over all polyvector-valued indices (antisymmetric tensors
of arbitrary rank) . Upon squaring eq-(5-5), one obtains the Clifford space
extensions of the D0-brane field equations found in [?] which are of the form
[ X AB (∂/∂XCD )−X CD (∂/∂XAB ) ] [ XAB (∂/∂X CD )−XCD (∂/∂X AB ) ] Ψ = 0.
(5.6)
where A, B = 1, 2, ..., 6. It is warranted to study all these equations in future
work and their relation to the physics of D-branes and Matrix Models [?]. Yang’s
Noncommutative algebra should be extended to superspaces, meaning non-anticommuting Grassmanian coordinates and noncommuting bosonic coordinates.
Acknowledgments
We are indebted to C. Handy and M.Bowers for encouragement and support.
14

Quantization in Astrophysics ...

175

References
[1] C.N Yang, Phys. Rev 72 ( 1947 ) 874. Proceedings of the International
Conference on Elementary Particles, ( 1965 ) Kyoto, pp. 322-323.
[2] S. Snyder: Phys. Rev. 71 (1947) 38. Phys. Rev. 71 (1947) 78.
[3] S.Tanaka, Nuovo Cimento 114 B (1999) 49. ”Contracted Represenation of
Yang’s Sapcetime Algrebra and Buniy-Hsu-Zee Discrete Spacetime ” hepth/0511023. ” Noncommutative Field Theory on Yang’s Space-Time Algebra, Covariant Moyal Star products and Matrix Model ” hep-th/0406166 . ”
Space-Time Quantization and Nonlocal Field Theory ...” hep-th/0002001 .
”Yang’s Quantized Space-Time Algebra and Holographic Hypothesis ” hepth/0303105 .
[4] A. Ashtekar, C. Rovelli and L. Smolin, Phys. Rev. Lett 69 (1992) 237.
[5] -C. Castro, Mod. Phys. Letts A 17 , No. 32 (2002) 2095. ( 2004) .
[6] C. Castro, ”Branes from Moyal deformations of Generalized Yang-Mills Theories” [arXiv: hep-th/9908115]. S. Ansoldi, C. Castro, E. Spallucci, Phys.
Lett B 504 ( 2001 ) 174 . Class. Quan. Gravity 18 ( 2001) L17-L23. Class.
Quan. Gravity 18 ( 2001) 2865 . S. Ansoldi, C. Castro, E. Guendelman and
E. Spallucci, Class. Quant. Gravity vol. 19 ( 2002 ) L 135.
[7] C. Castro, General Relativity and Gravitation 36 , No. 12 (2004) 2605.
[8] C. Castro, Jour. Math. Phys. 34 (1993) 681. Phys Letts B 288 (1992) 291.
[9] Q.H. Park, Int. Jour. Mod. Phys A 7 ( 1992) 1415. H. Garcia-Compean,
J. Plebanski, M. Przanowski, ”Geometry associated with SDYM and chiral
approaches to Self Dual Gravity” [ arXiv : hep-th/9702046] .
[10] J. Hoppe, ” Quantum Theory of a Relativistic Surface” Ph.D Thesis MIT
(1982).
[11] M. Kontsevich, ” Deformation Quantization on Poisson Manifolds I ” qalg/9709040.
[12] B. Fedosov, J. Diff. Geom 40 , no. 2 (1994 ) 213.
[13] C. Castro and M. Pavsic, Progress in Physics, vol. 1 ( 2005 ) 31-64 .
[14] C. Castro, Foundations of Physics 35 , no. 6 (2005) 971.
[15] C.Castro, Phys. Letts B 626 (2005) 209. Foundations of Physics 8 ( 2000
) 1301.
[16] T. Biro, B. Mueller and S. Matinyan, ” Chaotic Quantization: maybe the
Lord plays dice after all ?” hep-th/0301131.

15

Quantization in Astrophysics ...

176

[17] O. Aharony, S. Gubser, J. Maldacena, H. Ooguri and Y. Oz, ” Phys. Rep
323 (2000) 183.
[18] B. de Witt and I. Herger, ” Anti de Sitter Supersymmetry” hep-th/9908005.

16

Quantization in Astrophysics ...

177

RUNNING NEWTONIAN COUPLING AND HORIZONLESS
SOLUTIONS IN QUANTUM EINSTEIN GRAVITY
C. Castro∗ 1 , J. A. Nieto? 2 , J.F. Gonzalez†3
∗

Center for Theoretical Studies of Physical Systems, Clark Atlanta University, Atlanta,
GA. 30314, USA.
?
Facultad de Ciencias Fı́sico-Matemáticas de la Universidad Autónoma de Sinaloa,
80010, Culiacán Sinaloa, México.
November 2006

Abstract
It is shown how the exact Nonperturbative Renormalization Group flow of the running
Newtonian coupling G(r) in Quantum Einstein Gravity is consistent with the existence
of an ultra-violet cutoff R(r = 0) = 2GN Mo in the most general Schwarzschild solutions.
After setting gtt = 1 − 2GN Mo /R(r) = 1 − 2G(r)M (r)/r, and due to the condition G(r =
0) = 0 and M (r = 0) ∼ 1/2GN Mo , we prove why there is no horizon, since gtt (r = 0) = 0,
and there is a delta function scalar curvature singularity at r = 0. Similar results follow in
generalized Anti de Sitter-Schwarzschild metrics with a running cosmological parameter
Λ(r) and Newtonian coupling G(r). The ultra-violet cutoff in this latter case is no longer
given by 2GN Mo , but instead is given by a real-valued positive root R∗ of a cubic equation
associated with the condition gtt (R(r = 0)) = gtt (R∗ ) = 0. A running Newtonian coupling
G(r) can also be accommodated naturally in a Jordan-Brans-Dicke scalar-tensor theory
of Gravity via a trivial conformal transformation of the Schwarzschild metric. However,
the running Newtonian coupling G(r) = (16πΦ2 )−1 corresponding to the scalar field
Φ does not satisfy the asymptotic freedom condition G(r = 0) = 0 associated with
the ultra-violet non-Gaussian fixed point of Nonperturbative Quantum Einstein Gravity.
Nevertheless, our results exhibit an interesting ultra-violet/infrared duality behaviour of
G(r) that warrants further investigation. Some final remarks are added pertaining naked
singularities in higher derivative gravity, Finsler geometry, metrics in phase spaces and the
connection between an ultra-violet cutoff in Noncommutative spacetimes and the general
Schwarzschild solutions.
Keywords: Renormalization Group, Quantum Gravity, General Relativity, Strings,
Black Holes. PACS numbers: 04.60.-m, 04.65.+e, 11.15.-q, 11.30.Ly
1

castro@ctsps.cau.edu
nieto@uas.uasnet.mx
3
jfgh.teorfizikisto@gmail.com
2

1
Quantization in Astrophysics ...

178

1

1.1

Renormalization Group Flow and Schwarzschild
solution
Introduction

We begin by writing down the class of static spherically symmetric (SSS) solutions of
Einstein’s equations [1] studied by [5], [8], [7], [6] among others, and most recently [12]
given by a inf inite family of solutions parametrized by a family of admissible radial
functions R(r)
(ds)2 = g00 (dt)2 − gRR (dR)2 − R2 (dΩ)2 =
dR 2
) (dr)2 − R2 (dΩ)2 = g00 (dt)2 − grr (dr)2 − (R(r))2 (dΩ)2 (1.1a)
dr
where the solid angle infinitesimal element is
g00 (dt)2 − gRR (

(dΩ)2 = sin2 (φ)(dθ)2 + (dφ)2 .
and
g00 = (1 −

2 G N Mo
);
R(r)

gRR =

grr = gRR (dR/dr)2 = (1 −

(1.1b)

1
1
.
=
g00
1 − (2 GN Mo /R(r))
2 GN Mo −1 dR(r) 2
) (
).
R(r)
dr

(1.1c)

Notice that the static spherically symmetric (SSS) vacuum solutions of Einstein’s
equations, with and without a cosmological constant, do not determine the form of the
radial function R(r) [12], [10]. There are two classes of solutions; ( i ) those solutions
whose radial functions obey the condition R(r = 0) = 0, like the Hilbert textbook black
hole solution R(r) = r with a horizon at r = 2GN Mo ; and ( ii ) those horizonless solutions
with an ultraviolet cutoff R(r = 0) = 2GN Mo . In particular, for radial functions like
R(r) = r + 2GN Mo ;

R(r) = [r3 + (2GN Mo )3 ]1/3 ;

R(r) =

2GN Mo
.
1 − e−2GN Mo /r

(1.2)

found by Brillouin [3] , Schwarzschild [2] and Fiziev-Manev [7] respectively obeying the
conditions that R(r = 0) = 2GN Mo and when r >> 2GN Mo ⇒ R(r) → r.
It is very important to emphasize that despite the fact that one can always relabel
the variable r for R in such a way that the metric in eq-(1.1) has exactly the
same f unctional f orm as the standard Hilbert textbook solution [4] (black-holes solutions with a horizon at r = 2GN Mo ) this does not mean that the Hilbert textbook
metric is dif f eomorphic to the metric in eq-(1.1). The reason is that the values of r
range from 0 to ∞ while the values of R range from 2GN Mo to ∞. The physical explanation why there is an ultra-violet cutoff at R = 2GN Mo was provided long ago by
Abrams [5], and rather than imposing this ultraviolet ( UV ) cutoff R = 2GN Mo by fiat
2
Quantization in Astrophysics ...

179

(by decree, by hand) there is a deep physical reason for doing so; namely it has been
argued that the Hilbert textbook solution R(r) = r does not properly represent the static
gravitational field of a point mass centered at the origin r = 0 [5], [7], [8], [6] because the
Hilbert textbook solution is not static in the region 0 < r < 2GN Mo after performing the
Fronsdal-Kruskal-Szekeres analytical continuation in terms of the new u, v coordinates.
In section 3 we will explain the physical meaning of this UV cutoff R(r = 0) = 2GN Mo
resulting from the noncommutativity of the spacetime coordinates. Since the point r = 0
is fuzzy and delocalized, it has an area. Another interpretation as to why the proper
area of the point mass at r = 0 is not zero ( while the volume is zero ) may be due to
the stringy nature of a ”point” and can be understood if one formulates the problem in
phase space, in particular within the framework of the Finsler geometry associated with
the co-tangent bundle of spacetime. Thus a nonzero area of the point mass at r = 0
stems from the additional momentum degrees of freedom in phase space after imposing
the mass-shell condition pµ pµ = M 2 .
There are many physical differences among the Hilbert textbook solution that has
a horizon at r = 2GN Mo and the original 1916 Schwarzchild’s horizonless solution [2].
The Schwarzschild 1916 solution is not a naive radial reparametrization of the Hilbert
solution because the radial function chosen by Schwarzschild R3 = |r|3 + (2GN Mo )3 can
never zero. The absolute value |r| properly accounts for the field of a point mass source
located at r = 0. Thus, the lower bound of R is given by 2GN Mo , and R cannot be zero
for a nonvanishing point mass source.
The Fronsdal-Kruskal-Szekeres analytical continuation of the Hilbert textbook solution
for r < 2GN Mo yields a spacelike singularity at r = 0 and the roles of t and r are
interchanged when one crosses r = 2GN Mo ; so the interior region r < 2GN Mo is no
longer static. The Schwarzchild solution is static for all values of r and in particular for
r < 2GN Mo ; there is no horizon at r = 2GN Mo and there is a timelike naked singularity
at r = 0, the true location of the point mass source. Notice that when r >> 2GN Mo the
Schwarzchild solution reduces to the Hilbert solution and one has the correct Newtonian
limit.
Colombeau [11] developed the rigorous mathematical treatment of tensor-valued distributions in General Relativity, new generalized functions (nonlinear distributional geometry) and multiplication of distributions in nonlinear theories like General Relativity
since the the standard Schwarz theory of linear distributions is invalid in nonlinear theories. This treatment is essential in order to understand the physical singularity at the
point-mass location r = 0. In [10] we studied the many subtleties behind the introduction
of a true point-mass source at r = 0 ( that couples to the vacuum field ) and the physical
consequences of the delta function singularity (of the scalar curvature) at the location of
the point mass source r = 0. Those solutions were obtained from the vacuum SSS solutions simply by replacing r for |r|. For instance, the Laplacian in spherical coordinates
in flat space of 1/|r| is equal to −(1/r2 )δ(r), but the Laplacian of 1/r is zero. Thus,
to account for the presence of a true mass-point source at r = 0 one must use solutions
depending on the modulus |r| instead of r.
One can have an infinite number of metrics parametrized by a family of arbitrary
radial functions R(r) with the desired behaviour at r = 0 and r = ∞, whose values for
3
Quantization in Astrophysics ...

180

the scalar curvature (parametrized by a family of arbitrary radial functions R(r)) are
given by [10]
2 GN Mo δ(r)
; in units of c = 1.
(1.3a)
R = −
R2 (dR/dr)
Since the scalar curvature R (1.3a) is a coordinate invariant quantity, this result in
eq-(1.3a) that depends explicitly on the family of radial functions R(r) corroborates once
more that one cannot view the role of the radial function R(r) as a naive change of radial
coordinates from r to R. Hence, one must view the radial function squared R2 (r) as just
one of the metric tensor-field components gφφ (r) ≡ R2 (r); i.e. R(r)2 is a f unction of
the radial coordinate r that has a lower cutoff given by gφφ (r = 0) = (2GN Mo )2 . One
must not confuse R with r and even after relabeling r for R, the metric in eq-(1.1) is
not diffeomorphic to the Hilbert textbook solution due to the cutoff R = 2GN Mo . If one
chooses the radial functions to obey the condition R(r = 0) = 0 and R(r → ∞) ∼ r
then only in this case these metrics are diffeomorphic to the Hilbert textbook black hole
solution.
The relevant invariant physical quantity independent of the any arbitrary choice of
R(r) is the Einstein-Hilbert action, whether it obeys the condition R(r = 0) = 0 or
R(r = 0) = 2GN Mo . In particular, the Euclideanized action after a compactification of
the temporal interval yields an invariant quantity which is precisely equal to the ”black
hole” entropy in Planck area units. The invariant area is the proper area at r = 0
given by 4πR(r = 0)2 = 4π(2GN Mo )2 . We shall see that the source of entropy is due
entirely to the scalar curvature delta function singularity at the location of the point
mass source given by R = −[2GN Mo /R2 (dR/dr)]δ(r) [10] after using the 4-dim measure
4πR2 (|gRR |1/2 dR) (|gtt |1/2 dt) = 4πR2 dR dt in the Euclidean Einstein-Hilbert action.
Therefore, the Einstein-Hilbert action associated with the scalar curvature delta function in eq-(1.3a) when the four-dim measure is
d4 x = 4π R2 dR dt.

(1.3b)

is
S=−

Z
2M δ(r)
1
(4π R2 dR dt) (− 2
) =
16πGN
R (dR/dr)

Z
1
2GN Mo
(
δ(r) ) (4πr2 dr dt).
(1.3c)
16πGN
r2
Notice that the action (1.3c) is truly invariant and independent of any arbitrary choice
of the radial function R(r) , whether or not it is the Hilbert textbook choice R(r) = r, or
any other choice for R(r). The Euclideanized action (1.3c) becomes, after reinserting the
Newtonian coupling G = L2P lanck in order to have the proper units,

4π(GN Mo )2
4π (2GN Mo )2
Area
S(Euclidean) =
=
=
.
GN
4 L2P lanck
4 L2P lanck

(1.3d)

when the Euclidean time coordinate interval 2πtE is defined in terms of the Hawking
temperature TH and Boltzman constant kB as 2πtE = (1/kB TH ) = 8πGN Mo . It is
4
Quantization in Astrophysics ...

181

interesting that the Euclidean action (1.3c) is the same as the ”black hole” entropy (1.3d)
in Planck area units. The source of entropy is due entirely to the scalar curvature delta
function singularity at the location of the point mass source. Furthermore, this result
that the Euclidean action is equal to the entropy in Planck units can be generalized to
higher dimensions upon recurring to Schwarzschild-like metrics in higher dimensions.
The fact that a point-mass can have a non-zero proper area 4πR(r = 0)2 =
4π(2GN Mo )2 , but no volume, due to the metric and curvature singularity at r = 0 seems
to indicate a stringy nature underlying the very notion of a point-mass itself. The string
world-sheet has a non-zero area but zero volume. Aspinwall [13] has studied how a string
(an extended object) can probe space-time points due to the breakdown of our ordinary
concepts of Topology at small scales. In [12] it was shown how the Bars-Witten stringy
1 + 1-dim black-hole metric [14] can be embedded into the 4-dim conf ormally re-scaled
metrics displayed in eq-(1.1), if and only if, the radial function R(r) was given implicitly
by the following relationship involving R and r ( the left hand side has the same functional
form as the radial tortoise coordinate) :
r
R − 2GN Mo
dR
) = 2GN Mo ln [ sinh
].
= R + 2GN Mo ln (
1 − 2GN Mo /R
2Mo
2GN Mo
(1.4a)
one can verify that there is an ultra-violet cutoff at r = 0
Z

R(r = 0) = 2GN Mo ;

R(r → ∞) → R ∼ r.

(1.4b)

which precisely has the same behaviour at r = 0 and ∞ as the radial functions displayed
in this section. The fact that the stringy black-hole 1 + 1-dim solution can be embedded
into the conformally rescaled solutions of this section, for a very specific functional form
of the radial function R(r), with the same ”boundary” conditions at r = 0 and r = ∞
as the radial functions displayed in this section, is very appealing. Similar conclusions
apply to horizonless solutions in higher dimensions D > 4 [12] with a cutoff R(r = 0) =
[16πGD Mo /(D − 2)ΩD−2 ]1/D−3 where the point-mass has a nonzero D − 2-dimensional
measure and a zero D − 1-dim ”volume”. The point-mass in this case is p − branelike in
nature with p + 1 = D − 2. For example, in D = 5 one has a membrane-like behaviour
of a point mass. In D = 6 one has a 3-brane-like behaviour of a point mass, etc.... The
D = 4 case is special since it corresponds to the string.

1.2

Renormalization Group Flow and Horizonless Solutions

The purpose of this section is to explain the meaning of the ultra-violet cutoff R(r =
0) = 2GN Mo within the context of the exact Nonperturbative Renormalization Group
flow of the Newtonian coupling G = G(r) in Quantum Einstein Gravity [16] where a nonGaussian ultra-violet fixed point was found G(r = 0) = 0. The presence of an ultra-violet
cutoff R = 2GN Mo originates from the mere presence of matter and permits to relate
the metric component gtt = 1 − 2GN Mo /R(r) to gtt = 1 − 2G(r)M (r)/r, in such a way
5
Quantization in Astrophysics ...

182

that the the small distance behaviour of G(r) eliminates the presence of a horizon at
r = 2GN Mo : we will see why the metric component gtt evaluated at the location of the
point mass source r = 0 is gtt (r = 0) = 0, due to G(r = 0) = 0, M (r = 0) = f inite but
it does not eliminate the delta function singularity of the scalar curvature at r = 0. This
result is compatible with the ultra-violet cutoff of the radial function R(r = 0) = 2GN M .
GN is the value of the Newtonian coupling in the deep infrared and M = Mo is the Kepler
mass as seen by an observer at asymptotic infinity.
The momentum dependence of G(k 2 ) was found by Reuter et al [16] to be
GN
.
1 + α GN k 2
The momentum-scale relationship is defined
G(k 2 ) =

k2 = (

β 2
),
D(R)

(1.5a)

β = constant.

(1.5b)

in terms of the proper radial distance D(R)
D(R) =

Z

√

R

2GN Mo

gRR dR =

Z

R

dR
q

1 − (2 GN Mo /R)

2GN Mo

s

q

R ( R − 2 GN Mo ) + 2 GN Mo ln [

R
+
2GN Mo

s

=

R − 2GN Mo
].
2GN Mo

(1.6)

where the lower (ultra-violett cutoff ) is R(r = 0) = 2GN Mo . The proper distance
corresponding to r = 0 is D(R(r = 0)) = D(R = 2GN Mo ) = 0 as it should since the
proper distance from r = 0 is zero when one is located at r = 0.
Hence,
GN D(R)2
GN
=
,
G = G(R) =
1 + α GN k 2
D(R)2 + αβ 2 GN

(1.7)

such that G(R(r = 0)) = G(R = 2GN Mo ) = 0 consistent with the findings [16] since
D(R(r = 0)) = D(R = 2GN Mo ) = 0.
An important remark is in order. There is a f undamental dif f erence between the
work of Reuter et al [16] and ours . The metric components studied by [16] were of the
form, gtt = 1−2G(r)Mo /r, .... and are not solutions of Einstein’s field equations. Whereas
in our case, the metric components (1.1) gtt = 1 − 2G(r)M (r)/r = 1 − 2GN Mo /R(r), ....
are solutions of Einstein’ equations displayed in eq-(1.1). This is one of the most salient
features in working with the most general metric (1.1) involving the radial functions R(r)
instead of forcing R(r) = r.
Hence, given that R = R(r), by imposing the following conditions valid for all values
of r
(1 −

2 G N Mo
2 G(r) M (r)
) = (1 −
).
R(r)
r

6
Quantization in Astrophysics ...

183

(1.8)

( dR
)2
dr
=
N Mo
(1 − 2 GR(r)
)
(1 −

1

.

(1.9)

dR
= 1 ⇒ R(r) = r + 2GN Mo .
dr

(1.10)

2 G(r) M (r)
)
r

from eqs-(1.7, 1.8, 1.9) one infers that

which is the Brillouin choice for the radial function as well as the relation
r
Mo
GN D(R)2
G(r) = GN ( ) (
) = (
) ⇒
R M (r)
D(R)2 + αβ 2 GN
r D(R)2 + αβ 2 GN
M (r) = Mo ( ) (
).
R
D(R)2

(1.11)

that allows us to determine the form of the M (r) once the radial function R(r) = r +
2GN Mo is plugged into D(R) given by eq-(1.6). The constant found by Reuter et al [16]
is αβ 2 = 118/15π and the proper distance D(R) is given by eq-(1.6).
When r = 0 a careful analysis reveals
M (r → 0) → (constant)

1
2 GN Mo

..

(1.12)

therefore, the running mass parameter at r = 0, M (r = 0) ∼ 1/R(r = 0) = 1/(2GN Mo )
is f inite instead of being infinite. The running mass at r = 0 has a cutoff given by
the inverse of the ultra-violet cutoff R(r = 0) = 2GN Mo ( up to a numerical constant
). When r = 0 one has in eqs-(1.7, 1.11) that G(r = 0) = 0. When r → ∞ one has
M (r → ∞) → Mo as expected, where Mo is the Kepler mass observed by an observer at
asymptotic infinity ( deep infrared ) and G(r → ∞) → GN .
The running flow M (r) was never studied by [16]. Our ansatz in eqs-(1.8, 1.9) is an
heuristic one ( a conjecture ). In the special case when M (r = 0) = Mo one gets the
interesting result for the value of Mo given by Mo ∼ MP lanck which is the same, up to
a trivial numerical factor, to the Planck mass remnant in the final state of the Hawking
black hole evaporation process found by [16] after a Renormalization Group improvement
of the Vaidya metric was performed.
Concluding, R = r + 2GN Mo is the sought after relation between r and R, out of an
infinite number of possible functions R(r) obeying the SSS vacuum solutions of Einstein’s
equations. We may notice that r = r(R) = D(R) given by eq-(1.6) is the appropriate
choice for the radial function if, and only if, the spatial area coincides with the proper
area 4πR(r)2 . The spatial area A(r) is determined in terms of the infinitesimal spatial
volume dV (r) as follows :
(dR/dr)
.
dV (r) = A(r)dr ⇒ A(r) = 4πR(r)2 q
1 − 2GN Mo /R(r)
When A(r) = 4πR2 then
7
Quantization in Astrophysics ...

184

(1.13a)

(dR/dr)
q

1 − 2GN Mo /R

= 1.

(1.13b)

since the integration of eq-(1.12) was performed in eq-(1.6), one can infer then that r =
r(R) = D(R) is the choice in this case for the functional relationship between R and r;
in particular A(r = 0) = 4π(2GN Mo )2 , which is not true in general when the proper area
is not equal to the spatial area. The volume is zero at r = 0.
To finalize this subsection, when the radial function R = r +2GN Mo has been specified
by the RG flow solutions [16] , the scalar curvature is
R = −

2 GN Mo δ(r)
2 GN Mo δ(r)
= −
.
2
R (dR/dr)
(r + 2GN Mo )2

(1.14a)

and has a delta function singularity at r = 0 of the form
−

δ(r = 0)
2 GN Mo δ(r = 0)
= −
.
2
(2GN Mo )
2GN Mo

(1.14b)

compared to the stronger singular behaviour of the Hilbert textbook solution at r = 0
when R = r
R(Hilbert) = −

2 GN Mo δ(r)
2 GN Mo δ(r)
= −
⇒
2
R (dR/dr)
r2

2 GN Mo δ(r = 0)
.
(1.14c)
02
The reason the singularity of (1.14b) is softer than in (1.14c) is because when there is
an ultra-violet cutoff of the radial function R(r = 0) = 2GN Mo (due to the presence
of matter) the proper area 4π(2GN Mo )2 is finite at r = 0 and so is the surface mass
density. However, since the volume is zero at the location r = 0 of the point-mass, the
volume mass density is inf inite and one cannot eliminate the singularity at r = 0 given
by R = −δ(r = 0)/(2GN Mo ) .
R(r = 0) = −

1.3

Anti de Sitter-Schwarzschild Metrics and running Cosmological Constant

We begin with the generalized de Sitter and Anti de Sitter metrics that will help us
understand the nature of the infrared cutoff required to solve the cosmological constant
problem. In [10] we proved why the most general static form of the ( Anti ) de SitterSchwarzschild solutions are given in terms of an arbitrary radial function by
2GN Mo Λo
−
R(r)2 )−1 (dR(r)/dr)2 .
R(r)
3
(1.15)
2
2
The angular part is given as usual in terms of the solid angle by −(R(r)) (dΩ) .
g00 = ( 1 −

2GN Mo Λo
−
R(r)2 ),
R(r)
3

grr = −( 1 −

8
Quantization in Astrophysics ...

185

Λo is the cosmological constant. The Λo < 0 case corresponds to Anti de SitterSchwarzschild solution and Λo > 0 corresponds to the de Sitter-Schwarzschild solution.
The physical interpretation of these solutions is that they correspond to ”black holes”
in curved backgrounds that are not asymptotically flat. For very small values of R one
recovers the ordinary Schwarzschild solution. For very large values of R one recovers
asymptotically the ( Anti ) de Sitter backgrounds of constant scalar curvature.
Since the radial function R(r) can be arbitrary, one particular expression for the
radial function R(r) , out of an inf inite number of arbitrary expresions, in the de SitterSchwarzschild (Λo > 0) case one may choose [10]
1
1
=
+
R − (2GN Mo )
r

s

Λo
.
3

(1.16)

When Λo = 0 one recovers R = r + (2GN Mo ) that has a similar behaviour at r = 0 and
r = ∞ as the original Schwarzschild solution of 1916 given by R3 = r3 + (2GN Mo )3 ; i.e.
R(r = 0) = 2GN Mo and R(r → ∞) ∼ r respectively. When Mo = 0 one recovers the
pure de Sitter case and the radial function becomes
s

1
Λo
1
= +
.
(1.17)
R
r
3
In this case, one encounters the reciprocal situation ( the ”dual” picture ) of the
Schwarzschild solutions : ( i ) when r tends to zero ( instead of r = ∞ ) the radial
function behaves R(r → 0) → r ; in particular R(r =q0) = 0 and (ii) when r = ∞ (
instead of r = 0 ) the value of R(r = ∞) = RHorizon = Λ3o and one reaches the location
of the horizon given by the condition g00 [R(r = ∞)] = 0.
A reasonable and plausible argument as to why the cosmological constant is not zero
and why it is so tiny was given by [10] : In the pure de Sitter case, the condition
g00 (r = ∞) = 0 ⇒ 1 −

Λo
R(r = ∞)2 = 0
3

(1.18)

has a real valued solution
s

R(r = ∞) =

3
= RHorizon = Inf rared cutof f.
Λo

(1.19)

and the correct order of magnitude of the observed cosmological constant can be derived
from eq-(1.19) by equating R(r = ∞) = RHorizon = Hubble Horizon radius as seen today
since the Hubble radius is constant in the very late time pure inflationary de Sitter phase
of the evolution of the universe when the Hubble parameter is constant Ho . The metric in
eq-(1.15) is the static form of the generalized de Sitter ( Anti de Sitter ) metric associated
with a constant Hubble parameter.
Therefore, by setting the Hubble radius to be of the order of 1061 LP lanck and by
setting G = L2P lanck ( h̄ = c = 1 units) in
8π G ρvacuum = Λo =

3
3
=
⇒
2
R(r = ∞)2
RH

9
Quantization in Astrophysics ...

186

ρvacuum =

3 1 LP 2
3 1 1
=
(
) ∼ 10−123 (MP lanck )4 ,
2
2
8π LP RH
8π L4P RH

.

(1.20)

we obtain a result which agrees with the experimental observations when RHubble ∼
1061 LP lanck .
Notice the importance of using the radial function R = R(r) in eq-(1.17). Had one used
R = r in eq-(1.17) one would have obtained a zero value for the cosmological constant
when r = ∞ . Thus, the presence of the radial function R(r) is essential to understand
why the cosmological constant is not zero and why it is so tiny .
The idea now is to relate the metric components in the Anti de Sitter-Schwarzcshild
case involving the running G(r), M (r), Λ(r) parameters with the metric components
of (1.15) involving the unique and sough-after radial function R(r) and the constants
GN , Mo , Λo (as seen by an asymptotic observer in the deep infrared region). The equations which determine the forms of M (r) and R(r) are given by
( 1−

( 1−

2 G(r) M (r) Λ(r) 2
2 GN Mo Λo
−
R(r)2 ) = ( 1 −
−
r ).
R(r)
3
r
3

(1.21)

2 GN Mo Λo
2G(r) M (r) Λ(r) 2 −1
−
R(r)2 )−1 (dR(r)/dr)2 = ( 1 −
−
r ) . (1.22)
R(r)
3
r
3

then from eqs-(1.21, 1.22) one infers that
dR
= 1 ⇒ R(r) = r + R∗
dr

(1.23)

where the constant of integration R∗ is now the root of the cubic equation, and not the
value 2GN Mo , given by

1−

2 GN Mo Λo 2
2 GN Mo Λo
+
R(r = 0)2 = 1 −
R = = 0.
+
R(r = 0)
3
R∗
3 ∗

(1.24)

such that gtt (R(r = 0)) = gtt (R∗ ) = 0. The real positive root of the cubic equation (found
after multiplying (1.24) by R∗ 6= 0) is
v
u

v
u

2
3GN Mo u
(3GN Mo )2
1
3GN Mo u
1/3
t (3GN Mo ) + 1 ]1/3 .
R∗ = [
+
]
+
[
+t
−
|Λo |
Λ2o
|Λo |3
|Λo |
Λ2o
|Λo |3
(1.25)
Because Anti de Sitter space has ΛAdS < 0, we have already aken into account the negative
sign in the expression in eq-(1.25) by writing ΛAdS = −|Λo | and we must disregard the
two complex roots (a pair of complex conjugates).
The values of R range from 0 < R∗ ≤ R ≤ ∞ and correspond to the values of
r ranging from 0 ≤ r ≤ ∞. This is very reasonable since R has an ultra-violet cutoff
given by the root of the cubic R∗ > 0. If R was allowed to attain the values of zero the
metric component gtt would blow up. r can in fact attain the zero value, but not the

10
Quantization in Astrophysics ...

187

radial function R(r) = r + R∗ . The metric component grr in (1.15) blows up at r = 0,
location of the singularity.
Notice that one cannot take the limits Λ0 → 0 in eq-(1.25) af ter having found the
roots of the cubic equation because that limit is singular. One must take the limit |Λo | → 0
of eq-(1.24) before and afterwards find the root of gtt (R∗ ) = 0 given by R∗ = 2GN Mo
(when |Λo | = 0).
After having found the root R∗ of the cubic equation, from eq-(1.21) one infers
Λo
2 G(r) M (r)
Λ(r) 2
2 G N Mo
+
(r + R∗ )2 =
+
r .
r + R∗
3
r
3

(1.26)

which yields M (r)
M (r) =

r
2GN Mo
Λo
Λ(r) 2
[
+
(r + R∗ )2 −
r ].
2G(r) r + R∗
3
3

(1.27)

where now the proper distance D(R) associated with the metric (1.15) is given the elliptic
integral whose lower limit of integration is now given by the cubic root R∗ (instead of
2GN Mo ) :

D(R) =

Z

R

R∗

√

gRR dR =

Z

R

R∗

dR
q

1−

2 GN Mo
R

+

|Λo |
3

= Elliptic Integral. (1.28a)
R2

such
D(R(r = 0)) = D(R = R∗ ) = 0.

(1.28b)

The running coupling is the one given by [16]
G = G(R) =

GN
GN D(R)2
=
.
1 + α GN k 2
D(R)2 + αβ 2 GN

(1.29)

where D(R) is given by the elliptic integral and the running cosmological parameter is
[16]
|Λ(k)| = |Λo | +

b GN
β4
b GN 4
(k ) = |Λo | +
.
4
4 D(R)4

(1.30)

where the momentum-scale relation is k 2 = (β 2 /D(R)2 ) .
As expected, in eq-(1.27) we have the correct limits : M (r → ∞) → Mo , since when
r → ∞, R(r) → r, |Λ(r)| → |Λo | and G(r) → GN . M (r = 0) ∼ 1/R∗ is finite also because
r/G(r) and Λ(r)r2 are f inite as r → 0.
In the case of de Sitter-Schwarzschild metric , Λo > 0, one has a negative real root
and a positive double root [10] R2 = R3 > 0, R1 < 0; however, there is no horizon since
gtt does not change signs as once crosses the double-root location ; there is problem with
the R1 < 0 solutions and there is a pole of gtt at R = 0 . For this reason we have focused
on the Anti de Sitter-Schwarzschild metric in this subsection.

11
Quantization in Astrophysics ...

188

2

Jordan-Brans-Dicke Gravity

We wish now to relate the metric of eq-(1.1) that solves the vacuum Einstein field equations
for r > 0 written in terms of GN , Mo , R(r) with a metric written in terms of G(r), M (r), r
that does not solve the vacuum field equations but instead the field equations in the
presence of a scalar field Φ associated with the Jordan-Brans-Dicke theory of gravity.
Such metric is given by
(ds)2 = gtt (r) (dt)2 − grr (r) (dr)2 − ρ(r)2 (dΩ)2 .

(2.1)

0
A conformal transformation gµν
= e2λ gµν relating the two metrics can be attained by
starting with the Brans-Dicke-Jordan scalar-tensor action

Z

d4 x

√

g [ Φ2 R + 6 (∇µ Φ) (∇µ Φ) ].

(2.2)

and which can be transformed into a pure gravity action by means of a conformal transformation
q

0
gµν
= e2λ gµν ;

√

q

g 0 R0 (g 0 ) =

g 0 = e4λ

√

g.

g e2λ [ R − 6 (∇µ ∇µ λ) − 6 (∇µ λ)(∇µ λ) ]

(2.3)
(2.4)

By setting
e2λ ≡

Φ2
GN
.
=
Φ2o
G(r)

(2.5)

[ Φ2 R − 6 Φ (∇µ ∇µ Φ) ]

(2.6)

one can rewrite :
√
q

g0

due to the fact that (∇µ
√

0

0

R (g ) =

√

g

Φ2o

g) = 0 then

√
√
g Φ(∇µ ∇µ Φ) = ∇µ ( g Φ∇µ Φ) − g (∇µ Φ)(∇µ Φ).

(2.7)

since total derivative term drops from the action one has the equalities
Z

4

dx

√

2

µ

g [ Φ R + 6 (∇µ Φ) (∇ Φ) ] =

Z

d4 x

√

g [ Φ2 R − 6 Φ (∇µ ∇µ Φ) ] =

Z
q
1
d4 x g 0 R0 (g 0 )
16πGN

(2.8)

0
therefore, one can solve the Einstein vacuum field equations for the metric gµν
( for r > 0
0
) and perform a conformal transformation gµν
= e2λ gµν to obtain the metric that solves
the field equations corresponding to the Jordan-Brans-Dicke action.

12
Quantization in Astrophysics ...

189

The running Newtonian coupling G(r) is now defined explicitly in terms of the scalar
field as follows
1
1
Φ2 =
; Φ2o =
.
(2.9a)
16πG(r)
16πGN
and the dimensionless scaling factor e2λ is given by the ratio :
2λ

e

Φ2
GN
= 2.
=
G(r)
Φo

(2.9b)

such that the equalities among the three lines of eq-(2.5) are satisfied.
The scalar field Φ that determines the functional form of G(r) must solve the generalized Klein-Gordon equation obtained from a variation of the action (2.5) w.r.t the scalar
field Φ
1
(2.10)
(∇µ ∇µ − R) Φ = 0, f or r > 0.
6
and the latter equation is equivalent to the equation R0 (g 0 ) = 0 since the scalar curvature
0
R, for r > 0, is fixed by eq-(2.6) after setting R0 (g 0 ) = 0 because the metric gµν
is a
0 0
solution of the Einstein vacuum field equations for r > 0. When R (g ) = 0, for r > 0,
yields the scalar curvature
6
(∇µ ∇µ Φ).
(2.11)
R(g) =
Φ
which is precisely equivalent to the generalized Klein-Gordon equation (2.10). This means
that the scalar Φ field does not have dynamical degrees of freedom since it is identified
with the conformal factor eλ = Φ/Φo . Therefore one can safely equate the scalar field Φ2
with (1/16πG(r)) giving
6
(∇r ∇r Φ) =
Φ
q
6
1
√ rr
q
∂
(
g
g
∂
G(r) ).
√ r
r
G(r) g
R(r) =

(2.12)

where the metric components gµν necessary to evaluate the Laplace-Beltrami operator are
obtained directly via the conformal scaling of the metric that solves the vacuum static
spherical solutions of Einstein’s equations of the previous section :
gtt = e−2λ (1 −
gRR = e−2λ

1
1−

2GN Mo
R

,

gφφ = e−2λ R(r)2 = ρ(r)2 ,
√

2GN Mo
).
R(r)
grr = gRR (dR/dr)2 .

gθθ = e−2λ R(r)2 sin2 (φ).

g = e−4λ R(r)2 (dR/dr) sin(φ).

13
Quantization in Astrophysics ...

190

(2.13)
(2.14)
(2.15)
(2.16)

Since e−2λ = G(r)/GN and G(r = 0) = 0 then the radial rho function obeys the condition
ρ(r = 0) = 0.
The new proper distance D(R) is now given by

D(R) =

Z

e−λ

R

2GN Mo

q

1 − (2GN Mo /R)

dR =

Z

(G(R)/GN )1/2

R

q

1 − (2GN Mo /R)

2GN Mo

dR (2.17)

and dif f ers from the expressions of eq-(1.6) because of the conformal factor.
However, there is a caveat if we now try to use the running flow of the Newtonian
coupling of the previous section [16]
v
u

u (αβ 2 GN ) G(R)
GN D(R)2
t
⇒
D(R)
=
.
G(R) =
D(R)2 + αβ 2 GN
GN − G(R)

(2.18)

because the RG flow equations must dif f er now due to the presence of the scalar field
Φ. To prove why one cannot use the running flow equation (2.18) for G used in section
1.2, 1.3, let us differentiate both sides of the expression for D(R) in eq-(2.18) and upon
equating the result with the integrand of eq-(2.17) leads to the dif f erential equation
obeyed by G(R) :
αβ 2 G2N
dG(R)
dD(R)
r
=
2
dR
dR
(αβ GN ) G(R)
2 (GN − G(R))2
GN − G(R)

= q

(G(R)/GN )1/2

1 − (2GN Mo /R)

.

(2.19)

subject to the boundary conditions G(R(r = 0)) = G(R = 2GN Mo ) = 0 and G(r → ∞) =
G(R → ∞) → GN . The differential equation (2.19) is the equation that determines the
functional form of G(R). Notice that functional form of G(R) which obeys the above
differential equation is not the same as the result obtained for G(R) in eq-(1.7) of the
previous section because the proper distance D(R) given by the integral of eq-(2.17)
dif f ers from the integral of eq-(1.6). The constant found by Reuter et al [16] is αβ 2 =
118/15π.
One can integrate eq-(2.19) giving the functional relationship between G and R :
√ 2 2 Z
Z R
G
αβ GN
dG
dR
q
q
=
=
2
Go G
2GN Mo
(GN − G)3
1 − (2GN Mo /R)
q
√ 2 2
2 arctanh [ 1 − (G/GN ) ]
αβ GN
2
q
[
−
] − I[Go ] =
2
(GN )3/2
GN (GN − G)
s

q

R ( R − 2 GN Mo ) + 2 GN Mo ln [

R
+
2GN Mo

s

R − 2GN Mo
].
2GN Mo

(2.20)

where Go ≡ G(R = 2GN Mo ).
One can immediately deduce that the first integral diverges when G = GN which is
compatible with the condition G(R → ∞) = GN . But there is a problem in enforcing the
14
Quantization in Astrophysics ...

191

behaviour of G(r = 0) = 0; one cannot impose the condition Go ≡ G(R = 2GN Mo ) = 0
because the G integral also diverges when G = Go = 0 ! ( the integral is −∞ ).
Therefore, one must have the condition Go ≡ G(R = 2GN Mo ) 6= 0. The value of Go
obeying GN > Go = G(r = 0) > 0 can be determined from solving the transcendental
equation derived from the condition
√
I[Go ] =

q

2 arctanh [ 1 − (Go /GN ) ]
αβ 2 G2N
2
q
[
−
] = 0. (2.21)
2
(GN )3/2
GN (GN − Go )

The result I[Go ] = 0 is now compatible with the behaviour of the R integral which is zero
when R(r = 0) = 2GN M0 . To sum up : one cannot satisfy the condition G(R(r = 0)) = 0
required by eqs-(1.7, 2.18) found by [16].
The same conclusions apply ( one is forced to impose Go > 0 ) if we had taken a minus
sign in front of the square root in the R integral which leads to G(r → ∞) = 0 ( R ∼ r
when r → ∞), as opposed to the desired behaviour G(r → ∞) → GN It is interesting
that this result G(r → ∞) = 0, when the minus sign in front of the square root is chosen,
is ”dual” to the behaviour found in the RG flow solutions by [16] where at r = 0 (instead
of r = ∞ ) one encounters G(R(r = 0)) = G(R = 2GN Mo ) = 0 ( asymptotic freedom).
Concluding, the fact that G integral (2.20) diverges at G = 0 is a signal that one
cannot use the running flow equation (2.18) for G in the presence of the Jordan-BransDicke scalar Φ. One would have to solve the modif ied RG equations that will involve the
beta functions for the Φ field in addition to the metric gµν . A similar divergence problem
was encountered by [17]. One can bypass this divergence problem by imposing G(r = 0) =
G(R = 2GN M0 ) = Go > 0 where Go is given by a solution of the trascendental equation.
By taking the minus sign in front of the square root we found an ultraviolet/infrared
”duality” behaviour of the couplings, at r = 0 and R ∼ r → ∞, which warrants further
investigation.

3

Concluding Remarks : On Noncommutative and
Finsler Geometries

We conclude by discussing some speculative remarks. It is well known (see references in
[17]) that by replacing GN → G(k 2 ) = GN (1 + GN k 2 )−1 leads to 1/k 4 modifications of
the propagator
GN
1
1
G(k 2 )
= 2
= GN [ 2 − 2
],
2
2
k
k (1 + GN k )
k
k + MP2 lanck

GN MP2 lanck = 1.

(3.1)

that correspond to quadratic curvatures R2 of perturbative quantum gravity. The
Lanczos-Lovelock theories of Gravity involving higher powers of the curvature have the attractive feature that the equations of motion are no more than second order in derivatives
15
Quantization in Astrophysics ...

192

of the metric and contain no ghosts. The authors [18] have found black hole solutions,
topological defects, and naked singularities as well, in pure Lanczos-Lovelock Gravity with
only one Euler density term. The fact that naked singularities were found by [18] deserve
further investigation within the context of modified propagators induced by a running
Newtonian coupling.
Another interesting field of study is Noncommuttaive Geometry, Fuzzy spaces, Fractal
geometries, etc... The standard noncommutative algebra ( there are far more fundamental
algebras like Yang’s algebra in noncommutative phase spaces ) is of the form
[xµ , xν ] = iΘµν . [pµ , pν ] = 0 [xµ , pν ] = iη µν

(3.2)

where η µν is a flat space metric and the structure constants (c-numbers ) Θµν = −Θνµ
are c-numbers that commute with x, p and that have dimensions of length2 ; the Θµν are
proportional to the L2P lanck . A change of coordinates
1
0
0
x µ = xµ + Θµρ pρ . p µ = pµ .
2
leads to an algebra with commuting coordinates and momenta
0

0

0

0

0

0

[x µ , x ν ] = 0. [p µ , p ν ] = 0. [x µ , p ν ] = iη µν .

(3.3)

(3.4)

Due to the mixing of coordinates and momentum in the new commuting variables
x0 , p0 one can envisage coordinate and momentum dependent metrics in phase space,
in particular Finsler geometries, and whose average over the momentum coordinates
< πµν (x, p) >p = gµν (x) yield the effective spacetime metric. This momentum averaging procedure is very similar to the averaging of the momentum-scale dependent
metrics employed in the Renormalization Group flow of the effective average action by
[16]. Morever, the momentum dependence
of the new coordinates x0 leads to a momentum
q
dependent radial coordinate r0 = x0i x0i involving commuting x0µ coordinates
s

1
1
(3.5)
(xi + Θiρ pρ ) (xi + Θiτ pτ ).
2
2
Similar attempts to study the Noncommutative effects on black holes by modifying r → r0
have been made by many other authors , [29], [30] however, to our knowledge its relation
to phase spaces and Finsler geometries has not been explored. The impending question
is to find another interpretation of the radial function R(r) and the physical meaning of
the cutoff R(r = 0) = 2GN Mo in terms of the momentum dependent radial coordinate r0 .
When xi = 0 ⇒ r = 0 and (3.5) becomes
0

r =

1 q iρ
Θ pρ Θiτ pτ .
(3.6)
2
The expression inside the square root can be written in terms of pµ pµ = Mo2 , in the static
case when |~p| = pi = 0, i = 1, 2, 3, after the following steps. Firstly, due to the static
condition pi = 0, p0 = E = Mo one has
r0 =

Θiρ Θiτ pρ pτ = Θµρ Θµτ pρ pτ − Θ0i Θ0j pi pj = Θµρ Θµτ pρ pτ ,
16
Quantization in Astrophysics ...

193

(3.7)

this last expression may be recast as
Θµρ Θµτ pρ pτ = λ pτ pτ = λ Mo2 .

(3.8)

if, and only if, the 4 × 4 antisymmetric matrix Θµν obeys the eigenvalue condition :
Θµρ Θµτ pρ = λ pτ .

(3.9)

In the static case pρ = (Mo , 0, 0, 0), the eigenvalue condition yields the following 4 conditions
Θµ0 Θµ0 p0 = λ p0 , Θµ0 Θµi p0 = pi = 0, i = 1, 2, 3.
(3.10)
that will restrict the values of the 6 components of the 4 × 4 antisymmetric matrix Θµν ;
i.e. the 6 components are not independent.
Therefore, in the static case pi = p~ = 0, upon imposing the eigenvalue condition
and after adjusting the value of the constant λ = 16 L4P lanck = 16 G2N , gives then the
ultra-violet cutoff
1 q iρ
Θ pρ Θiτ pτ = 2 L2P Mo = 2 GN Mo .
r (r = 0) =
2
0

consistent with R(r = 0) = 2GN Mo with the only subtlety that that r =
now noncommuting coordinates xµ .
When r 6= 0, the terms

(3.11)
√

xi xi involves

Θµρ pρ xµ + Θµτ xµ pτ = Θµρ pρ xµ + Θµτ xµ pτ =
Θµρ pρ xµ + Θµρ xµ pρ = Θµρ ( xµ pρ − i ηµρ ) + Θµρ xµ pρ = 2 Θµρ xµ pρ . (3.12)
due to the antisymmetric property of Θµρ , one has Θµρ ηµρ = 0.
The quantity Θiρ xi pρ involving the angular momentum operator, xi pρ −xρ pi does not
preserve the spherically symmetry unless one imposes a condition (constraint) in phase
space like
Θiρ xi pρ ∼ L2P lanck Mo ω(r) r2 = GN Mo ω(r) r2

(3.13)

where ω(r) is a scale-dependent frequency. Concluding, in the most general case one has
:
r0 = r0 (r) =

q

r2 + 2Θiρ xi pρ + (2GN Mo )2 .

(3.14)

Since eq-(3.14) involves the phase space variabes x, p the question is to see whether or
not phase space metrics solutions of the form gµν (x, p) = gµν (xµ + Θµρ pρ ) solve the field
equations corresponding to Moyal-Fedosov star product deformations of Noncommutative Finsler Gravity associated with the contangent bundle [20]. For a recent status of
Noncommutative Riemannian gravity see [21] and references therein. However, we must
believe that it is Finslerian geometry the appropriate one to study and the proper arena
to quantize gravity. When r = 0 one recovers the cutoff r0 (r = 0) = 2GN Mo . Therefore this procedure to relate the effects of the Noncommutativity of coordinates with the
17
Quantization in Astrophysics ...

194

ultra-violet cutoff R(r = 0) = 2GN Mo is quite promising . We shall leave it for future
work.
Let us summarize the main conclusions of this work :
1. The original Schwarzschild’s 1916 solution has no horizons and is static for all
values of r with a timelike naked singularity at r = 0. The radial function R =
[r3 + (2GN Mo )3 ]1/3 has an UV cutoff in R(r = 0) = 2GN Mo .
2. The ”black hole” entropy expression is the same as the Euclideanized EinsteinHilbert action corresponding to the scalar curvature delta function singularity due
to the presence of a mass point at the origin r = 0. Such delta function scalar
curvature singularity can account for the ”black hole” entropy. For this reason a
microscopic theory of a point-mass is needed to understand key aspects of Quantum
Gravity. A point-mass may be stringy in Nature since due to the ultra-violet cutoff
R(r = 0) = 2GN Mo , a point-mass source at r = 0 has non-zero area but zero
volume; a string world-sheet has non-zero area and zero volume.
3. In section 1.2 we showed how the exact Nonperturbative Renormalization Group
flow of the running Newtonian coupling G(r) in Quantum Einstein Gravity [16]
was consistent with the existence of an ultra-violet cutoff R(r = 0) = 2GN Mo
of the Schwarzschild solutions in eq-(1.1), after setting gtt = 1 − 2GN Mo /R(r) =
1 − 2G(r)M (r)/r, ..... We proved that due to the condition G(r = 0) = 0 and
M (r = 0) ∼ 1/2GN Mo , there was no horizon since it is at the location r = 0 that
gtt (r = 0) = 0.
4. Similar results followed in the case of Anti de Sitter-Schwarzschild metrics in section
1.3 with a running cosmological parameter Λ(r) and Newtonian coupling G(r).
The ultra-violet cutoff in this case was no longer given by 2GN Mo but instead by
a real-valued positive root R∗ of the cubic equation associated with the condition
gtt (R(r = 0)) = gtt (R∗ ) = 0. There was a singularity at r = 0.
5. Generalized de Sitter metrics led to an inf rared cuttoff R(r = ∞) = RHubble =
(3/Λo )1/2 in the very late time de Sitter inflationary phase of the evolution of the
universe ( when the Hubble parameter is constant ) and provided a plausible argument why the cosmological constant is not zero and why it is so tiny [10].
6. In section 2 we studied how a running Newtonian coupling G(r) could also be
accommodated naturally in a Jordan-Brans-Dicke scalar-tensor theory of Gravity via
a trivial conformal transformation of the Schwarzschild metric solution. However,
the running Newtonian coupling G(r) = (16πΦ2 )−1 corresponding to the scalar field
Φ could not satisfy the asymptotic freedom condition G(r = 0) = 0 found by [16].
Nevertheless, our results in section 2 exhibited an interesting ultra-violet/infrared
duality behaviour of G(r) that warrants further investigation. A combinatorial
geometry and dual nature of gravity was proposed by [19] using Matroid theory.

18
Quantization in Astrophysics ...

195

To finalize we should stress the search for the foundational (quantum equivalence)
principle of Quantum Gravity which is related to the true origin of inertia (mass/energy).
Mach’s principle is an intriguing concept with several formulations and applications [22],
[24], [25], [26], [27], [23]. A proper and precise implementation of Mach’s principle, beyond
the equivalence’s principle of General Relativity, in modern physics is still lacking, to our
knowledge. Furthermore, it is very likely that our naive notions of Topology break down at
small scales [13] and for this reason we must redefine our notion of a ”point” such that this
novel ”fuzzy” topology is compatible with the stringy geometry. For the role of Fractals
in the construction of a Scale Relativity theory based on scale resolutions of ”points” and
the minimal Planck scale see [15]. A Phase Space Extended Relativity theory involving
an ultra-violet ( minimal scale ) and infrared cutoff ( maximum scale ) in Clifford spaces
has been advanced by [27] based on Max Born [28] Reciprocal principle of Relativity in
Phase spaces where there is a limiting speed and limiting force (acceleration).
Acknowledgments
C.C thanks M. Bowers for hospitality. J. A. N. would like to thank to O. Velarde, L.
Ruiz and J. Silvas for helpful comments. The work of J.A.N was partially supported by
PROFAPI and PIFI 3.2.
Appendix A
Consider the conformal map
0
gµν
= e2λ gµν .

(A.1)

Here, the indices µ, ν run from 0, 1, ..., d − 1. The Christoffel symbols become
µ
µ
0
Γ0µ
αβ (g ) = Γαβ (g) + Σαβ ,

(A.2)

Σµαβ = δαµ λ,β +δβµ λ,α −gαβ λ,µ .

(A.3)

where

Using (A.2) one finds that the Riemann tensor can be written as
µ
µ
µ
σ
σ
0
µ
µ
R0µ
ναβ (g ) = Rναβ (g) + ∇α Σνβ − ∇β Σνa + Σσα Σνβ − Σσβ Σνα

(A.4)

where ∇α denotes covariant derivative in terms of Γµαβ (g). By straightforward computation, using (A.3) we find
µ
µ
0
µ
,µ
,µ
R0µ
ναβ (g ) = Rναβ (g) + {δβ ∇α λ,ν −δα ∇β λ,ν −gνβ ∇α λ + gνα ∇β λ }

(A.5)
+{(δαµ λ,β

−δβµ λ,α

)λ,ν −(δαµ gνβ

−

δβµ gνα )λ,σ

,σ

λ − (gνα λ,β −gνβ λ,α )λ }.

From (A.5) we get the Ricci tensor

19
Quantization in Astrophysics ...

,µ

196

R0νβ (g 0 ) = Rνβ (g) − {(d − 2)∇β λ,ν +gνβ ∇µ λ,µ }
(A.6)
+(d − 2){λ,β λ,ν −gνβ λ,µ λ,µ },
which in turn gives us the scalar curvature
R0 = e−2λ {R − 2(d − 1)∇µ λ,µ − (d − 2)(d − 1)λ,µ λ,µ }.

(A.7)

Therefore we get
√
−g 0 R0 = −ge(d−2)λ {R − 2(d − 1)∇µ λ,µ − (d − 2)(d − 1)λ,µ λ,µ }.
√
Since ∇µ −g = 0, (A.8) can also be written as
q

√

√

−g 0 R0 =

(A.8)

√
−ge(d−2)λ R − ∇µ {( 2(d−1)
) −g(e(d−2)λ ),µ }
d−2

√
+(d − 2)(d − 1) −ge(d−2)λ λ,µ λ,µ .

(A.9)

We observe that the second term is a total derivative and therefore can be dropped. So,
we have
q

−g 0 R0 =

√

−ge(d−2)λ (R + (d − 2)(d − 1)λ,µ λ,µ ).

(A.10)

For d = 4 the expression (A.10) is reduced to
q

−g 0 R0 =

√

−ge2λ (R + 6λ,µ λ,µ ).

(A.11)

Some times it becomes convenient to write eλ = Φ. In this case, we have λ,µ = Φ−1 Φ,µ .
Consequently, we see that (A.11) can also be written as
q

−g 0 R0 =

√

−g(Φ2 R + 6Φ,µ Φ,µ )

(A.13)

−g(Φ2 R + 6∇µ Φ∇µ Φ).

(A.14)

or
q

−g 0 R0 =

√

since ∇µ Φ = Φ,µ .

References
[1] A. Einstein, Sitzungsber Preuss Akad Berlin II, 831 (1915).
[2] K. Schwarzschild, Sitzungsber Preuss Akad Berlin I, 189 ( 1916).
[3] M. Brillouin, Jour. Phys. Rad 23 ( 1923) 43.

20
Quantization in Astrophysics ...

197

[4] D. Hilbert, Nachr. Ges. Wiss Gottingen Math. Phys K1, 53 (1917). H. Weyl, Ann.
Physik 54, 117 (1917) . J. Droste, Proc. Ned. Akad. West Ser A 19, 197 (1917).
[5] L. Abrams, Can. J. of Physics 67, 919 (1989). Physical Review D 20, 2474 (1979).
Physical Review D 21, 2438 (1980). Physical Review D 21, 2941 (1980) .
[6] A. Loinger, On Black Holes and Gravitational Waves ( La Goliardica Pavese, June
2002 )
00

[7] P. Fiziev, Gravitational field of massive point particle in general relativity”,
00
[arXiv.org: gr-qc/0306088]. P. Fiziev and S.V. Dimitrov, Point electric charge in
general relativity” [arXiv.org: hep-th/0406077].
00

[8] S. Antoci, D.E. Liebscher, Reinstating Schwarzschild’s original manifold and its
singularity” [arXiv.org : gr-qc/0406090].
[9] C. Fronsdal, Phys. Rev 116, 778 ( 1959) . M. Kruskal, Phys. Rev 119 1743, (1960).
G. Szekers, Publ. Mat. Debreca 7, 285 (1960).
[10] C.Castro, Mod. Phys Lett A 21 , no. 35 (2006) 2685. ”On Dark Energy, Weyl’s
Geometry, Different Derivations of the Vacuum Energy Density and the Pioneer
Anomaly”, to appear in Foundations of Physics.
[11] J.F. Colombeau, New generalized functions and multiplication of distributions
(North Holland, Amsterdam, 1984). Elementary introduction to generalized functions (North Holland, Amsterdam, 1985). M. Grosser, M. Kunzinger, M. Oberguggenberger and R. Steinbauer, Geometric theory of generalized functions with
applications to relativity, (Kluwer series on Mathematics and its Applications vol.
537, Kluwer, Dordrecht, 2001). J. Heinzke and R. Steinbauer, Jour. Math. Phys 43,
1493 (2002). ” Remarks on the distributional Schwarzschild geometry” [arXiv.org:
gr-qc/0112047].
00

[12] C. Castro and J. A. Nieto, On 2+2 spacetimes, strings and black holes” (submitted
to Int. J. Mod. Phys A , October 2006).
[13] P. S. Aspinwall, ”The Breakdown of Topology at Small Scales” JHEP 0407 (2004)
021 (hep-th/0312188). ”A Point’s Point of View of Stringy Geometry” JHEP 0301
(2003) 002 ( hep-th/0203111).
[14] E. Witten, ”On black holes in string theory” Lecture given at Strings ’91 Conf.,
Stony Brook, N.Y., Jun 1991, Published in Strings: Stony Brook 1991, 0184-192
(QCD161:S711:1991); hep-th/9111052 I. Bars, Lecture at Strings 91, Stonybrook,
June 1991. E. Witten, Phys. Rev. D 44, 314 (1991).
[15] L. Nottale, Fractal Spacetime and Microphysics : Towards Scale Relativity ( World
Scientific, Singapore, 1992 )

21
Quantization in Astrophysics ...

198

[16] A. Bonanno and M. Reuter, ”Renormalization group improved black hole spacetime” [arXiv.org : hep-th/0002196]. M. Reuter and J.M. Schwindt, ”A Minimal
Length from Cutoff Modes in Asymptotically Safe Quantum Gravity” [arXiv.org
: hep-th/0511021]. M. Reuter and J.M. Schwindt, ” Scale-dependent structures
and causal structures in Quantum Einstein Gravity ” [arXiv.org:hep-th/0611294].
A.Bonanno, M.Reuter ” Spacetime Structure of an Evaporating Black Hole in Quantum Gravity ” hep-th/0602159, Phs. Rev. D 73 (2006) 0830005.
[17] H. Emoto, ”Asymptotic safety of quantum gravity and improved spacetime of black
hole singularity by cutoff identification” hep-th/0511075
[18] R.G. Cai and N. Ohta, ” Black Holes in Pure Lovelock Gravities” hep-th/0604088.
Phys.Rev. D 74 (2006) 064001. J. Matyjasek, M. Telecka and D. Tryniecki, ”Higher
dimensional black holes with a generalized gravitational action” hep-th/0606254
Phys.Rev. D73 (2006) 124016.
[19] J.A. Nieto, ”Matroids and P-branes”, Adv. Theor. Math. Phys. 8, 177 (2004); hepth/0310071. J.A. Nieto and M.C. Marn, ”Search for a ”gravitoid” theory”, Int. J.
Mod. Phys. A18, (2003) 5261; hep-th/0302193.
[20] S. Vacaru, Phys. Letts B 498 (2001) 74. Jour. Math Phys 46 (2005) 042503. Jour.
Math Phys 46 (2005) 032901. Jour. Math Phys 47 (2006) 093504. S. Vacaru,
P. Stavrinos, E. Gaburov, D. Gonta,Clifford and Riemann-Finsler Structures in
Geometric Mechanics and Gravity. Geometry Balkan Press, Monograph 7 (2006)
693 pages.
[21] J. Wess, ”Eisntein-Rieman Gravity on Deformed Spaces” hep-th/0611025.
[22] H. Bondi and J. Samuel, ”The Lense-Thirring Effect and Mach’s Principle” grqc/9607009
[23] F.Wilczek, ”Total Relativity”, Physics Today vol. 57.
[24] H. Lichtenegger and B. Mashhoon, ”Mach’s Principle” physics/0407078
[25] D.F.Roscoe ”Clock and rods- or something more fundamental?” physics/0107044.
[26] V.M.Logunov, ”Mach’s principle and cosmology term” astro-ph/0210013 ”
[27] C. Castro, Foundations of Physics, 35, no. 6 (2005) 971-1041. ” On dual phasespace relativity, the Machian principle and modified newtonian dynamics, Progress
in Physics, 1 (2006) 20-30.
[28] M. Born, Proc. Royal Society A 165 (1938) 291.
[29] L. Colatto, A. Penza and W. Santos, ” Noncommutative geometry induced by spin
effects” [arXiv.org : hep-th/0512266]. F. Nasseri and S. Alavi, ” Schwarzschild black
hole in Noncommuttaive spacetime” [arXiv.org: hep-th/0508051.
22
Quantization in Astrophysics ...

199

[30] A. Lewis, ” Coulomb potential of a point mass in theta noncommutative geometry
” [arXiv.org:hep-th/0605140].

23
Quantization in Astrophysics ...

200

1

Annales de la Fondation Louis de Broglie, Volume 31, no 1, 2006

On the origin of macroquantization in astrophysics and
celestial motion
V. CHRISTIANTO a
a

email: vxianto@yahoo.com, http://reachme.at/coolbit

Reprinted with kind permission from editor of
Annales de la Fondation Louis de Broglie
ABSTRACT. Despite the use of Bohr radius formula to predict celestial
quantization has led to numerous verified observations, the cosmological
origin of this macroquantization remains an open question. In this article
various plausible approaches are discussed. Further observation to verify or
refute this proposition is recommended, in particular for exoplanets.
RÉSUMÉ: En dépit de l'utilisation de la formule de rayon de Bohr de
prévoir la quantification céleste a mené aux nombreuses observations vérifiées, l'origine cosmologique de ce macroquantization est une question en
suspens. En cet article de diverses approches plausibles sont discutées.
Promouvez l'observation pour vérifier ou réfuter cette proposition est recommandée, en particulier pour des exoplanets.

1

Introduction

It is known that the use of Bohr radius formula [1] to predict celestial
quantization has led to numerous verified observations [2][3]. This approach
was based on Bohr-Sommerfeld quantization rules [4][5]. Some implications
of this quantum-like approach include exoplanets prediction, which has
become a rapidly developing subject in recent years [6][7]. While this kind
of approach is not widely accepted yet, this could be related to a recent suggestion to reconsider Sommerfeld’s conjectures in Quantum Mechanics [8].
While this notion of macroquantization seems making sense at least in the
formation era of such celestial objects, i.e. “all structures in the Universe,
from superclusters to planets, had a quantum mechanical origin in its
earliest moments” [9], a question arises as to how to describe the physical
origin of wave mechanics of such large-scale structures [5].

URL: http://www.ensmp.fr/aflb/AFLB-311/aflb311m370.pdf

Quantization in Astrophysics ...

202

2

V. Christianto, vxianto@yahoo.com

A plausible definition of the problem of quantization has been given by
Grigorescu [10]: “select an infinite, discrete number of quantum possible
real motions, from the continuous manifold of all mechanically possible
motions.” While this quantization method has been generally acceptable to
describe physical objects at molecular scale, there is not much agreement
why shall we also invoke the same notion to describe macrophenomena,
such as celestial orbits. Nonetheless, there are plenty efforts in the literature
in attempt to predict planetary orbits in terms of wave mechanics, including
a generalisation of Keplerian classical orbits [11].
In this article we discuss some plausible approaches available in the
literature to describe such macroquantization in astrophysics, in particular to
predict celestial motion:
a. Bohr-Sommerfeld’s conjecture;
b. Macroquantum condensate, superfluid vortices;
c. Cosmic turbulence and logarithmic-type interaction;
d. Topological geometrodynamics (TGD) approach.
While these arguments could be expected to make the notion of macroquantization a bit reasonable, it is beyond the scope of this article to conclude which of the above arguments is the most consistent with the observed
data. There is perhaps some linkage between all of these plausible arguments. It is therefore recommended to conduct further research to measure
the reliability of these arguments, which seems to be worthwhile in our attempt to construct more precise cosmological theories.
2

Bohr-Sommerfeld’s quantization rules

In an attempt to describe atomic orbits of electron, Bohr proposed a conjecture of quantization of orbits using analogy with planetary motion. From
this viewpoint, the notion of macroquantization could be considered as returning Bohr’s argument back to the celestial orbits. In the meantime it is not
so obvious from literature why Bohr himself was so convinced with this idea
of planetary quantization [12], despite such a conviction could be brought
back to Titius-Bode law, which suggests that celestial orbits can be described using simple series. In fact, Titius-Bode were also not the first one
who proposed this kind of simple series [13], Gregory-Bonnet started it in
1702.
In order to obtain planetary orbit prediction from this hypothesis we could
begin with the Bohr-Sommerfeld’s conjecture of quantization of angular
momentum. As we know, for the wavefunction to be well defined and

Quantization in Astrophysics ...

203

On the origin of macroquantization in astrophysics and celestial motion

3

unique, the momenta must satisfy Bohr-Sommerfeld’s quantization condition [14]:

∫ p.dx = 2π .nh

(1)

Γ

for any closed classical orbit Γ. For the free particle of unit mass on the unit
sphere the left-hand side is
T

∫ v .dτ = ω
2

2

.T = 2π .ω

(2)

0

where T=2π/ω is the period of the orbit. Hence the quantization rule
amounts to quantization of the rotation frequency (the angular momentum): ω = nh . Then we can write the force balance relation of Newton’s
equation of motion:

GMm / r 2 = mv 2 / r

(3)
Using Bohr-Sommerfeld’s hypothesis of quantization of angular momentum (2), a new constant g was introduced:
(4)
mvr = ng / 2π
Just like in the elementary Bohr theory (before Schrödinger), this pair of
equations yields a known simple solution for the orbit radius for any quantum number of the form:

r = n 2 .g 2 /( 4π 2 .GM .m 2 )

(5)

or

r = n 2 .GM / vo2

(6)
where r, n, G, M, vo represents orbit radii (semimajor axes), quantum number (n=1,2,3,…), Newton gravitation constant, and mass of the nucleus of
orbit, and specific velocity, respectively. In this equation (6), we denote
vo = (2π / g ).GMm
(7)
The value of m is an adjustable parameter (similar to g).
Nottale [1] extends further this Bohr-Sommerfeld quantization conjecture
to a gravitational-Schrödinger equation by arguing that the equation of motion for celestial bodies could be expressed in terms of a scale-relativistic
Euler-Newton equation. For a Kepler potential and in the time independent
case, this equation reads (in Ref [1c] p. 380):

2 D 2 ∆Ψ + ( E / m + GM / r ).Ψ = 0

(8)
Solving this equation, he obtained that planetary orbits are quantized
according to the law:

Quantization in Astrophysics ...

204

4

V. Christianto, vxianto@yahoo.com

an = GMn 2 / vo

2

(9)
where an,G,M,n,vo each represents orbit radius for given n, Newton gravitation constant, mass of the Sun, quantum number, and specific velocity
(vo=144 km/sec for Solar system and also exoplanet systems), respectively.
These equations (8)-(9) form the basis of Nottale’s Scale Relativity prediction of planetary orbits [1]; and equation (9) corresponds exactly with equation (6) because both were derived using the same Bohr-Sommerfeld’s quantization conjecture. Another known type of observed quantization in astronomy is Tifft’s 72 km/sec quantization [13].
3

Macroquantum condensate, superfluid vortices

Provided the above Bohr-Sommerfeld description of macroquantization
corresponds to the facts, then we could ask further what kind of physical
object could cause such orbital quantization. Thereafter we could come to
the macroquantum condensate argument. In this regard, astrophysical objects
could be seen as results of vacuum condensation [15][16]. For instance Ilyanok & Timoshenko [17] took a further step by hypothesizing that the universe resembles a large Bose Einstein condensate, so that the distribution of
all celestial bodies must also be quantized. This conjecture may originate
from the fact that according to BCS theory, superconductivity can exhibit
macroquantum phenomena [18]. There is also a known suggestion that the
vacua consist of hypercrystalline: classical spacetime coordinate and fields
are parameters of coherent states [19].
It is perhaps interesting to remark here that Ilyanok & Timoshenko do not
invoke argument of non-differentiability of spacetime, as Nottale did [1]. In
a macroquantum condensate context, this approach appears reasonable because Bose-Einstein condensate with Hausdorff dimension DH~2 could exhibit fractality [20], implying that non-differentiability of spacetime conjecture is not required. The same fractality property has been observed in various phenomena in astrophysics [21], which in turn may also correspond to
an explanation of the origin of multifractal spectrum as described by Gorski
[22]. In this regard, Antoniadis et al. have discussed CMBR temperature
(2.73o K) from the viewpoint of conformal invariance [23], which argument
then could be related to Winterberg’s hypothesis of superfluid Planckian
phonon-roton aether [24].
Based on previous known analogy and recent research suggesting that
there is neat linkage between gravitation and condensed matter physics
[25][26], we could also hypothesize that planetary quantization is related to
quantized vortex. In principle, this hypothesis starts with observation that in

Quantization in Astrophysics ...

205

On the origin of macroquantization in astrophysics and celestial motion

5

quantum fluid systems like superfluidity, it is known that such vortexes are
subject to quantization condition of integer multiples of 2π, or

∫ v .dl = 2π .nh / m
s

4.

Furthermore, such quantized vortexes are distributed

in equal distance, which phenomenon is known as vorticity [4]. In large
superfluid system, usually we use Landau two-fluid model, with normal and
superfluid component. The normal fluid component always possesses some
non-vanishing amount of viscosity and mutual friction. Similar approach
with this proposed model has been considered in the context of neutron stars
[27], and this quantized vortex model could also be related to Wolter’s vortex [28].
4

Cosmic turbulence and logarithmic type interaction

Another plausible approach to explain the origin of quantization in astronomy is using turbulence framework. Turbulence is observed in various
astrophysical phenomena [21], and it is known that such turbulence could
exhibit a kind of self-organization, including quantization.
Despite such known relations, explanation of how turbulence could exhibit orbital quantization is not yet clear. If and only if we can describe such
a flow using Navier-Stokes equation [29], then we can use R.M. Kiehn’s
suggestion that there is exact mapping from Schrödinger equation to NavierStokes equation, using the notion of quantum vorticity [30]. But for fluid
which cannot be described using Navier-Stokes equation, such exact mapping would not be applicable anymore. In fact, according to Kiehn the Kolmogorov theory of turbulence is based on assumption that the turbulent state
consists of “vortices” of all “scales” with random intensities, but it is not
based on Navier-Stokes equation explicitly, in fact “the creation of the turbulent state must involve discontinuous solutions of Navier-Stokes equations.” [31] However, there is article suggesting that under certain conditions, solutions of 3D Navier-Stokes equation could exhibit characteristic
known as Kolmogorov length [32]. In this kind of hydrodynamics approach,
macroquantization could be obtained from solution of diffusion equation
[33].
In order to make this reasoning of turbulence in astrophysics more consistent
with the known analogy between superfluidity and cosmology phenomena [26],
we could also consider turbulence effect in quantum liquid. Therefore it seems
reasonable to consider superfluid turbulence hypothesis, as proposed for instance
by Kaivarainen [34]. There are also known relations such as discrete scale invariant turbulence [35], superstatistics for turbulence [36], and conformal turbu-

Quantization in Astrophysics ...

206

6

V. Christianto, vxianto@yahoo.com

lence. Furthermore, such a turbulence hypothesis could lead to logarithmic interaction similar to Kolmogorov-type interaction across all scales [28].
Another way to put such statistical considerations into quantum mechanical framework is perhaps using Boltzmann kinetic gas approach. It is known
that quantum mechanics era began during Halle conference in 1891, when
Boltzmann made a remark: “I see no reason why energy shouldn’t also be
regarded as divided atomically.” Due to this reason Planck subsequently
called the quantity 2πh after Boltzmann – ‘Boltzmann constant.’ Using the
same logic, Mishinov et al. [37] have derived Newton equation from TDGL:
(10)
m * d tV p (t ) = e * .E − m *V p (t ) / τ p
This TDGL (time-dependent Ginzburg-Landau) equation is an adequate
tool to represent the low-frequency fluctuations near Tc, and it can be considered as more universal than GPE (Gross-Pitaevskii equation).
5

TGD viewpoint on the origin of macroquantization in astrophysics
and celestial motion

Topological geometrodynamics (TGD) viewpoint on this macroquantization subject [38] was based on recognition that this effect could be
considered as simple substitution of Planck constant:
h → h gr = GMm / v0
(11)
provided we assert that h = c = 1 . The motivation is the earlier proposal
inspired by TGD [39] that the Planck constant is dynamical and quantized.
−4

As before vo=144.7+0.7 km/sec, giving v0 / c = 4.82 x10 km / sec . This
value is rather near to the peak orbital velocity of stars in galactic halos. As a
sidenote, this is not the only plausible approach to make extension from
geometrodynamics to Planck scale, and vice versa [41].
A distinction of TGD viewpoint [42] from Nottale’s fractal hydrodynamics approach is that many-sheeted spacetime suggests that astrophysical
systems are not only quantum systems at larger space-time sheets but correspond to a gigantic value of gravitational Planck constant. The Bohr’s rules
for the visible matter reflect the quantum dynamics of the dark matter at
larger space-time sheets. Furthermore, TGD predicts the value of the parameter vo appearing in equation (9) and explains its harmonic and subharmonics. There is also a plausible linkage between hydrodynamics approach
and Kähler structure to describe the Schrödinger equation [43].

Quantization in Astrophysics ...

207

On the origin of macroquantization in astrophysics and celestial motion

7

5.1. Consistency with TGD based model of galactic dark matter
The first step is to see whether the TGD based model for dark matter is
consistent with the gravitational Schrödinger equation. The following argument was based on Bohr quantization rules [41].
a. The gravitational potential energy V(r) for a mass distribution
M(r)=xTr (T denotes string tension) is given by:
Ro

V (r ) = Gm ∫ M (r ).dr / r 2 = GmxT log(r / Ro )
r

b.

(12)

Here Ro corresponds to a large radius so that the potential is negative,
as it should in the region where binding energy is negative.
The Newton equation for circular orbit:

mv 2 / r = GmxT / r

(13)

which gives
c.

(14)
v = xGT
Bohr quantization condition for angular momentum by equation (11)
reads as
(15)
mvr = nh gr
and gives:

d.

rn = nh gr /(mv) = n.r1

(16)

r1 = GM /(vvo )

(17)

where v is rather near to vo.
Bound state energies are given by

En = mv 2 / 2 − xT log(r1 / Ro ) + xT log(n)

(18)

The energies depend only weakly on the radius of the orbit.
e.

2

The centrifugal potential l (l + 1) / r in the Schrödinger equation is
negligible as compared to the potential term at large distances so that
one expects that degeneracies of orbits with small values of l do not
depend on the radius.

5.2. TGD based model of planetary system
The magnetic flux quanta (shells and flux tubes) are the carriers of the
quantum coherent dark matter and behave effectively like quantum rigid
bodies. This leads to a simple model for the generation of planetary system
via a breaking of rotational symmetry. For inner planets this process leads
from spherical shells with a full rotational symmetry to flux tubes with reduced rotational symmetry inside with planet are eventually formed. Earth

Quantization in Astrophysics ...

208

8

V. Christianto, vxianto@yahoo.com

and outer planets were formed by a splitting of a flattened flux tube in the
common orbital plane to 5 flux tubes corresponding to Earth and outer planets except Pluto, which indeed has orbital parameters differing dramatically
from those of other planets. The replacement of vo by its subharmonic vo/5
for these Jovian planets corresponds topologically to the splitting of a magnetic flux tube to five separate tubes.
Flux tubes and spherical cells containing quantum dark matter are predicted to be still there. The amazing finding is that the quantum time scales
associated with Bohr orbits seem to correspond to important biological time
scales. For instance, the time scale
T = h gr / E
(19)
associated with n=1 orbit is precisely 24 hours. This apparently supports the
prediction of TGD based theory of living matter in with quantum coherent
dark matter plays a fundamental role [40].
The inclinations of planetary orbits could be a test problem for the hypothesis outlined above. The prediction is not merely statistical like the
predictions given by Nottale and others [1d][1e]. The minimal value of inclination for a given principal quantum number n follows from semiclassical
view about angular momentum quantization for maximal value of zcomponent of angular momentum m=j=n [38]:

cos(φ ) = n / n(n + 1)

(20)

where φ is the angle between angular momentum and quantization axis and
thus also between orbital plane and (x,y)-plane. This angle defines the tilt
angle between the orbital plane and (x,y)-plane. For n=3,4,5 (Mercury,
o

o

o

Earth, Venus) this equation gives φ = 30.0 ,26.6 ,24.0 respectively.
Only the relative tilt angle can be compared with the experimental data.
Taking Earth’s orbital plane as reference will give ‘inclination’ angle, i.e. 6
degrees for Mercury, and 2.6 degrees for Venus. The observed values are 7.0
and 3.4 degrees, respectively, which are in good agreement with prediction.
Bohr-Sommerfeld rules allow also estimating eccentricities and the prediction is [38]:

e 2 = 2.( 1 − m 2 / n 2 ) /(1 + 1 − m 2 / n 2 )

(21)
The eccentricities are predicted to be very large for m<n unless n is very
large and the only possible interpretation is that planets correspond in the lowest
order approximation to m=n and e=0 whereas comets with large eccentricities
could correspond to m<n orbits. In particular, for m<n comets in Oort Clouds

Quantization in Astrophysics ...

209

On the origin of macroquantization in astrophysics and celestial motion

9

(n<700) the prediction is e>0.32. This could be a good test problem for further
astronomical observation.

Concluding remarks
In this article, some plausible approaches to describe the origin of macroquantization in astrophysics and also celestial motion are discussed. While
all of these arguments are interesting, it seems that further research is required to verify which arguments are the most plausible, corresponding to
the observed astrophysics data.
After all, the present article is not intended to rule out the existing methods in the literature to predict quantization of celestial motion, but instead to
argue that perhaps this macroquantization effect in various astronomy phenomena requires a new kind of theory to describe its origin.
Acknowledgement
Special thanks go to Prof. M. Pitkänen for insightful discussions in particular on his TGD theory, and also for reading the draft version and suggesting improvement. The writer also expresses his sincere thanks to Profs. C.
Castro, R.M. Kiehn, A. Rubćić, and E. Scholz for discussions and various
suggestions during development of ideas presented herein. Kindful translation of the abstract by Prof. Ezzat G. Bakhoum is gratefully appreciated.

References
[1] Nottale, L., G. Schumacher, & E.T. Levefre, Astron. Astrophys. 361, 379-387
(2000); [1b] Nottale, L., “Non-differentiable space-time and scale relativity,”
in Proc. Inter. Colloquium Géométrie au XXè siècle, Paris, 24-29 Sept. 2001,
Ed. D. Flament; [1c] Nottale, L., Astron. Astrophys. 327, 867-889 (1997); [1d]
Nottale, L., Chaos, Solitons and Fractals 7, 877-938 (1996) in particular
equation (156); [1e] Galopeau, P.H.M., et al., Geophysical Research Abstract 5,
11864 (2003); [1f] Da Rocha, D., & L. Nottale, arXiv:astro-ph/0310036, astroph/0310031, preprint at http://www.daec.obspm.fr/users/nottale
[2] Rubćić, A., & J. Rubćić, “The quantization of solar-like gravitational systems,”
Fizika B 7 Vol. 1, 1-13 (1998).
[3] Agnese, A.G., & R. Festa, Proc. Workshop on Modern Modified Theories of
Gravitation and Cosmology 1997, preprint at arXiv:astro-ph/9807186 (1998).
Also Agnese, A.G., astro-ph/9910534; Neto, M., et al., arXiv:astro-ph/0205379.

Quantization in Astrophysics ...

210

10

V. Christianto, vxianto@yahoo.com

[4] Chechelnitsky, A., “Hot points of the Wave Universe concept,” in JINR,

[5]
[6]
[7]

[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]

[18]
[19]
[20]
[21]
[22]

Dubna, Aug. 22-26, 2000; preprint at arXiv:physics/0102036 (2001); [4b]
Chechelnitsky., A., “Epoch of quantization and Wave Universe,” (1995)
http://web.ccr.jussieu.fr.
Coles, P., arXiv:astro-ph/0209576 (2002); [5b] Chavanis, P., arXiv:astroph/9912087 (1999).
Armitage, P.J., et al., Mon.Not.R. Astron. Society, (2002); preprint at
arXiv:astro-ph/0204001.
Lineweaver, C. et al., Scientific Frontiers in Research on Extrasolar Planets,
ASP Conf. Series Vol. 28, Deming et al. (eds), (2003) preprint at arXiv:astroph/0209382.
Oudet, X., “The quantum state and the doublets,” Annales de la Fondation
Louis de Broglie Vol. 25 No.1, 24p (2000).
Sakharov, A.S., & H. Hofer, “Development of the universe and new
cosmology,” CERN-TH/2003-101, arXiv:astro-ph/0309026 (2003).
Grigorescu, M., “Physical framework of quantization problem,”
arXiv:physics/0306079 (2003).
Daboul, J., & M.M. Nieto, “Exact E=0 solutions for general power law
potentials. I. Classical orbits,” arXiv:hep-th/9408057 (1994).
Kleppner, D., & R. Jackiw, Science Vol. 289, No. 5481, Issue of 11 (Aug
2000), pp. 893-898.
Trimble, V., “Astrophysics faces the Millennium part III: The ratios of small
whole numbers. Misadventures in astronomical quantization,” (2000).
Van Holten, J., arXiv:gr-qc/0107041 (2001).
Chapline, G., arXiv:hep-th/9812129 (1998).
Chapline, G., et al., arXiv:gr-qc/0012904 (2001), gr-qc/0407033.
Ilyanok, A., & I.A. Timoshenko, “Quantization of masses in the solar system,”
arXiv: astro-ph/0201057(2002). Also astro-ph/9912537 (1999), astroph/0111182, astro-ph/0111183.
Schrieffer, J.R., “Macroscopic quantum phenomena from pairing in
superconductors,” Lecture, December 11th (1972).
Finkelstein, D.R., et al., “Hypercrystalline vacua,” arXiv:quant-ph/9608024
(1996).
Kolomeisky, E.B., et al., arXiv:cond-mat/0002282 (2000).
Combes, F., arXiv:astro-ph/9906477 (1999). Also Chappell, D., & J. Scallo,
astro-ph/9707102; Baryshev, Y.V., astro-ph/9912074.
Gorski, A.Z., arXiv:chao-dyn/9804034 (1998).

Quantization in Astrophysics ...

211

On the origin of macroquantization in astrophysics and celestial motion

11

[23] Antoniadis, I., P. Mazur, & E. Mottola, “Conformal invariance and Cosmic

[24]

[25]

[26]

[27]
[28]
[29]

[30]
[31]
[32]
[33]
[34]

[35]

[36]
[37]

Background Radiation,” Phys.Rev.Lett. 79 (1997) 14-17; preprint at arXiv:
astro-ph/9611208. Also E. Scholz, arXiv:astro-ph/0403446.
Winterberg, F., “Planck mass rotons as cold dark matter and quintessence,”
presented at the 9th Canadian Conf. on General Relativity and Relativistic
Astrophysics, Edmonton, May 24-26th (2001); also in Z. Naturforschung 57a
(2002) 202-204.
Zurek, W., “Cosmological experiments in superfluids and superconductors,” in
Proc. Euroconference Formation and Interaction of Topological Defects, A.
Davis & R. Brandenberger (eds.) Plenum (1995); preprint at arXiv:condmat/9502119. [25b] Barcelo, C., et al., “Analogue gravity from Bose-Einstein
condensate,” Class. Quantum Grav. 18, 1137-1156 (2001). [25c] D.L.
Kovrizhin & L.A. Maksimov, arXiv:cond-mat/0109236.
Volovik, G., “Superfluid analogies of cosmological phenomena,” arXiv:grqc/0005091 (2000); [26b] Volovik, G., “Links between gravity and dynamics of
quantum liquids,” in Proc. Inter. Conf. “Cosmology, Relativistic Astrophysics,
Cosmoparticle Physics” (COSMION-99), preprint at arXiv:gr-qc/0004049;
[26c] Volovik, G., arXiv:gr-qc/0104046; [26d] Nozieres, P., & D. Pines, The
theory of quantum liquids: Superfluid Bose Liquid. Wesley Publ. Co. Inc., 116124 (1990).
Sedrakian, A., et al., arXiv:astro-ph/9801188 (1998).
Rosu, H., arXiv:quant-ph/9506015 (1995).
Gibson, C., “Turbulent mixing, diffusion and gravity in the formation of
cosmological structures,” Proc. FEDSM99, July 18-23, 1999, San Fransisco
(1999). Also Gibson, C., & R. Schild, preprint for The Astronomical Journal,
arXiv:astro-ph/0304483 (2003).
Kiehn, R.M., “An interpretation of wave-function as a measure of vorticity,”
http://www22.pair.com/csdc/pdf/cologne.pdf (1989).
Kiehn, R.M., “Topology and turbulence,” arXiv:physics/0102003 (2001).
Grassi, V., et al., arXiv:math-ph/9912008 (1999).
Castro, C., J. Mahecha, & B. Rodriguez, arXiv:quant-ph/0202026v1 (2002).
Kaivarainen, A., “Hierarchic models of turbulence, superfluidity and
superconductivity,”
arXiv:physics/0003108
(2000).
Also
in
http://www.karelia.ru/~alexk
Sornette, D., “Discrete scale invariance in turbulence?” Proc. 7th European
Turbulence Conf. (ETC-7), June 1998, Kluwer, U. Frisch (ed.), preprint at
arXiv:cond-mat/9802121 (1998).
Beck, C., arXiv:physics/0303061(2003).
Mishonov, T.M., et al., “Kinetics and Boltzmann kinetic equation for
fluctuation Cooper pairs,” arXiv:cond-mat/0302046 (2003).

Quantization in Astrophysics ...

212

12

V. Christianto, vxianto@yahoo.com

[38] Pitkänen, M., http://www.physics.helsinki.fi/~matpitka/articles/nottale.pdf
[39] Pitkänen, M., "Equivalence of Loop Diagrams with Tree Diagrams and

[40]
[41]
[42]
[43]

Cancellation of Infinities in Quantum TGD" of "Topological
Geometrodynamics", www.physics.helsinki.fi/~matpitka/tgd.html#bialgebra
Pitkänen,
M.,
"Time,
Space-Time,
and
Consciousness",
http://www.physics.helsinki.fi/~matpitka/cbookII.html#time
Spaans, M., arXiv:gr-qc/9612027 (1996). Also M. Blasone, P. Jizba, G.
Vitiello, arXiv:math-ph/0402034 (2004).
Pitkänen, M., “Topological Geometrodynamics I, II,” Chaos, Solitons, Fractals
13, no. 6 (2002) 1205, 1217.
Reginatto, M., arXiv:quant-ph/9909065 (1999).

First version: Oct 8th 2004, 1st revision: Oct 13th 2004, 2nd revision: Jan
18 2005.
th

Quantization in Astrophysics ...

213

Apeiron, Vol. 10, No. 3, July 2003

231

The Cantorian Superfluid
Vortex Hypothesis
V. Christianto, vxianto@yahoo.com
Reprinted with kind permission from Apeiron editor

The present article suggests a preliminary version of Cantorian
superfluid vortex hypothesis as a plausible model of nonlinear cosmology. Using the proposed model we explain the
physical origin of quantum-like approach to describe planetary
orbits as proposed in the recent literature. The meaning of the
Cantorian superfluid vortex hypothesis is discussed,
particularly in the context of offering a plausible mechanism
of gravitation-related phenomena from boson condensation.
Some advantages and unsolved questions are discussed.
Keywords: superfluid aether, Bose-Einstein condensate, phion,
multiple vortices, gravitational instability.

Introduction
In recent years, there has been a growing interest in the quantum
approach to describing orbits of celestial bodies. While this approach
has not been widely accepted, the motivating idea of this approach
was Bohr-Sommerfeld’s hypothesis of quantization of angular
momentum, and therefore it shows some resemblance to the
Schrödinger wave equation (Chavanis 1999, Nottale 1996, Neto et al.
2002). The application of wave mechanics to large-scale structures
(Coles 2002) has led to impressive results in terms of prediction of
© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

214

Apeiron, Vol. 10, No. 3, July 2003

232

planetary semimajor axes, especially orbits of exoplanets (Nottale et
al. 1997, 2000). However, a question arises as to how to describe the
physical origin of wave mechanics of such large-scale structures. This
leads to the Volovik-Winterberg hypothesis of the superfluid phononroton as a quantum vacuum aether (Volovik 2001, Winterberg 2002a,
2002b).
To extend the superfluid aether hypothesis further in order to
explain nonlinear phenomena in cosmology, we propose a new
Cantorian Superfluid Vortex (CSV) hypothesis. The present article
discusses some questions related to this hypothesis, including:
a. What is the meaning of Cantorian Superfluid Vortex?
b. Why do we require this model?
c. How can we represent various high-temperature phenomena
in cosmology using low-temperature superfluid physics?
d. What are its advantages and implications compared to present
theories?
e. What are the unsolved questions and possible future research?
We begin with question b, in particular with reference to reconciling
Quantum Mechanics and GTR. Further discussion of the proposed
hypothesis will be reserved for a forthcoming article.

QM, GTR, QED, Sachs
For almost eight decades theoretical physicists have toiled to
reconcile Quantum Mechanics and Einstein’s (General) Theory of
Relativity, beginning with Dirac, and continuing with leading
scientists up to this time. As a result, several different approaches
are taken by theoretical physicists today, including such theories
as:
• QED & QFT: these can be considered as two of the best
experimentally confirmed theories up to this day. For an
© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

215

Apeiron, Vol. 10, No. 3, July 2003

233

introduction, see for example Weinberg (1993, 1997) and
Siegel (1999).
• Sachs’s theory: in principle Sachs has attempted to bring the
four-dimensional geometrical world into QM.1
• Other refinements of GTR such as Weyl’s (conformal
gravity) solution, etc.
• Various versions of string theories: supergravity, superstring,
supersymmetry, brane universe, etc.
• One lesser known approach is the diametrical opposite of
Sachs’s approach: it claims that quantum (wave) mechanics
theory is sufficient to explain the phenomena corresponding
to GTR (Coles 2002).
A major obstacle here is how to reconcile the four-dimensional
geometrisation of GTR with common three-dimensional QM. As is
well known, GTR was constructed as a geometrification of physical
reality: GTR’s attempt to describe gravity is purely geometric and
macroscopic. As such, there are some known limitations in GTR,2
including:
a. Classical general relativity by itself is unable to
predict the sign of the gravitational force (attraction
rather than repulsion). Consoli (2000) also noted:
“Einstein had to start from the peculiar properties of
Newtonian gravity to get the basic idea of
transforming the classical effects of this type of
interaction into a metric structure.” In other words, it
seems that GTR is not the complete theory Einstein
was looking for.
b. There is no mechanism for gravitational forces: the
‘graviton’ has never been observed.

© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

216

Apeiron, Vol. 10, No. 3, July 2003

234

c. There is no convincing mechanism to describe the
interaction between matter, inertia, and space (Mach
principle is merely postulated).
d. There is no description of the medium of space.
Although Einstein apparently considered a perfect
fluid to describe this medium in his Leiden lecture in
1921 (Einstein 1921), he never attempted to theorize
this medium formally—perhaps for good reason.3
e. It is quite difficult to imagine how matter can affect
the spacetime curvature and vice versa as postulated
by GTR (for instance H. Arp).
f. Using GTR it is also quite difficult to explain the socalled ‘hidden matter’ which is supposed to exist in
order to get average density of matter in the universe
that required for flat universe, Ω=1 (Chapline 1998).
Alternatively some theorists have shown we can
reconcile this issue using Navier-Stokes model
(Gibson 1999).
g. The spacetime curvature hypothesis cannot explain
phenomena in the micro world of Quantum
Mechanics. In contrast, by the Ehrenfest theorem,
Quantum Mechanics reduces to classical physics if we
use classical parameters consistently (see also Signell
2002).
However, we should recognize that the strong point of GTR is to
generalize the Maxwell equations to the gravity field and to introduce
the equivalence principle, as observed by recent experiments.
Therefore according to Consoli (2000): “all classical experimental
tests of general relativity would be fulfilled in any theory
incorporating the Equivalence Principle.” We should also note that
Einstein was quite right in pointing out the incompleteness of QM (as
© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

217

Apeiron, Vol. 10, No. 3, July 2003

235

described by the Copenhagen school). Therefore, we would expect to
find a reformulation of QM, which is capable of describing known
phenomena in support of GTR, such as the bending of light rays,
clock delay due to the gravitational field and also the precession of the
perihelion of planet Mercury. Attempts to generalise (QM) wave
mechanics to describe the motion and distribution of celestial objects
have been made, for instance by Coles (2002), Neto et al. (2002),
Nottale et al. (1997, 2000) and Zakir (1999).
Therefore, we may conclude the following: to reconcile GTR
(phenomena) and QM, we have to begin by finding the mechanism of
gravitation and its interaction with the medium of space. This leads us
to the scalar field hypothesis as discussed below.

Whittaker, scalar field, phion condensate
The scalar field hypothesis as a description of gravitation is not a
recent idea at all. Whittaker, a leading physicist and mathematician in
his time, originated the idea of a (longitudinal) scalar field while
studying the nature of partial differential equations.4 To quote
Whittaker:
…the gravitational force in each constituent field will be
perpendicular to the wave-front: the waves will be
longitudinal… this undulatory theory of gravity would
require gravity should be propagated with a finite
velocity, which however need not be the same as of light,
and may be enormously greater.
Whittaker’s student, Dirac, upon reading Whittaker’s idea, then
came up with his idea of the ‘electron sea’, though this was later
found to be at odds with observation. Therefore the scalar field must
be closely linked to the medium of space (aether, or its modern
© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

218

Apeiron, Vol. 10, No. 3, July 2003

236

version ‘quantum vacuum fluctuation’; see Chapline 1998, Rothwarf
1998). In Whittaker’s formulation, one of the features of this scalar
field is that its speed is much higher than the speed of light c. This
hypothesis is recently supported by Van Flandern’s theory on the
‘speed of gravity’.5
Now if we accept that a scalar field can describe the mechanism of
gravitation, the question then arises: what is the physical nature of this
scalar field. Some physicists have argued that gravitation is actually a
long-wavelength excitation of a scalar condensate inducing
spontaneous symmetry breaking (Consoli 2000, 2002). This scalar
field is represented by the ‘phion condensate’. In this sense, the Mach
principle represents an inextricable linkage between inertia and
gravity due to the common origin of the phenomena: condensation of
the scalar field.6
We now come to the core hypothesis of CSV theory: the ‘phion
condensate’ can be modeled by zero temperature superfluid physics
(Consoli 2000). Therefore, we treat the ‘superfluid’ as the quantum
vacuum aether medium (as proposed by Winterberg 2002a, 2002b).
In this way, we are no longer considering superfluidity merely as a
useful analogy to describe various phenomena of cosmology
(Volovik 2000b, 2001), but instead as a real fluid medium in
accordance with Gibson’s model (Gibson 1999).7 In this regard, it
becomes very convenient to consider the Navier-Stokes equations
(Zalaletdinov 2002). Furthermore to represent a real superfluid model
in cosmology, we propose a new term: ‘superfluid cosmology.’ This
conjecture implies that there should be various nonlinear phenomena
in cosmology which are thus far inexplicable using the
‘geometrification’ approach, including the ‘hidden matter’ problem.
In other words, if we use a real fluid model for nonlinear cosmology,
we do not have to invoke some kind of exotic matter to explain the
nature of ‘hidden matter’.
© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

219

Apeiron, Vol. 10, No. 3, July 2003

237

Now, with regard to GTR experiments, we also consider Consoli’s
(2000) idea that “all classical experimental tests of general relativity
would be fulfilled in any theory incorporating the Equivalence
Principle.” Therefore, because the CSV hypothesis was in principle
also based on the same phion condensate mechanism, we can predict
the same effects as were predicted by Consoli (2000).
Furthermore, the real Cantorian superfluid model also implies that
it is possible to conduct a set of laboratory experiments to replicate
real cosmological objects (Volovik 2001, Zurek 1995), provided we
take into consideration proper scale modeling (similitude) theories.

What is the Cantorian superfluid vortex?
Once we agree with the above proposition on the role of phion
condensate in describing the gravitational interaction, we are now
ready to consider the meaning of the Cantorian Superfluid Vortex
(CSV) hypothesis. Term ‘Cantorian’8 here represents the transfinite
set introduced by Georg Cantor. As we know, the transfinite set
introduces the mapping of a set onto itself, better known as a ‘selfsimilar’ pattern. This pattern is observed in various natural
phenomena, including vortex phenomena. The notion of Cantorian
vortices can be defined in simple terms as the tendency of multiple
vortices to be present in a real fluid medium, including superfluidity.
(See Nozieres & Pines 1990, Quist 2002, Volovik 2000a, 2000b,
2000c.)9 Therefore, with regards to superfluid cosmology, in principle
the Cantorian Superfluid Vortex hypothesis suggests that there is a
tendency in nature as follows:
Lemma I: “There are mini vortices within bigger vortices
ad infinitum.”
A flow pattern where the streamlines are concentric circles is
known as a circular vortex. If the fluid particles rotate around the
© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

220

Apeiron, Vol. 10, No. 3, July 2003

238

vortex centre, the vortex is called rotational. It also follows that the
vortex moves with the fluid. It is also known that real fluid flow is
never irrotational, though the mean pattern of turbulent flow
outside the boundary layer resembles the pattern of irrotational
flow. In rotational flow of real fluids, vorticity can develop as an
effect of viscosity. The term ‘vorticity’ is defined as the number of
circulations in a certain area, and it equals the circulation around
an elemental surface divided by the area of the surface (assuming
the vortex lattice exists). Since the vortex moves with the fluid, the
vortex tube retains the same fluid elements. and these elements
retain their vorticity. And provided other factors remain the same,
vortices can neither be created nor destroyed in a non-viscous
fluid.
In quantum fluid systems like superfluidity, it is known that such
vortices are subject to a quantization condition of integer multiples of
2π, or ∫ vs .dl = 2π .nh / m4 = n.κ o . Such quantized vortices are

distributed at equal distance from one another, which is known as
vorticity. Furthermore, in large superfluid systems usually we use
Landau two-fluid model, with normal and superfluid components.
The normal fluid component always possesses some nonvanishing
amount of viscosity and mutual friction.
This vortex formation phenomenon is well known in various
turbulence-related fluid phenomena such as tornadoes and tropical
hurricanes; and it can be represented by the Navier-Stokes equation
(Zalaletdinov 2002). Therefore, mathematically we treat the ‘vortex’
as a stable solution (Kivshar et al. 1999) and a consequence of
Navier-Stokes equation. Furthermore it is known there is exact
mapping between the Schrödinger equation and Navier-Stokes
equation (Kiehn 1989, 1999), therefore the Cantorian Superfluid
Vortex hypothesis requires a second conjecture:
© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

221

Apeiron, Vol. 10, No. 3, July 2003

239

Lemma II: “Vortices are considered stable solutions of
the Navier-Stokes equations.”
Since we know the Navier-Stokes equation leads us to nonlinear fluid
phenomena in cosmology (Gibson 1999) and also superfluid vortices
(Godfrey et al. 2001, Prix 2000), then the Cantorian Superfluid
Vortex hypothesis also proposes:
Lemma III: “Cantorian Superfluid Vortex theory is
capable to represent various phenomena of nonlinear
cosmology.”
Nottale’s Scale Relativity Theory (Nottale 1996, 1997, 2001, 2002)
leads us to some interesting implications including:
I. The Euler-Newton equation can be generalized to represent
various phenomena in cosmology across different scales.
Because the Euler-Newton equation can be considered a
subset (in the inviscid limit) of the Navier-Stokes equation,
then the Navier-Stokes equation can also be considered
applicable to any scale (scale covariant).
II. Because Scale Relativity Theory can be used to derive the
Dirac equation (Celerier & Nottale 2002), we also conclude
that Scale Relativity Theory implies there is an ‘electron sea’
medium, in Dirac’s words, to represent interactions across
different scales.
Hence we may also conclude that:

© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

222

Apeiron, Vol. 10, No. 3, July 2003

240

Lemma IV: “The Cantorian Superfluid Vortex is a
plausible medium to describe the motion of various
celestial objects governed by the Navier-Stokes
equation, and to represent a medium for interactions
across various scales.”
In other words, and considering the exact correspondence between
the Schrödinger equation and the Navier-Stokes equation, the
Cantorian Superfluid Vortex hypothesis also suggests:10
Lemma V: “Schrödinger equation can be treated as a
real diffusion theory, capable of describing various
celestial phenomena at various scales.”
In this sense, despite some similarities in their consequences and
cosmological implications, the Cantorian Superfluid Vortex model is
quite different from Nottale’s Scale Relativity Theory, since it relies
on a real fluid model right from the beginning.11 Using this model, we
can expect to get a proper mechanism and medium for gravity
interactions, which GTR is lacking.
A question arises here concerning whether the proposed Cantorian
Superfluid Vortex hypothesis is really different from Nottale’s Scale
Relativity Theory. Therefore it is perhaps worth mentioning here
Nottale’s own opinion (Nottale 1996):
We stress once again the fact, diffusion here is only an
interpretation. Our theory is not statistical in its essence,
contrarily to quantum mechanics or to diffusion
approaches. In scale relativity, the fractal space-time can
be completely ‘determined’, while the undeterminism of
trajectories is not set as a founding stone of the theory,
but as a consequence of the nondifferentiability of spacetime. In our theory, ‘God does not play dice’, …
© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

223

Apeiron, Vol. 10, No. 3, July 2003

241

In summary, our point of view is quantum objects are
neither ‘waves’ nor ‘particles’, … while our experiments,
being incomplete, put into evidence only the module.
There is no ‘complementarity’ here, since the phase is
never directly seen,…. There is therefore no mystery when
one can jump instantaneously from observing the ‘wave’
behavior to observing the ‘particle’ behavior without
physically disturbing the system, but only by changing the
observing way. Both properties were present before the
observation, even if only one of them was seen.
In other words, we argue here that Nottale’s Scale Relativity Theory
is insightful in its representation of a scale covariant theory of
gravitation, but it is lacking an explanation of the medium of the
gravitation interaction mostly due to th evagueness of the distinction
between the real diffusion theory and the statistical interpretation of
QM (in particular, Schrödinger equation).12
Furthermore, this could have been anticipated, because Nottale’s
Scale Relativity Theory tends to neglect the significance of real
medium modeling: it has some inherent limitations in predicting
nonlinear phenomena in cosmology (Gibson 1999).
In this regard, the Cantorian Superfluid Vortex hypothesis can be
considered an extended version of Nottale’s scale relativity theory
toward a real fluid model of nonlinear cosmology. In other words, the
proposed Cantorian Superfluid Vortex theory considers Scale
Relativity Theory merely a transformation theory, such as STR or the
Ehrenfest theorem: its contribution is to show the generality and
applicability of the Schrödinger equation for predicting phenomena at
cosmological scales. However, in the present author’s opinion,
Nottale’s Scale Relativity Theory lacks a convincing description of

© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

224

Apeiron, Vol. 10, No. 3, July 2003

242

why and what kind of medium and mechanism can represent these
phenomena.

What are its advantages over the present
theories
From the Cantorian Superfluid Vortex hypothesis we can expect
certain advantages over existing theories, including:
a. Describes the origin of outer planet distribution in a (planar)
solar system, without invoking an ad hoc second quantum
number as Nottale (1996) or Neto et al. (2002) did;
b. Predicts the existence of a vortex center in galaxies (similar to
the ‘eye’ in hurricane and tornadoes);
c. Predicts new planets in the outer orbits beyond Pluto;
d. Explains the same phenomena as predicted by GTR
(precession of perihelion of Planet Mercury, etc.) similar to
what has been suggested by Consoli (2000);
e. Describes the physical nature of the quantum vacuum aether
medium and also the mechanism of the gravitation interaction
(Chapline 1998, Consoli 2000, 2002);
f. Simplicity preserved by retaining the notion of three
dimensional space and one dimension time; thus QM can be
generalized to cosmological scales naturally (Coles 2002,
Neto et al. 2002, Signell 2002, Zakir 1999, Zurek 1995);
g. Explains why the universe is observed as flat Euclidean, not
as curved spacetime as predicted by Einstein (flat spacetime
has also been considered for instance by K. Akama and P.V.
Moniz).13 This is because there is no such thing as curved
spacetime, at least not in the proposed Cantorian Superfluid
Vortex theory (see also Chapline 1998, Winterberg 2002a,
2002b);
© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

225

Apeiron, Vol. 10, No. 3, July 2003

243

h. Solves some known paradoxes in QM.

Unsolved questions and possible future
research
Despite the above advantages, there are unsolved questions that
require further research, including:
• Explain other nonlinear cosmological phenomena from
superfluidity viewpoint, including nebulae, pulsars, neutron
stars, gamma ray bursts, etc. (DeAquino 2002, 2002a, Gibson
1999, Sedrakian & Cordes 1997);
• Reconcile the proposed Cantorian Superfluid Vortex theory
with various phenomena at quantum scale, as predicted by
QED, etc. (Nottale 1996, 1997, 2001, 2002a, 2002b);
• Provide a mathematical explanation of various known QM
paradoxes;
• Explain known electromagnetic theories of Maxwell, etc.;
• Provide a measurable prediction of the smallest entity in
nature. The proposed Cantorian Superfluid Vortex theory
prefers ‘vorton’ instead of ‘photon’ as the smallest entity in
nature.
Other phenomena may have been overlooked here. The above list is
merely an introductory ‘to-do list’.
In the present article we have discussed some reasons for
considering Cantorian superfluid vortices as the basis of cosmology
modeling. While of course this approach has not been widely
accepted yet, in the author’s opinion it could reconcile some known
paradoxes both in quantum mechanics (e.g., duality of wave-particle),
and also in cosmology (clustering, inhomogeneity, hidden matter).
Further discussion of the proposed hypothesis will be reserved for a
forthcoming article where some implications and open questions will
© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

226

Apeiron, Vol. 10, No. 3, July 2003

244

be discussed. Furthermore, in the near future we expect that there will
be other theories based on a real fluid model, which are capable of
predicting various cosmological phenomena in a more precise way.
References
1.

Barcelo, C., S. Liberati, & M. Visser, “Analogue gravity from field theory
normal modes,” Class. Quantum Grav. 18 (2000) 3595-3610.
2. Barge, P., & J. Sommeria, “Did planet formation begin inside persistent
gaseous vortices?” published in arXiv:astro-ph/9501050. Also in
Astronomy & Astrophysics 19.8.2002 (1995),
3. Castro, C., A. Granik, & M. El Naschie, “Scale relativity in Cantorian
space,” arXiv:hep-th/0004152 (2000).
4. Castro, C. & A. Granik, “Scale relativity in Cantorian space and average
dimensions of our world,” Chaos, Solitons & Fractals, Vol. 12 No. 10
(2001) 1793-1816; also arXiv:hep-th/0004152.
5. Celerier, M.N. & L. Nottale, “A scale-relativistic derivation of the Dirac
Equation,” Electromagnetic Phenomena, in press, special issue dedicated
to the 75th anniversary of the discovery of the Dirac equation (Oct 2002).
Published at http://www.daec.obspm.fr/users/nottale. Also in arXiv:hepth/0210027.
6. Chapline, G., “The vacuum energy in a condensate model of spacetime,”
arXiv:hep-th/9812129 (1998).
7. Chavanis, P., “Trapping of dust by coherent vortices in the solar nebula,”
arXiv:astro-ph/9912087 (1999).
8. Christianto, V., “Scale effect in quantization of semimajor axes of solarlike systems” (Dec 2002) available at http://reachme.at/coolbit.
9. Coles, P., “The wave mechanics of large-scale structure,” arXiv:astroph/0209576 (2002).
10. Consoli, M., “Gravitational forces from Bose-Einstein condensate,”
arXiv:hep-ph/0002098 (2000).
11. Consoli, M., “A connection between gravity and the Higgs field,”
arXiv:hep-ph/0204106 (2002).
12. De Aquino, F., “The gravitational mass at the superconducting state,”
arXiv:physics/0201508 (2002).
© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

227

Apeiron, Vol. 10, No. 3, July 2003
245
13. De Aquino, “Some cosmological consequences of the correlation between
the gravitational and inertial mass,” arXiv:physics/0205040 (2002a).
14. Dereli, T., & R. Tucker, “On the detection of scalar field induced
spacetime torsion,” arXiv:gr-qc/0104050 (2001).
15. Einstein, A., Relativity: The Special and the General Theory. Random
House, London (1952).
16. Einstein, A., Sidelights on Relativity. Methuen and Co., London (1921).
17. Gibson, C., “Turbulent mixing, diffusion and gravity in the formation of
cosmological structures,” Proceedings of FEDSM99, July 18-23, 1999, San
Fransisco (1999).
18. Godfrey, S.P. et al., “A new interpretation of oscillating flow experiments
in superfluid Helium II,” J. Low Temp. Physics, Vol. 125, Nos. 112 (Oct
2001).
19. Kiehn, R.M., “An interpretation of wave-function as a cohomological
measure of quantum vorticity,” Florida,
http://www22.pair.com/csdc/pdf/cologne.pdf (1989).
20. Kiehn, R.M., “An extension of quantum theory to include Non-gradient
Potentials and the production of vortices,”
http://www22.pair.com/csdc/pdf/bohmplus.pdf (1999).
21. Kivshar, Y., et al., “Dynamics of optical vortex solitons,” Optics
Communications 152, Elsevier (1998) 198-206.
22. Munera, H., “Michelson-Morley experiments revisited: systematic errors,
consistency among different experiments, and compatibility with absolute
space,” Apeiron Vol. 5 No. 1-2, Jan-Apr (1998).
23. Neto, M., et al., “An alternative approach to describe planetary systems
through a Schrodinger-type diffusion equation,” arXiv:astro-ph/0205379
(Oct 2002).
24. Nottale, L., “Scale Relativity and Fractal Space-Time: Application to
Quantum Physics, Cosmology and Chaotic systems,” Chaos, Solitons and
Fractals, 7, (1996) 877-938. Also published at
http://www.daec.obspm.fr/users/nottale. See particularly p. 50-53.
25. Nottale, L. et al., “Scale relativity and quantization of the Solar System,”
Astron. Astrophys. 322, (1997) 1018. Also published at
http://www.daec.obspm.fr/users/nottale.
© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

228

Apeiron, Vol. 10, No. 3, July 2003
246
26. Nottale, L., “Scale relativity and quantization of the universe I: Theoretical
framework,” Astron. Astrophys. 327, (1997) 867-889. Also published at
http://www.daec.obspm.fr/users/nottale.
27. Nottale, L., Schumacher, G., & E.T. Levefre, “Scale-relativity and
quantization of exoplanet orbital semi-major axes,” Astron. Astrophys. 361,
(2000) 379-387. Also published at http://www.daec.obspm.fr/users/nottale.
28. Nottale, L., “Non-differentiable space-time and scale relativity,” in
“Géométrie au XXè siècle”, Proc. of International Colloquium, Paris, 2429 Septembre 2001, Ed. D. Flament, in press (2002a) As published at
http://www.daec.obspm.fr/users/nottale.
29. Nottale, L., “Scale relativistic cosmology,” Chaos, Solitons and Fractals
(2002b) As published at http://daec.obspm.fr/users/nottale.
30. Nozieres, P., & D. Pines, The theory of quantum liquids: Superfluid Bose
Liquid. Wesley Publ. Co. Inc., (1990) p. 116-124.
31. Prix, R., “Covariant vortex in superconducting-superfluid-normal fluid
mixtures with stiff equation of state,” arXic:gr-qc/0004076 (2000).
32. Quist, M., “Vortex dynamics in the nonlinear Schrödinger equations,”
arXiv:cond-mat/0211424 (Nov. 2002).
33. Roberts, M., “Vacuum energy,” arXiv:hep-th/0012062 (2001).
34. Rosu, H.C., “Pedestrian notes on quantum mechanics,” arXiv:grqc/9411035 (1994).
35. Rothwarf, A., “An aether model of the universe,” Phys. Essay 11 (1998)
444-466.
36. Sedrakian, A. & J. Cordes, “Generation of pulsar glitches: superfluid core
model,” Proc. 18th Texas Symposium on Relativistic Astrophysics, eds. A.
Olinto, J. Frieman & D. Schram, World Scientific Press, (1997),
arXiv:astro-ph/9709277.
37. Siegel, W., “Fields,” C.N. Yang Institute for Theoretical Physics.
arXiv:hep-th/9912205 (Sep 2002).
38. Signell, P., “Newton’s second law from quantum physics,” Project
PHYSNET, http://www.physnet.org/home/modules/license.html (2002) 8p.
39. Volovik, G., “Links between gravity and dynamics of quantum liquids,”
Int. Conf. “Cosmology. Relativistic Astrophysics. Cosmoparticle Physics”
(COSMION-99) (2000a). As published at arXiv:gr-qc/0004049.
© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

229

Apeiron, Vol. 10, No. 3, July 2003
247
40. Volovik, G., “Superfluid analogies of cosmological phenomena,” arXiv:grqc/0005091 (2000b).
41. Volovik, G., “Vortices observed and to be observed,” arXiv:condmat/0005431 (2000c).
42. Volovik, G., “Vacuum in quantum liquids and in general relativity,”
arXiv:gr-qc/0104046 (2001).
43. Weinberg, S., Dreams of final theory, Vintage, London. (1993) 180-182,
214-215.
44. Weinberg, S., “What is quantum field theory and what did we think it is?”
arXiv:hep-th/9702027 (Feb 1997).
45. Winterberg, F., “Maxwell’s Aether, the Planck Aether hypothesis, and
Sommerfeld’s Fine Structure Constant.” http://www.cet.sunderland.
ac.uk/webedit/allweb/news/Philosophy_of_Science/Maxwell's%20Aether,
%20the%20Planck%20Aether%20Hypothesis%20and%20Sommerfeld's%
20Finestructure%20Constant.doc (2002a) see also
http://www.znaturforsch.com/56a/56a0681.pdf .
46. Winterberg, F., “Planck mass rotons as cold dark matter and quintessence,”
presented at the 9th Canadian Conf. on General Relativity and Relativistic
Astrophysics, Edmonton, May 24-26 2001; also in Z. Naturforsch 57a,
202-204 (2002b).
47. Zakir, Z., “Gravitation as quantum diffusion,” arXiv:gr-qc/9906079
(1999).
48. Zalaletdinov, R., “Averaging out inhomogenous Newtonian cosmologies:
Newtonian Cosmology and the Navier-Stokes-Poisson Equations,”
arXiv:gr-qc/0212071 (Dec 2002).
49. Zurek, W.H., “Cosmological experiments in superfluids and
superconductors,” in Proc. Euroconference Formation and Interaction of
Topological Defects, A.C. Davis & R.N. Brandenberger (eds.) Plenum
(1995). Also in Los Alamos preprint LAUR 95-170, arXiv: condmat/9502119.

Notes
1

See the articles by Mendel Sachs at http://www.compukol.com/mendel, also
Annales Foundation Louis de Broglie vol. 27, 85 (2002). Also Chapter 11 in
© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

230

Apeiron, Vol. 10, No. 3, July 2003

248

Modern Nonlinear Optics Part I. Advances in Chemical Physics, Volume 119,
Series editors: I. Prigogine et al., John Wiley & Sons, Inc. (2002).
2
For more discussion on this issue, we refer to C. Will’s report: ‘The
confrontation between general relativity and experiments: 1998 update,’
McDonnell Center for the Space Sciences, Washington University. Recently
there are also some articles discussing some features indicating incompleteness
of GTR, for example arXiv:gr-qc/0102056, particularly related to the so-called
Pioneer anomaly.
3
See Munera (1998), who provides calculation to show Michelson-Morley
experiments actually never were null. Since Michelson-Morley experiments are
often considered as the building block of relativity theory (STR), we know what
this article suggests.
4
See Whittaker, E., On the partial differential equations in mathematical
physics, Cambridge Univ., (1903).
5
See articles by T. Van Flandern at http://www.metaresearch.org
6
Of course, there are several other interpretations of the nature of the scalar field
besides the ‘phion condensate.’ See for instance Barcelo et al. (2000), Dereli &
Tucker (2000), Roberts (2001), Siegel (2002).
7
See also other articles by Gibson at arXiv.org:astro-ph/9904230, 9904237,
9904260, 9904284, 9904283, 9904317, 0003147, 9911264, 9904362, 9904269,
9904366, 9908335, 0002381.
8
Recently Castro, Granik, & El Naschie (2000) reintroduced this term to
describe the exact dimension of our universe.
9
There is already literature describing vortices in some cosmology phenomena,
for instance Barge & Sommeria (1995) and also Chavanis (1999).
10
In this regards, see Coles (2002), Neto et al. (2002), Rosu (1994), Zakir
(1999).
11
For a discussion on the meaning of interpreting Schrödinger equation as real
fluid phenomena, see also Rosu (1994).
12
See also Neto et al. (2002), Rosu (1994), Zakir (1999).
13
See for instance P.V. Moniz (arXiv:gr-qc/0011098) and K. Akama
(arXiv:hep-th/0007001, hep-th/0001113).

© 2003 C. Roy Keys Inc.

Quantization in Astrophysics ...

231

A NOTE ON GEOMETRIC AND INFORMATION FUSION
INTERPRETATION OF BELL’S THEOREM AND QUANTUM
MEASUREMENT1
F. SMARANDACHE2 AND V. CHRISTIANTO3

1. Introduction
It is generally accepted that Bell’s theorem [1] is quite exact to describe the
linear hidden-variable interpretation of quantum measurement, and hence ‘quantum
reality.’ Therefore null result of this proposition implies that no hidden-variable
theory could provide good explanation of ‘quantum reality.’
Nonetheless, after further thought we can find that Bell’s theorem is nothing
more than another kind of abstraction of quantum observation based on a set of
assumptions and propositions [7]. Therefore, one should be careful before making
further generalization on the null result from experiments which are ‘supposed’ to
verify Bell’s theorem. For example, the most blatant assumption of Bell’s theorem
is that it takes into consideration only the classical statistical problem of chance
of outcome A or outcome B, as result of adoption of Von Neumann’s definition of
‘quantum logic’. Another critic will be discussed here, i.e. that Bell’s theorem is
only a reformulation of statistical definition of correlation; therefore it is merely
tautological. [5]
Therefore in the present paper we will discuss a few plausible extension of Bell’s
theorem:
(a) Bayesian and Fuzzy Bayesian interpretation.
(b) Information Fusion interpretation. In particular, we propose a modified
version of Bell’s theorem, which takes into consideration this multivalued outcome, in particular using the information fusion theory of DezertSmarandache [2, 3, 4]. We suppose that in quantum reality the outcome of
P (A ∪ B) and also P (A ∩ B) shall also be taken into consideration. This is
where DSmT theory could be found useful. [2]
(c) Geometric interpretation, using a known theorem connecting geometry
and imaginary plane. In turn, this leads us to 8-dimensional extendedMinkowski metric.
(d) As an alternative to this geometric interpretation, we submit the viewpoint
of photon fluid as medium for quantum interaction. This proposition leads
us to Gross-Piteavskii equation which is commonly used to describe bose
condensation phenomena. In turn we provide a route where Maxwell equations and Schrodinger equation could be deduced from Gross-Pitaevskii
equation by using known algebra involving bi-quaternion number. In our
1 . Note: The notion ‘hronir wave’ introduced here was inspired from Borges’ Tlon, Uqbar,
Orbis Tertius.

1

Quantization in Astrophysics ...

232

F. SMARANDACHE2 AND V. CHRISTIANTO3

2

opinion, this new proposition provides us a physical mechanism of quantum interaction, beyond conventional ‘quantum algebra’ which hides causal
explanation.
By discussing these various approaches, we use an expanded logic beyond ‘yes’
or ‘no’ type logic [3]. In other words, there could be new possibilities to describe
quantum interaction: ‘both can be wrong’, or ‘both can be right’, as described in
Table 1 below:
Table 1. Going beyond classical logic view of QM
Alternative
QM is nonlocal

Bell’s theorem
Invalid

QM is local with
hidden variable

Valid

Both can be right

Valid, but there is
a way to explain
QM without violating Special Relativity

Both can be wrong Invalid, and so
Special Relativity
is. We need a new
theory

Implications
Special relativity
Causality breaks Is not always apdown; Observer plicable
determines the
outcome
Causality
pre- No interaction can
served;
The exceed the speed
moon is there of light
even
without
observer.
QM,
special Can
be
exrelativity
and panded
using
Maxwell electro- 8-dimensional
magnetic theory Minkowski metric
can be unified. with
imaginary
New worldview plane
shall be used.
New
nonlocal Is not always apQM theory is plicable
required, involving
quantum
potential

It could be expected that a combined interpretation represents multiple-facets of
quantum reality. And hopefully it could bring better understanding on the physical mechanism beneath quantum measurement, beyond simple algebraic notions.
Further experiments are of course recommended in order to verify or refute this
proposition.
2. Bell’s theorem. Bayesian and Fuzzy Bayesian Interpretation
Despite widespread belief of its ability to describe hidden-variables of quantum
reality [1], it shall be noted that Bell’s theorem starts with a set of assumptions
inherent in its formulation. It is assumed that each pair of particles possesses a
particular value of λ, and we define quantity p(λ) so that probability of a pair being
produced between λ and λ + dλ is p(λ)dλ. It is also assumed that this is normalized
so that:
Z
p(λ)dλ = 1.
(1)

Quantization in Astrophysics ...

233

A NOTE ON GEOMETRIC AND INFORMATION FUSION. . .

3

Further analysis shows that the integral that measures the correlation between
two spin components that are at an angle of (δ − φ) with each other, is therefore
equal to C 00 (δ − φ). We can therefore write:
|C 00 (φ) − C 00 (δ)| − C 00 (δ − φ) ≤ 1

(2)

which is known as Bell’s theorem, and it was supposed to represent any local hiddenvariable theorem. But it shall be noted that actually this theorem cannot be tested
completely because it assumes that all particle pairs have been detected. In other
words, we find that a hidden assumption behind Bell’s theorem is that it uses
classical probability assertion [12], which may or may be not applicable to describe
Quantum Measurement.
It is wothnoting here that the standard interpretation of Bell’s theorem includes
the use of Bayesian posterior probability [13]:
p(α)p(x | α)
.
P (α | x) = P
β p(β)p(x | β)

(3)

As we know Bayesian method is based on classical two-valued logic. In the
meantime, it is known that the restriction of classical propositional calculus to a
two-valued logic has created some interesting paradoxes. For example, the Barber
of Seville has a rule that all and only those men who do not shave themselves are
shaved by the barber. It turns out that the only way for this paradox to work is if
the statement is both true and false simultaneously. [14]. This brings us to fuzzy
Bayesian approach [14] as an extension of (3):
P (si |M ) =

p(M | si )p(si )
p(M )

.

(4)

Where [14, p. 339]:
p(M | si ) =

r
X

p(xk | si )µM (xk ).

(5)

k=1

Nonetheless, it should also be noted here that there is shortcoming of this
Bayesian approach. As Kracklauer points out, Bell’s theorem is nothing but a
reformulation of statistical definition of correlation [5]:
Corr(A, B) =

h|AB|i − hAihBi
p
.
hA2 ihB 2 i

(6)

When hAi or hBi equals to zero and hA2 ihB 2 i = 1 then equation (6) reduces to
Bell’s theorem. Therefore as such it could be considered as merely tautological [5].
3. Information Fusion interpretation of Bell’s theorem. DSmT
modification
In the context of physical theory of information [8], Barrett has noted that
“there ought to be a set theoretic language which applies directly to all quantum
interactions.” This is because the idea of a bit is itself straight out of classical set
theory, the definitive and unambiguous assignment of an element of the set {0, 1},
and so the assignment of an information content of the photon itself is fraught with
the same difficulties [8]. Similarly, the problem becomes more adverse because the
fundamental basis of conventional statistical theories is the same classical set {0, 1}.
Not only that, there is also criticism over the use of Bayesian approach, i.e.: [13]

Quantization in Astrophysics ...

234

F. SMARANDACHE2 AND V. CHRISTIANTO3

4

(a) In real world, neither class probabilities nor class densities are precisely
known;
(b) This implies that one should adopt a parametric model for the class probabilities and class densities, and then use empirical data.
(c) Therefore, in the context where multiple sensors can be used, information
fusion approach could be a better alternative to Bayes approach.
In other words, we should find an extension to standard proposition in statistical
theory [8, p.388]:
P (AB | C) = P (A | BC)P (B | C)
= P (B | AC)P (A | C)
P (A | B) + P (Ā | B) = 1.

(7)
(8)

Such an extension is already known in the area of information fusion [2], known
as Dempster-Shafer theory:
m(A) + m(B) + m(A ∪ B) = 1.

(9)

Interestingly, Chapline [13] noted that neither Bayesian theory nor DempsterShafer could offer insight on how to minimize overall energy usage in the network. In
the meantime, Dezert-Smarandache (DSmT) [2] introduced further improvement of
Dempster-Shafer theory by taking into consideration chance to observe intersection
between A and B:
m(A) + m(B) + m(A ∪ B) + m(A ∩ B) = 1.

(10)

Therefore, introducing this extension from equation (10) into equation (2), one
finds a modified version of Bell’s theorem in the form:
|C 00 (φ) − C 00 (δ)| − C 00 (δ − φ) + C 00 (δ ∪ φ) + C 00 (δ ∩ φ) ≤ 1

(11)

which could be called as modified Bell’s theorem according to Dezert-Smarandache
(DSmT) theory [2]. Its direct implications suggest that it could be useful to include
more sensors in order to capture various possibilities beyond simple {0, 1} result,
which is typical in Bell’s theorem.
Further generalization of DSmT theory (10) is known as Unification of Fusion
Theories [15, 16, 17]:
m(A) + m(B) + m(A ∪ B) + m(A ∩ B) + m(Ā) + m(B̄) = 1

(12)

where Ā is the complement of A and B̄ is the complement of B (if we consider the
set theory).
(But if we consider the logical theory then Ā is the negation of A and B̄ is the
negation of B. The set theory and logical theory in this example are equivalent,
hence doesn’t matter which one we use from them.) In equation (12) above we have
a complement/negation for A. We might define the Ā as the entangle of particle
A. Hence we could expect to further extend Bell’s inequality considering UFT;
nonetheless we leave this further generalization for the reader.
Of course, new experimental design is recommended in order to verify and to
find various implications of this new proposition.

Quantization in Astrophysics ...

235

A NOTE ON GEOMETRIC AND INFORMATION FUSION. . .

5

4. An alternative geometric interpretation of Bell-type
measurement. Gross-Pitaevskii equation and the ‘hronir wave’
Apart from the aforementioned Bayesian interpretation of Bell’s theorem, we
can consider the problem from purely geometric viewpoint. As we know, there is
linkage between geometry and algebra with imaginary plane [18]:
x + iy = ρeiφ .

(13)

Therefore one could expect to come up with geometrical explanation of quantum
interaction, provided we could generalize the metric using imaginary plane:
X + iX 0 = ρeiφ .

(14)

Interestingly, Amoroso and Rauscher [19] have proposed exactly the same idea,
i.e. generalizing Minkowski metric to become 8-dimensional metric which can be
represented as:
µ
µ
Z µ = Xre
+ iXim
= ρeiφ .
(15)
A characteristic result of this 8-dimensional metric is that ‘space separation’
vanishes, and quantum-type interaction could happen in no time.
Another viewpoint could be introduced in this regard, i.e. that the wave nature of photon arises from ‘photon fluid’ medium, which serves to enable photonphoton interaction. It has been argued that this photon-fluid medium could be
described using Gross-Pitaevskii equation [20]. In turns, we could expect to ‘derive’ Schrodinger wave equation from the Gross-Pitaevskii equation.
It will be shown, that we could derive Schrodinger wave equation from GrossPitaevskii equation. Interestingly, a new term similar to equation (13) arises here,
which then we propose to call it ‘hronir wave’. Therefore one could expect that this
‘hronir wave’ plays the role of ‘invisible light’ as postulated by Maxwell long-time
ago.
Consider the well-known Gross-Pitaevskii equation in the context of superfluidity
or superconductivity [21]:
~2
∂Ψ
=−
∆Ψ + (V (x) − γ|Ψ|p−1 )Ψ,
(16)
∂t
2m
where p < 2N/(N − 2) if N ≥ 3. In physical problems, the equation for p = 3 is
known as Gross-Pitaevskii equation. This equation (16) has standing wave solution
quite similar to Schrodinger equation, in the form:
i~

Ψ(x, t) = e−iEt/~ · u(x).

(17)

Substituting equation (17) into equation (16) yields:
~2
∆u + (V (x) − E)u = |u|p−1 u,
(18)
2m
which is nothing but time-independent linear form of Schrodinger equation, except
for term |u|p−1 [21]. In case the right-hand side of this equation is negligible,
equation (18) reduces to standard Schrodinger equation. Using Maclaurin series
expansion, we get for (17):


(Et/~)2
(−iEt/~)3
Ψ(x, t) = 1 − iEt/~ +
+
+ · · · · u(x).
(19)
2!
3!
−

Therefore we can say that standing wave solution of Gross-Pitaevskii equation
(17) is similar to standing wave solution of Schrodinger equation (u), except for

Quantization in Astrophysics ...

236

F. SMARANDACHE2 AND V. CHRISTIANTO3

6

nonlinear term which comes from Maclaurin series expansion (19). By neglecting
third and other higher order terms of equation (19), one gets an approximation:
Ψ(x, t) = [1 − iEt/~] · u(x).

(20)

Note that this equation (20) is very near to hyperbolic form z = x + iy [18].
Therefore one could conclude that standing wave solution of Gross-Pitaevskii equation is merely an extension from ordinary solution of Schrodinger equation into
Cauchy (imaginary) plane. In other words, there shall be ‘hronir wave’ part
of Schrodinger equation in order to describe Gross-Pitaevskii equation. We will
use this result in the subsequent section, but first we consider how to derive biquaternion from Schrodinger equation.
It is known that solutions of Riccati equation are logarithmic derivatives of
solutions of Schrodinger equation, and vice versa [22]:
u00 + vu = 0.

(21)

Bi-quaternion of differentiable function of x = (x1 , x2 , x3 ) is defined as [22]:
Dq = − div(q) + grad(qo ) + rot(q).

(22)

By using alternative representation of Schrodinger equation [22]:
[−∆ + u]f = 0,

(23)

where f is twice differentiable, and introducing quaternion equation:
Dq + q 2 = −u.

(24)

Then we could find q, where q is purely vectorial differentiable bi-quaternion
valued function [22].
We note that solutions of (23) are related to (24) as follows [22]:
** For any nonvanishing solution f of (23), its logarithmic derivative:
q=

Df
,
f

(25)

is a solution of equation (24), and vice versa. [22]
Furthermore, we also note that for an arbitrary scalar twice differentiable function f , the following equality is permitted [22]:
[−∆ + u]f = [D + M h ][D − M h ]f,

(26)

provided h is solution of equation (24).
Therefore we can summarize that given a particular solution of Schrodinger
equation (23), the general solution reduces to the first order equation [22, p.9]:
[D + M h ]F = 0,
where

(27)

√
D ε
h=
.
(28)
ε
Interestingly, equation (27) is equivalent to Maxwell equations. [22] Now we
can generalize our result from the preceding section, in the form of the following
conjecture:

Quantization in Astrophysics ...

237

A NOTE ON GEOMETRIC AND INFORMATION FUSION. . .

7

Conjecture 1. Given a particular solution of Schrodinger equation (23), then the
approximate solution of Gross-Pitaevskii equation (16) reduces to the first order
equation:
[1 − iEt/~][D + M h ]F = 0.
(29)
Therefore we can conclude here that there is neat linkage between Schrodinger
equation, Maxwell equation, Riccati equation via biquaternion expression [22, 23,
24]. And approximate solution of Gross-Pitaevskii equation is similar to solution
of Schrodinger equation, except that it exhibits a new term called here ‘the hronir
wave’ (29).
Our proposition is that considering equation (29) has imaginary plane wave,
therefore it could be expected to provided ‘physical mechanism’ of quantum interaction, in the same sense of equation (13). Further experiments are of course
recommended in order to verify or refute this proposition.
5. Some astrophysical implications of Gross-Pitaevskii description
Interestingly, Moffat [25, p.9] has also used Gross-Pitaevskii in his ‘phion condensate fluid’ to describe CMB spectrum. Therefore we could expect that this
equation will also yield interesting results in cosmological scale.
Furthermore, it is well-known that Gross-Pitaevskii equation could exhibit topologically non-trivial vortex solutions [26, 27], which can be expressed as quantized
vortices:
I
p • dr = Nv 2π~.
(30)
Therefore an implication of Gross-Pitaevskii equation [25] is that topologically
quantized vortex could exhibit in astrophysical scale. In this context we submit
the viewpoint that this proposition indeed has been observed in the form of Tifft’s
quantization [28, 29]. The following description supports this assertion of topological quantized vortices in astrophysical scale.
We start with standard definition of Hubble law [28]:
z=

Hr
δλ
=
.
λ
c

(31)

Or

c
z.
(32)
H
Now we suppose that the major parts of redshift data could be explained via
Doppler shift effect, therefore [28]:
r=

δλ
v
= .
(33)
λ
c
In order to interpret Tifft’s observation of quantized redshift corresponding to
quantized velocity 36.6 km/sec and 72.2 km/sec, then we could write from equation
(33):
 
δλ
δv
= δz = δ
.
(34)
c
λ
z=

Or from equation (32) we get:
δr =

Quantization in Astrophysics ...

c
δz.
H

238

(35)

8

F. SMARANDACHE2 AND V. CHRISTIANTO3

In other words, we submit the viewpoint that Tifft’s observation of quantized
redshift implies a quantized distance between galaxies [28], which could be expressed
in the form:
rn = ro + n(δr).
(35a)
It is proposed here that this equation of quantized distance (35a) is resulted
from topological quantized vortices (30), and agrees with Gross-Pitaevskii (quantum phion condensate) description of CMB spectrum [25]. Nonetheless, further
observation is recommended in order to verify the above proposition.
Concluding remarks
In the present paper we review a few extension of Bell’s theorem which could take
into consideration chance to observe outcome beyond classical statistical theory, in
particular using the information fusion theory. A new geometrical interpretation of
quantum interaction has been considered, using Gross-Pitaevskii equation. Interestingly, Moffat [25] also considered this equation in the context of cosmology.
It is recommended to conduct further experiments in order to verify and also
to explore various implications of this new proposition, including perhaps for the
quantum computation theory [8, 13].
Acknowledgment
The writers would like to thank to Profs. C. Castro, J. Dezert, P. Vallin, T. Love,
D. Rabounski, and A. Kaivarainen for valuable discussions. The new term ‘hronir
wave’ introduced here was inspired from Borges’ Tlon, Uqbar, Orbis Tertius. Hronir
wave is defined here as ‘almost symmetrical mirror’ of Schrodinger-type wave.
References
[1] Shimony, A., http://plato.stanford.edu/entries/bell-theorem/ (2004), [1a]
http://plato.stanford.edu/entries/kochen-specker/
[2] Smarandache, F., & J. Dezert, Advances and Applications of of DSmT for Information
Fusion, American Research Press, Rehoboth (2004) 438p.
[3] Smarandache, F., “An introduction to the Neutrosophic probability applied in quantum
physics,” hBulletin of Pure and Applied Sciencesi , Physics, 13-25, Vol. 22D, No. 1 (2003) 11
p; www.gallup.unm.edu/~smarandache/physics.htm.
[4] Smarandache, F., & V. Christianto, Multivalued Logic, Neutrosophy and Schrodinger equation, Hexis-Phoenix (2005).
[5] Kracklauer, A., ‘La theorie de Bell, est-elle la plus grande meprise de l’histoire de la
physique?,’ Annales de la Fondation Louis de Broglie, 25 (2000) 193
[6] Aharonov, Y., et al., arXiv:quant-ph/0311155 (2003).
[7] Rosu, H. C., arXiv:gr-qr-qc/9411035 (1994) p. 2–8.
[8] Zurek, W. (ed.), Complexity, Entropy and the Physics of Information, Santa Fe Institute
Studies, Addison-Wesley Publ. (1990) 378.
[9] Schrieffer, J. R., “Macroscopic quantum phenomena from pairing in superconductors,” Lecture, December 11th, 1972.
[10] Anandan, J. S., in Quantum Coherence and Reality, Proc. Conf. Fundamental Aspects of
Quantum Theory, Columbia SC., edited by J.S. Anandan & J.L. Safko, (World Scientific
1994). arXiv:gr-qc/9504002 (1995).
[11] Goldstein, S., “Quantum Theory without Observers – Part One,” Physics Today, March 1998,
p. 42–46
[12] Pitowski, I., arXiv:quant-ph/0510095 (2005).
[13] Chapline, G., arXiv:adap-org/9906002 (1999). Also [13a] arXiv:quant-ph/9912019
(1999), [13b] A. Granik, G. Chapline, arXiv:quant-ph/0302013 (2003), [13c]
www.whatsnextnetwork.com/technology/index.php/2006/03/

Quantization in Astrophysics ...

239

A NOTE ON GEOMETRIC AND INFORMATION FUSION. . .

9

[14] Ross, T. J., Fuzzy Logic with Engineering Applications. McGraw-Hill, Inc. (1995) 196–197,
334–341
[15] Smarandache, F. “Unification of Fusion Theories (UFT)”, International Journal of Applied
Mathematics & Statistics, Roorkee, India, Vol. 2, 1-14, December 2004.
[16] Smarandache, F., “An In-Depth Look at Information Fusion Rules and Unification of Fusion
Theories”, Invited speech at NASA Langley Research Center, Hampton, VA, USA, November
5, 2004.
[17] Smarandache, F. “Unification of the Fusion Theory (UFT)”, Invited speech at NATO Advance Study Institute, Albena, Bulgaria, 16–27 May, 2005.
[18] Gogberashvili, M., arXiv:hep-th/0212251 (2002) 4–5
[19] Rauscher, E. A., & R. Amoroso, The physical implications of multidimensional geometries
and measurement, Inter. J. of Comp. Anticipatory Systems, D. Dubois (ed.) (2006)
[20] Chiao, R., et al., arXiv:physics/0309065 (2003).
[21] Dinu, arXiv:math.AP/0511184 (2005)
[22] Kravchenko, V., arXiv:math.AP/0408172 (2004)
[23] Lipavsky, P., et al., arXiv:cond-mat/0111214 (2001)
[24] de Haas, E. P., “A renewed theory of electrodynamics in the framework of Dirac ether,” PIRT
(2005), http://www.physics.nl
[25] Moffat, J., arXiv:astro-ph/0602607 (2006)
[26] Smarandache, F., & V. Christianto, Progress in Physics Vol.2 No. 2 (2006),
http://www.ptep-online.com
[27] Fischer, U., arXiv:cond-mat/9907457 (1999); [27a] arXiv:cond-mat/0004339
[28] Humphreys, TJ Archive Vol.16, http://answersingenesis.org/tj/v16/i2/index.asp
(2002)
[29] Setterfield, B., http://www.journaloftheoretics.com

First version: 8th June 2006, 1st revision 28th Aug. 2006.
2.

Department of Mathematics, University of New Mexico, NM 87301, USA,
E-mail address: smarand@unm.edu

3.

http://www.sciprint.org,
E-mail address: admin@sciprint.org

Quantization in Astrophysics ...

240

1

Plausible explanation of quantization of
intrinsic redshift from Hall effect and
Weyl quantization
1

F. Smarandache1 & V. Christianto2
. Department of Mathematics, University of New Mexico, NM 87301, USA, email:
smarand@unm.edu
2
. http://reachme.at/coolbit, email: vxianto@yahoo.com

Introduction
In a recent paper by Moffat [1] it is shown that quantum phion condensate
model with Gross-Pitaevskii equation yields an approximate fit to data corresponding to CMB spectrum, and it also yields a modified Newtonian acceleration law which is in good agreement with galaxy rotation curve data. It
seems therefore interesting to extend further this hypothesis to explain quantization of redshift, as shown by Tifft et al. [2][6][7]. We also argue in other
paper that this redshift quantization could be explained as signature of topological quantized vortices, which also agrees with Gross-Pitaevskiian description [3][5].
Nonetheless, there is remaining question in this quantized vortices interpretation, i.e. how to provide explanation of ‘intrinsic redshift’ argument by
Bell [6]. In the present paper, we argue that it sounds reasonable to interpret
the intrinsic redshift data from the viewpoint of rotating Hall effect, i.e.
rotational motion of clusters of galaxies exhibit quantum Hall effect which
can be observed in the form of ‘intrinsic redshift’. While this hypothesis is
very new, it could be expected that we can draw some prediction, including
possibility to observe small ‘blue-shift’ effect generated by antivortex part of
the Hall effect. [5a]
Another possibility is to explain redshift quantization from the viewpoint
of Weyl-Moyal quantization theory [25]. It is shown that Schrödinger equation can be derived from Weyl approach [8], therefore quantization in this
sense comes from ‘graph’-type quantization. In large scale phenomena like
galaxy redshift quantization one could then ask whether there is possibility
of ‘super-graph’ quantization.
Further observation is of course recommended in order to verify or refute
the propositions outlined herein.
Interpreting quantized redshift from Hall effect. Cosmic String
In a recent paper, Moffat [1, p.9] has used Gross-Pitaevskii in conjunction
with his ‘phion condensate fluid’ model to describe CMB spectrum data.
Therefore we could expect that this equation will also yield interesting results in galaxies scale. See also [1b][1c][13] for other implications of lowenergy phion fluid model.
Interestingly, it could be shown, that we could derive (approximately)
Schrödinger wave equation from Gross-Pitaevskii equation. Consider the

Quantization in Astrophysics ...

241

2

F. Smarandache & V. Christianto

well-known Gross-Pitaevskii equation in the context of superfluidity or
superconductivity [14]:

ih

∂Ψ
h2
=−
∆Ψ + (V ( x) − γ Ψ
∂t
2m

p −1

)Ψ,

(1)

where p<2N/(N-2) if N>3. In physical problems, the equation for p=3 is
known as Gross-Pitaevskii equation. This equation (1) has standing wave
solution quite similar to solution of Schrödinger equation, in the form:

Ψ ( x, t ) = e − iEt / h .u ( x)
Substituting

−

equation

(2)

into

h2
∆u + (V ( x) − E )u = u
2m

p−1

equation

(1)

u,

(2)
yields:
(3)

which is nothing but a time-independent linear form of Schrödinger equation, except for term

u

p −1

[14]. If the right-hand side of this equation is

negligible, equation (3) reduces to standard Schrödinger equation.
Now it is worth noting here that from Nottale et al. we can derive a gravitational equivalent of Bohr radius from generalized Schrödinger equation.
[4] Therefore we could also expect a slight deviation of this gravitational
Bohr radius in we consider Gross-Pitaevskii equation instead of generalized
Schrödinger equation.
According to Moffat, the phion condensate model implies a modification
of Newtonian acceleration law to become [1, p.11]:

a(r ) = −

exp(− µ φ r )
G∞ M
+K
(1 + µ φ r )
2
r
r2

(4)

Where


M0 
G∞ = G 1 +

M 


(5)

Therefore we can conclude that the use of phion condensate model implies a modification of Newton gravitational constant, G, to become (5).
Plugging in this new equation (5) into a Nottale’s gravitational Bohr radius
equation [4] yields:

rn ≈ n 2

M0 
GM 
2 GM
1+
 ≈ χ .n
2 
M 
vo 
vo2

(6)

where n is integer (1,2,3…) and:



χ = 1 +


M0 

M 

(6a)

Therefore we conclude that --provided the higher order Yukawa term of
equation (4) could be neglected-- one has a modified gravitational Bohrradius in the form of (6). It can be shown (elsewhere) that using similar
argument one could expect to explain a puzzling phenomenon of receding
Moon at a constant rate of +1.5” per year. And from this observed fact one
could get an estimate of this χ factor. It is more interesting to note here, that
a number of coral reef data also seems to support the same idea of modification factor in equation (5), but discussion of this subject deserves another
paper.

Quantization in Astrophysics ...

242

3

Plausible explanation of quantization of intrinsic redshift from Hall effect ….

A somewhat similar idea has been put forward by Masreliez [18] using
the metric:

[

ds 2 = eαβ dx 2 + dy 2 + dz 2 − (ic.dt ) 2

]

(7)
Another alternative of this metric has been proposed by Socoloff & Starobinski [19] using multi-connected hypersurface metric:

ds 2 = dx 2 + e −2 x (dy 2 + dz 2 )

(8)

−x

With boundaries: e = Λ .
Therefore one can conclude that the use of phion condensate model has
led us to a form of expanding metric, which has been discussed by a few
authors.
Furthermore, it is well-known that Gross-Pitaevskii equation could exhibit topologically non-trivial vortex solutions [4][5], which also corresponds to quantized vortices:

∫ p ⋅ dr = N

v

2πh

(9)

Therefore an implication of Gross-Pitaevskii equation [1] is that topologically quantized vortex could exhibit in astrophysical scale. In this context we submit the viewpoint that this proposition indeed has been observed
in the form of Tifft’s redshift quantization [2][6]:

δr =

c
δz
H

(10)

In other words, we submit the viewpoint that Tifft’s observation of quantized redshift implies a quantized distance between galaxies [2][5], which
could be expressed in the form:
rn = ro + n(δr )
(11)
where n is integer (1,2,3,…) similar to quantum number. Because it can be
shown using standard definition of Hubble law that redshift quantization
implies quantized distance between galaxies in the same cluster, then one
could say that this equation of quantized distance (11) is a result of topological quantized vortices (9) in astrophysical scale [5]; and it agrees with
Gross-Pitaevskii (quantum phion condensate) description of CMB spectrum
[1]. It is perhaps more interesting if we note here, that from (10) then we
also get an equivalent expression of (11):

c
c
c
z n = z o + n( δz )
H
H
H

(12)

z n = z o + n(δz )

(13)

Or
Or


δz 
z n = z o 1 + n( )
zo 


(13a)

Nonetheless, there is a problem here, i.e. how to explain intrinsic redshift
related to Tifft quantization as observed in Fundamental Plane clusters and
also from various quasars data [6][6a]:
z iQ = z f [ N − 0.1M N ]
(13b)
Where zf=0.62 is assumed to be a fundamental redshift constant, and N
(=1,2,3…), and M is function of N.[6a] Meanwhile, it is interesting to note

Quantization in Astrophysics ...

243

4

F. Smarandache & V. Christianto

here similarity between equation (13b) and (13a). Here, the number M seems
to play a role similar to second quantum number in quantum physics. [7]
Now we will put forward an argument that intrinsic redshift quantization
(13b) could come from rotating quantum Hall effect. [5a]
It is argued by Fischer [5a] that “Hall quantization is of necessity derivable from a topological quantum number related to this (quantum) coherence.” He used total particle momentum [5a]:
(14)
p = mv + mΩ × r + qA
The uniqueness condition of the collective phase represented in (9) then
leads, if we take a path in the bulk of electron liquid, for which the integral
of mv can be neglected, to the quantization of the sum of a Sagnac flux, and
the magnetic flux [5a]:

Φ = q ∫ A ⋅ dr + m ∫ Ω × r ⋅ dr = ∫ ∫ B ⋅ dS = N v 2πh

(15)

This flux quantization rule corresponds to the fact that a vortex is fundamentally characterised by the winding number N alone [5a]. In this regard
the vortex could take the form of cosmic string [22]. Now it is clear from
(15) that quantized vortices could be formed by different source of flux.
After a few more reasonable assumptions one could obtain a generalised
Faraday law, which in rotating frame will give in a non-dissipative Hall state
the quantization of Hall conductivity [5a].
Therefore one could observe that it is quite natural to interpret the quantized distance between galaxies (11) as an implication of quantum Hall effect in rotating frame (15). While this proposition requires further observation, one could think of it in particular using known analogy between condensed matter physics and cosmology phenomena. [10][22] If this proposition corresponds to the facts, then one could think that redshift quantization
is an imprint of generalized quantization in various scales from microphysics
to macrophysics, just as Tifft once put it [2]:
“The redshift has imprinted on it a pattern that appears to have its origin
in microscopic quantum physics, yet it carries this imprint across cosmological boundaries.”
In the present paper, Tifft’s remark represents natural implication of topological quantization, which could be formed at any scale [5]. We will explore further this proposition in the subsequent section, using Weyl quantization.
Furthermore, while this hypothesis is very new, it could be expected that
we can draw some new prediction, for instance, like possibility to observe
small ‘blue-shift’ effect generated by the Hall effect from antivortexgalaxies. [23] Of course, in order to observe such a ‘blue-shift’ one shall first
exclude other anomalous effects of redshift phenomena [6].
One could expect that further observation in particular in the area of lowenergy neutrino will shed some light on this issue.[20] In this regard, one
could view that the Sun is merely a remnant of a neutron star in the past,
therefore it could be expected that it also emits neutrino similar to neutron
star [21].
An alternative interpretation of astrophysical quantization from Weyl
quantization. Graph and quantization.
An alternative way to interpret the above proposition concerning topological quantum number and topological quantization [5a], is by using Weyl
quantization.

Quantization in Astrophysics ...

244

5

Plausible explanation of quantization of intrinsic redshift from Hall effect ….

In this regards, Castro [8, p.5] has shown recently that one could derive
Schrödinger equation from Weyl geometry using continuity equation:

1
∂ρ
+
∂i
∂t
g

(

g ρv i

)

(16)

And Weyl metric:

RWeyl = (d − 1)(d − 2) Ak A k − 2(d − 1)∂ k A k

(

)

(17)

Therefore one could expect to explain astrophysical quantization using
Weyl method in lieu of using generalised Schrödinger equation as Nottale
did [4]. To our knowledge this possibility has never been explored before
elsewhere.
For instance, it can be shown that one can obtain Bohr-Sommerfeld type
quantization rule from Weyl approach [24, p.12], which for kinetic plus
potential energy will take the form:
∞

2πNh = ∑ h j S j ( E )

(18)

j =0

Which can be solved by expressing

E = ∑ h k E k as power series in h .

[24]. Now equation (9) could be rewritten as follows:
∞

j
∫ p ⋅ dr = N v 2πh = ∑ h S j ( E )

(19)

j =0

Or if we consider quantum Hall effect, then equation (15) can be used instead of equation (9), which yields:
∞

Φ = q ∫ A ⋅ dr + m ∫ Ω × r ⋅ dr = ∫ ∫ B ⋅ dS = ∑ h j S j ( E )

(19a)

j =0

The above method is known as ‘graph kinematic’ [25] or Weyl-Moyal’s
deformation quantization [26]. We could also expect to find Hall effect
quantization from this deformation quantization method.
Consider a harmonic oscillator, which equation can be expressed in the
form of deformation quantization instead of Schrödinger equation [26]:
2
2


  x + ih ∂ p  +  p − ih ∂ x  − 2 E  f ( x, p ) = 0


2
2 
 



(20)

This equation could be separated to become two simple PDEs. For imaginary part one gets [26]:
x∂ p − p∂ x f = 0
(21)

(

)

Now, considering Hall effect, one can introduce our definition of total
particle momentum (14), therefore equation (21) may be written:
x∂ p − (mv + mΩ × r + qA)∂ x f = 0
(22)

(

)

Our proposition here is that in the context of deformation quantization it
is possible to find quantization solution of harmonic oscillator without
Schrödinger equation. And because it corresponds to graph kinematic [25],
generalized Bohr-Sommerfeld quantization rule for quantized vortices (19)
in astrophysical scale could be viewed as signature of ‘super-graph’ quantization.

Quantization in Astrophysics ...

245

6

F. Smarandache & V. Christianto

This proposition, however, deserves further theoretical considerations.
Further experiments are also recommended in order to verify and explore
further this proposition.
Concluding remarks
In a recent paper, Moffat [1] has used Gross-Pitaevskii in his ‘phion condensate fluid’ to describe CMB spectrum data. We extend this proposition to
explain Tifft redshift quantization from the viewpoint of topological quantized vortices. In effect we consider that the intrinsic redshift quantization
could be interpreted as result of Hall effect in rotating frame.
Another alternative to explain redshift quantization is to consider quantized vortices from the viewpoint of Weyl quantization which could yield
Bohr-Sommerfeld quantization.
It is recommended to conduct further observation in order to verify and
also to explore various implications of our propositions as described herein.

Acknowledgment
The writers would like to thank to Profs. C. Castro, T. Love, E. Scholz, D.
Rabounski, and A. Kaivarainen for valuable discussions.

References
[1] Moffat, J., arXiv:astro-ph/0602607 (2006); [1a] Consoli, M., arXiv:hepph/0109215 (2001); [1b] Consoli, M. et al, arXiv:physics/0306094

[2] Humphreys, TJ Archive Vol.16 http://answersingenesis.org/tj/v16/i2/ (2002)

[3]

[4]
[5]
[6]

[7]
[8]

[9]
[10]
[11]

http://www.answersingenesis.org/home/area/magazines/tj/docs/TJv16n2_CEN
TRE.pdf
Smarandache, F., & V. Christianto, ‘A note on geometric and information
fusion interpretation of Bell theorem and quantum measurement,’ submitted to
Progress in Physics (2006), http://www.ptep-online.com
Smarandache, F., & V. Christianto, Progress in Physics Vol.2 No. 2 (Apr.
2006), http://www.ptep-online.com
Fischer, U., arXiv:cond-mat/9907457 (1999); [5a] arXiv:cond-mat/0004339
Bell, M.B., arXiv: astro-ph/0111123 (2001); [6a] Bell, M.B.,
http://arxiv.org/PS_cache/astro-ph/pdf/0305/0305112.pdf; [6b] Bell, M.B.,
arXiv: astro-ph/0305060 (2003).
Setterfield, B., http://www.journaloftheoretics.com, www.setterfield.org
Castro, C. & J. Mahecha, ‘On Nonlinear Quantum Mechanics, Brownian
motion, Weyl Geometry, and Fisher Information,’ Progress in Physics Vol.2
No. 1 (Jan. 2006), http://www.ptep-online.com
Schrieffer, J.R., “Macroscopic quantum phenomena from pairing in
superconductors,” Lecture, December 11th, 1972.
Zurek, W. (ed.), in Proc. Euroconference in Formation and Interaction of
Topological Defects, Plenum, 1995, arXiv :cond-mat/9502119 (1995).
Anandan, J.S., in Quantum Coherence and Reality, Proc. Conf. Fundamental
Aspects of Quantum Theory, Columbia SC., edited by J.S. Anandan & J.L.
Safko, (World Scientific 1994). arXiv:gr-qc/9504002 (1995).

Quantization in Astrophysics ...

246

Plausible explanation of quantization of intrinsic redshift from Hall effect ….

7

[12] Rauscher, E.A., & R. Amoroso, The physical implications of multidimensional

[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]
[26]

geometries and measurement, Inter. J. of Comp. Anticipatory Systems, D.
Dubois (ed.) (2006)
Chiao, R., et al., arXiv:physics/0309065 (2003).
Dinu, arXiv:math.AP/0511184 (2005)
Kravchenko, V., arXiv:math.AP/0408172 (2004)
Lipavsky, P., et al., arxiv:cond-mat/0111214 (2001)
de Haas, E.P., “A renewed theory of electrodynamics in the framework of Dirac
ether,” PIRT (2005), http://www.physics.nl
Masreliez, J., Apeiron Vol. 12 (2005)
Marc, L-R., & J-P. Luminet, arXiv:hep-th/9605010 (1996) 59, 116.
Lanou, R., arXiv:hep-ex/9808033 (1998).
Yakovlev, D., et al., arXiv:astro-ph/0012122 (2001).
Volovik, G., arXiv:cond-mat/0507454 (2005).
Balents, L., et al., arXiv:cond-mat/9903294 (1999).
Gracia-Saz, A., Ann. Inst. Fourier, Grenoble, Vol. 55, 5 (2005) 1001-1008.
Asribekov, V.L., arXiv:physics/0110026 (2001).
Zachos, C., arXiv:hep-th/0110114 (2001).

First version: 18th Aug. 2006, 1st revision 21th Aug. 2006.

Quantization in Astrophysics ...

247

EJTP 3, No. 12 (2006) 117–144

Electronic Journal of Theoretical Physics

A New Wave Quantum Relativistic Equation from
Quaternionic Representation of Maxwell-Dirac
Isomorphism as an Alternative to
Barut-Dirac Equation
V. Christianto∗
V. Christianto Via Florentin Smarandache,
Department of Mathematics, University of New Mexico Gallup, NM 87301, USA
Reprinted with kind permission from EJTP editor

Received 5 November 2005, Accepted 5 January 2006, Published 20 September 2006
Abstract: It is known that Barut’s equation could predict lepton and hadron mass
with remarkable precision. Recently some authors have extended this equation, resulting
in Barut-Dirac equation. In the present article we argue that it is possible to derive
a new wave equation as alternative to Barut-Dirac’s equation from the known exact
correspondence (isomorphism) between Dirac equation and Maxwell electromagnetic equations
via biquaternionic representation. Furthermore, in the present note we submit the viewpoint
that it would be more conceivable if we interpret the vierbein of this equation in terms of
superfluid velocity, which in turn brings us to the notion of topological electronic liquid. Some
implications of this proposition include quantization of celestial systems. We also argue that it
is possible to find some signatures of Bose-Einstein cosmology, which thus far is not explored
sufficiently in the literature. Further experimental observation to verify or refute this proposition
is recommended.
c Electronic Journal of Theoretical Physics. All rights reserved.
°
Keywords: Relativistic Quantum Mechanics, Barut’s equation, Barut-Dirac’s equation
PACS (2006): 03.65.Pm, 03.65.Ca, 11.10.-z

1.

Introduction

It is known that Barut’s equation could predict lepton and hadron mass with remarkable precision [1]. A plausible extension of Barut’s equation is by using Barut-Dirac ’s
model via inclusion of electron self-field. Furthermore, a number of authors has extended
∗

vxianto@yahoo.com

Quantization in Astrophysics ...

248

118

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

this equation using non-linear field theory [2a][5][5a]. Barut’s equation is as follows [5a]:
£
¤
iγν ∂ν − a∂µ2 /m + κ Ψ = 0
(1)
where ∂ν = ∂/∂xν and repeated indices imply a summation [5a]. The remaining parameters come from substitution of variables: m = κ/α1 and a/m = −α2 /α1 [5a]. In the
meantime Barut-Dirac-Vigier’s equation could be written as:
£
¤
cα.p − E + β(mc2 + ∈ e2 /r) Ψ = −[(∈ α~e2 )/(4πmc2 r2 )]iβαΨ
(2)
Despite this apparently remarkable result of Barut’s equation, nonetheless there is question concerning the physical meaning of his equation, in particular from the viewpoint of
non-linear field theory [2a]. This question seems very interesting, in particular considering the unsolved question concerning the physical meaning of wavefunction in Quantum
Mechanics [4a]. It is known that some proponents of ‘realism’ interpretation of Quantum Mechanics predict that there should be a complete ‘realism’ description of physical
model of electron, where non-local hidden variables could be included [4][1a]. We consider
that this question remains open for discussion, in particular in the context of plausible
analog between classical electrodynamics and non-local quantum interference effect, via
Aharonov-Casher effect [8].
In the present article we argue that it is possible to derive a new wave quantum
relativistic equation as an alternative to Barut-Dirac-Vigier’s equation. Our description
is based on the known exact correspondence (isomorphism) between Dirac equation and
Maxwell electromagnetic equations via biquaternionic representation. In fact, we will
discuss five approaches as alternative to Barut-Dirac equation. And we would argue that
the question of which of these approaches is the most consistent with experimental data
remains open. Our proposition of alternative to Barut(-Dirac) equation was based on
characteristics of Barut equation:
• it is a second-order differential equation (1);
• it shall include the physical meaning of vierbein in quantum mechanical equation;
• it has neat linkage with other known equations in Quantum Mechanics including
Dirac equation [5a], while its solution could be different from Dirac approach [11];
• our observation asserts that it shall also include a proper introduction of Lorentz
force, and acceleration from relativistic fluid dynamics.
Furthermore, in the present note we submit the viewpoint that it would be more conceivable if we interpret the vierbein of this equation in terms of superfluid velocity [12][13],
which in turn brings us to the notion of topological electronic liquid [27]. Its implications to quantization of celestial systems lead us to argue in favor of signatures of
Bose-Einstein cosmology, which thus far has not been explored sufficiently yet in the
literature [49a][49b].
What we would argue in the present note is that one could expect to extend further this quaternion representation into the form of unified wave equation, in particular
using Ulrych’s representation [7]. While such an attempt to interpret vierbein of Dirac
equation has been made by de Broglie (in terms of ‘Dirac fluid’ [41]), it seems that an

Quantization in Astrophysics ...

249

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

119

exact representation in terms of superfluid velocity has never been made before. From
this viewpoint one could argue that the superfluid vierbein interpretation will make the
picture resembles superfluid bivacuum model of Kaivarainen [20][21]. Furthermore, this
proposition seems to support previous hypothetical argument by Prof. J-P. Vigier on the
further development of theoretical Quantum Mechanics [6]:
“..a revival, in modern covariant form, of the ether concept of the founding fathers
of the theory of light (Maxwell, Lorentz, Einstein, etc.). This is a crucial question, and
it now appears that the vacuum is a real physical medium, which presents surprising
properties (superfluid, i.e. negligible resistance to inertial motions) . . . “
Provided this proposition of unified wave equation in terms of superfluid velocity
vierbein corresponds to the observed facts, and then it could be used to predict some
new observations, in particular in the context of condensed-matter analog of astrophysics
[16][17][18]. Therefore in the last section we will extend this proposition to argue in
favor of signatures of Bose-Einstein cosmology, including some recent relevant observation
supporting this argument.
While quaternionic Quantum Mechanics has been studied before by Adler etc. [14c][28],
and also biquaternionic Quantum Mechanics [2][3], it seems that interpreting the righthand-side of the unified wave equation as superfluid 4-velocity has not been considered
before, at least not yet in the context of cylindrical relativistic fluid of Carter and SklarzHorwitz.
In deriving these equations we will not rely on exactitude of the solutions, because
as we shall see the known properties, like fine structure constant of hydrogen, can be
derived from different approaches [11][15][19][22a]. Instead, we will use ‘correspondence
between physical theories’ as a guiding principle, i.e. we argue that it is possible to derive
some alternatives to Barut equation via generalization of various wave equations known
in Quantum Mechanics. More linkage between these equations implies consistency.
Further experimental observation to verify or refute this proposition is recommended.

2.

Biquaternion, Imaginary algebra, Unified relativistic Wave
Equation

Before we discuss biquaternionic Maxwell equations from unified wave equation, first
we should review Ulrych’s method [7] by defining imaginary number representation as
follows [7]:
x = x0 + j.x1 ,

j 2 = −1

(3)

This leads to the multiplication and addition (or substraction) rules for any number,
which is composed of real part and imaginary number:

Quantization in Astrophysics ...

(x ± y) = (x0 ± y0 ) + j.(x1 ± y1 ),

(4)

(xy) = (x0 y0 + x1 y1 ) + j.(x0 y1 + x1 y0 ).

(5)

250

120

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

From these basic imaginary numbers, Ulrych [7] argues that it is possible to find a new
relativistic algebra, which could be regarded as modified form of standard quaternion
representation.
Once we define this imaginary number, it is possible to define further some relations
as follows [14]. Given w = x0 + j.x1 , then its D-conjugate of w could be written as:
w̄ = x0 − j.x1

(6)

Also for any given two imaginary numbers w1 , w2 ∈ D, we get the following relations [14]:
w1 + w2 = w̄1 + w̄2

(7)

w1 • w2 = w̄1 • w̄2

(8)

|w|2 = w̄ • w = x20 − x21

(9)

|w1 • w2 |2 = |w1 |2 • |w2 |2

(10)

All of these provide us nothing new. For extension of these imaginary numbers in Quantum Mechanics, see [33]. Now we will review a few elementary definitions of quaternions
and biquaternions, which are proved to be useful.
It is known that biquaternions could describe Maxwell equations in its original form,
and some of the use of biquaternions was discussed in [2][34].
Quaternion number, Q is defined by [33][60]:
Q = a + b.i + c.j + d.k

a, b, c, d ∈ R,

(11a)

where
i2 = j 2 = k 2 = ijk = −1

(11b)

Alternatively, one could extend this quaternion number to Clifford algebra [3a][3][6][25][41],
because higher-dimensions Clifford algebra and analysis give the possibility to generalize
the factorisations into higher spatial dimensions and even to space-time domains [70a].
In this regard quaternions H ∼ C`0,2 , while standard imaginary numbers C∼ C`0,1
[70a].
Biquaternion is an extension of this quaternion number, and it is described here using
Hodge-bracket operator, in lieu of known Hodge operator (∗∗ = −1) [5a]:
{Q}∗ = (a + iA) + (b + iB).i + (c + iC).j + (d + iD).k,

(12a)

where the second part (A,B,C,D) is normally set to zero in standard quaternions [33].
For quaternion differential operator, we define quaternion Nabla operator:
~
∇q ≡ c−1 .∂/∂t + (∂/∂x)i + (∂/∂y)j + (∂/∂z)k = c−1 .∂/∂t + ~i.∇

(12b)

And for biquaternion differential operator, we define a quaternion Nabla-Hodgebracket operator:
~
{∇q }∗ ≡ (c−1 .∂/∂t + c−1 .i∂/∂t) + {∇}∗
(12c)

Quantization in Astrophysics ...

251

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

121

where Nabla-Hodge-bracket operator is defined as:
~
{∇}∗
≡ (∂/∂x + i∂/∂X).i + (∂/∂y + i∂/∂Y ).j + (∂/∂z + i∂/∂Z).k.

(13a)

It is worthnoting here that equations (4)-(10) are also applicable for biquaternion number.
While equations (3)-(12a) are known in the existing literature [33][59], and sometimes
called ‘biparavector’ (Baylis), we prefer to call it ‘imaginary algebra’ with emphasis on
the use of Hodge-bracket operator. It is known that determinant and differentiation of
quaternionic equations are different from standard differential equations [59], therefore
solution for this problem has only been developed in recent years.
The Hodge-bracket operator proposed herein could become more useful if we introduce
quaternion number (11a) in the paravector form [70]:
~q =

3
X

qk .ek when {qk } ⊂ C, {ek |k = 1, 2, 3 }

(13b)

k=0

and e0 is the unit. Therefore, biquaternion number could be written in the same form
[70]:
3
3
X
X
qk .ek }
(13c)
qk .ek + i{
{~q}∗ = ~q + i~q =
k=0

k=0

Now we are ready to discuss Ulrych’s method to describe unified wave equation [7], which
argues that it is possible to define a unified wave equation in the form [7]:
Dφ(x) = m2φ .φ(x),
where unified (wave) differential operator D is defined as:
h
¡
¢µ i
D = (P − qA)µ P̄ − qA
.

(14)

(15)

To derive Maxwell equations from this unified wave equation, he uses free photon fields
expression [7]:
DA(x) = 0,
(16)
where potential A(x) is given by:
A(x) = A0 (x) + jA1 (x),

(17)

E i (x) = −∂ 0 Ai (x) − ∂ i A0 (x),

(18)

B i (x) =∈ijk ∂j Ak (x).

(19)

and with electromagnetic fields:

Inserting these equations (17)-(19) into (16), one finds Maxwell electromagnetic equation
[7]:
−∇ • E(x) − ∂ 0 C(x)
+ij∇ • B(x)

(20)

−j(∇xB(x) − ∂ 0 E(x) − ∇C(x))
−i(∇xE(x) + ∂ 0 B(x)) = 0

Quantization in Astrophysics ...

252

122

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

The gauge transformation of the vector potential A(x) is given by [7]:
A0 (x) = A(x) + ∇Λ(x)/e,

(21)

where Λ(x) is a scalar field. As equations (17)-(18) only use simple definitions of imaginary numbers (3)-(5), then an extension from (20) and (21) to biquaternionic form of
Maxwell equations is possible [2][34].
In order to define biquaternionic representation of Maxwell equations, we could extend
Ulrych’s definition of unified differential operator [7] to its biquaternion counterpart, by
using equation (12a), to become:
h
¡
¢µ i
{D}∗ ≡ ({P } ∗ −q{A}∗)µ {P̄ } ∗ −q{A}∗
,
(22a)
or by definition P = −i~∇and (13a), equation (22a) could be written as:
h
i
µ
{D}∗ ≡ (−~{∇} ∗ −q{A}∗)µ (−~{∇} ∗ −q{A}∗) ,

(22b)

where each component is now defined in its biquaternionic representation. Therefore the
biquaternionic form of unified wave equation takes the form:
{D} ∗ φ(x) = m2φ .φ(x),

(23)

if we assume the wavefunction is not biquaternionic, and
{D} ∗ {φ(x)}∗ = m2φ .{φ(x)} ∗ .

(24)

if we suppose that the wavefunction also takes the same biquaternionic form.
Now, biquaternionic representation of free photon fields could be written in the same
way with (16), as follows:
{D} ∗ A(x) = 0
(25)
We will not explore here complete solution of this biquaternion equation, as it has been
discussed in various literatures aforementioned above, including [2][33][34][59]. However,
immediate implications of this biquaternion form of Ulrych’s unified equation can be
described as follows.
Ulrych’s fermion wave equation in the presence of electromagnetic field reads [7]:
h
¡
¢µ i
(P − qA)µ P̄ − qA ψ = −m2 .ψ,
(26)
which asserts c=1 (conventionally used to write wave equations). In accordance with
Ulrych [7] this equation implies that the differential operator of the quantum wave equation (LHS) is composed of the momentum operator P multiplied by its dual operator,
and taking into consideration electromagnetic field effect qA. And by using definition of
momentum operator:
P = −i~∇.
(27)

Quantization in Astrophysics ...

253

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

123

So we get three-dimensional relativistic wave equation [7]:
[(−i~∇µ − qAµ ) (−i~∇µ − qAµ ) ψ] = −m2 .c2 .ψ.

(28)

which is Klein-Gordon equation. Its 1-dimensional version has also been derived by
Nottale [67, p,29]. A plausible extension of equation (28) using biquaternion differential
operator defined above (22a) yields:
[(−~{∇µ } ∗ −q{Aµ }∗) (−~{∇µ } ∗ −q{Aµ }∗) ψ] = −m2 .c2 .ψ,

(29)

which could be called as ‘biquaternionic’ Klein-Gordon equation.
Therefore we conclude that there is neat correspondence between Ulrych’s fermion
wave equation and Klein Gordon equation, in particular via biquaternionic representation. It is also worthnoting that it could be shown that Schrodinger equation could be
derived from Klein-Gordon equation [11], and Klein-Gordon equation also neatly corresponds to Duffin-Kemmer-Petiau equation. Furthermore it could be proved that modified
(quaternion) Klein-Gordon equation could be related to Dirac equation [7]. All of these
linkages seem to support argument by Gursey and Hestenes who find plenty of interesting features using quaternionic Dirac equation. In this regard, Meessen has proposed a
method to describe elementary particle from Klein-Gordon equation [30].
By assigning imaginary numbers to each component [7, p.26], equation (26) could be
rewritten as follows (by writing c=1):
h
i
µ
i
i
2
(P − qA)µ (P − qA) − eE ijσi − eB σi + m ψ = 0,
(30)
where Pauli matrices σi are written explicitly. Now it is possible to rewrite equation
(30) in complete tensor formalism [7], if Pauli matrices and electromagnetic fields are
expressed with antisymmetric tensor, so we get:
h
i
¡
¢µ
(P − qA)µ P̄ − qA − eσµν F µν + m2 ψ = 0,
(31)
where
Fµν = (∂µ Aν − ∂ν Aµ ).

(32)

Note that equation (31) is formal identical to quadratic form of Dirac equation [7], which
supports argument suggesting that modified (quaternion) Klein-Gordon equation could
be related to Dirac equation. Interestingly, equation (31) is also known in the literature
as Feynman-Gell-Mann’s equation, and its implications will be discussed in subsequent
section [5]. Interestingly, if we neglect contribution of the electromagnetic field (q and e)
component, and using only 1-dimensional of the partial differentiation, one gets a wave
equation from Feynman rules [56, p.6]:
¡

¢
∂µ ∂ µ + m2 Ψ = 0,

(33)

which has been used to describe quantum-electrodynamics without renormalization [56].

Quantization in Astrophysics ...

254

124

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

Further extension of equation (28) could be made by expressing it in terms of 4velocity:
[(−i~∇µ − qAµ ) (−i~∇µ − qAµ ) ψ] = −pµ pµ .ψ.
(34)
In the context of relativistic fluid [10][11], one could argue that this 4-velocity corresponds
to superfluid vierbein [13][16][17]. Therefore we could use Carter-Langlois’ equation [12]:
µρ .µρ = −c2 .µ2 ,

(35)

by replacing m with the effective mass variable µ. This equation has the meaning of
cylindrically symmetric superfluid with known metric [12]:
gρσ .dxρ .dxσ = −c2 .dt2 + dz 2 + r2 .dφ2 + dr2 .

(36)

Further extension of equation (35) is possible, as discussed by Fischer [13], where the
effective mass variable term also appears in the LHS of velocity equation, by defining
momentum of the continuum as:
pα = µ.uα .
(37)
Therefore equation (35) now becomes:
µ2 .uα .uα = −c2 .µ2 ,

(38)

where the effective mass variable now acquires the meaning of chemical potential [13]:

and

µ = ∂ ∈ /∂ρ,

(39)

¡
¢
ρ.pα /µ = K/~2 pα = jα ,

(40)

K = ~2 (ρ/µ) .

(41a)

The quantity K is defined as the stiffness coefficient against variations of the order parameter phase. Alternatively, from macroscopic dynamics of Bose-Einstein condensate
containing vortex lattice, one could write the chemical potential in the form [57]:
£
¤2/5
µ = µ0 . 1 − (Ω0 /ω⊥ )2

(41b)

where the quantity Ω corresponds to the angular frequency of the sample and is assumed
to be uniform, ω is the oscillator frequency, and chemical potential in the absence of
rotation is given by [57]:
µ0 = (~ωho /2) (N a/0.0667aho )2/5

(41c)

and N represents the number of atoms and a is the corresponding oscillator length [57]:
aho =

Quantization in Astrophysics ...

p

~/M ωho

255

(41d)

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

125

Now the sound speed cs could be related to the equations above, for a barotropic fluid
[13], as:
¡
¢
cs = d (ln µ) /d (ln ρ) = K/~2 d2 ∈ /dρ2 .
(42)
Using this definition, then equation (42) could be rewritten as follows:
¡
¢
pα = K −1 ~2 jα = (jα /cs ).d2 ∈ /dρ2 ,

(43)

Introducing this result (43) into equation (34), we get:
¡
¢2
[(−i~∇µ − qAµ ) (−i~∇µ − qAµ ) ψ] = − (jα /cs ).d2 ∈ /dρ2 .ψ

(44)

which is an alternative expression of relativistic wavefunction in terms of superfluid sound
speed, cs . Note that this equation could appear only if we interpret 4-velocity in terms
of superfluid vierbein [11][12]. Therefore this equation is Klein-Gordon equation, where
vierbein is defined in terms of superfluid velocity. Alternatively, in condition without
electromagnetic charge, then we can rewrite equation (44) in the known form of standard
Klein-Gordon equation [36]:
¡
¢2
[Dµ Dµ ψ] = − (jα /cs ).d2 ∈ /dρ2 .ψ.
(45)
Therefore, this alternative representation of Klein-Gordon equation (45) has the physical
meaning of relativistic wave equation for superfluid phonon [37][38].
A plausible extension of (44) is also possible using our definition of biquaternionic
differential operator (22a):
¡
¢2
{D} ∗ ψ = − (jα /cs ).d2 ∈ /dρ2 ψ

(46)

which is an alternative expression from Ulrych’s [7] unified relativistic wave equation,
where the vierbein is defined in terms of superfluid sound speed, cs . This is the main
result of this section. As alternative, equation (46) could be written in compact form:
[{D} ∗ +Γ]Ψ = 0,
where the operator Γ is defined according to the quadratic of equation (43):
¢2
¡
Γ = (jα /cs ).d2 ∈ /dρ2 .

(47)

(48a)

For the solution of equation (44)-(47), one could refer for instance to alternative description of quarks and leptons via SU(4) symmetry [28][58]. As we note above, equation
(31) is also known in the literature as Feynman-Gell-Mann’s equation, and it has been
argued that it has neat linkage with Barut equation [5]. This assertion could made more
conceivable by noting that equation (31) is quadratic form of Dirac equation. In this
regard, recently Kruglov has considered a plausible generalization of Barut equation via
third-order differential extension of Dirac equation [60]:
(γµ ∂µ + m1 ) (γν ∂ν + m2 ) (γα ∂α + m3 ) ψ = 0.

Quantization in Astrophysics ...

256

(48b)

126

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

It is also interesting to note that in his previous work, Kruglov [60a] has argued in favor
of Dirac-Kahler equation:
(d − δ + m) ψ = 0,
(48c)
where the operator (d − δ) is the analog of Dirac operator γµ ∂ µ . It seems plausible,
therefore, in the context of Kruglov’s recent attempt to generalize Barut equation [60] to
argue that further generalization to biquaternionic form is possible by rewriting equation
(47) in the third-order equation, by using our definition (12c):
~ q } ∗ +pν ][{∇
~ q } ∗ +pα ]Ψ = 0.
~ q } ∗ +pµ ][{∇
[{∇
α
µ
ν

(48d)

Therefore, we could consider this equation as the first alternative to (generalized) Barut
equation. Note that we use here equation (12c) instead of (22a), in accordance with
Kruglov [60] definition:
∂ν = ∂/∂xν = (∂/∂xm , ∂/∂(it))
(48e)
In subsequent sections, we will consider a number of other plausible alternatives to BarutDirac’s equation, in particular from the viewpoint of superfluid vierbein.

3.

Alternative #2: Barut-Dirac-Feynman-Gell-Mann Equation

It is argued [5, p. 4] that Barut equation is the sum of Dirac equation and FeynmanGell-Mann’s equation (31). But from the aforementioned argument, it should be clear
that the Feynman-Gell-Mann’s equation is nothing more than Ulrych’s fermion wave
equation, which is indeed a quadratic of Dirac equation. Therefore, it seems that there
should be other route to derive Barut-Dirac type equation. In this regard, we submit
the viewpoint that the introduction of electron self-field would lead to an alternative of
Barut equation.
First, let us rewrite equation (31) with assigning the real c in lieu of c=1:
h
i
¡
¢µ
(P − qA)µ P̄ − qA − eσµν F µν + m2 c2 ψ = 0,
(49)
By using equation (34), then Feynman-Gell-Mann’s equation becomes:
[(−i~∇µ − qAµ ) (−i~∇µ − qAµ ) − eσµν F µν + pµ pµ ] Ψ = 0,

(50)

[(−i~∇µ − qAµ ) (−i~∇µ − qAµ ) + pµ pµ ] Ψ = (eσµν F µν )Ψ,

(51)

or
which can be called Feynman-Gell-Mann’s equation with superfluid vierbein interpretation, in particular if we then introduce equation (43) into the LHS.
In this regard, we can introduce Ibison’s description of electron self-energy from ZPE
[38]:
£
¤
(52)
eσµν F µν = m0 aµ − m0 τ0 daλ /dτ + aλ aλ uµ /c2
where
τ0 = e2 /6πε0 m0 c3

Quantization in Astrophysics ...

257

(53)

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

127

The first term in the right hand side of equation (52) could be written in the Lorentz
form [42] [24a, p.12]:
m0 aµ = m[dv/dt] = e[E + vxB]
(54)
where:
E = −∇φ,

(55)

B = ∇xA.

(56)

Therefore, by defining a new parameter [24a, p.12]:
£
¤
∀ = e[E + vxB]µ − m0 (e2 /6πε0 m0 c3 ) daλ /dτ + aλ aλ uµ /c2 ,
one could rewrite equation (51) in term of equation (43):
h
¡
¢i
µ
µ
2
2 2
(−i~∇µ − qAµ ) (−i~∇ − qA ) + (jα /cs ).d ∈ /dρ
Ψ = ∀Ψ,

(57)

(58)

which could be regarded as a second alternative expression of Barut equation. Therefore
we propose to call it Barut-Dirac-Feynman-Gell-Mann equation. Implications of this
equation should be verified via experiments, in particular with condensed-matter physics.

4.

Alternative #3: Second Order Differential Form of SchrödingerType Equation

It is known that Barut equation is a typical second-order differential equation, which
is therefore non-linear. Therefore a good alternative to Barut equation could be derived
from similar approach with Schrödinger’s original equation, but this time it should be
differentiated twice.
In this regard, it seems worthnoting here that it is more proper to use Noether’s expression of total energy in lieu of standard derivation of Schrödinger’s equation (E = p~2 /2m).
According to Noether’s theorem [39], the total energy of the system corresponding to the
time translation invariance is given by:
Z ∞
¢
¡ 2
2
E = mc + (cw/2).
γ .4πr2 .dr = kµc2
(59)
0

where k is dimensionless function. It could be shown, that for low-energy state the total
energy could be far less than E = mc2 . Interestingly Bakhoum [22] has also argued in
favor of using E = mv 2 for expression of total energy, which expression could be traced
back to Leibniz. Therefore it seems possible to argue that expression E = mv 2 is more
generalized than the standard expression of special relativity, in particular because the
total energy now depends on actual velocity [39].
From this new expression, it is plausible to rederive quantum relativistic wave equation
in second-order differential expression, and it turns out the new equation should also
include a Lorentz-force term in the same way of equation (57). This feature is seemingly
interesting, because these equations are derived from different approach from (57).

Quantization in Astrophysics ...

258

128

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

We start with Bakhoum’s assertion that it is more appropriate to use E = mv 2 ,
instead of more convenient form E = mc2 . This assertion would imply [22]:
H 2 = p2 .c2 − m2o .c2 .v 2 .

(60)

Therefore, for phonon speed (cs ) in the limit p → 0, we write [37]:
E(p) ≡ cs . |p| .

(61)

A bit remark concerning Bakhoum’s expression, it does not mean to imply or to interpret
E = mv 2 as an assertion that it implies zero energy for a rest mass. Actually the problem
comes from ’mixed’ interpretation of what we mean with ’velocity’. In original Einstein’s
paper (1905) it is defined as ’kinetic velocity’, which can be measured when standard
’steel rod’ has velocity approximates the speed of light. But in quantum mechanics,
we are accustomed to make use it deliberately to express ’photon speed’=c. According
to Bakhoum, to get a consistent interpretation between special relativity and quantum
mechanics, we should treat this definition of velocity according to its context, in particular
to its linkage with electromagnetic field. Therefore, in special relativity 1905 paper, it
should be better to interpret it as ’speed of free electron’, which approximates c. For
muon, Spohn [42] has obtained v=0.9997c which is very near to c, but not exactly =c.
For hydrogen atom with 1 electron, the electron occupies the first excitation (quantum
number n=1), which implies that their speed also approximate c, which then it is quite
safe to assume E ∼ mc2 . But for atoms with large amount of electrons occupying large
quantum numbers, as Bakhoum showed that electron speed could be far less than c,
therefore it will be more exact to use E = mv 2 , where here v should be defined as
’average electron speed’. Furthermore, in the context of relativistic fluid, we could use
Eα = µ.uα uα from equation (37).
In the first approximation of relativistic wave equation, we could derive Klein-Gordontype relativistic equation from equation (60), as follows. By introducing a new parameter:
ζ = i(v/c),

(62)

then we can rewrite equation (60) in the known procedure of Klein-Gordon equation:
E 2 = p2 .c2 + ζ 2 m2o .c4 ,

(63)

where E = mv 2 . [22] By using known substitution:
E = i~.∂/∂t,

p = ~∇/i,

(64)

and dividing by (~c)2 , we get Klein-Gordon-type relativistic equation:
0

2

−c−2 ∂Ψ/∂t + ∇2 Ψ = ko Ψ,

(65)

where
0

ko = ζmo c/~.

Quantization in Astrophysics ...

259

(66)

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

129

One could derive Dirac-type equation using similar method. But the use of new parameter
(62) seems to be indirect, albeit it simplifies the solution because here we can use the
same solution from Klein-Gordon equation [30].
Alternatively, one could derive a new quantum relativistic equation, by noting that
expression of total energy E = mv 2 is already relativistic equation. We will derive here
two approaches to get relativistic wave equation from this expression of total energy.
The first approach, is using Ulrych’s [7] method as follows:
E = mv 2 = p.v

(67)

Taking square of this expression, we get:
E 2 = p2 .v 2

(68)

p2 = E 2 /v 2

(69)

or
Now we use Ulrych’s substitution [7]:
h
¡
¢µ i
(P − qA)µ P̄ − qA
= p2 ,

(70)

and introducing standard substitution in Quantum Mechanics (64), one gets:
h
¡
¢µ i
(P − qA)µ P̄ − qA
Ψ = v −2 .(i~.∂/∂t)2 Ψ,

(71)

or

£

¤
(−i~∇µ − qAµ ) (−i~∇µ − qAµ ) − (i~/v.∂/∂t)2 Ψ = 0.

(72a)

which can be called as Noether-Ulrych-Feynman-Gell-Mann’s (NUFG) equation. This is
the third alternative to Barut-Dirac equation.
Alternatively, by using standard definition p=m.v, we can rewrite equation (71) in
form of equation (43):
h
¡
¢µ i
¡
¢−2
(P − qA)µ P̄ − qA
Ψ = m2 (jα /cs ).d2 ∈ /dρ2
.(i~.∂/∂t)2 Ψ.
(72b)
In order to verify that we can use the same method with Schrödinger equation to derive
nonlinear wave equation, let us consider Oleinik’s nonlinear wave equation. It is argued
that the proper equation of motion is not the Dirac or Schrödinger equation, but an equation with a new self-energy term [24]. This would mean that there is a pair wavefunction
to include electron interaction with its surrounding medium. Therefore, the standard
Schrödinger equation becomes nonlinear equations of motion [24]:
µ
¶
£
¤ Ψ(x)
2
¯
i∂/∂t + ∇ /2m − U (x)
=0
(73)
Ψ̄(x)
where we use ~ = 1 for convenience.
From this equation, one can get the relativistic version corresponding to Dirac equation [24]. Interestingly, Froelich [66] has considered equation of motion for the few-body

Quantization in Astrophysics ...

260

130

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

systems associated with the hydrogen-antihydrogen pairs using radial Schrödinger-type
equation. Therefore, it seems interesting to consider equation (73) also in the context of
hydrogen-antihydrogen molecule.
And because equation (73) is derived from the standard definition of total energy
E = p~2 /2m, then our method to use equation (60) seems to be a logical extension
of Oleinik’s method. To get nonlinear version similar to equation (73), then we could
rewrite equation (72a) as:
µ
¶
£
¤ Ψ(x)
µ
µ
2
(−i~∇µ − qAµ ) (−i~∇ − qA ) − (i~/v.∂/∂t)
= 0.
(74)
Ψ̄(x)
What’s more interesting here, is that Oleinik [24a, p.12] has shown that equation (73)
could lead to an expression of Newtonian-Lorentz force similar to equation (54):
m0 aµ = m[d2 r/dt2 ] = e[E + v × B]

(75)

This verifies our aforementioned proposition that a good alternative to Barut’s equation
should include a Lorentz-force term in wave equation. In other words, from equation
(73) we find neat linkage between Schrödinger equation, nonlinear wave, and Lorentzforce. We will use this linkage in the following section. It turns out that we can find
a proper generalization of Barut’s equation via introduction of Newtonian-acceleration
from velocity of the relativistic fluid in similar form of Lorentz force.

5.

Alternative #4: Lorentz-force & Newtonian Acceleration
Method

For the fourth method, we will introduce Leibniz rule [40] into equation (67) via
differentiation with respect to time, which yields:
dE/dt = d[p.v]/dt = v.[dp/dt] + p.[dv/dt]

(76)

The next step is taking derivation of the known substitution in QM:
dE/dt = i~.∂ 2 /∂t2 ,

(77)

˙
dp/dt = d(−i~∇)/dt = −i~∇
Now, substituting back equation (77) and (64) into equation (76), we get:
˙ − [dv/dt].i~∇)Ψ.
(i~.∂ 2 /∂t2 )Ψ = (v.[−i~∇]

(78)

At this point, we note that the second term in the right hand side of equation (78) could
be written in the Lorentz force form [42], and following equation (54):
[dv/dt] = e/m.(E + vxB)

(79)

E = −∇φ,

(80)

where:

Quantization in Astrophysics ...

261

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

B = ∇xA.

131

(81)

Therefore, we can rewrite equation (78) in the form:
˙ − e/m.[E + vxB].i~∇)Ψ,
(i~.∂ 2 /∂t2 )Ψ = (v.[−i~∇]

(82)

which is a new wave relativistic quantum equation as alternative to Barut equation.
To our present knowledge, this alternative wave equation (82) has never been derived
elsewhere.
As an alternative to equation (79), we can rewrite Lorentz form in term of Newtonian acceleration. In this regard, it is worthnoting that the definition of acceleration of
relativistic fluid is not widely accepted yet [10]. Therefore we will use here result from
relativistic field equations from Poisson process [46], from which we get an expression of
acceleration [46]:
dv/dt = ~/2m.(∂ 2 u/∂x2 ) − v.∂u/∂x + u.∂v/∂x − m−1 .∂V /∂x = ∃

(83)

Therefore, by substituting this equation into (78), we get:
˙ − ∃.i~∇)Ψ,
(i~.∂ 2 /∂t2 )Ψ = (v.[−i~∇]

(84)

which can be considered as a better alternative to equation (82).

6.

Alternative #5: Schrödinger-Ginzburg-Landau Equation and
Quantization of Celestial Systems

In the preceding section (#4), we have found the neat linkage between Schrödinger
equation, nonlinear wave, and Lorentz-force, which indicates a possibility to be considered
as alternative to Barut equation. Now, as the fifth alternative method, it will be shown
that we can expect to generalize Schrödinger equation to describe quantization of celestial
sytems. While this notion of macro-quantization is not widely accepted yet, as we will
see the logarithmic nature of Schrödinger equation is sufficient to ensure its applicability
to larger systems. As alternative, we will also discuss an outline for deriving Schrödinger
equation from simplification of Ginzburg-Landau equation. It is known that GinzburgLandau equation exhibits fractal character.
First, let us rewrite Schrödinger equation (73) in its common form:
£
¤
¯ 2 /2m − U (x) Ψ = 0
i∂/∂t + ∇
(85)
where we use ~ = 1for convenience, or
(i∂/∂t)Ψ = H.Ψ

(86)

Now, it is worthnoting here that Englman & Yahalom [4a] argue that this equation
exhibits logarithmic character:
ln Ψ(x, t) = ln (|Ψ(x, t)|) + i. arg(Ψ(x, t))

Quantization in Astrophysics ...

262

(87)

132

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

Schrödinger already knew this expression in 1926, which then he used it to propose
his equation called ‘eigentliche Wellengleichung’ [4a]. Therefore equation (85) can be
rewritten as follows:
¯ ln |Ψ| .∇
¯ arg[Ψ] + ∇.
¯ ∇
¯ arg[Ψ] = 0
2m(∂ ln |Ψ| /∂t) + 2∇

(88)

Interestingly, Nottale’s scale-relativistic method [43][44] was also based on generalization
of Schrödinger equation to describe quantization of celestial systems. It is known that
Nottale-Schumacher’s method [45] could predict new exoplanets in good agreement with
observed data. Nottale’s scale-relativistic method is essentially based on the use of firstorder scale-differentiation method defined as follows [43][44]:
∂V /∂(ln δt) = β(V ) = a + bV + ...

(89)

Now it seems clear that the logarithmic derivation, which is essential in scale-relativity
approach, also has been described properly in Schrödinger’s original equation [4a]. In
other word, its logarithmic form ensures applicability of Schrödinger equation to describe
macroquantization of celestial systems.
To emphasize this assertion of the possibility to describe quantization of celestial
systems, let us return for a while to the preceding section where we use Fischer’ description
[13] of relativistic momentum of 4-velocity (37)-(38). Interestingly Fischer [13] argues that
the circulation leading to equation (37)-(38) is in the relativistic dense superfluid, defined
as the integral of the momentum:
I
γs = pµ dxµ = 2π.Nv ~,
(90)
and is quantized into multiples of Planck’s quantum of action. This equation is the covariant Bohr-Sommerfeld quantization of γs . And then Fischer [13] concludes that the
Maxwell equations of ordinary electromagnetism can be cast into the form of conservation
equations of relativistic perfect fluid hydrodynamics [10], in good agreement with Vigier’s
guess as mentioned above. Furthermore, the topological character of equation (90) corresponds to the notion of topological electronic liquid, where compressible electronic liquid
represents superfluidity [27].
It is worthnoting here, because here vortices are defined as elementary objects in the
form of stable topological excitations [13], then equation (90) could be interpreted as
signatures of Bohr-Sommerfeld quantization from topological quantized vortices. Fischer
[13] also remarks that equation (90) is quite interesting for the study of superfluid rotation
in the context of gravitation. Interestingly, application of Bohr-Sommerfeld quantization
to celestial systems is known in literature [47][48], which here in the context of Fischer’s
arguments it seems plausible to suggest that quantization of celestial systems actually
corresponds to superfluid-quantized vortices at large-scale [27]. In our opinion, this result
supports known experiments suggesting neat correspondence between condensed matter
physics and various cosmology phenomena [16]-[19].

Quantization in Astrophysics ...

263

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

133

To make the conclusion that quantization of celestial systems actually corresponds to
superfluid-quantized vortices at large-scale a bit conceivable, let us consider an illustration
of quantization of celestial orbit in solar system.
In order to obtain planetary orbit prediction from this hypothesis we could begin with
the Bohr-Sommerfeld’s conjecture of quantization of angular momentum. This conjecture
may originate from the fact that according to BCS theory, superconductivity can exhibit
macroquantum phenomena [16][65]. In principle, this hypothesis starts with observation
that in quantum fluid systems like superfluidity, it is known that such vortexes are subject
H
to quantization condition of integer multiples of 2π, or vs .dl = 2π.n~/m4 . As we know,
for the wavefunction to be well defined and unique, the momenta must satisfy BohrSommerfeld’s quantization condition:
I
p.dx = 2π.n~
(91)
Γ

for any closed classical orbit Γ. For the free particle of unit mass on the unit sphere the
left-hand side is [49]:
ZT
v 2 .dτ = ω 2 .T = 2π.ω
(92)
0

where T=2π/ω is the period of the orbit. Hence the quantization rule amounts to quantization of the rotation frequency (the angular momentum):ω = n~. Then we can write
the force balance relation of Newton’s equation of motion [49]:
GM m/r2 = mv 2 /r

(93)

Using Bohr-Sommerfeld’s hypothesis of quantization of angular momentum, a new constant g was introduced:
mvr = ng/2π
(94)
Just like in the elementary Bohr theory (before Schrödinger), this pair of equations yields
a known simple solution for the orbit radius for any quantum number of the form [49]:
r = n2 .g 2 /(4π 2 .GM.m2 )

(95)

which can be rewritten in the known form [43][44]:
r = n2 .GM/vo2

(96)

where r, n, G, M, vo represents orbit radii, quantum number (n=1,2,3,. . . ), Newton
gravitation constant, and mass of the nucleus of orbit, and specific velocity, respectively.
In this equation (96), we denote:
vo = (2π/g).GM m
The value of m is an adjustable parameter (similar to g). [43][44]

Quantization in Astrophysics ...

264

(97)

134

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

Using this equation (96), we could predict quantization of celestial orbits in the solar
system, where for Jovian planets we use least-square method and define M in terms of
reduced mass µ = (M1 .M2 )/(M1 + M2 ). From this viewpoint the result is shown in Table
1 below [49]:
Table 1. Comparison of prediction and observed orbit distance of
planets in Solar system (in 0.1 AU unit) [49]
Object

No.

Bode

Nottale

CSV

1

0.4

0.428

2

1.7

1.71

Observed

∆(%)

Mercury

3

4

3.9

3.85

3.87

0.52

Venus

4

7

6.8

6.84

7.32

6.50

Earth

5

10

10.7

10.70

10.00

-6.95

Mars

6

16

15.4

15.4

15.24

-1.05

Hungarias

7

21.0

20.96

20.99

0.14

Asteroid

8

27.4

27.38

27.0

1.40

Camilla

9

34.7

34.6

31.5

-10.00

Object

No.

Bode

Nottale

CSV

Observed

∆(%)

Jupiter

2

52

45.52

52.03

12.51

Saturn

3

100

102.4

95.39

-7.38

Uranus

4

196

182.1

191.9

5.11

Neptune

5

284.5

301

5.48

Pluto

6

409.7

395

-3.72

2003EL61

7

557.7

520

-7.24

Sedna

8

728.4

760

4.16

2003UB31

9

921.8

970

4.96

Unobserved

10

1138.1

Unobserved

11

1377.1

388

722

For comparison purpose, we also include some recent observation by M. Brown et
al. from Caltech [50][51][52][53]. It is known that Brown et al. have reported not less
than four new planetoids in the outer side of Pluto orbit, including 2003EL61 (at 52AU),
2005FY9 (at 52AU), 2003VB12 (at 76AU, dubbed as Sedna.) And recently Brown and his
team reported a new planetoid finding, called 2003UB31 (97AU). This is not to include
Quaoar (42AU), which has orbit distance more or less near Pluto (39.5AU), therefore this
object is excluded from our discussion. It is interesting to remark here that all of those

Quantization in Astrophysics ...

265

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

135

new ‘planetoids’ are within 8% bound from our prediction of celestial quantization based
on the above Bohr-Sommerfeld quantization hypothesis (Table 1). While this prediction
is not so precise compared to the observed data, one could argue that the 8% bound limit
also corresponds to the remaining planets, including inner planets. Therefore this 8%
uncertainty could be attributed to macroquantum uncertainty and other local factors.
While our previous prediction only limits new planet finding until n=9 of Jovian
planets (outer solar system), it seems that there are enough reasons to suppose that
more planetoids are to be found in the near future. Therefore it is recommended to
extend further the same quantization method to larger n values. For prediction purpose,
we include in Table 1 new expected orbits based on the same quantization procedure we
outlined before. For Jovian planets corresponding to quantum number n=10 and n=11,
our method suggests that it is likely to find new orbits around 113.81 AU and 137.71 AU,
respectively. It is recommended therefore, to find new planetoids around these predicted
orbits.
As an interesting alternative method supporting this proposition of quantization from
superfluid-quantized vortices (90), it is worthnoting here that Kiehn has argued in favor
of re-interpreting the square of the wavefunction of Schrödinger equation as the vorticity
distribution (including topological vorticity defects) in the fluid [61]. From this viewpoint,
Kiehn suggests that there is exact mapping from Schrödinger equation to Navier-Stokes
equation, using the notion of quantum vorticity [61]. Interestingly, de Andrade & Sivaram
[62] also suggest that there exists formal analogy between Schrödinger equation and the
Navier-Stokes viscous dissipation equation:
∂V /∂t = ν.∇2 V

(98)

where ν is the kinematic viscosity. Their argument was based on propagation torsion
model for quantized vortices [62]. While Kiehn’s argument was intended for ordinary fluid,
nonetheless the neat linkage between Navier-Stokes equation and superfluid turbulence
is known in literature [63][64][21].
Therefore, it seems interesting to consider a plausible generalization of Schrödinger
equation in particular in the context of viscous dissipation method. First, we could write
Schrödinger equation for a charged particle interacting with an external electromagnetic
field [61] in the form of equation (28) and (85):
[(−i~∇µ − qAµ ) (−i~∇µ − qAµ ) Ψ] = [−i2m.∂/∂t + 2mU (x)] Ψ.

(99)

In the presence of electromagnetic potential [69], one could include another term into the
LHS of equation (99):
[(−i~∇µ − qAµ ) (−i~∇µ − qAµ ) + eAo ] Ψ = 2m [−i∂/∂t + U (x)] Ψ.

(100)

This equation has the physical meaning of Schrödinger equation for a charged particle interacting with an external electromagnetic field, which takes into consideration Aharonov
effect [69]. Topological phase shift becomes its immediate implication, as already considered by Kiehn [61].

Quantization in Astrophysics ...

266

136

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

Therefore, in the context of quaternionic representation of Schrödinger equation [70],
one could write equation (100) in terms of equation [22a]:
[{D} ∗ +eAo ] Ψ = 2m [−i∂/∂t + U (x)] Ψ.

(101)

In the context of topological phase shift [69], it would be interesting therefore to find the
scalar part of equation (101) in experiments [8].
As described above, one could also derive equation (96) from scale- relativistic Schrödinger
equation [43][44]. It should be noted here, however, that Nottale’s method [43][44] differs appreciably from the viscous dissipative Navier-Stokes approach of Kiehn, because
Nottale only considers his equation in the Euler-Newton limit [67][68]. Nonetheless, as
we shall see, it is possible to find a generalization of Schrödinger equation from Nottale’s
approach in similar form with equation (101). In order to do so, first we could rewrite
Nottale’s generalized Schrödinger equation via diffusion method [67][71]:
£
¤
i2mγ − (iγ + a(t)/2) (∂ψ/∂x)2 ψ −2 + ∂ ln ψ/∂t
2

(102)

2

+iγa(t). (∂ ψ/∂x ) /ψ = Φ + a(x)
where ψ,a(x), Φ, γ each represents classical wave function, an arbitrary constant, scalar
potential, and a constant, respectively. If the function f(t) is such that
a(t) = −i2γ,

α(x) = 0,

γ = ~/2m

(103)
(104)

then one recovers the original Schrödinger equation (85).
Further generalization is possible if we rewrite equation (102) in quaternion form
similar to equation (101):
i2mγ [− (iγ + a(t)/2) ({∇}∗)2 ψ −2 + ∂ ln ψ/∂t]

(105)

0

+iγ.a(t). ({∇ }∗) /ψ = Φ + a(x)
Alternatively, with respect to our superfluid dynamics interpretation [13], one could also
get Schrödinger equation from simplification of Ginzburg-Landau equation. This method
will be discussed subsequently. It is known that Ginzburg-Landau equation can be used
to explain various aspects of superfluid dynamics [16][17][18].
According to Gross, Pitaevskii, Ginzburg, wavefunction of N bosons of a reduced mass
m* can be described as [55]:
−(~2 /2m∗).∇2 ψ + κ |ψ|2 ψ = i~.∂ψ/∂t

(106)

For some conditions (where the temperature dependence of the density of Cooper pairs,
ns , is just the square of order parameter. Or |ψ|2 ≈ ns = A(Tc − T )), then it is possible

Quantization in Astrophysics ...

267

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

137

to replace the potential energy term in equation (106) with Hulthen potential. This
substitution yields:
−(~2 /2m∗).∇2 ψ + VHulthen .ψ = i~.∂ψ/∂t

(107)

VHulthen (r) = κ |ψ|2 ≈ −Ze2 .δ.e−δr /(1 − e−δr )

(108)

where
This equation (108) has a pair of exact solutions. It could be shown that for small
values of δ, the Hulthen potential (108) approximates the effective Coulomb potential, in
particular for large radius [14b]:
ef f
VCoulomb
= −e2 /r + `(` + 1).~2 /(2mr2 )

Therefore equation (109) could be rewritten as:
£
¤
−~2 ∇2 ψ/2m ∗ + −e2 /r + `(` + 1).~2 /(2mr2 ) .ψ = i~.∂ψ/∂t

(109)

(110)

For large radii, second term in the square bracket of LHS of equation (110) reduces to
zero [54],
`(` + 1).~2 /(2mr2 ) → 0
(111)
so we can write equation (110) as:
(−~2 ∇2 ψ/2m ∗ +U ).ψ = i~.∂ψ/∂t

(112)

where Coulomb potential can be written as:
U = −e2 /r

(113)

This equation (112) is nothing but Schrödinger equation (85). Therefore we have rederived Schrödinger equation from simplification of Ginzburg-Landau equation, in the
limit of small screening parameter. Calculation shows that introducing this Hulthen
effect (108) into equation (107) will yield different result only at the order of 10−39 m
compared to prediction using equation (110), which is of course negligible. Therefore, we
conclude that for most celestial quantization problems the result of TDGL-Hulthen (110)
is essentially the same with the result derived from equation (85). Now, to derive equation
(96) from Schrödinger equation, the reader is advised to see Nottale’s scale-relativistic
method [43][44].
What we would emphasize here is that this derivation of Schrödinger equation from
Ginzburg-Landau equation is in good agreement with our previous conjecture that equation (90) implies macroquantization corresponding to superfluid-quantized vortices. This
conclusion is the main result of this section. It is also worthnoting here that there is
recent attempt to introduce Ginzburg-Landau equation in the context of microtubule
dynamics [72], which implies wide applicability of this equation.
In the following section, we would extend this argument by noting that macroquantization of celestial systems implies the topological character of superfluid-quantized vortices, and cosmic microwave background radiation is also an indication of such topological
superfluid vortices.

Quantization in Astrophysics ...

268

138

7.

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

Further Note: Signatures of Bose-Einstein Cosmology

It is known that CMBR temperature (2.73K) is conventionally assumed to come from
the hot early Universe, which then cools adiabatically to the present epoch. Nonetheless
this description is not without problems, such as how to consider the small temperature
fluctuations of CMBR as the seeds that give rise to large-scale structure such as galaxy
formation [73]. Furthermore it is known that CMBR follows Planck radiation law with
high precision, so one could argue whether it also indicates that large-scale structures
obey quantum-mechanical principles. Therefore we will consider here some alternative
hypothesis, which support the idea of low-energy quantum mechanics corresponding to
superfluid vortices described in the preceding section.
In recent years, there are alternative arguments suggesting that the Universe indeed
resembles the dynamics of N number of Planckian oscillators. Using similar assumption,
for instance Antoniadis et al. [74] argue that CMBR temperature could be derived using
conformal invariance symmetry, instead of using Harrison-Zel’dovich spectrum. Other
has derived CMBR temperature from Weyl framework [74a]. Furthermore, if the CMBR
temperature 2.73K could be interpreted as low-energy part of the Planck distribution
law, then it seems to indicate that the Universe resembles Bose-Einstein condensate [75].
Pervushin et al. also argued that CMBR temperature could be derived from conformal
cosmology with relative units [76]. These arguments seem to support Winterberg’s hypothesis that superfluid phonon-roton aether could explain the origin of cosmic microwave
background radiation [18][19].
Of course, it does not mean that CMBR data fits perfectly with Planck distribution
law. It has been argued that CMBR data more corresponds to q-deformed Planck radiation distribution [77]. However, this argument requires further analysis. What interests
us here is that there are reasons to believe that a quantum universe based on Planck
scale is not merely a pure hypothetical notion, in particular if we consider known analogy
between superfluidity and various cosmology phenomena [16][17].
Another argument comes from fractality argument. It has been discovered by Feynman that the typical quantum mechanical paths are non-differentiable and fractal [67].
In this regard, it has been argued that the Universe is embedded in Cantorian fractal
spacetime having non-integer Hausdorff dimension [78], and from this viewpoint it could
be inferred that the correlated fluctuations of the fractal spacetime is analogous to the
Bose-Einstein condensate phenomenon. Interestingly, there is also hypothesis suggesting
that Hausdorff dimension could be related to temperature of ideal Bose gas [79].
From these aforementioned arguments, it seems plausible to suppose that that CMBR
temperature 2.73K could be interpreted as a signature of Bose-Einstein condensate cosmology. In particular, one could consider [22b] that “this relationship comes directly from
Boltzmann’s law N= B.k.T, where N is the background noise power; T is the background
temperature in degrees Kelvin; and B is the bandwidth of the background radiation. It
follows that the ratio (N/kB) for the cosmic background radiation is approximately equal
to ”e”, because we usually convert the equation to decibels by taking natural logarithm.

Quantization in Astrophysics ...

269

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

139

The relationship is a solid one in fact.” From this viewpoint, it seems quite conceivable to
explain why CMBR temperature 2.73K is near enough to known number e= 2.71828. . . ,
which seems to suggest that the logarithmic form of Schrödinger equation (‘eigentliche
Wellengleichung’) [4a] may have a deep linkage with this number e= 2.71828. . .
Nonetheless, we recognize that this proposition requires further analysis before we
could regard it as conclusive. But we can describe here some arguments to support the
new interpretation supporting this Bose-Einstein cosmology argument:
• From Fischer’s argument [13] we know that Bohr-Sommerfeld quantization from
superfluid vortice could exhibit at all scales, including celestial quantization. This
proposition comes directly from his assertion of the topological character of superfluid
vortices, because superfluid is topological electronic liquid [27].
• Extending further the aforementioned hypothesis of topological superfluid vortices,
then it seems interesting to compare it with topological analysis of COBE-DMR
data. G. Rocha et al. [80] argue using wavelet approach with Mexican Hat potential
that it is possible to interpret the data as clue for a finite torus Universe, albeit not
conclusive enough.
• Interestingly, this conjecture could be related to Bulgadaev’s argument [81] suggesting that topological quantum number could be related to torus structure as stable
soliton [81a]. In effect, this seems to imply that the basic structure of physical
phenomena throughout all scales could take the form of topological torus.
In other words, the topological character of superfluid vortices implies that it is possible to
generalize superfluid vortices to large scales. And the topological character of CMBR data
seems to support our proposition that the universe indeed exhibits topological structures.
It follows then that CMBR temperature is topological [80] in the sense that the superfluid
nature of background temperature [18][19] could be explained from topological superfluid
vortices.
Interestingly, similar argument has been pointed out by a number of authors by
mentioning non-Gaussian part of CMBR spectrum. However, further discussion on this
issue requires another note.

8.

Concluding Remarks

It is known that Barut equation could predict lepton mass (and also hadron mass)
with remarkable precision. Therefore, in the present article, we attempt to find plausible
linkage between Dirac-Maxwell’s isomorphism and Barut-Dirac-Vigier equation. From
this proposition we could find a unified wave equation in terms of superfluid velocity
(vierbein), which then could be used as basis to derive some alternative descriptions of
Barut equation. Further experiment is required to verify which equation is the most
reliable.
In the present note we submit the viewpoint that it would be more conceivable if we
interpret the vierbein of the unified wave equation in terms of superfluid velocity, which
in turn brings us to the notion of topological electronic liquid. Nonetheless, the proposed

Quantization in Astrophysics ...

270

140

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

imaginary algebra discussed herein is only at its elementary form, and it requires further
analysis in particular in the context of [5a][7][14][28]. It is likely that this subject will
become the subject of subsequent paper.
Furthermore, the notion of topological electronic liquid could lead to topological superfluid vortices, which may explain the origin of macroquantization of celestial systems
and perhaps also topological character of Cosmic Microwave Background Radiations.
Nonetheless, such a proposition requires further analysis before it can be considered as
conclusive.
Provided the aforementioned propositions of using superfluid velocity (vierbein) to
describe unified wave equation correspond to the observed facts, and then in principle it
seems to support arguments in favor of possibility to observe condensed-matter hadronic
reaction.

Acknowledgement
The author would thank to Profs. C. Castro, M. Pitkänen, R.M. Kiehn, Ezzat G.
Bakhoum, A. Kaivarainen, P. LaViolette, F. Smarandache, and E. Scholz, for insightful
discussions and remarks. Special thanks go to Prof. C. Castro for suggesting finding linkage between quaternionic Klein-Gordon equation and Duffin-Kemmer-Petiau equation,
to Prof. Ezzat G. Bakhoum for his remark on Boltzmann distribution and its linkage to
CMBR temperature.

Quantization in Astrophysics ...

271

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

141

References
[1] Barut, A.O., Phys. Rev. Lett. Vol. 42 (1979). [1a] A.O. Barut, “On the status of hidden
variable theories in Quantum mechanics,” Apeiron Vol. 2 No. 4, Oct. 1995, p. 97.
[2] Gsponer, A., & J-P. Hurni, Found. Phys. Lett. Vol. 14 No.1 (2001) 77-85, preprint
arXiv:math-ph/0201049. [2a] Gsponer, A., & J-P. Hurni, hep-ph/0201193. [2b] Gsponer,
A., Int. J. Theor. Phys. 41 (2002) 689-694, preprint arXiv:math-ph/0201053.
[3] Varlamov, V.V., Annales Fondation Louis de Broglie Vol. 27 no. 2(2002), [3a]
arXiv:hep-th/9709051 (1997).
[4] Post, E.J., Annales Fondation Louis de Broglie no. 2, 217-240 (2002). [4a] Englman,
R., & H. Yahalom, arXiv:physics/0406149 (2004).
[5] Dvoeglazov, V.V., arXiv:math-ph/0503008 (2005). [5a] Kruglov, S.I., Annales
Fondation Louis de Broglie Vol. 29 (2004) arXiv:quant-ph/0408056 (2004)
[6] Vigier, J-P., “Fundamental problems in quantum physics,” Apeiron Vol. 2 No. 4, Oct.
1995, (1995) p.114.
[7] Ulrych, S, arXiv:hep-th/9904170 (1999); [7a] Ulrych, S., physics/0009079.
[8] Aharonov, Y., et al., arXiv:quant-ph/0311155 (2003).
[9] Hofer, W.A., “Beyond Uncertainty: the internal structure of electron and photon,”
arXiv:physics/9611009 (1996).
[10] Sklarz, S., & L. Horwitz, arXiv:physics/0104019 (2001).
[11] Cui, H.Y., arXiv:physics/0103019 (2001); [11a] arXiv:physics/0408025 (2004); [11b]
arXiv:physics/0409023 (2004).
[12] Carter, B., & D. Langlois, arXiv:hep-th/9507059 (1995). [12a] Carter, B., & D.
Langlois, Phys. Rev. D 51 (1995), 5855-64.
[13] Fischer, U.W., arXiv:cond-mat/9907457 (1999).
[14] Antonuccio, F., arXiv:hep-th/9812036 (1998); [14a] arXiv:hep-th/9408166; [14b]
Alhaidari, A., arXiv:math-ph/0405022 (2004); [14c] Adler, S.L., arXiv:hep-th9306009.
[15] Oudet, X., “The quantum state and the doublets,” Annales Fondation Louis de
Broglie Vol. 25 No.1, 24p (2000).
[16] Zurek, W., “Cosmological experiments in superfluids and superconductors,” in
Proc. Euroconference Formation and Interaction of Topological Defects, A. Davis &
R. Brandenberger (eds.) Plenum (1995); preprint at arXiv:cond-mat/9502119. [16b]
Barcelo, C., et al., “Analogue gravity from Bose-Einstein condensate,” Class. Quantum
Grav. 18, 1137-1156 (2001).
[17] Volovik, G., “Superfluid analogies of cosmological phenomena,” arXiv:gr-qc/0005091
(2000a); [17b] Volovik, G., “Links between gravity and dynamics of quantum liquids,”
Int. Conf. “Cosmology. Relativistic Astrophysics. Cosmoparticle Physics” (COSMION99) (2000) preprint at arXiv:gr-qc/0004049; [17c] Volovik, G., arXiv:gr-qc/0104046
(2001); [17d] Nozieres, P., & D. Pines, The theory of quantum liquids: Superfluid Bose
Liquid. Wesley Publ. Co. Inc., 116-124 (1990).
[18] Winterberg,F., Z. Naturforsch., 57a (2002) 202-204; presented at the 9 th Canadian
Conf. on General Relativity and Relativistic Astrophysics, Edmonton, May 24-26, 2001.

Quantization in Astrophysics ...

272

142

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

[19] Winterberg, F., “Maxwell’s Aether, the Planck Aether hypothesis, and
Sommerfeld’s Fine Structure Constant, ”http://www.cet.sunderland.ac.uk (2002),
http://www.znaturforsch.com/56a/56a0681.pdf .
[20] Kaivarainen, A., arXiv:physics/0207027 (2002)
[21] Kaivarainen, A., “Hierarchic models of turbulence, superfluidity and
superconductivity,” arXiv:physics/0003108 (2000). Also in http://www.karelia.ru
[22] Bakhoum, E., “Fundamental disagreement of wave mechanics with relativity,”
arXiv:physics/0206061 (2002) p. 5-7. [22a] Bakhoum, E., physics/0207107. [22b]
Bakhoum, E., email communication, Sept. 24th 2005.
[23] Goenner, H.F.M., “On the history of the unified field theories”, Living Rev. Relativity
Vol. 7, 102 (2004) http://www.livingreviews.org/lrr-2004-2
[24] Oleinik, V.P., Nonlin. Math. Phys., Vol. 4 No. 1-2 (1997) 180-189. [24a] Oleinik, V.P.
et al., “On quantum dynamics of the self-acting electron,” ( ).
[25] Castro., C., & M. Pavsic, “The extended relativity in Clifford-spaces,” commissioned
review to appear in Int. J. Mod. Phys. D (2004).
[26] Feynman, R., et al., The Feynman Lectures on Physics, Addison-Wesley, Reading,
MA, Vol. II, (1964) p. 1-3.
[27] Wiegmann, P., arXiv:cond-mat/9808004 (1998).
[28] Dahm, R., arXiv:hep-ph/9601207 (1996).
[29] Rowlands, P., arXiv:hep-ph/0301071 (2003).
[30] Meessen, A., Found. of Phys., no. 29 (2000) 281-316.
[31] Henley, E., & J. Schiffer, arXiv:nucl-th/9807041 (1998) p.11.
[32] Baez, J., arXiv:math.RA/0105155 (2001).
[33] Waser, A., “Quaternions in electrodynamics,” (2001) http://www.aw-verlag.ch
[34] Kassandrov, V.V., Grav. & Cosmology Vol.1(1995) arXiv:gr-qc/0007027 (2000); [34a]
arXiv:gr-qc/00070276 (2000).
[35] Baylis, W.E., arXiv:quant-ph/0202060 (2002).
[36] van Dongen, J., arXiv:gr-qc/0009087 (2000) p.5
[37] Consoli, M., arXiv:gr-qc/0204106 (2002). [37a] Consoli, M., & E. Costanzo,
arXiv:hep-ph/0311317 (2003).
[38] Ibison, M., “Electrodynamics in the Zero-Point Field: On the equilibrium spectral
energy distribution, and the origin of inertial mass.” Found. Phys. Letter Vol. 16 No.
1 (2003) 83-90.
[39] Ying-Qiu, G., “Some properties of the spinor soliton,” Advances in Applied Clifford
Algebras 8 No. 1 (1998) 17-29.
[40] Woon, S., DAMTP-R-97/33, arXiv:hep-th/9707206 (1997) p. 18-21.
[41] Fauser, B., arXiv:hep-th/9908200 (1999).
[42] Spohn, H., arXiv:quant-ph/9911065 (1999).

Quantization in Astrophysics ...

273

Electronic Journal of Theoretical Physics 3, No. 12 (2006) 117–144

143

[43] Nottale, L., et al., Astron. Astrophys. 322, (1997) 1018.
[44] Nottale, L., Astron. Astrophys. 327, (1997) 867-889.
[45] Nottale, L., G. Schumacher, & E.T. Levefre, Astron. Astrophys. 361, (2000) 379-387;
preprint at http://www.daec.obspm.fr/users/nottale
[46] Ohba, I., “Some stochastic aspect of quantization”, Pramana J.Phys., Indian
Academy of Sciences, Vol. 59. No. 2 (2002) 397-404.
[47] Rubi, A., & J. Rubi, “The quantization of solar-like gravitational systems,” Fizika
B 7 Vol. 1, 1-13 (1998).
[48] Agnese, A.G., & R. Festa, “Discretization of the cosmic scale inspired from the Old
Quantum Mechanics,” Proc. Workshop on Modern Modified Theories of Gravitation
and Cosmology 1997, preprint arXiv:astro-ph/9807186 (1998).
[49] Christianto, V., Apeiron Vol. 11 No. 3 (2004), preprint at http://reachme.at/coolbit
[49a] Christianto, V., AFLB Vol. 31 no. 1 (2006)
URL:http://www.ensmp.fr/aflb/AFLB-311/aflb311m370.pdf
[49b] Christianto, V., Apeiron VOl. 11 (2004)
http://www.slac.stanford.edu
[50] NASA News Release (Jul 2005),
http://www.nasa.gov/vision/universe/solarsystem/newplanet-072905.html
[51] BBC News (Oct 2004), http://news.bbc.co.uk/1/hi/sci/tech/4730061.stm
[52] Brown, M., et al., ApJ. Letters (Aug. 2004). Preprint arXiv: astro-ph/0404456
(2004); ApJ. forthcoming issue (2005), astro-ph/0504280 (2005).
[53] Brown, M. (Jul. 2005), http://www.gps.caltech.edu/‘mbrown/planetlila/
[54] Pitkänen, M., http://www.physics.helsinki.fi/∼matpitka/articles/nottale.pdf
[55] Infeld, E., et al., arXiv:cond-mat/0104073 (2001).
[56] Lodder, J.J., submitted to Physica A, arXiv:hep-th/9405265 (1994).
[57] Cozzini, M., & S. Stringari, arXiv:cond-mat/0211294 (2002).
[58] Marchuk, N., “A model of the composite structure of quarks and leptons with SU(4)
gauge symmetry,” arXiv:hep-ph/9801382 (1998).
[59] Cohen, N., et al., Electronic Journal of Linear Algebra 7 (2000), arXiv:mathph/9907015 (1999).
[60] Kruglov, S.I., arXiv:hep-ph/0507027 (2005); [60a] Kruglov, S.I., “Dirac-Kahler
equation,” arXiv:hep-th/0110060; [60b] Kruglov, S.I., hep-th/0110251 (2001).
[61] Kiehn, R.M., “An interpretation of wavefunction as a measure of vorticity,”
http://www22.pair.com/csdc/pdf/cologne.pdf (1989).
[62] de Andrade, L.G., & C. Sivaram, arXiv:hep-th/9811067 (1998). [62a] de Andrade,
L.G., gr-qc/0405062.
[63] Kivotides, D., arXiv:physics/0405033 (2004).

Quantization in Astrophysics ...

274

Torsion Fields, Cartan-Weyl Space-Time and State-Space Quantum
Geometries, Brownian Motions, and their Topological Dimension
Diego L. Rapoport
Department of Sciences and Technology, Universidad Nacional de Quilmes
Buenos Aires, Argentina 1
Summary: We review the relation between space-time geometries with torsion fields (the so-called Riemann-Cartan-Weyl (RCW) geometries) and their
associated Brownian motions. In this setting, the metric conjugate of the tracetorsion one-form is the drift vector field of the Brownian motions. Thus, in the
present approach Brownian motions are -in distinction with Nelson’s Stochastic
Mechanics- space-time structures. We extend this to the state-space of nonrelativistic quantum mechanics and discuss the relation between a non-canonical
quantum RCW geometry in state-space associated with the gradient of the
quantum-mechanical expectation value of a self-adjoint operator. A particular
case is given by the generalized laplacian operator defined by a RCW geometry, which is the generator of the space-time Brownian motions. We discuss
the reduction of the wave function in terms of a RCW quantum geometry in
state-space. We characterize the Schroedinger equation in terms of the RCW geometries and Brownian motions, for systems under observation as well as those
unobserved. Thus, in this work, the Schroedinger field is a torsion generating
field. In this work the U and R processes -in the sense of R. Penrose- are associated to RCW geometries and their Brownian motions, the former to RCW
space-time geometries and their associated Brownian motions, and the latter to
their extension to the state-space of nonrelativistic quantum mechanics given by
the projective Hilbert space. In this setting, the Schroedinger equation can be
either linear or nonlinear. We discuss the problem of the many times variables
and the relation with dissipative processes. We present as an additional example
of RCW geometries and their Brownian motions, the dynamics of viscous fluids
obeying the invariant Navier-Stokes equations. We introduce in the present setting an extension of R. Kiehn’s approach to dynamical systems starting from the
notion of the topological dimension of one-forms, to apply it to the trace-torsion
one-form whose metric conjugate is the Brownian motion’s drift vector-field and
discuss the topological notion of turbulence. We discuss the relation between
our setting and the Nottale theory of Scale Relativity, and the work of Castro
and Mahecha in this volume in nonlinear quantum mechanics, Weyl geometries
(which are not to be confused with the RCW geometries) and the quantum
potential. In our setting, the quantum potential is found to coincide (up to
a conformal factor) with the metric scalar curvature. We discuss the possible
relations between the present approach and the nonlocal universal correlations
1 E-mail

address: diego.rapoport@gmail.com

1

Quantizatin in Astrophysics ...

276

between dissipative systems, first found by Kozyrev, and subsequently in diverse geophysical, solar and ionospheric observations. We discuss the possible
relations between the present gauge theory that can be introduced in terms of
Einstein’s lambda transformations, and the so-called Global Scaling theory due
to Hartmutt Muller, and his predictions of the existence of a universal fractal
structure associated to the logarithmic scale for universal scales in Nature.

1

INTRODUCTION

In a series of articles [1,2], we have presented a fusion between space-time
structures and Brownian motions, in which a complementarity of the objects
characterizing the Brownian motion, i.e. the noise tensor which produces a
metric, and the drift vector field which describes the average velocity of the
Brownian motion whenever this takes place in space-time. These space and
time structures, which can be defined starting from flat Euclidean or Minkowski
space-time, have in addition to a metric, a torsion tensor which is formed from
the metric conjugate of the drift vector field, and the laplacian operator defined
by this geometrical structure is the differential generator of the Brownian motions. Thus , in this equivalence, one can choose the Brownian motions as the
original structures determining a space-time structure, or conversely, the spacetime structures produce a Brownian motion process. Thus, in view that the
space and time geometries can be seen as associated with an extension of the
theory of gravitation which in fact was first explored in joint work by Einstein
with Cartan [3], then the foundations for the gravitational field, at least those
associated to this restricted case of torsion reduced to the trace, can be found
in these Brownian motions. Thus, in this equivalence, lies a characterization
of the Universe in which due to the self-similarity of Brownian motions with
its associated fractal structures, and the infinite velocity propagation of diffusion processes, point to a phenomenology which is not the classical mechanical
metaphor, but one in which interactions at a point are imparted in no time
to the whole Universe, while an hologram picture of reality (which recalls the
Bohm conception of implicit order [68]), appears as its natural expression of
universal scales that have been gauged to produce the actual geometries and
the associated Brownian motions. Indeed, these space-time geometrical structures can be introduced by the Einstein λ transformations on the tetrad fields,
from which the usual Weyl scale transformations can be deduced, but contrarily to Weyl geometries, these structures have torsion and they are integrable in
contrast with Weyl’s theory (see Castro and Mahecha in this volume). We have
called these connections as RCW structures (short for Riemann-Cartan-Weyl)
[2,4]. A different meaning for RCW structures is found in [77] in which it is
2

Quantizatin in Astrophysics ...

277

refered to connections which are metric compatible with torsion, not necessarily
restricted to the trace-torsion, in spite of the common designation in [78].
This description in terms of gauging the scale transformations, begs the
question about how universal scales can be to be able to produce a Universe
of diversity which gives place to the phenomenae we call life, the quantum
mechanical scales and still the planetary and galactic scales?2
We can further enquire what is the relation between the aether and a Universe described in terms of this equivalence, in which due to the fact that torsion
is a non-metric geometrical object describing a topological obstruction to triviality, i.e. the breaking of closure of infinitesimal parallelograms in the particular
scale we are describing this equivalence, so that the presence of a flow is intuitively evoked by this geometry, and the aether? This may seem strange to most
of the readers, since the Michelson-Morley experiments seemingly disproved the
existence of a background fluid, which years later reappeared in quantum field
theory in the guise of the vacuum fluctuations 3
This negative result called for the fusion of space and time, in a single structure which we know as Minkowski space , but for which the founding fathers
of modern physics found initially to be abtruse. The fact is that the Lorentz
group does not depend on the existence or not of an aether, and they have
been associated by V. Fock to particles as space-time structures associated with
solutions of the eikonal equation for which in Minkowski space this equation is
Lorentz-invariant; see [35]. Furthermore, if an aether would exist, the Lorentz
transformations, in contrary to common belief, does not loose its place, because
they become the set of transformations by which two arbitrary observers can
2 The answer to this may come from the research by Hartmutt Muller, currently called
Global Scaling, starting from research by the Russian biologist, Cislenko. This researcher
discovered that biological species sizes can be represented in a logarithmic scale where they
appear concentrated in specific equally distanced intervals; see. Cislenko, Structure of Fauna
And Flora With Regard to Body Size of Organisms ( Lomonosov-University Moscow, 1980).
Analysis of data of natural processes and structures on all scales, from the cosmological to
the quantum, have shown a similar behavior. The scales in which the Universe appears to be
related to a fractal structure on this logarithmic scale, and the void sets of this vacuum are
nodal sets for a stationary wave that appears from a model of interacting classical particles,
and associated with the creation or annhilitation of particles, or, more universally, of structures. Thus, one can apply as a general method Muller’s findings, to the analysis with this
fractal structure of a time series of experimental data of arbitrary phenomenae, and thus to be
able to deduce the possibility of creation (or annihilation) of hitherto unkwown structures; see
[2]. Remarkably, this resonant effects of the vacuum through the nodes, has allowed to produce transference of information, concretely, computer files between computers unconnected
through internet or other tangible communication scheme; see [30]. In the context of our
theory, the logarithmic scale is gauged and introduces the exact term of the trace-torsion oneform, whose metric conjugate is part of the drift of the underlying Brownian motion through
which the teleportation may be produced. Thus, the universality of global scaling laws lead
to gauge dependent Brownian motions of all systems.
3 There is a strong controversy about the interpretations of the Michelson-Morley experiments. The experiments were repeated extensively by Miller who interpreted his results as
yielding a positive result for the existance of the aether [41]. More contemporary experiments
with different settings, may point out to the existence of the aether [42][43].

3

Quantizatin in Astrophysics ...

278

agree in the existence of a lump of space-time associated to the solution of the
eikonal equation. Furthermore, the velocity of light as a universal factor does
not loose its place. What about General Relativity (GR), vis-a-vis the existence
of background Brownian motions, where we recall that GR appeared to give a
geometrical invariant extension of Minkowksi space-time precisely to account of
the existence of massive objects as deformations of the flat space-time? Does
the principle of general covariance looses its ground as a basic tenant for the
universality of the laws of physics? In this regard the fact that the theory of
Brownian motion cannot be formulated without a diffeomorphism invariant distinction between the first and second moments, i.e. between the drift and the
noise tensor, for which it is indispensable to introduce the notion of a linear
connection [24,25,26]. This gives further support, albeit from an unexpected
quantum status, to the relevance of the general principle of covariance, but now
stemming from a more fundamental non-differentiable fractal level 4 .
In this article we shall treat the problem of non-relativistic quantum mechanics in terms of diffusion processes both in spacetime and the state-space of
quantum mechanics. Thus, in this approach, it will appear that the Schroedinger
field can be associated with a scale field producing a distortion in the vacuum,
and introducing as well the associated Brownian motions. There have been
numerous attempts to relate non-relativistic quantum mechanics to diffusion
equations; the most notable of them is Stochastic Mechanics , due to Nelson
[14]. Already Schroedinger proposed in 1930-32 that his equation should be
related to the theory of Brownian motions, and proposed a scheme he was not
able to achieve, the so-called interpolation problem which requires to describe
the Brownian motion and the wave functions in terms of interpolating the initial and final densities in a given time-interval [27]. More recently Nagasawa
presented a solution to this interpolation problem and further elucidated that
the Schroedinger equation is in fact a Boltzmann equation [38]. Neither Nagasawa nor Nelson presented these Brownian motions as spacetime structures,
but rather as matter fields on the vacuum. While Nelson introduced artificially a forward and backward stochastic derivatives to be able to reproduce
the Schroedinger equation as a formally time-symmetric equation, Nagasawa
was able to solve the interpolation problem in terms of the forward diffusion
process and its adjoint backward process, from which without resort to the adhoc constructions due to Nelson, he was able to prove that this was related
to the Kolmogorov characterization of time-irreversibility of diffusion processes
in terms of the non-exact terms of the drift, here related to the trace-torsion.
4 Einstein somewhat conceded to the criticism of the so called operationalists, as Bridgeman and Kretschmann, on downplaying the role of the Principle of General Covariance; see E.
Kretschmann, Ann.der Physik 53, 575 (1917), P.W. Bridgeman, Natural of Physical Theory,
Princeton Univ. Press (1936); if it would not have been by the developments of the mathematical theory of Brownian motions, and still, the inception of gauge-theoretical geometrical
methods in statistical and condensed matter physics at its very roots, this criticism of the
geometrical approach, and further, of a topological approach, would have prevailed.

4

Quantizatin in Astrophysics ...

279

In spite of the ad-hoc character of Nelson’s approach, a similar approach to
quantization in terms of an initial fractal structure of space-time and the introduction of Nelson’s forward and backward stochastic derivatives, was developed
by Nottale in his Scale Theory of Relativity [31]. Remarkably, his approach
has promoted the Schroedinger equation as valid for large scale structures, and
predicted the existence of exo-solar planets which were observationally verified
to exist [34]. This may further support the idea that the RCW structures introduced in the vacuum by scale transformations, are valid, as any topological
approach would be, independently of the scale in which the associated Brownian motions and equations of quantum mechanics are posited. Furthermore,
Kiehn has proved that the Schroedinger equation in spatial 2D can be exactly
transformed into the Navier-Stokes equation for a compressible fluid, if we furh̄
with m the mass of the electron;;
ther take the kinematical viscosity ν to be m
see ref. [33]. As we proved in [1] and [32] the Navier-Stokes equations share
with the Schroedinger equation, that both have a RCW geometry at their basis;
while in the Navier-Stokes equations the trace-torsion is −1
2ν u with u the timedependent velocity one-form of the viscous fluid, in the Schroedinger equation,
the trace-torsion one-form incorporates the logarithmic differential of the wave
function -just like in Nottale’s theory [31]- and further the electromagnetic potential terms of the trace-torsion. This correspondence between trace-torsion
one-forms is what lies at the base of Kiehn’s correspondance, with an important
addendum: While in the approach of the Schroedinger equation the probability
density is related to the Schroedinger scale factor (in incorporating the complex
phase) and the Born formula turns out to be a formula and not an hypothesis,
under the transformation to the Navier-Stokes equations it turns out that the
probability density of non-relativistic quantum mechanics, is the entrosphy density of the fluid, i.e. the square of the vorticity, which thus plays a geometrical
role that substitutes the probability density. Thus, in this approach, while
there may be virtual paths sustaining the random behaviour of particles (as is
the case also of the Navier-Stokes equations [1],[32]) and the interference such
as in the two-slit experiments can be interpreted as a superposition of Brownian paths [38], the probability density has a purely geometrical fluid-dynamical
meaning (the squared length of the vorticity vector field). Finally, we shall
present the relation between what we can now call RCW quantum geometries,
with the representation of the Schroedinger equation in the projective statespace of non-relativistic quantum mechanics, and further present the problem
of the reduction of the wave function, as related to a non-canonical geometry in
state-space. This quantum RCW geometry has a metric which is not the usual
Fubini-Study metric, but is related to an extension of the classical symplectic
geometry treatment of the Schroedinger function in state-space, to include the
observation process in terms of a noise term and a trace-torsion drift given by
(a modification of) the gradient of the Hamiltonian function corresponding to
the symplectic formalism. This Hamiltonian function is none other that the
quantum mechanical expectation function defined by the quantum Hamiltonian
5

Quantizatin in Astrophysics ...

280

operator, or more specifically, it can be the Laplacian operator associated to
the RCW geometry which has a correlate as a Brownian motion in space-time.
Thus, if one incorporates the observation process into the theory, still RCW geometries will play an important role, since the Schroedinger symplectic vector
field is the natural drift vector field in state-space whenever the noise coefficient
is zero.

2

RIEMANN-CARTAN-WEYL GEOMETRIES

In this section we follow our articles in [1,2]. In this article M denotes a
smooth connected compact orientable n-dimensional manifold (without boundary). While in our initial works, we took for M to be spacetime, there is no
intrinsic reason for this limitation, in fact if can be an arbitrary configuration manifold and still a phase-space associated to a dynamical system. The
paradigmatical example of the latter, is the projective space associated to a
finite-dimensional Hilbert-space of a quantum mechanical system. We shall further provide M with a linear connection described by a covariant derivative
operator ∇ which we assume to be compatible with a given metric g on M , i.e.
∇g = 0. Given a coordinate chart (xα ) (α = 1, . . . , n) of M , a system of func∂
tions on M (the Christoffel symbols of ∇) are defined by ∇ ∂ ∂x∂ γ = Γ(x)α
βγ ∂xα .
∂xβ
The Christoffel coefficients of ∇ can be decomposed as:
 
α
1 α
α
Γβγ =
+ Kβγ
.
(1)
βγ
2
The first term in (1) stands for the metric
coefficients of the Levi α Christoffel
Civita connection ∇g associated to g, i.e. βγ
= 21 ( ∂x∂ β gνγ + ∂x∂ γ gβν − ∂x∂ ν gβγ )g αν ,
and
α
α
α
α
Kβγ
= Tβγ
+ Sβγ
+ Sγβ
,

(2)

α
α
κ
α
= g αν gβκ Tνγ
, and Tβγ
= (Γα
is the cotorsion tensor, with Sβγ
βγ − Γγβ ) the
skew-symmetric torsion tensor. We are interested in (one-half) the Laplacian
operator associated to ∇, i.e. the operator acting on smooth functions on M
defined as

H(∇) := 1/2∇2 = 1/2g αβ ∇α ∇β .

(3)

A straightforward computation shows that H(∇) only depends in the trace of
the torsion tensor and g, since it is
H(∇) = 1/24g + Q̂,

(4)

ν
with Q := Qβ dxβ = Tνβ
dxβ the trace-torsion one-form and where Q̂ is the
vector field associated to Q via g: Q̂(f ) = g(Q, df ), for any smooth function

6

Quantizatin in Astrophysics ...

281

f defined on M . Finally, 4g is the Laplace-Beltrami operator of g: 4g f =
divg gradf , f ∈ C ∞ (M ), with divg the Riemannian divergence. Thus for any
1
1
smooth function, we have 4g f = 1/[det(g)] 2 g αβ ∂x∂ β ([det(g)] 2 ∂x∂α f ). Furthermore, the second term in (4), i.e. Q̂ coincides with the Lie-derivative with
respect to the vectorfield Q̂: LQ̂ = iQ̂ d + diQ̂ , where iQ̂ is the interior product
with respect to Q̂: for arbitrary vectorfields X1 , . . . , Xk−1 and φ a k-form defined on M , we have (iQ̂ φ)(X1 , . . . , Xk−1 ) = φ(Q̂, X1 , . . . , Xk−1 ). Then, for f a
scalar field, iQ̂ f = 0 and
LQ̂ f = (iQ̂ d + diQ̂ )f = iQ̂ df = g(Q, df ) = Q̂(f ).

(5)

Thus, our laplacian operator admits being written as
H0 (g, Q) =

1
4g + LQ̂ .
2

(6)

Consider the family of zero-th order differential operators acting on smooth
k-forms, i.e. differential forms of degree k (k = 0, . . . , n) defined on M :
Hk (g, Q) := 1/24k + LQ̂ ,

(7)

In the first summand of the r.h.s. of (7) we have the Hodge operator acting on
k-forms:
4k = (d − δ)2 = −(dδ + δd),

(8)

with d and δ the exterior differential and codifferential operators respectively,
i.e. δ is the adjointR operator of d defined through the pairing of k-forms
on M : (ω1 , ω2 ) := ⊗k g −1 (ω1 , ω2 )volg , for arbitrary k-forms ω1 , ω2 , where
1
volg (x) = det(g(x)) 2 dx is the volume density, g −1 denotes the induced metric
on 1-forms and ⊗k g −1 the induced metric on k-forms . The last identity in
eq. (7) follows from the fact that d2 = 0 so that δ 2 = 0. Since this operator
when k = 0 coincides with the Laplace-Beltrami operator 4g , we see that from
the family defined in eq. (7) we retrieve for scalar fields (k = 0) the operator
H(∇) defined in (4). The Hodge laplacian can be further written expliciting
the Weitzenbock metric curvature term, so that when dealing with M = Rn
provided with the Euclidean metric, 4k is the standard Euclidean laplacian
acting on the components of a k-form defined on Rn (0 ≤ k ≤ n).
Therefore, assuming that g is non-degenerate, we have defined a one-to-one
mapping
∇ ; Hk (g, Q) = 1/24k + LQ̂
between the space of g-compatible linear connections ∇ with Christoffel coefficients of the form
 
 α
α
2
Γα
=
+
δ Qγ − gβγ Qα , n 6= 1
(9)
βγ
βγ
(n − 1) β
7

Quantizatin in Astrophysics ...

282

and the space of elliptic second order differential operators on k-forms (k =
0, . . . , n).
Remarkably enough, the full torsion does not appear in the Laplacian operator associated to the connection, only the trace-torsion one-form Q that gives
rise through its metric conjugate, to the drift interaction term. But the torsion
tensor has as irreducible decomposition the form
α
Tβγ

=

1 n
2
α
δ α Qγ] +
 T̂ δ + T̄βγ
,
n − 1 [β
n − 1 βγδ

(10)

where
T̂β =

1
jinsT ins
2

(11)

m
is the pseudovector term and the completely skew-symmetric term, T̄αβ
which
then satisfies

T̄αβγ + T̄βγα + Tγαβ = 0,

(12)

δ
where T̄αβγ = gαδ T̄βγ
. This is the term that was introduced in the joint collaborator by Einstein and Cartan, without identification of the physical nature
of the term [3]], and later, retaken in the framework of the Poincaré -gauge
theory of gravitation , as the spin-angular-density of elementary particles or
macroscopic objects [17]. As we shall seen already, the pseudovector and completely skew-symmetric terms do not appear in the generalized laplacian, and a
fortiori do not appear in the expression of the Brownian motions that generate
the RCW geometries. Thus, angular momentum is not a geometrical object
which generates the Brownian motions, only the metric through the noise term
that generates the metric through the relation we shall see in the next section,
and the drift vector field given by the metric-conjugate of the trace-torsion. In
other terms, the probability law of the Brownian motions are determined only
by the noise and the trace-torsion, so that the angular momentum density plays
no fundamental role in this respect. Nevertheless, since the Brownian motions
of tensors and ultimately of differential forms, is determined by the probability
law and the Brownian motions of the scalar particles and this information is
determined by the scalar laplacian, so the diffusion of the angular-momentum
is determined by the diffusion of the scalar fields, and naturally we would like
to study the diffusion of angular momentum along the paths of the scalar fields.
Thus, when considering the Navier-Stokes equations for viscous fluids, where
the drift vector field associated to the geometrical-stochastic characterization
of these equations is minus the fluid’s velocity one-form obeying the NavierStokes equations, the diffusion of the angular momentum of the fluid, i.e. of the
vorticity two-form could be identically characterized in terms of the diffusion
of the Navier-Stokes laplacian, as an operator acting on scalars associated to a

8

Quantizatin in Astrophysics ...

283

RCW connection. In this case, the diffusion equation for angular momentum
is the Navier-Stokes equations for the vorticity, derived by simply applying the
exterior differential to the Navier-Stokes equations for the velocity. In fact we
can introduce a non-static completely antisymmetric torsion starting from the
RCW connections in a most natural form which implies that it can be taken
as derived from it and therefore it will propagate along the paths of the scalar
particles generated by it. Indeed, it simply amounts to introduce the duality
operation given by the Hodge star operator defined by the metric g,
∗ : sec(Λk T ∗ M ) → sec(Λn−k T ∗ M ), Ak ; ∗Ak ,

(13)

and further apply it to the trace-torsion one-form, i.e. we consider the seudothree-form ∗Q. Thus, if Q̂ denotes the drift vector field given by the g-conjugate
of Q, then ∗Q = iQ̂ volg ; see page 362 in Frankel [48]. Thus we note that this
duality depends on the choice of an orientation, and thus ∗Q has a built-in
chirality associated to it. While this pseudo-three-form does not appear in the
RCW laplacian, it is not an additional element of the structure, since it is
naturally derived from the RCW geometrical structure. As a final comment,
the equations of motion for the skew-symmetric torsion thus introduced, have
to be deduced from the equations for Q itself, but we shall not elaborate on this
further in the present article.

3

RIEMANN-CARTAN-WEYL DIFFUSIONS

In this section we shall present recall the correspondence between RCW
connections defined by (9) and diffusion processes of scalar fields having H(g, Q)
as infinitesimal generators (i.g. for short, in the following). For this, we shall
see this correspondence in the case of scalars. Thus, naturally we have called
these processes as RCW diffusion processes.. For the extensions to describe the
diffusion processes of differential forms, see [1].
For the sake of generality, in the following we shall further assume that
Q = Q(τ, x) is a time-dependent 1-form. The stochastic flow associated to
the diffusion generated by H0 (g, Q) has for sample paths the continuous curves
τ 7→ x(τ ) ∈ M satisfying the Itô invariant non-degenerate s.d.e. (stochastic
differential equation)
dx(τ ) = X(x(τ ))dW (τ ) + Q̂(τ, x(τ ))dτ.

(14)

In this expression, X : M × Rm → T M is such that X(x) : Rm → T M is linear
for any x ∈ M , the noise tensor, so that we write X(x) = (Xiα (x)) (1 ≤ α ≤ n,
1 ≤ i ≤ m) which satisfies
Xiα Xiβ = g αβ ,
(15)
where g = (g αβ ) is the expression for the metric in covariant form, and {W (τ ), τ ≥
0} is a standard Wiener process on Rm . Now , it is important to remark that
9

Quantizatin in Astrophysics ...

284

here m can be arbitrary, i.e. we can take noise tensors defined on different
spaces, and obtain the space diffusion process. In regards to the equivalence
between the stochastic and the geometric picture, this enhances the fact that
there is a freedom in the stochastic picture, which if chosen as the originator
of the equivalence, points out to a more fundamental basis of the stochastic
description. This is satisfactory, since it is impossible to identify all the sources
for noise, and in particular those coming from the vacuum, which we take as
the source for the randomness.
Here τ denotes the time-evolution parameter of the diffusion (in a relativistic setting it should not be confused with the time variable; we shall discuss
more this issue further below), and for simplicity we shall assume always that
τ ≥ 0. Indeed, taking in account the rules of stochastic analysis for which
dW α (τ )dW β (τ ) = δβα dτ (the Kronecker tensor), dτ dW (τ ) = 0 and (dτ )2 = 0,
we find that if f : R × M → R is a C 2 function on the M -variables and C 1 in
the τ -variable, then a Taylor expansion yields
f (τ, x(τ )) = f (0, x(0)) + [

∂f
∂f
+ H0 (g, Q)f ](τ, x(τ ))dτ + α (τ, x(τ ))Xiα (x(τ ))dW i (τ )
∂τ
∂x

∂
and thus ∂τ
+ H0 (g, Q) is the infinitesimal generator of the diffusion represented by integrating the s.d.e. (14). Furthermore, this identity sets up the
so-called martingale problem approach to the random integration of linear evolution equations for scalar fields [1], and for the integration of the Navier-Stokes
equation [49].Note, that if we start with eq. (14), we can reconstruct the associated RCW connection by using eq.(15) and the fact that the trace-torsion is
the g-conjugate of the drift, i.e., in simple words, by lowering indexes of Q̂ to
obtain Q.

3.1

The Time Variables

Since the Michelson-Morley experiment on the existence of an aether were
interpreted as giving negative results with regard to its existence, the introduction of the observer’s time variable to account for the Lorentz transformations
in the same status of the space variables, was the scheme of development of
physics thereafter. Thus the notion of spacetime was born, the Minkowski metric was introduced as its first example, and the geometrization of physics ensued
in terms of Lorentzian manifolds, in great measure due to the dissatisfaction of
Einstein with regards to Special Relativity. In spite that a Lorentz invariant
Brownian motion has been recently constructed by Oron and Horwitz [5] -and
further applied to the equivalence of the Maxwell and Dirac-Hestenes equation
[2]- in terms of a modification of the Gaussian distribution which turns out
to be is invariant by Lorentz transformation, the whole program of quantum

10

Quantizatin in Astrophysics ...

285

mechanics from the point of view of Feynman path integrals and its applications to quantum field theory requires an Euclidean signature for spacetime.
Also, the construction of Brownian motions starting from the stochastic differential equations introduces an Euclidean spacetime structure in contrast with
the Lorentzian degenerate metrics of General Relativity. So, if we wish to relate
the spacetime geometry to Brownian motions and quantum mechanics, we need
an Euclidean metric. The receipt for this has been to take the analytical continuation in the observer’s time variable. Another way of handling this time variable
which has to do with an Euclidean signature, is to work with the universal time
variable initially proposed by Stuckelberg [7] which by the way was the parameter used in quantum field theory, as we proposed before [2]. This choice can be
further substantiated from the divergence-free classical theory of the electron
recently proposed by Gill, Zachary and Lindesay [6]. In this theory, we equate
the Minkowski metric (dt)2 − (dx)2 − (dy)2 − (dz)2 , where t is the time of the
observer, with (dτ )2 , where τ is the time of the source, or still, we can write this
in the equivalent Euclidean metric (dt)2 = (dτ )2 + (dx)2 + (dy)2 + (dz)2 . If we
write the Lorentz-invariant equations of electromagnetism in the new Euclidean
variables (τ, x, y, z), then we get a mathematically equivalent set of equations for
electromagnetism; these equations in particular apply to the non-exact terms of
the trace-torsion Q, as we shall see in this article. But, from the point of view
of physics, there is a transformation between a passive time registered by the
observer to a different quality of process, which we call time, and is proper to the
source. To start with, τ is a non-integrable parameter, i.e. it is path-dependent
[6], and thus it has to do with non-conservative processes. Thus the equations of
electromagnetism while being mathematically equivalent in the Euclidean and
Minkowski space, in the former case they have an additional term which is dissipative (and describes the radiation reaction) appearing in the wave equations
of the electric and magnetic terms; this longitudinal term is proportional to the
inner product of the velocity with the acceleration. In this setting, a classical
theory for the electron without divergences is achieved. It was further proved
that for a closed system of particles, a global inertial frame and unique invariant
global time parameter for all observers is defined in ref. [6]. Thus τ which is the
time-evolution parameter of the diffusion process (and in the general space and
time manifold M case is not to be confused with the time variable t of General
Relativity 5 ) may be related with the time variable introduced by Stueckelberg
(and then introduced in quantum field theory), further elaborated by Piron and
Horwitz and in several works by Horwitz and coworkers [8], Fanchi [9], Trump
and Schieve [10], Pavsic [44], and in the context of a Schroedinger spacetime
operator, by Kyprianidis [11], Collins and Fanchi [12] and the present author
5 David Bohm proposed in a paper that appeared in www.duversity, and presently inaccesible (yet, this discussion can be found in elaborate form in the book, The Bohm-Bennett
Correspondence 1962-64, DuVersity, 1997), that time was three-fold: time of the source, time
of the observer and time of repetition, which was called hyparxis by J.G. Bennett.

11

Quantizatin in Astrophysics ...

286

[2] 6 . Thus, the modification from the passive observer’s time to the Euclidean
time of the source allows to define simultaneity, while from the physical point
of view, it has the meaning of a dissipative process being ascribed to the source.
So, we are very far from the trivial passive linear time variable which was incorporated by the Minkowski metric substituted here by a non-integrable function
which allows to establish the universality of the observer itself. The fact that
the evidence of the time variable which is no longer a mere registration by the
observer, is the dissipative process associated to this transformation from the
Minkowski space to the Euclidean one is remarkable. If one would downplay
the sheer subsistence of Special Relativity with regards to the existence of the
aether, if proven to exist, the role of geometries to describe physical processes is
enhanced precisely if the Brownian motions described above are the very essence
of this aether.
There has been in the last fifty years a number of experiments, mostly carried out in the former Soviet Union by Kozyrev, that have shown the existance
of another role for time that the mere relational linear variable that we have
inherited from Newtonian mechanics, and that in Special Relativity has been
incorporated to the Minkowski metric. In these experiments 7 the role of time
appears precisely in terms of dissipative processes and it is evidenced through a
field which cannot be shielded and propagates with an estimated velocity of 109 c
[19]. Kozyrev interpreted his experiments as a proof of the reality of Minkowski
space [20]. From the so called causal mechanics due to Kozyrev [13], Levich [22]
and M.M. Lavrenteiev [21], it follows that asymmetrical (irreversible) time is an
active substance, through which the transaction of distant dissipative processes
of any nature can take place, being this transaction not only universal in nature, but also running both with retardation and advancement. The proposal of
this formative character of time was forwarded not from an abstract quest, but
from the need for solving astronomical and astrophysical problems. Kozyrev
rejected the idea that the source for the stars energy were fusion reactions [13]
8
, and proposed instead that a substantial active time was related to this [20].
In fact, recent measurements of the Sun, seem to confirm Kozyrev’s rejection
to the present theory [28]. In this regard, the transactional interpretation of
quantum mechanics was proposed as a possible explanation, and as a second
perspective, the existence of nonlocal correlations in the strong macroscopic
limit. This was applied to the forecast of geomagnetic and solar processes, with
very good approximation with the actual processes that came to being after
123 days of observations [23]. In Kozyrev’s theory, the active time parameter is
6 Furthermore, the relativistic theory with the τ parameter predicts the interference in
time of the wave function (see Horwitz and Rabin [45] which has been recently been verified
experimentally [46]. We shall discuss further below a serious of experiments carried out by
Kozyev and followers, where time appear as having an active role.
7 A number of these experiments have been repeated recently by Kaivarainen [69].
8 Kozyrev was a reknown astronomer of his time, he predicted the volcano eruptions that
were observed in the Moon in the late fifties.

12

Quantizatin in Astrophysics ...

287

realized through angular momentum, and thus it can be naturally be associated
with a completely skewsymmetric torsion tensor. In the presentation of the relation between the Schroedinger equation, torsion fields and Brownian motions,
we shall see that the actual irreversible time invariant Brownian motion of the
process that can be linked with the RCW connection with trace-torsion given
by electromagnetic potential and the exact logarithmic differential of the distribution density of the Brownian motion, this density is formed by interpolation
between the initial and final distributions of the density, which by the way,
form the Schroedinger wave function. In this perspective, non-relativistic quantum mechanics which is designed in terms of the time variable which coincides
with the time-universal parameter, has the same features that these remarkable
processes observed by Kozyrev and followers, it incorporates the past and the
future into its setting. While being formally time-symmetric, the Schroedinger
equation admits a realization in terms of the future evolving Brownian process
built from a RCW connection in which the Schroedinger field is part of its drift
through the gradient of its logarithm. As we have seen already, we can take
the Hodge dual of the trace-torsion, say, the one that through metric conjugation yields the drift of the Brownian motion associated to the wave function of
the Universe, and thus obtain a pseudo-three-form that may be associated with
the angular momentum field characteristic of the experiments carried out by
Kozyrev. A different approach to explain the Kozyrev phenomenae, but which
may be related to the present one, has been developed in remarkable unified
theory of physics, biology and consciousness by Kaivarainen in terms of the so
called bivacuum structures; for the explanation of the Kozyrev phenomenae,
the so called spin guide fields play a fundamental role [69]. In our approach, it
is the universal Brownian motions linked to the wave function of the Universe,
and the Hodge dual of the trace-torsion of these diffusions, which would produce
the same effect that the spin guide fields due to Kaivarainen.

4

THE HODGE DECOMPOSITION OF THE
TRACE-TORSION FIELD

To obtain the most general form of the RCW laplacian in the non-degenerate
case, we only need to know the most general decomposition of 1-forms. To start
with, in this section, we have a smooth orientable n-manifold M provided with a
Riemannian metric g. We consider as above, the Hilbert space given by the completion of the pre-Hilbert space of square-integrable smooth differential forms of
degree k (0 ≤ k ≤ n) on M , with respect to the Riemannian volume volg , which
13

Quantizatin in Astrophysics ...

288

we denote as L2 (sec(Λk (T ∗ M )). We shall focus on the decomposition of 1-forms,
so let ω ∈ L2 (sec(T ∗ M )); then we have the Hilbert space decomposition
ω = df + Acoex + Aharm ,

(16)

where f is a smooth real valued function on M , Acoex is a smooth coexact 1form, i.e. there exists a smooth 2-form, β such that δβ2 = Acoex 9 , so that Acoex
is coclosed, i.e.
δAcoex = δ(δβ2 ) = 0,

(17)

and Aharm is a closed and coclosed smooth 1-form, then
δAharm = 0, dAharm = 0,

(18)

or equivalently, Aharm is harmonic, i.e.
41 Aharm ≡ trace(∇g )2 Aharm − Rβα (g)(Aharm )α γ α = 0,

(19)

µβ
with Rβα (g) = Rµα
(g) the Ricci metric curvature tensor. Eq. (16) is the sourceless Maxwell-de Rham equation. An extremely important fact is that this is a
Hilbert space decomposition, so that it has unique terms, which are furthermore
orthogonal in Hilbert space, i.e.

((df, Acoex )) = 0, ((df, Aharm )) = 0, ((Acoex , Aharm )) = 0,

(20)

so that the decomposition of 1-forms (as we said before, this is also valid for
k-forms, with the difference that f is a k − 1-form, β2 is really a k + 1-form and
Aharm is a k-form) has unique terms, and a fortiori, this is also valid for the
Cartan-Weyl 1-form. We have proved that Acoex and Aharm are further linked
with Maxwell’s equations, both for Riemannian and Lorentzian metrics. For
the stationary state which we shall describe in the next section, they lead to
the equivalence of the Maxwell equation and the relativistic quantum mechanics equation of Dirac-Hestenes in a Clifford bundle setting [2,49] whenever the
coclosed (Hertz potential) term and the Aharonov-Bohm harmonic term are
both dependent on all the 4D variables while they are infinitesimal rotations
defined on the spin-plane. 10 . Further, in regards to the above mentioned classical theory of the electron due to Gill, Zachary and Lindesay [6], the validity
9 Here

δ denotes the codifferential operator, the adjoint of d, introduced above.
problem of equivalence of the Maxwell and Dirac-Hestenes equations has been presented in a general framework in Rodrigues and Capelas de Oliveira [77]. In that work, torsion
appears as a mathematical entity in terms of which the whole theory is constructed , yet is
presented as unrelated to any recognizable physical field and particularly having no relation
with Brownian motions; furthermore, the Hertz potential which is in our formalism is related
to the coclosed term, has a fundamental role but is unrelated to torsion, and it is claimed
that electromagnetism is a separate phenomena to gravitation, while the present approach
following [2] claims the opposite. Furthermore, the equivalence between these equations is
established for the exact term of the trace-torsion one-form only [79], while in [2] and [49]
it is proved for the general case for Q, not restricted to the exact term as described above.
The equivalence between these equations in a biquaternionic setting has been established by
Yefremov [71].
10 The

14

Quantizatin in Astrophysics ...

289

of this decomposition in a Riemannian metric, say , Euclidean space, points
to the validity of having this theory of Brownian motion formulated in a nondegenerate (albeit trivial) space-time: these electromagnetic potential terms
can be associated with a classical electron which does not require a quantum
treatment and allows the introduction of a global time parameter. Furthermore,
by studying the topological dimension of the trace-torsion, i.e. the irreducible
number of minimal dimensions in space and time on which its coefficients are
dependent, we can introduce helicity, spinor structures, minimal surfaces associated to them, superconductivity, turbulence and coherent structures, in short, a
topological theory of processes, following the studies by R. Kiehn [47]. Thus, in
this approach we can introduce spinor structures on looking to the topological
features of the trace-torsion.

4.1

The Decomposition Of The Cartan-Weyl Form And
The Stationary State

We wish to elaborate further on the decomposition of Q in the particular
state in which the diffusion process generated by H0 (g, Q) and its extensions
to differential forms, in the case M has a Riemannian metric g, and has a τ invariant state corresponding to the asymptotic stationary state. Thus, we shall
concentrate on the diffusion processes of scalar fields generated by
H0 (g, Q) =

1
(4 + LQ̂ ), with Q = dlnψ 2 + Acoex + Aharm .
2

(21)

This is the invariant form of the (forward) Fokker-Planck operator of this theory (and furthermore of the Schroedinger operator when introducing the phase
function to the exact term of Q). Through this identification, we note that ψ
is the scale field in the Einstein λ transformations from which in the vacuum,
the RCW geometry can be obtained ; see [2]. We are interested now in the
volg -adjoint operator defined in L2 (sec(Λn (T ∗ M ))), which we can think as an
operator on densities, φ. Thus,
H0 (g, Q)† φ =

1
(4g φ − divg (φgrad ln φ) − divg (φÂ)).
2

(22)

The operator described by eq. (20) is the backward Fokker-Planck operator.
The transition density p∇ (τ, x, y) is determined by the fundamental solution
(i.e. p∇ (τ, x, −) → δx (−) as τ → 0+ ) of the equation on the first variable
∂u
= H0 (g, Q)(x)u(τ, x, −).
∂τ
15

Quantizatin in Astrophysics ...

290

(23)

Then , the diffusion process {x(τ ) : τ ≥ 0}, gives rise to the Markovian semigroup {Pτ = exp(τ H0 (g, Q)) : τ ≥ 0} defined as
Z
(Pτ f )(x) = p∇ (τ, x, y)f (y)volg (y).
(24)
It has a unique τ -independant-invariant state described by a probability density ρ independant of τ determined as the fundamental weak solution (in the
sense of the theory of generalized functions) of the τ -independent Fokker-Planck
equation:
1
(25)
H0 (g, Q)† ρ ≡ (−δdρ + δ(ρQ)) = 0.
2
Let us determine the corresponding form of Q, say Qstat = dlnψ 2 + Astat . We
choose a smooth real function U defined on M such that
H0 (g, Qstat )† (e−U ) = 0,

(26)

−de−U + e−U Q = δ(−δΠ + Aharm ),

(27)

so that

for a 2-form Π and harmonic 1-form Aharm ; thus, if we set the invariant density
to be given by ρ = e−U volg , then
Qstat = dlnψ 2 +

A
, with A = −δΠ2 + Aharm .
ψ2

(28)

Now we project ψA2 into the Hilbert-subspaces of coexact and harmonic 1-forms,
to complete thus the decomposition of Qstat obtaining thus Hertz and AharonovBohm potential 1-forms for the stationary state respectively. Yet these potentials have now a built-in dependence on the invariant distribution, and although
they give rise to Maxwell’s theory, the interpretation is now different. 11 Indeed, we have an inhomogeneous random media, and these potentials depend
on the τ -invariant distribution of the media. We have seen in [2] that these
potentials appear in the context of the equivalence of the Maxwell sourceless
equation on Minkowski space written in terms of a Dirac-Hestenes spinor field,
and the non-linear Dirac-Hestenes equation for these fields, albeit in Minkowski
space provided with a RCW connection with trace-torsion given by Qstat . Yet,
we can exploit further the Hodge-decomposition of Qstat to manifest the quantum potential as built-in. Indeed, if we multiply it by ψ and apply ∂, then we
11 A word of caution. In principle, −δΠ/ρ and A
harm /ρ may not be the coexact and harmonic
components of A/ρ respectively. If this would be the case, then we obtain that dlnψ is g −1 orthogonal to both −δΠ and Aharm ; furthermore dlnψ ∧ Aharm = 0, so furthermore they are
collinear. This can only be for null Aharm or constant ρ, so that the normalization of the
electromagnetic potentials is by a trivial constant. In the first case the invariant state has the
sole function of determining the exact term of Q to be (up to a constant) dlnψ.

16

Quantizatin in Astrophysics ...

291

get that dlnψ, and the coexact and harmonic terms of Qstat decouple in the
resultant field equation which turns out to be
4g ψ = [g −1 (dlnψ, dlnψ) − δdlnψ]ψ,

(29)

with nonlinear potential V := g −1 (dlnψ, dlnψ) − δdlnψ, which has the form of
(twice) a relativistic quantum potential extending Bohm’s potential in nonrelativistic quantum mechanics [51]. We have seen in [2], that from scaleinvariance it follows that the quantum potential coincides up to a conformal
factor with the metric scalar curvature, as we shall elaborate below.
Finally, we want to recall the essence of the problem of time-invariance of
the diffusion processes on the invariant state. In this setting, following the
Kolmogorov characterization [70], τ -reversibility is verified whenever for any
two smooth compact supported functions f, h defined on M , we have that
Z
Z
(H0 (g, Q)f )(x)h(x)ρ(x)volg (x) = f (x)(H0 (g, Q)h(x))ρ(x)volg (x) (30)
and thus it can be seen that this is the case if and only if δΠ and Aharm vanish
completely, and thus Q = 21 dlnρ. This will be of importance when studying the
problem of the reduction of the wave function when considering the representation of the Laplacian operator, or still, as it have the same eigenstates, the
Schroedinger operator on the state-space of quantum mechanics.

5

ENERGY FORMS, THE QUANTUM POTENTIAL AND RCW DIFFUSIONS

In this section we shall show that the RCW geometries yield a natural
formulation of quantum mechanics on manifolds, as an operator theory on two
Hilbert spaces [2]. So, this section and the next, we will discuss basic issues
which on the usual setting have been somehow obviated and are far from being
obvious. The basic formalism which leads to this is the well known remarkable
correspondence explored in flat Euclidean space between the Dirichlet forms of
potential theory, Markovian semigroups and their diffusion processes [37][73]
and RCW laplacian operators [2], and originates in the canonical commutation
relations. In fact, in quantum field theory on curved space-time, the starting
point is an energy functional for the field associated to a self-adjoint operator
on the Hilbert space determined by the Riemannian volume element [71]. In our
theory, this self-adjoint operator will appear to be the conformal transform of the
self-adjoint extension of the RCW laplacian as defined on an adequate subspace
of the ground-state Hilbert space with a weighted inner product defined by
the invariant density . Thus, two Hilbert spaces are needed: the ground-state
Hilbert space on which we have a diffusion generated by the RCW laplacian
which acts as the Fokker-Planck operator, and the Hilbert space defined by the
17

Quantizatin in Astrophysics ...

292

Riemannian volume in which this operator transforms into the Schroedinger
operator. We shall present below the above mentioned correspondences.
We assume that M has a Riemannian metric; we assume further that is
four-dimensional space-time (and thus, we are in the situation discussed in [2]
and references therein) and a diffusion process with stationary state ψ 2 volg with
null electromagnetic terms in eq. (28), generated by H0 (g, dlnψ 2 ) = 12 (4g +
gradlnψ 2 ), a Hamiltonian operator on the Hilbert space L2 (ψ 2 volg ); thus, the
drift vector field is gradlnψ. With abuse of notation, let us denote still as
H0 (g, dln ψ 2 ) the Friedrichs self-adjoint extension [21,43] of the infinitesimal
generator given in eq. (27) with domain given by D, the space of compact
supported infinitely differentiable functions on M ; for related discussions on
this extension, we ellaborate further in Section VII below). We can now define
the inner product
Z
(f1 , f2 )ρ = 1/2 g −1 (df1 , df2 )ψ 2 volg
(31)
By integration by parts, we obtain
(f1 , f2 )ρ = −(f1 , H(g, dln ψ 2 )f2 )ρ
ρ

2

(32)
2

where (., .) denotes the weighted inner product in L (ψ volg ). Let us consider
now the closed quadratic form, (the Dirichlet form) q associated to (., .)ρ , i.e.
q(f ) = (f, f )ρ . We see from eq.(32) that there is a unique Hamiltonian operator which generates q, it is the self-adjoint operator −H0 (g, dlnψ 2 ). Since the
quadratic form is positive, q(f ) ≥ 0 , for any f ∈ L2 (ψ 2 volg ), then H0 (g, dlnψ 2 )
is a negative self-adjoint operator on L2 (ψ 2 volg ) and the Markovian semigroup
exp(τ H(g, dlnψ 2 )) is defined. Let us see how this construction is related to
the usual formulation of Quantum Mechanics in terms of quadratic forms in
L2 (volg ), which in the non- relativistic flat case has been elaborated by several
authors [73]. Consider the mapping Cψ : L2 (ψ 2 volg ) → L2 (volg ) defined by
multiplication by ψ; this is the groundstate transformation and defines a conformal isometry between the two Hilbert spaces. This map takes C0∞ (M ) into
itself. For any f in C0∞ (M ) we have
q(ψ −1 f )

(ψ −1 f, ψ −1 f )ρ
Z
= 1/2 {g −1 (df, df ) − 2g −1 (df, d ln ψ)f + g −1 (dlnψ, dlnψ)f 2 }volg
Z
= 1/2 {g −1 (df, df ) + (divg (b)f 2 + g(b, b)f 2 }volg
Z
1
(33)
=
f {− 4g + V }f volg = (f, Hf )L2 (volg )
2
=

where we denoted b = grad lnψ which is the drift vector field of the process
generated by H0 (g, dlnψ 2 ) since by eqs. (47) and (55) this is 21 grad lnψ 2 and
H = Cψ ◦ H(g, dln ψ 2 ) ◦ Cψ−1 = −1/24g + V,
18

Quantizatin in Astrophysics ...

293

(34)

where in the weak sense,
V =

4g ψ
1
(divg b + g(b, b)) =
,
2
2ψ

(35)

is the relativistic quantum potential ; here, in distinction with Bohm’s quantum
potential in non-relativistic Quantum Mechanics [68] (which is retrieved in the
case of n = 3 and g the Euclidean metric), it depends on both the space and
time-t coordinates. Then, we have proved that −H(g, dlnψ 2 ) is unitarily equivalent to the Hamiltonian operator H := − 21 4g + V defined on L2 (volg ) and ψ is
a generalized groundstate eigenfunction of H with 0 eigenvalue. The non-linear
dependence of V on the invariant density introduced by ψ introduces non-local
correlations on the quantum system We shall see below that this dependence
of V on ψ is removed due to conformal invariance. This will establish that
the Schroedinger operator H has for quantum potential one-twelfth of the Riemannian scalar metric and thus H coincides with the Riemannian conformal
invariant wave operator considered in quantum gravity in curved spaces [71].
We shall now elaborate on these aspects.

6

THE MEAN CURVATURE EXTREMAL PRINCIPLE

Since at the level of constitutive equations for Q, the electromagnetic
potentials decouple from the ψ-field (see the discussion that lead to eq. (29))
we can study independently the field equations from which the RCW connection
with exact Q can be derived. We shall assume that n = 4. We start with a
general Riemann-Cartan connection (Γab
α ), (where Greek letters denote spacetime indices as until now, and Latin letters denote anholonomic indices), and
we introduce its scalar curvature
β ..ab
R(Γ) = eα
a eb Rαβ ,

(36)

a b
where the eα
a is a field of invertible tetrads with gαβ = δab eα eβ , with δab the
12
..ab
ab
Euclidean metric , and Rαβ is the curvature tensor of (Γα ) [37]. Now we recall
the Einstein’s λ transformations of above (here ρ will be substituted by a scalar
12 All the following definitions of the λ transformations and the ensuing field equations
are valid as well if we take here the Minkowski metric; since we do not know whether our
construction of a relativistic Brownian motion carries from the Minkowski space to general
Lorentzian metrics , in this section we shall keep the metric to be positive-definite for which
we take the initial metric to be Euclidean. Brownian motions the Schwarszild metric has been
recently constructed on the unit tangent manifold (see J. Franchi and Y. Le Jan,Relativistic
Diffusions, arXiv:math.PR/0403499). The relation of this construction, with the Lorentzinvariant Brownian motions on Minkowski space presented in [5, 2] and the present article is
unknown.

19

Quantizatin in Astrophysics ...

294

field φ): Let φ be a real function on M . Then λ(Γaαb ) := Γaαb , and λ(eα
a ) :=
2 αβ
φ−1 eα
so
that
λ(g
)
=
φ
g
and
then
the
scalar
curvature
transforms
as
αβ
a
λ(R(Γ)) = φ−2 R(Γ), and finally volλ(g) = φ4 volg . Since the scalar fields ψ
transform as λ(ψ) = φ−1 ψ, we get that the functional
Z
A(Γ, ψ, g) = R(Γ)ψ 2 volg ,
(37)
is invariant by the set of λ transformations, i.e.: A(λ(Γ), λ(ψ), λ(g)) = A(Γ, ψ, g).
Notice that if from the field equations we obtain that ψ 2 volg can be identified with the unique invariant density of the diffusion process generated by
H0 (g, dln ψ 2 ), then (37) is the mean Riemann-Cartan scalar curvature. Taking
variations with respect to g we obtain that
Rαβ (Γ) − 1/2gαβ R(Γ) = 0,

(38)

i.e. the Einstein-Cartan equations for Γ in the vacuum, while by taking variations with respect to Γγαβ , we obtain that torsion tensor is a particular case of
the one we derive from the anticommutator of eq. (9), since we have
γ
Tαβ
= δαγ ∂β lnψ − δβγ ∂γ lnψ,

(39)

so that, up to factor of 3 which we shall absorb so we shall take Q = dln ψ and
thus the field equations have yielded a RCW structure with exact Q. Taking
variations with respect to ψ we get the teleparallelism: R(Γ) = 0; replacing
eq. (39) in eq. (38) we get the field for the Einstein metric tensor Gαβ (g) =
Rαβ (g) − 21 R(g):
Gαβ (g) = −

1
6
∂α ψ ∂β ψ − 1/2gαβ ∂γ ψ∂ γ ψ − (∇α ∇β ψ 2 − gαβ 4g ψ 2 ),
2
ψ
6

(40)

where in the r.h.s. we identify (up to a factor) minus the improved energymomentum density of the scalar field in renormalizable gauge theories. Now,
by taking the trace in this equation we finally get
1
(4g − R(g))ψ = 0,
6

(41)

so that ψ is a generalized groundstate of the conformal invariant wave operator
defined on L2 (volg ). Note that from eqs. (34, 35, 41) we conclude that the
1
quantum potential is 12
R(g) which does not depend on the scalar field ψ at
all. Therefore, the correlations on the quantum system under Brownian motion
with drift given by b = grad lnψ are mediated by the metric scalar curvature
(which, of course, does not depend on ψ any more; this is the form invariance
of the quantum potential [3])! Otherwise stated and in view of the relation
between the noise tensor and the Riemannian metric (see the discussion after
20

Quantizatin in Astrophysics ...

295

eq. (15)), when we have an anisotropic noise tensor we have constructed a nontrivial metric and quantum non-local correlations which are due to the metric
scalar curvature.
Solving the conformal invariant wave equation with Dirichlet regularity conditions on the closure of an open neighborhood of M [27], we obtain a conformally conjugate Dirichlet form whose associated Hamiltonian operator is
−H0 (g, dln ψ 2 ), with ψ a solution of eq. (41) and thus the Markovian semigroup
determined by it can be reconstructed by reversing the steps in the previous Section. We shall finally establish the relation between the heat kernel pconf (τ, x, y)
of the Markovian semigroup exp( τ2 H) and the heat kernel pψ (τ, x, y) of the RCW
semigroup. We have
exp(τ H0 (g, dln ψ 2 ))f (x)

τ
= ψ −1 (x)exp( H)(ψf )(x)
2
Z
−1
=
ψ (x)pconf (τ, x, y)φ(y)f (y)volg (y) (42)

so that we conclude that
pψ (τ, x, y) = ψ −1 (x)ψ(y)pconf (τ, x, y).

(43)

Thus, we have linked the kernels of the quantization in the two Hilbert
spaces, the groundstate Hilbert space L2 (ψ 2 volg ), and L2 (volg ). The former
corresponds to the RCW geometry, while the latter is the usual Hilbert space
for the quantization of the kinetic energy of a spinless massive free-falling testparticle, in terms of the Riemannian invariants of the manifold M described in
terms of g! We remark that the introduction of both spaces and the unitary
transformation between them, has allowed us to identify the quantum potential,
while working only in the usual Hilbert space would not have allowed for this
identification; finally, the scalar curvature term so much discussed has been
found to be a resultant of the λ invariance of the theory, and not the resultant
of technicalities in computing the propagators; as discusssed already in [2], this
theory has no ordering problem [44]. Thus, in the L2 (volg ) space we have found
the Hamiltonian operator considered by B.de Witt, and reencountered by several
researchers in quantum field theory in Riemannian geometries through the shortτ expansion of pconf (τ, x, x) [71] in geometrical and topological invariants , and
for the path integral representations for Fokker-Planck operators [72], which
as we already saw, when g is Riemannian, are precisely of the form H0 (g, Q).
Yet, our result is in disagreement with the path integral representation of the
classical kinetic energy of a massive particle in a Riemann-Cartan geometry due
to Kleinert, in which he obtains twice the quantum potential (see, chap. X,
[?])13 .
13 For a discussion on the work of Kleinert and the role of autoparallels we suggest the reader
to return to Remarks 1 and footnote no. 8 before Section 5 of [1].

21

Quantizatin in Astrophysics ...

296

7

RCW DIFFUSIONS AND NON-RELATIVISTIC
QUANTUM MECHANICS

From the previous section we know that for the stationary state defined
by ρ the laplacian defined by a RCW connection is symmetric with respect to
the measure defined by ρ if and only if the trace-torsion is given by Q = 21 dlnρ.
Futhermore, it is a non- positive-definite operator since for any functions in the
space D of compact supported functions u and v defined on M we have the
Green identity
Z
Z
Z
u(H0 (g, Q)v)ρvolg = −
g(∇u, ∇v)ρvolg =
v(H0 (g, Q)u)ρvolg . (44)
M

M

M

We wish to see if there exists a self-adjoint extension of H0 (g, Q)|D in the space
L2 = L2 (M, ρ) of square-integrable functions with respect to the density ρvolg .
Consider the space W 1 (M, ρ) = {f : M → C, f ∈ L2 , ∇f ∈ L2 } where we
mean by ∇f the distributional gradient. We can turn this space into a complex
Hilbert space by working with complex-valued functions provided with the inner
product
Z
Z
(u, v)W 1 =
uv̄ρvolg +
g(∇u, ∇v̄)ρvolg .
(45)
M

M

Let W01 be the closure of D in W 1 ; define W02 = W02 (M, ρ) = {f ∈ W01 /
H0 (g, dlnρ)f ∈ L2 } where the latter action of the operator is meant in the
distributional sense. Since D ⊂ W02 , then H0 (g, dlnρ)|W01 is an extension of
H0 (g, dlnρ)|D . Therefore −H0 (g, dlnρ) is a positive-definite self-adjoint extension defined in L2 . Furthermore, if M is geodesically complete, then H0 (g, dlnρ)|W02
is a unique self-adjoint extension of H(g, dlnρ)|D . 14
Consider next the Dirichlet problem for H0 (g, Q|W02 on a relatively compact
non-empty set Ω in M , so that

H0 (g, Q)u + λu = 0 in Ω,
u = 0 in ∂Ω,
where λ is constant. This can be considered in the weak sense: We look for a
non-zero function u ∈ W01 (Ω, ρ) such that for all v ∈ W01 (Ω, ρ),
Z
Z
−
g(∇u, ∇v)ρvolg + λ
uvρvolg = 0.
(46)
Ω

Ω

14 A

1
short approach to the proof. The first
R part is the fact that the space W0 is1a Hilbert
space so that the quadratic form E(u, v) =
g(∇u, ∇v)ρvolg with the domain W0 is closed
M
in L2 . Therefore, it has the generator, which is self-adjoint with domain W02 and hence, is
the Friedrich extension of H(g, 21 dlnρ)|D ; see [36][37].

22

Quantizatin in Astrophysics ...

297

It is easy to prove that u is a solution of this problem if and only if u ∈ W02 (Ω, ρ)
and H0 (g, Q)u + λu = 0. Considering then the manifold Ω provided with the
density ρ, we conclude that the eigenvalues of the weak Dirichlet problem in
Ω are exactly the eigenvalues of the self-adjoint operator −H(g, Q)|W02 (Ω,ρ) in
L2 (Ω, ρ).
We have a theorem due to Rosenberg [37]: For any non-empty relatively compact open set Ω ⊂ M , the spectrum of −H(g, Q)|W02 (Ω,ρ) is discrete and consists
of a sequence {λk (Ω)}∞
k=1 of non-negative real numbers such that λk (Ω) → ∞
as k → ∞. If in addition M − Ω̄ is non-empty, then λ1 (Ω) > 0.
Assuming that the eigenvalues are counted with multiplicity, we have the
Weyl asymptotic formula
λk (Ω) ≈ cn ( R

2
k
) n , ask → ∞,
ρvol
g
Ω

(47)

where n = dim(M ) and the constant cn > 0 is the same as in Rn .
If M is compact, then we have λ1 (M ) = 0, because the function f = constant
is an eigenfunction. Since H0 (g, Q)f = 0 implies f = 0 (we are assuming that
M is connected), the multiplicity of the bottom eigenvalue is 1 and then λ2 (M )
is strictly positive. In any case, the lowest eigenvalue of −H0 (g, Q)|W02 (M,ρ) can
be determined as follows.
Furthermore, we have a theorem (Rayleigh Principle) for the minimal eigenvalue [37]: For a manifold M provided with a density ρvolg ,
R
|∇f |2 ρvolg
R
λmin (M ) = inf f ∈T −0 M
,
(48)
f 2 ρvolg
M
where T is any class of test functions such that D ⊂ T ⊂ W01 .
Proof: It follows from the variational principle for the operator −H0 (g, Q)|W02
and by the Green formula, that
R
− M (H0 (g, Q)f, f )volg
(49)
λmin (M ) = inf f ∈W02
||f ||2L2
R
|∇f |2 ρvolg
= inf f ∈W02 −0 M
,
(50)
||f ||2L2
and by observing that D ⊂ W02 ⊂ W01 and D is dense in W01 .

8

GEOMETRIC QUANTUM-MECHANICS ON
STATE-SPACE

We consider a complex separable Hilbert space H and a self-adjoint operator H
defined on H. The time development of quantum systems is given by the oneparameter group {e−itH , t ∈ R} of unitary operators. A pure quantum state
23

Quantizatin in Astrophysics ...

298

ψ ∈ H, ||ψ|| = 1, develops according to
ψt = e−itH ψ

(51)

which can be reformulated in terms of the Schroedinger equation
∂ψt
= −iHψt .
∂t

(52)

Still, pure quantum states are described by equivalence classes [ψ] of unit vectors
ψ ∈ H, where two vectors are equivalent if they differ by a complex phase factor.
Then, the time development of the state [ψ] is given by
Φt ([ψ]) = [e−iHt ψ].

(53)

While eqs. (51) and (52) are equivalent, this is no longer the case of eqs. (52)
(53), since ψt can contain a complex time dependant factor. The proper setting
for quantum mechanical evolution in terms of the Schroedinger equation requires
to take in account this indeterminate factor. So the state space is the projective
Hilbert space P(H), and the time evolution of quantum systems are curves on
this space of the form γ(t) = Φt ([ψ]) = [e−iHt ψ].
There are two ways in which one can construct from a heat semigroup defined by a RCW diffusion process its quantum Schroedinger representation. In
this case, the hamiltonian operator is H(g, Q) associated to a RCW connection
with Q = 12 dlnρ and the corresponding unitary group [e−iτ H(g,Q) ] defined on
the natural complex extension of a real Hilbert space as we have taken in the
previous section, corresponds to the so-called Euclidean analytical Schroedinger
representation for the diffusion semigroup defined by this space-time structure,
yet with some differences we would like to remark. Firstly, there is a freedom
upon the choice of the time, it can be τ for a relativistic theory in which g
can depend on t as well as Q and our space-time manifold is a 4-dimensional
manifold , M , or, we can write down a non-relativistic theory, for which τ and
t coincide [23] and space-time is R × M where M is a 3-manifold, but still
we have in this foliated manifold a Riemannian metric which may depend on
t as well as the trace-torsion Q; in any case, due to the fact that in Quantum
Mechanics observables are self-adjoint operator (real eigenvalues) we have to
restrict Q to be exact of the form Q = 12 dlnρ because the inclusion of the electromagnetic terms, following the Kolmogorov characterization of τ -symmetric
diffusion processes, produces H(g, Q) for general Q to be a non-symmetric operator in L2 (M, ρ), so we cannot introduce the self-adjoint extension of it. The
other possibility is to develop a covariant formulation of non-relativistic Quantum Mechanics in R × M in which we transform the diffusion processes into the
Schroedinger equation without applying the Euclidean time scheme, but in this
case Q does not necessarily restrict to the exact differential term, including thus
the electromagnetic terms and the Schroedinger operator is associated to the
RCW laplacian in an indirect way in which will adquire the form 4g + V , where
24

Quantizatin in Astrophysics ...

299

V is a potential which can eventually depend on the wave function or not,which
for appropiate classes of potentials V can result in a self-adjoint operator; see
page 34 in Schechter [39]. Both theories we know already how to formulate as an
infinite-dimensional Hamiltonian system (in the sense of classical mechanics), as
long as the spectrum of H(g, Q) is discrete, which in the case of Q restricted to
be exact, is already the case as discussed above. In this article, we shall present
both alternatives. Finally, having set the geometric approach to quantum mechanics in Hilbert space, we can further study the so-called stochastic extension
of the Schroedinger equation, which amounts to write the s.d.e. which extend
the Hamiltonian flow with a noise term which drives the system to a particular
eingenstate, providing thus for the reduction of the wave function.

9

THE STATE-SPACE QUANTUM GEOMETRIES, BROWNIAN MOTIONS AND THE
REDUCTION OF THE WAVE FUNCTION

The notion of a geometric theory of quantum mechanics has been in most of
the works associated with the idea of placing in a purely geometrical context the
operator formalism of quantum mechanics and describing the processes of observation in terms of geometrical distance in state-space; the other approach that
can be named identically as quantum geometry, is the present approach that is
valid for both configuration manifolds and state-space manifolds . The former
geometrical approach has lead to formulate non-relativistic quantum mechanics
as a theory of Kahler manifolds, and to breach the gap with classical mechanics
which as well known, is formulated in terms of symplectic flows, and in particular, those associated with a Hamiltonian function independent of time. The
Hamiltonian function that generates the Schroedinger flow is non other that the
expectation value function defined on state-space of the quantum Hamiltonian
operator. In this so called quantum geometry (see [15,16,18] and references
therein), the Schroedinger equation is a symplectic flow in state-space, given
by a complex projective manifold, provided with the Fubini-Study metric, with
its naturally associated symplectic and Kahlerian structures. Furthermore, by
considering random perturbations of this symplectic flow to account for the role
of the environment in the quantum system, the reduction of the wave-function
has been described in terms of stochastic processes on the quantum geometry on
state-space [27,28]. This approach to the so-called open Schroedinger equation
has been elaborated as an emergent theory of a background statistical theory of
unitary matrices. In none of this approaches to the open Schroedinger equation,
no relation was established with the fact that there is a quantum geometry in
space-time and its association with Brownian motions. Thus, this chapter aims
to present a very short account of the fact that we can describe the stochas-

25

Quantizatin in Astrophysics ...

300

tic processes in state-space that describe the reduction of the wave-function in
terms of the same stochastic-geometrical structures of Riemann-Cartan, and
that the Schroedinger symplectic flow defined by the expectation value of the
Hamiltonian operator is (up to a modification which drives the measurement
process to a specific eigenstate) the natural choice for the drift. In particular,one can start with a stochastic differential equation, consider the connection
on space-time defined by it and its differential generator which is the Laplacian
operator of this geometry, and study the reduction of the wave function of the
quantum evolution of this space-time operator. In this sense, the role of spacetime structures in producing the reduction of the wave function. So in this case,
we have a two layer structure of RCW type, one related to the diffusion process
in space-time and the second one, with the diffusion process in state-space that
follows when studying the spectra of the RCW laplacian, or can be carried out
independently for an arbitrary quantum system described by its Hamiltonian
operator. In the following we shall present both quantum geometries in a single
setting. In the following we follow our discussion in [77].
Let us assume we have a Hilbert space with finite dimension n + 1 so
we are dealing with M being CP (n), the complex projective space of dimension n, the space of rays of Quantum Mechanics, although the more general
infinite-dimensional case is also possible. In fact, this space not only carries a Riemannian metric, the Fubini-Study (FS) metric, which we denote
as g but also a symplectic two-form Ω and still an almost complex structure
provided by an endomorphism Jz : Tz M → Tz M such that J 2 = −I and
g(u, Jv) = Ω(u, v) for all u, v ∈ CP (n), z ∈ M . Indeed, denote the hermitean product of the the n + 1-dimensional Hilbert space of the quantum
system as < u, v >= g(u, v) + iΩ(u, v) where g(u, v) = < < u, v > and
Ω(u, v) = = < u, v >, and g(u, v) = g(Ju, Jv). Furthermore J is compatible with g, i.e. ∇J = 0, where ∇ is the Levi-Civita covariant derivative. Thus,
M provided with (g, Ω, J) becomes a Kahler manifold. For a self-adjoint Hamiltonian operator H defined on CP (n), we define the quantum-expectation value
function (H) : CP (n) → R by (H)(u) = <u,Hu>
<u,u> . In this section we shall then
restrict ourselves to the Euclidean technique and take H = H(g, Q) the selfadjoint operator defined in a finite-dimensional complex subspace of the Hilbert
space H = W02 ; this amounts to fixing a cut-off which one can fix in accordance with the estimates given above 15 . We denote the general state vector
by |z > with z standing for the complex projections z 0 , . . . , z n of |z > on an
z̄ α H

zβ

αβ
arbitrary fixed basis. Thus, (H)(|z >) = <z|H|z>
. Since (H) is ho<z,z> =
z̄ δ z δ
α α
mogeneous of degree zero on both z , z̄ we define the new complex coordinates
j
j
tj = zz0 and t̄j = z̄z̄0 , j = 1, . . . , n, which are well defined whenever z 0 6= 0. The
1

15 Actually,

we can take for this operator, the self-adjoint extension of H(g, Q) for Q = dlnρ 2
now acting on CP (∞) [8]; for the purpose of keeping this article to some length, we prefer
not to deal with this more general case which does not imply major differences with the
finite-dimensional case.

26

Quantizatin in Astrophysics ...

301

real manifold structure of CP (n) is defined by taking the coordinate system
(xa ), a = 1, . . . , 2n with x1 = <t1 , x2 = =t2 , . . . , x2n−1 = <t2n−1 , x2n = =t2n .
Thus, the specification of the 2n-vector (xa ) determines the unique ray containing the unnormalized state |z >. The FS metric g = (gαβ dz α ⊗ dz̄ β ) with
2
gαβ = 4 ∂zα∂∂ z̄β lnz̄ γ z γ written on the real manifold is g = (gab dxa ⊗ dxb ) with
(see S. Adler and L. Horwitz [16])
gab = 4

[(1 + xd xd )δab − (xa xb + ωac xc ωbd xd )]
,
(1 + xl xl )2

(54)

g ab =

1
(1 + xl xl )(δab + xa xb + ωac xc ωbd xd ).
4

(55)

with inverse

where ωab is a skewsymmetric tensor whose only non-vanishing terms are
ωa=2j−1b=2j = 1, ωa=2j,b=2j−1 = −1. Furthermore, the complex structure
J = (Jab ) satisfies Jab Jbc = −δac , and the identities Jac Jcd gcd = gab and the symplectic form Ω = Ωab dxa ∧ dxb satisfies Ωab = gbc Jac with inverse Ωab = g ac Jcb .
Then (SE) takes the form (we take h̄ = 1) of the Hamiltonian flow on M given
a
ab
by dx
dt = 2Ω(∇(H)), where ∇ f = g ∂b f (a = 1, . . . , 2n) is the FS gradient of
f : M → R [18]. To consider the dynamics of the quantum system under the
influence of a measurement, we have to include the random variations due to
the measurement. Thus, we extend the hamiltonian flow defined by the function
(H), by considering the Ito s.d.e. (originally in [15])
dxa = (2Ωab ∂b (H) + ρa )dt + σg ab ∂b (H)dW (t)

(56)

with ρa = − 41 σ 2 g ab ∂b V , where V = g ab ∂a (H)∂b (H) = (H 2 ) − (H)2 is the
variance of the Hamiltonian, or still, the squared energy quantum uncertainty.
Thus, we have modified the drift with a term which depends on ∇(H), and
still there is a noise tensor which is in this case a vector of the form σ∇(H),
with σ a constant, and we have a one-dimensional Wiener process (i = 1) in
eq. (15). Thus, if we start from the s.d.e. (56), the metric that arises from
the noise vector turns to be not the original FS metric g, but the contravariant
tensor with components ∇a (H)∇b (H) = g ad ∂d (H)g be ∂e (H), times the factor
σ 2 , which on setting it to be equal to zero, we get the original SE written in
CP (n). Furthermore, the trace-torsion one-form Q = Qe dxe has for components
the functions gae ((2Ωab ∂b (H) + ρa ) = 2Jeb ∂b (H) − 41 σ 2 ∂e V , so that
Q = Jd(H) −

σ2
d(V ),
4

(57)

an exact differential up to an infinitesimal rotation. Next we consider two realvalued stochastic processes defined on terms of the solution curves x(t) ∈ CP (n)
of eq. (56), the Hamiltonian process defined by (H)(x(t)) and the variance process V (x(t)). Then, from applying the Ito formula and formulae of Kahlerian
27

Quantizatin in Astrophysics ...

302

geometry, we find that (H)(x(t)) satisfies a s.d.e. with zero drift, more specifically, it is a square-integrable martingale on R, while the variance process is
a supermartingale, the latter describing the reduction of the wave function to
a particular eigenstate; see [15,16]. In the present geometro-stochastic setting,
we have associated to the reduction of the wave function in terms of the open
Schroedinger equation, a geometry which is not riemannian, it has torsion given
by the difference between the infinitesimal rotation of the differential of (H)
2
and the differential of σ4 V ; the metric is not the original FS, and as a covariant
tensor it has a singularity whenever (H) is constant, i.e. on a fixed eigenstate,
for which the flow of eq.(56), becomes constantly equal to it if choosen for initial
value.

10

RCW GEOMETRIES, BROWNIAN MOTIONS
AND THE SCHROEDINGER EQUATION

We have seen that one can represent the space and time quantum geometries for the relativistic diffusion associated with the invariant distribution,
so that Q = 12 dlnρ, and H0 (g, Q) has a self-adjoint extension for which we
can construct the quantum geometry on state-space and still the stochastic extension of the Schroedinger equation defined by this operator on taking the
analytical continuation on the time variable for the evolution parameter. In
this section which follows the solution of the Schroedinger problem of interpolation by Nagasawa [38] interpreted in terms of the RCW geometries and
the Hodge decomposition of the trace-torsion, we shall present the equivalence
between RCW geometries, their Brownian motions and the Schroedinger equation. The fact that nonrelativistic quantum mechanics can be linked to torsion
fields, has remained unseen till today, and we have proved this already for the
stochastic Schroedinger equation. Thus, we shall now present the construction
of non-relativistic quantum mechanics for the case that includes the full Hodge
decomposition of the trace-torsion, so that Q = Q(t, x) = dlnft (x) + A(t, x)
where f (t, x) = ft (x) is a function defined on the configuration manifold given
by [a, b] × M (where M is provided with a metric, g), to be determined below,
and A(t, x) is the sum of the harmonic and co-closed terms of the Hodge decomposition of Q, which we shall write as A(t, x) = At (x) as a time-dependent
form on M . The scheme to determine f will be to manifest the time-reversal
invariance of the Schroedinger representation in terms of a forward in time diffusion process and its time-reversed representation for the original equations for
creation and destruction diffusion processes produced by the electromagnetic
potential term of the trace-torsion of a RCW connection whose explicit form we
shall determine in the sequel. From now onwards, the exterior differential, and
the divergence operator will act on the M manifold variables only, for which we
shall write then as dft (x) to signal that the exterior differential acts only on the

28

Quantizatin in Astrophysics ...

303

x variables of M . We should remark that in this context, the time-variable t of
non-relativistic theory and the evolution parameter τ , are identical [50]. This
section stems from article [80]. Let
L=

∂
1
∂
+ A(t, x).∇ =
+ H(g, At )
∂t 2
∂t

(58)

(here, for unburdening the notation we omit the subscript 0 on H that recalls
that operates on scalar fields) with
δ Ât = −divg At = 0.

(59)

In this setting, we start with a background trace-torsion restricted to an electromagnetic potential. We think of this electromagnetic potential and the associated Brownian motion having its metric conjugate as its drift, as the background
geometry of the vacuum, which we shall subsequently relate to a creation and
destruction of particles and the equation of creation and annihilation is given
by the following equation.
Let p(s, x; t, y) be the weak fundamental solution of
Lφ + cφ = 0.

(60)

The interpretation of this equation as one of creation (whenever c > 0) and
annihilation (whenever c < 0) of particles is warranted by the Feynman-Kac
representation for the solution of this equation. Then φ = φ(t, x) satisfies the
equation
Z
φ(s, x) =
p(s, x; t, y)φ(t, y)dy,
(61)
M

where
for the sake of simplicity, we shall write in the sequel dy = volg (y) =
p
det(g)dy 1 ∧ . . . ∧ dy 3 . Note that we can start for data with aRgiven function
φ(a, x), and with the knowledge of p(s, x; a, y) we define φ(t, x) = M p(t, x; a, y)dy.
Next we define
q(s, x; t, y) =

1
p(s, x; t, y)φ(t, y),
φ(s, x)

which is a transition probability density, i.e.
Z
q(s, x; t, y)dy = 1,

(62)

(63)

M

while
Z
p(s, x; t, y)dy 6= 1.
M

29

Quantizatin in Astrophysics ...

304

(64)

Having chosen the function φ(t, x) in terms of which we have defined the probability density q(s, x; t, y) we shall further assume that we can choose a second
bounded non-negative measurable function φ̂(a, x) on M such that
Z
φ(a, x)φ̂(a, x)dx = 1,
(65)
M

We further extend it to [a, b] × M by defining
Z
φ̂(t, y) = φ̂(a, x)p(a, x; t, y)dx, ∀(t, y) ∈ [a, b] × M,

(66)

where p(s, x; t, y) is the fundamental solution of eq. (60).
Let {Xt ∈ M, Q} be the time-inhomogeneous diffusion process in M with
the transition probability density q(s, x; t, y) and a prescribed initial distribution
density
µ(a, x) = φ̂(t = a, x)φ(t = a, x) ≡ φ̂a (x)φa (x).

(67)

The finite-dimensional distribution of the process {Xt ∈ M, t ∈ [a, b]} with
probability measure on the space of paths which we denote as Q; for a = t0 <
t1 < . . . < tn = b, it is given by
Z
EQ [f (Xa , Xt1 , . . . , Xtn −1 , Xb )] =
dx0 µ(a, x0 )q(a, x0 ; t1 , x1 )dx1 q(t1 , x1 ; t2 , x2 )dx2 . . .
M

. . . q(tn−1 , xn−1 , b, xn )dxn f (x0 , x1 , . . . , xn−1 , xn )
:= [µa q >>
(68)
which is the Kolmogorov forward in time (and thus time-irreversible) representation for the diffusion process with initial distribution µa (x0 ) = µ(a, x0 ), which
using eq. (62) can still be rewritten as
Z
1
1
p(a, x0 ; t1 , x1 )φt1 (x1 )dx1
dx1 p(t1 , x1 ; t2 , x2 )φt2 (x2 )dx2 . . .
dx0 µa (x0 )
φ
(x
)
φ
a 0
t1 (x1 )
M
1
...
p(tn−1 , xn−1 ; b, xn )φb (xn )dxn f (x0 , . . . , xn ) (69)
φ(tn−1 , xn−1 )
which in account of µa (x0 ) = φ̂a (x0 )φa (x0 ) and eq.(62) can be written in the
time-reversible form
Z
φa (x0 )dx0 p(a, x0 ; t1 , x1 )dx1 p(t1 , x1 ; t2 , x2 )dx2 . . . p(tn−1 , xn−1 ; b, xn )φb (xn )dxn f (x0 , . . . , xn ) (70)
M

which we write as
= [φ̂a p >><< pφb ].

(71)

This is the formally time-symmetric Schroedinger representation with the transition (but not probability) density p. Here, the formal time symmetry is seen
30

Quantizatin in Astrophysics ...

305

in the fact that this equation can be read in any direction, preserving the physical sense of transition. This representation, in distinction with the Kolmogorov
representation, does not have the Markov property.
We define the adjoint transition probability density q̂(s, x; t, y) with the φ̂transformation
q̂(s, x; t, y) = φ̂(s, x)p(s, x; t, y)

1
φ̂(t, y)

(72)

which satisfies the Chapmann-Kolmogorov equation and the time-reversed normalization
Z
dxq̂(s, x; t, y) = 1.
(73)
M

We get
Z
EQ̂ [f (Xa , Xt1 , . . . , Xb )]

=

f (x0 , . . . , xn )q̂(a, x0 ; t1 , x1 )dx1 q̂(t1 , x1 ; t2 , x2 )dx2 . . .
M

. . . q̂(tn−1 , xn−1 ; b, xn )φ̂(b, xn )φ(b, xn )dxn ,

(74)

which has a form non-invariant in time, i.e. llegible from right to left, as
<< q̂ φ̂b φb ]] =<< q̂ µ̂b ]],

(75)

which is the time-reversed representation for the final distribution µb (x) =
φ̂b (x)φb (x). Now, starting from this last expression and rewriting in a similar form that in the forward process but now with φ̂ instead of φ, we get
Z
1
1
dx1 φ̂(t1 , x1 )p(t1 , x1 ; t2 , x2 )
dx2
dx0 φ̂a (x0 )p(a, x0 ; t1 , x1 )
φ̂t1 (x1 ))
φ̂t2 (x2 )
M
1
φ̂b (xn )φ(b, xn )dxn f (x0 , . . . , xn ) (76)
. . . dxn−1 φ̂(tn−1 , xn−1 )p(tn−1 , xn−1 ; b, xn )
φ̂(b, xn )
which coincides with the time-reversible Schroedinger representation [φ̂a p >><<
pφb ].
We therefore have three equivalent representations for the diffusion process,
one the forward in time Kolmogorov representation, the backward Kolmogorov
representation, both of them are naturally irreversible in time, and the timereversible Schroedinger representation, so that we can write succintly,
[µa q >>= [φ̂a p >><< pφb ]] =<< q̂µb ]], withµa = φa φ̂a , µb = φb φ̂b .

(77)

In addition of this formal identity,we have to establish the relations between
the equations that have lead to them. We first note, that in the Schroedinger
representation, which is formally time-reversible, we have an interpolation of
31

Quantizatin in Astrophysics ...

306

states between the initial data φ̂a (x) and the final data, φb (x). The information
for this interpolation is given by a filtration of interpolation Far ∪ Fbs , which
is given in terms of the filtration for the forward Kolmogorov representation
F = Fat , t ∈ [a, b] which is used for prediction starting with the initial density
φa φ̂a = µa and the filtration Ftb for retrodiction for the time-reversed process
with initial distribution µb .
We observe that q and q̂ are in time-dependent duality with respect to the
measure
µt (x)dx = φ̂t (x)φt (x)dx,
since if we define the time-homogeneous semigroups
Z
Qt−s f (s, x) =
q(s, x; t, y)f (t, y)dy, s < t
Z
g Q̂t−s (t, y) =
dxg(s, x)q̂(s, x; t, y), s < t,
then
Z
dxµs (x)g(s, x)Qt−s f (s, x)

(78)

(79)
(80)

Z

1
p(s, x; t, y)φt (y)f (t, y)dy
φs (x)
Z
1
=
dxg(s, x)φ̂s (x)p(s, x; t, y)
f (t, x)φ̂t (y)φt (y)dy
φ̂t (y)
Z
=
dxg(s, x)q̂(s, x; t, y)f (t, y)φ̂t (y)φ
Z
=
dxg(s, x)Q̂t−s (t, y)f (t, y)µt (y)dy
(81)
=

dxg(s, x)φs (x)φ̂s (x)

i.e.
< g, Qt−s f >µs =< g Q̂t−s , f >µt , s < t.

(82)

We shall now extend the state-space of the diffusion process to [a, b] × M , to
be able to transform the time-inhomogeneous processes into time-homogeneous,
while the stochastic dynamics is still taken place exclusively in M . This will
allow us to define the duality of the processes to be with respect to µt (x)dtdx
and to determine the form of the exact term of the trace-torsion, and ultimately,
to establish the relation between the diffusion processes and Schroedinger equations, both for potential linear and non-linear in the wave-functions. If we define
time-homogeneous semigroups of the processes on {(t, Xt ) ∈ [a, b] × M } by

Qs,s+r f (s, x) , s ≥ 0
Pr f (s, x) =
(83)
0
, otherwise
and


P̂r g(t, y) =

gQt−r,t (t, y) , r ≥ 0
0
, otherwise
32

Quantizatin in Astrophysics ...

307

(84)

then
Z
< g, Pr f >µt dtdx

b−r

=

Z

a+r

ds < g, Qs,s+r f > µs =
r

Z

< g, Qt−r,t f > µt−r (x)dx
b

b

dt < g P̂t−r , f >µt dx =< P̂r g, f >µt dtdx ,

=

(85)

a+r

which is the duality of {(t, Xt )} with respect to the µt dtdx density. Consequently, if in our space-time case we define for at (x), ât (x) time-dependent oneforms on M (to be determined later)
∂α
+ H(g, At + at )αt
∂t
∂µ
= −
+ H(g, At + at )† µt ,
∂t

Bα :

=

B0µ :

(86)
(87)

and its adjoint operators
B̂β = −

∂β
− H(g, −At + ât )† βt ,
∂t

(88)

∂µt
− H(g, −At + ât )† µt ,
(89)
∂t
where by H † we mean the volg -adjoint of the operator H defined as in eq.(22).
Now
Z b Z
Z b Z
dt 1Dt [(Bαt )βt ] − αt (B̂βt )]µt (x)dx =
dt 1Dt αt βt (B 0 µt )dx
(B̂)0 µt =

a

a

Z
−

b

Z
1Dt αt g([at + ât ] − dlnµt , dβt )µt dx

(90)

a

for arbitrary α, β smooth compact supported functions defined on [a, b] × M
which we have denoted as time-dependent functions αt , βt , where 1Dt denotes
the characteristic function of the set Dt (x) := {(t, x) : µt (x) = φt (x)φ̂t (x) > 0}.
Therefore, the duality of space-time processes
< Bα, β >µt (x)dtdx =< α, B̂β >µt (x)dtdx ,

(91)

at (x) + ât (x) = dlnµt (x) ≡ dlnφt (x)φ̂t (x),
B 0 µt (x) = 0,

(92)
(93)

is equivalent to

and the latter equation being the Fokker-Planck equation for the diffusion with
trace-torsion given by a + A, then the Fokker-Planck equation for the adjoint
(time-reversed) process is valid, i.e.
(B̂)0 µt (x) = 0.
33

Quantizatin in Astrophysics ...

308

(94)

Substracting eqts. (93, 94) we get the final form of the duality condition
at − ât
∂µ
+ divg [(At +
)µt )]
∂t
2

= 0, for µt (x) = φ̂t (x)φt (x)

(95)

Therefore, we can establish that the duality conditions of the diffusion equation
in the Kolmogorov representation and its time reversed diffusion lead to the
following conditions on the additional elements of the drift vectorfields:
at (x) + ât (x) = dlnµt (x) ≡ dlnφt (x)φ̂t (x),
at − ât
∂µ
+ divg [(At +
)µt ] = 0.
∂t
2

(96)
(97)

If we assume that at −ât is an exact one-form, i.e., there exists a time-dependent
differentiable function S(t, x) = St (x) defined on [a, b]×M such that for t ∈ [a, b],
at − ât = dln

φt (x)
φ̂t (x)

= 2dSt

(98)

which together with
at + ât = dlnµt ,

(99)

implies that on D(t, x) we have
at

= dlnφt ,

(100)

ât

= dlnφ̂t .

(101)

Remark. Note that the time-dependent function S on the 3-space manifold,
is defined by eq. (98) up to addition of an arbitrary function of t, and when
further below we shall take this function as defining the complex phase of the
quantum Schroedinger wave, this will introduce the quantum-phase indetermination of the quantum evolution, just as we discussed already in the setting of
geometry of the quantum state-space. In the other hand, this introduces as well
the subject of the multivaluedness of the wave function, which by the way, leads
to the Bohr-Sommerfeld quantization rules of quantum mechanics established
well before it was developed as an operator theory. It is noteworthy to remark
that these quantization rules, later encountered in superfluidity and superconductivity, or still in the physics of defects of condensed matter physics, are of
topological character. Later we shall see that the Schroedinger wave equation
contains the Navier-Stokes equations for a viscous fluid in 2D, and the probability density of the Brownian motions or still of the quantum system, will be
transformed into the enstrophy of the viscous fluid obeying the Navier-Stokes
equations. Thus, one might expect that Navier-Stokes equations could also have
multivalued solutions, namely in the 2D case of the already established relation,
the vorticity reduces to a time-dependent function. 16 . Multivaluedness of the
16 This comment follows Vic Christianto’s kind insistence in the need of exploring the possible existance of multivalued solutions to the Navier-Stokes equations.

34

Quantizatin in Astrophysics ...

309

solutions of the Schroedinger equation has been proposed [52] as evidence that
the multivalued logics due to Post have to be incorporated in physics, and cognition in general (which surely is the case in our uses of languages in daily life),
which would replace the probabilistic features of quantum mechanics.17
Introduce now Rt (x) = R(t, x) = 21 lnφt φ̂t and St (x) = S(t, x) = 12 ln φφ̂t , so
t
that
at (x)
â( x)

= d(Rt + St ),
= d(Rt − St ),

(102)
(103)

and the eq. (97) takes the form
∂R 1
+ 4g St + g(dSt , dRt ) + g(At , dRt ) = 0,
(104)
∂t
2
where we have taken in account that divg At = 0.
Therefore, together with the three different time-homogeneous representations {(t, Xt ), t ∈ [a, b], Xt ∈ M } of a time-inhomogeneous diffusion process
{Xt , Q) on M we have three equivalent dynamical descriptions. One description, with creation and killing described by the scalar field c(t, x) and the diffusion equation describing it is given by a creation-destruction potential in the
trace-torsion background given by an electromagnetic potential
∂p
+ H(g, At )(x)p + c(t, x)p = 0;
∂t

(105)

the second description has an additional trace-torsion a(t, x) , a 1-form on R×M
∂q
+ H(g, A + at )q = 0.
(106)
∂t
while the third description is the adjoint time-reversed of the first representation
given by φ̂ satisfying the diffusion equation on the background of the reversed
electromagnetic potential −A in the vacuum, i.e.
−

∂ φ̂
+ H(g, −At )φ̂ + cφ̂ = 0.
∂t

(107)

17 Pensinger and Paine, preceded Prigogine [74] -who stressed the formative role of time in all
systems and an approach through resonances in dynamical systems- in claiming an approach
to collective phenomenae in the social sciences based in quantum mechanics and the formative
role of time (the latter being absent in historiography, but certainly present in Nature, as we
have discussed above.). Furthermore, these authors claim that the probabilistic approach
attempted to replace the Aristoteles-Boole logic for its failure to account for the localization
problem, and that the probabilistic approach was conceived as a rejection of a multiple identity
conception. As we have seen in this article, the probabilistic approach to quantum mechanics
is equivalent to geometrical structures, and thus the multivalued logics would thus be linked
to RCW geometries through the multivaluedness of the logarithmic potential of the tracetorsion, when we extend the wave function to the complex domain, or still more generally, to
the quaternionic and octonionic spaces. In fact, the so called Smarandache geometries might
be related to this view of spacetime geometries as associated to multivalued logics and to the
coexistence of multiple structures [76].

35

Quantizatin in Astrophysics ...

310

The second representation for the full trace-torsion diffusion forward in time
Kolmogorov representation, we need to adopt the description in terms of the
fundamental solution q of
∂q
+ H(g, At + at )q = 0,
∂t

(108)

for which one must start with the initial distribution µa (x) = φ̂a (x)φa (x). This
is a time t-irreversible representation in the real world, where q describes the
real transition and µa gives the initial distribution. If in addition one traces the
diffusion backwards with reversed time t, with t ∈ [a, b] running backwards, one
needs for this the final distribution µb (x) = φ̂b (x)φb (x) and the time t reversed
probability density q̂(s, x; t, y) which is the fundamental solution of the equation
−

∂ q̂
+ H(g, −At + ât )q̂ = 0,
∂t

(109)

with additional trace-torsion one-form on R × M given by â, where
ât + at = dlnµt (x).

(110)

where the diffusion process in the time-irreversible forward Kolmogorov representation is given by the Ito s.d.e
dXti = σji (Xt )dWtj + (A + a)i (t, Xt )dt,

(111)

and the backward representation for the diffusion process is given by
dXti = σji (Xt )dWtj + (−A + â)i (t, Xt )dt,

(112)

where a, â are given by the eqs. (102) and (103), and (σσ † )αβ = g αβ
We follow Schroedinger in pointing that φ and φ̂ separately satisfy the
creation and killing equations, while in quantum mechanics ψ and ψ̄ are the
complex-valued counterparts of φ and φ̂, respectively, they are not arbitrary
but
φφ̂ = ψ ψ̄.

(113)

Thus, in the following , this Born formula, once the equations for ψ are determined, will be a consequence of the constructions, and not an hypothesis on the
random basis of non-relativistic mechanics.
Therefore, the equations of motion given by the Ito s.d.e.
dXti = (Â + gradφ)i (t, Xt )dt + σji (Xt )dWtj ,

(114)

which are equivalent to
∂u
+ H(g, At + at )u = 0
∂t
36

Quantizatin in Astrophysics ...

311

(115)

with a = dlnφ = d(R + S), determines the motion of the ensemble of nonrelativistic particles. Note that this equivalence requires only the Laplacian
for the RCW connection with the forward trace-torsion full one-form Q = A +
dlnφ = A+d(R+S). In distinction with Stochastic Mechanics due to Nelson, and
contemporary ellaborations of this applied to astrophysics as the theory of Scale
Relativity due to Nottale [31][34], we only need the form of the trace-torsion
for the forward Kolmogorov representation, and this turns to be equivalent
to the Schroedinger representation which interpolates in time-symmetric form
between this forward process and its time dual with trace-torsion one-form given
by −A + â = −A + dlnφ̂ = −A + d(R − S).
Finally, let us how this is related to the Schroedinger equation. Consider
now the Schroedinger equations for the complex-valued
wave function ψ and its
√
complex conjugate ψ̄, i.e. introducing i = −1, we write them in the form
∂ψ
+ H(g, iAt )ψ − V ψ
∂t
∂ ψ̄
+ H(g, −iAt )ψ̄ − V ψ̄
−i
∂t
i

=

0

(116)

=

0,

(117)

which are identical to the usual forms. So, we have the imaginary factor appearing in the time t but also in the electromagnetic term of the RCW connection
with trace-torsion given now by iA, which we confront with the diffusion equations generated by the RCW connection with trace-torsion A, i.e. the system
∂φ
+ H(g, At )φ + cφ = 0,
∂t
−∂ φ̂
+ H(g, −At )φ̂ + cφ̂ = 0,
∂t

(118)
(119)

and the diffusion equations determined by both the RCW connections with
trace-torsion A + a and −A + â, i.e.
∂q
+ H(g, At + at )q
∂t
−∂ q̂
+ H(g, −At + ât )q̂
∂t

=

0,

(120)

=

0,

(121)

=

0.

(122)

which are equivalent to the single equation
∂q
+ H(g, At + dlnφt )q
∂t

If we introduce a complex structure on the two-dimensional real-space with
coordinates (R, S), i.e. we consider
ψ = eR+iS , ψ = eR−iS ,

37

Quantizatin in Astrophysics ...

312

(123)

viz a viz φ = eR+s , φ̂ = eR−S , with ψ ψ̄ = φφ̂, then for a wave-function differentiable in t and twice-differentiable in the space variables, then, ψ satisfies the
Schroedinger equation if and only if (R, S) satisfy the difference between the
Fokker-Planck equations , i.e.
1
∂R
+ g(dSt + At , dRt ) + 4g St = 0,
∂t
2

(124)

and
V =−

1
∂S
+ H(g, dRt )Rt − g(dSt − At , dSt ).
∂t
2

(125)

which follows from substituting ψ in the Schroedinger equation and further
dividing by ψ and taking the real part and imaginary parts, to obtain the
former and latter equations, respectively.
Conversely, if we take the coordinate space given by (φ, φ̂), both non-negative
functions, and consider the domain D = D(s, x) = {(s, x) : 0 < φ̂(s, x)φ(s, x)} ⊂
[a, b] × M and define R = 12 lnφφ̂, S = 12 ln φφ̂ , with R, S having the same differentiabilty properties that previously ψ, then φ = eR+S satisfies in D the equation
∂φ
+ H(g, At )φ + cφ = 0,
∂t

(126)

if and only if
−c =
+

∂S
1
+ H(g, dRt )Rt − g(dSt , dSt ) − g(At , dSt )]
∂t
2
∂R
∂S
[
+ H(g, dRt )St + g(At , dRt )] + [2
+ g(dSt + 2At , dSt )].
(127)
∂t
∂t
[−

while φ̂ = eR−S satisfies in D the equation
−

∂φ
+ H(g, −At )φ̂ + cφ̂ = 0,
∂t

(128)

if and only if
∂S
1
+ H(g, dRt )Rt − g(dSt , dSt ) − g(At , dSt )]
∂t
2
∂R
∂S
− [
+ H(g, dRt )St + g(At , dRt )] + [2
+ g(dSt + 2At , dSt )].
(129)
∂t
∂t

−c =

[−

Notice that φ, φ̂ can be both negative or positive. So if we define ψ = eR+iS , it
then defines in weak form the Schroedinger equation in D with
V = −c − 2

∂S
− g(dSt , dSt ) − 2g(At , dSt ).
∂t
38

Quantizatin in Astrophysics ...

313

(130)

We note that from eq. (130) follows that we can choose S in a way such
that either c is independent of S and thus V is a potential which is non-linear
in the sense that it depends on the phase of the wave function ψ and thus
the Schroedinger equation with this choice becomes non-linear dependent of ψ,
or conversely, we can make the alternative choice of c depending non-linearly
on S, and thus the creation-annihilation of particles in the diffusion equation is
non-linear, and consequently the Schroedinger equation has a potential V which
does not depend on ψ .
With respect to the issue of nonlinearity of the Schroedinger equation, one
could argue that the former case means that the superposition principle of quantum mechanics is broken, but then one observes that precisely due to the fact
that the wave function depends on the phase, the superposition principle is invalid from the fact that we are dealing with complex-valued wave functions, and
what matters, is the evolution in state-space where the complex factor has been
quotiented. In the former case of a non-linear Schroedinger equation, we note
that the symplectic state-space formulation is still valid [18] and the quantum
geometry description incorporates non-linear quantum mechanics as is the case
of the Lie-isotopic theory of Santilli, when we place in evidence in the equation,
the isotopic unit of the Lie-isotopic Schroedinger-Santilli equation; see Santilli
[40]. In the case that V is such that the spectrum of H(g, A + a) is discrete,
we know already we can represent the Schroedinger equation in state-space and
further study the related stochastic Schroedinger equation as described above.
Finally, we have presented a construction in which by using two scalar diffusing
processes φ, φ̂ we have been able to subsume them into a single forward in time
process with additional trace-torsion given by at = dlnφt φ̂t , where µt = φt φ̂t
is the distribution of the diffusion process, and obtain under eqts. (118) the
Schroedinger equation (110). Alternatively, it is known that we can start with
2D space and the Schroedinger equation, we obtain a pair of equations, one
of them being the Navier-Stokes equations for a compressible fluid where now
φt φ̂t = ψt ψ̄t equals the enstrophy of the fluid. Thus, the formal-time reversible
representation can indeed be linked with the irreversible dynamics of a viscous
fluid, but now the density is given by the square of the vorticity, that in this
case can be associated with a function [33]; the case for this correspondance for
spatial 3D requires to be proved. This represents a mapping between two RCW
structures (inasmuch the correspondance between the sourceless Maxwell and
Dirac-Hestenes equations is another example [2], [49]), since as we have seen
in [1] and [32], the Navier-Stokes equations as well as the equations of passive
transport of a magnetic field on a fluid, are basic examples of RCW geometries whose dynamics can be represented in terms of Brownian motions, both
for boundaryless manifolds and the case of smooth boundary manifolds as well.
Finally, we would like to stress that from those Brownian motions, and in particular for the cases of the Schroedinger equation and its stochastic extension in
state-space, we can build Poincaré-Cartan random integral invariants [1]; this
will be presented in detail elsewhere.
39

Quantizatin in Astrophysics ...

314

Nonlinear Schroedinger equations have an important role in theoretical physics,
as well as the Lie-isotopic extensions of the linear Schroedinger equation and
of Quantum Mechanics, due to Santilli [40]. In the most interesting theory due
to Santilli developed along forty years of work, it is assumed that at very short
distances the quantum forces are no longer due to contact interaction representable by the quantum semigroup rules that extend the symplectic approach
to nondissipative classical mechanics. These interactions arise from the overlap of the wavefunctions, and thus cannot be formally represented as in the
usual approach. Thus, Santilli sets an epistemologic frontier in what is known
as the interior problem of hyperdense matter and noncontact interactions, and
the exterior problem which is the usually treated by to the theoretical physics
to nondissipative systems. To obtain a consistent theory, a modification of the
theory of numbers (known as isoarithmetic and isoalgebra) is produced incorporating an arbitrary unit (which will carry the information on the overlap of
the wavefunctions of the constitutive elements of the quantum system under
noncontact interaction,as well as information as the nonconstant viscosity or
diffraction index, temperature, high compression), which is further carried to
produce a modification of differential calculus in term of an isotopic differential,
and thus a modification of the Schroedinger equation follows. In terms of an
extension of the theory of general relativity, the corresponding modification is
thought in terms again of the so-called interior problem corresponding to ultradense matter or spin. In the large scale exterior problem, Lie-isotopic theory
recovers all the usual theories of quantum mechanics and relativity. The point of
view due to Santilli is different than the one presented here, in which we present
a basis for phenomenae in a form that although can be introduced in terms
of scale fields, the theory is essentially topological inasmuch the torsion field
is of topological origin: the nonclosure of infinitesimal parallelograms. Thus,
the Schroedinger equation as presented here as well as the Brownian motions
associated to RCW geometries, does not appear as linked to a particular scale,
they are universal structures. Furthermore, from our analysis above, the fact
that the Schroedinger equation be linear or nonlinear is not the main issue,
we can always choose where to set the nonlinearity, either in the creation or
annihilation potential, or in the potential function V that has been historically
attached to quantum physics. It is remarkable that Santilli’s theory can be
mapped into the present at least for certain types of units which as generators
of the trace-torsion [55]. From Santilli’s theory, a new formulation of chemical
bonds is produced [64].
Yet, if we remain in the context of the exterior problem for quantum systems
as described by quantum mechanics, in Santilli’s work there is no analysis of
the deeper structures and phenomenae that may arise in the exterior problem
at large, nor at the relation between the aether and the exterior problem at
large, as conceived in the present work, while at the level of the interior level,
the existance of an elementary particle is hypothized, the so-called aetherino.
While in the so-called interior problem, the torsion produced by the isotopic
40

Quantizatin in Astrophysics ...

315

unit which is the cornerstone of the Santilli- Lie-isotopic theory can depend on
additional parameters that represent the modifications due to the overlap of the
wavepackets of the quantum system and as well as due to the thermodynamics
irreversible processes taking part within the boundaries set for the system to
distinguish it from the canonical formalism for classical and quantum systems,
the present theory presents a view of phenomenae which is free of the establishment of boundaries (which can be somehow artificial or ad-hoc). In a theory of
the aether in which the non-trivial topological forces represented by geometrical
torsion are at the foundations, and the structures that arise from it are valid
in all scales such as vortices, spinor fields, minimal surfaces, as we shall briefly
present in the next section.
Returning to the issue of the nonlinearity of the potential function V in
quantum mechanics, the usual form is the known logarithmic expression V =
−b(ln|ψ|2 )ψ introduced by Bialnicky-Birula and Mycielski [57]. Its importance
in such diverse fields as quantum optics, superconductivity, atomic and molecular physics cannot be disregarded. Soliton solutions of nonlinear Schroedinger
equations may have a role central to molecular biology, in which the DNA
structure may be associated with a superconductive state. With regards as the
relation between geometries, Brownian motions and the linear and Schroedinger
equations, there is an alternative line of research which stems from two principles, one of them strongly related to the present one. The first is that all physical
fields have to be construed in terms of scale fields starting from the fields appearing in the Einstein lambda transformations,of which, the Schroedinger wave
function is an elementary example as shown here (see Rapoport [58]), and when
further associated to the idea of a fractal spacetime, this has lead to Nottale’s
theory of Scale Relativity [31]. Nottale’s theory starts from this fractal structure to construct a covariant derivative operator in terms of the forward and
backward stochastic derivatives introduced by Nelson in his theory of stochastic
mechanics [14]. In Nelson’s conception, Brownian motions and quantum systems are aggregates to spacetime, they are not spacetime structures themselves;
this is a completely different conception that the one elaborated in this article. Working with these stochastic derivatives, the basic operator of Nottale’s
theory, can be written in terms of our RCW laplacian operators of the form
h̄
∂
∂τ + H0 (iDg, V) where D is diffusion constant (equal to 2m in nonrelativistic
quantum mechanics), and V is a complex differentiable velocity
√ field, our complex drift appearing after introducing the imaginary unit i = −1 ; see Nottale
[61]. In the present conception, this fundamental operator in terms of which
Nottale constructs his theory which has lead to numerous predictions of the positions of exoplanets confirmed by observations [34], does not require to assume
that spacetime has a fractal structure a priori, from which stochastic derivatives
backward and forward to express the time asymmetry construct the dynamics
of fields. We rather assume that at a fundamental scale which is generally associated with the Planck scale, we can represent spacetime as a continuous in
which what really matters are the defects in these continuous, and thus torsion
41

Quantizatin in Astrophysics ...

316

has such a fundamental role. The fractal structure of spacetime arises from
the association between the RCW laplacian operators which as we said coincide
with Nottale’s covariant derivative operator, and the Brownian motions which
alternatively, can be seen as constructing the spacetime geometry. So there is
no place as to the discussion of what goes first, at least in the conception in
the present work. The flow of these Brownian motions under general analytical conditions, define for every trial Wiener path, an active diffeomorphism of
spacetime. But this primeval role of the Brownian motions and fractal structures, stems from our making the choice -arbitrary, inasmuch as the other choice
is arbitrary- as the fundamental structure instead of choosing the assumption
of having a RCW covariant derivative with a trace-torsion field defined on a
continuous model of spacetime. In some sense the primeval character of Brownian motions as a starting point is very interesting in regards that they can be
constructed as continous limits of discrete jumps, as every basic book in probability presents [66], and thus instead of positing a continuous spacetime, we can
think from the very beginning in a discrete spacetime, and construct a theory
of physics in these terms as suggested in [63] 18 In this case, instead of working
with the field of the real number or its complex or biquaternion extensions, one
can take a p-adic field, such as the one defined by the Mersenne prime number
2127 − 1 which is approximately equal to the square of the ratio between the
Planck mass and the proton mass [63]. This program and its relations with the
fundamental constants of physics, was elaborated independently by a number of
authors and an excellent presentation can be found in Castro [64] and references
therein.In fact, a theory of physics in terms of discrete structures associated to
the Mersenne prime numbers hierarchy, has been constructed in a program developed by P. Noyes, T. Bastin, P. Kilmister and others; see [67]. A remarkable
unified theory of physics, genetics and consciousness in terms of p-adic field
theory, has been elaborated by M.Pitkanen, which has been briefly presented in
this volume [75].
Returning to our discussion of the work by Nottale, we would like to comment that Castro and Mahecha (see [59] and chapter of this book) and Castro,
Mahecha and Rodriguez [60], following the Nottale constructions have derived
the nonlinear Schroedinger equation and associated it to a Brownian motion
with a complex diffusion constant. Futhermore, working with Weyl connections
(which are to be distinguished from the present work’s Riemann-Cartan-Weyl
connections) in that they are not integrable and they have zero torsion (they can
be introduced in terms of the reduced set of Einstein lambda transformations
when one does not posit the tetrad or cotetrad fields as fundamental and the
invariance of the Riemann-Cartan connection), they have derived the relativistic quantum potential in terms of the difference between the Weyl curvature
of this connection and the Riemannian curvature, while in the present theory,
18 Prof. Shan Gao,has initiated a program of construction of quantum mechanics as random
discontinuous motions in discrete spacetime, in his recent work Quantum Motion, Unveiling
the Mysterious Quantum World, Arima Publ., Suffolk (U.K.), 2006.

42

Quantizatin in Astrophysics ...

317

we have associated above the relativistic quantum potential with the Riemannian curvature, which is more closely related with the idea of Brownian motion
in spacetime (without additional internal degrees of freedom as the Weyl connections introduce) as being the generator of gravitation and all fundamental
fields.

11

THE NAVIER-STOKES EQUATIONS AND
RIEMANN-CARTAN-WEYL DIFFUSIONS

We have seen that quantum mechanics is an example of spacetime structures of RCW. We have shortly discussed the fact that the Navier-Stokes equations for viscous fluids are another example of this. In this section we shall
present the proofs of this statements.
In the sequel, M is a compact orientable ( without boundary) n-manifold
with a Riemannian metric g. We provide M with a 1-form u(τ, x) = uτ (x)
satisfying the invariant Navier-Stokes equations (NS in the following) ,
∂u
+ P [∇gûτ uτ ] − ν41 uτ = 0,
∂τ

(131)

where P is the projection operator to the co-closed term in the de RhamKodaira-Hodge decomposition of 1-forms. We have proved in [1,32], that we
can rewrite NS in the form of a non-linear diffusion equation19
∂u
−1
= P H1 (2νg,
uτ )uτ ,
∂τ
2ν

(132)

which means that NS for the velocity of an incompressible fluid is a a nonlinear diffusion process determined by a RCW connection. This connection has
2νg for the metric, and the time-dependant trace-torsion of this connection is
−u/2ν. Then, the drift of this process does not depend explicitly on ν, as it
coincides with the vectorfield associated via g to −uτ , i.e.−ûτ . Let us introduce
the vorticity two-form
Ωτ = duτ , τ ≥ 0.

(133)

Now, apply d to eq. (132); since d41 uτ = 42 duτ = 42 Ωτ and dLûτ =
Lûτ duτ = Lûτ Ωτ we obtain the evolution equation for the vorticity (the so
called Navier-Stokes equation for the vorticity):
−1
∂Ωτ
= H2 (2νg,
uτ )Ωτ .
∂τ
2ν

(134)

19 While in the boundaryless case P commutes with 4 , in the case of M with smooth
1
boundary this is no longer true so that we have to take P 41 uτ instead of the viscosity term
in eq. (131) , and we are left with the non-linear diffusion equation (132) in any case.

43

Quantizatin in Astrophysics ...

318

Now, if we know Ωτ for any τ ≥ 0, we can obtain uτ by inverting the definition
(133). Namely, applying δ to (133) we obtain the Poisson-de Rham equation
H1 (g, 0)uτ = −dδuτ − δΩτ , τ ≥ 0.

(135)

Thus, the vorticity Ωτ is a source for the velocity one-form uτ , for all τ together
with the predetermined expression for δuτ ; in the case that M is a compact
euclidean domain, eq. (135) is integrated to give the Biot-Savart law of Fluid
Mechanics. If furthermore the fluid is incompressible, i.e. δuτ = 0, then we get
the Poisson-de Rham equation for the velocity having the vorticity as a source,
H1 (g, 0)uτ = −δΩτ , τ ≥ 0.

(136)

In 3D this is none other that the Biot-Savart law but applied to fluid dynamics,
instead of electromagnetism.
Theorem . Given a compact orientable Riemannian manifold with metric
g, the Navier-Stokes equation (132) for fluid with velocity one-form u = u(τ, x),
assuming sufficiently regular conditions, are equivalent to a diffusion equation
for the vorticity given by (132) with uτ satisfying the Poisson-de Rham eq. (135)
for the compressible case and eq. (136) for the incompressible one. The RCW
connection on M generating this process is determined by the metric 2νg and
a trace-torsion 1-form given by −u/2ν.
Observations This characterization of NS in terms of a gauge structure,
will determine all the random representations for NS which we shall present
in this article. We would like to recall that in the gauge theory of gravitation
[17] the torsion is related to the translational degrees of freedom present in the
Poincaré group, i.e. to the gauging of momentum. Here we find a similar, yet
dynamical situation, in which the trace-torsion is related to the velocity and the
angular momentum is derived from it simply by considering the vorticity of the
fluid. We conclude this chapter noting that with this constructions we can finally
give the most general analytical representations for the Navier-Stokes equations
using the Brownian motions corresponding to the Navier-Stokes operator [32].

12

TURBULENCE AND RCW GEOMETRIES

Turbulence is a universal phenomenae inasmuch viscous fluids are universal. In particular, the role of turbulence in astrophysics has been discussed by
several authors [53] . Evidence of turbulence for the origin of galaxy formation
has been detected by observations [54]. Gibson has extensively discussed the
formation of the gravitational field, galaxies and the Universe from a turbulent
fluid [55] and contrasted with advantage the usual approach through the Jeans
law. In the present approach in which viscous fluids, gravitational fields, quantum mechanics are all instances of a single geometrical structure and its random
counterpart, this seems extremely natural.
44

Quantizatin in Astrophysics ...

319

In this section we want to introduce a treatment of turbulence which is independent of the particular equations of dynamics and is directly associated with
the RCW geometries through the structure of the trace-torsion one-form, Q,
whose conjugate vectorfield, whenever the metric is Minkowski or in an arbitrary
Riemannian (i.e. positive-definite) metric is established from the beginning,or
still, in the latter case, whenever we have a noise tensor which generates the
Riemannian metric through the eq. (15). The clue to this is through the ideas
elaborated by R. Kiehn in terms of a classical notion in the theory of differential
equations, the topological (also called, the Pfaffian dimension) dimension of Q.
So we consider the set of differential forms on 4-dimensional spacetime given by
{Q, dQ, Q ∧ dQ, dQ ∧ dQ},

(137)

which cannot have higher degree differential forms since d(dQ) = 0 whenever the
coefficient functions of Q are twice differentiable. Then, we follow Kiehn by recalling that the topological dimension of Q is the minimal number of coordinates
in M on which Q depend. Thus, if dQ = 0, then in a connected neighborhood
of M , we can find a differentiable function, say f , such that Q = df , i.e. Q is an
exact form in that neighbourhood. Trivially dQ = 0 as well as the higher degree
forms of the above set. In this case, it is clear that Q can be parametrized by a
one-dimensional set given by the inverse image by f of all its values in the real
line, and thus for an exact one-form the topological dimension is equal to 1. Let
us consider the case that Q is not exact and furthermore Q ∧ dQ = 0. By the
well known Frobenius integrability theorem, then Q has topological dimension
equal to 2, i.e. M can be at least locally foliated by two-dimensional submanifolds on which Q is defined; this corresponds to a reversible dynamics given
by the integral flow of Q that lies in this two-dimensional submanifold. Now
assume in the contrary that Q ∧ dQ 6= 0 and furthermore dQ ∧ dQ 6= 0, so that
being this a top degree differential form on M , in this case Q has topological
dimension equal to 4, and thus equal to the dimension of spacetime. In this
case, the integral flow of the drift vectorfield Q̂, for a positive-definite metric,
lies in a four dimensional submanifold. Otherwise, if dQ ∧ dQ = 0, then Q has
topological dimension equal to 3; in this case, the drift vector field has a flow
lying in a three-dimensional submanifold.
In the case of topological dimension equal to 4, we extend Kiehn [47] defining
a vector field called the topological torsion by the rule
Q ∧ dQ = iT volg .

(138)

If we introduce the Hodge star operator ∗ defined by g, we have that if T g denotes
the one-form given by the g conjugate of the vectorfield T (i.e. T g = aα dxα
with aα = gαβ T β , where T = T β ∂x∂ β is the coordinate expression for T , then
[48]
∗T g = iT volg = Q ∧ dQ,
(139)

45

Quantizatin in Astrophysics ...

320

which is Kiehn’s topological torsion three-form obtained by duality from T g .
When g is the Euclidean metric we retrieve the original definitions [47]. Although the present formulation retrieves the trivial metric case, it is more general since it includes the noise tensor of the Brownian motions having the drift
vector field produced by the g-conjugate of Q, producing the metric by eq. (15),
so in spite the exterior differential operator d 20 in terms of which define the
topological dimension is independent of the background noise, the topological
torsion one-form and the topological torsion vector field here introduced, do
depend on the metric (and the background noise) through the relations (138)
and (139). So the physical meaning of this terms incorporates the background
noise tensor, contrarily to Kiehn’s approach in which the topological approach
is unlinked to noise. In this respect, the presentation here introduced has incorporated the dynamics of the vacuum while in the approach due to Kiehn, the
vacuum is absent altogether in the definitions. As it stands, the present constructions are associated to the mean motion of the Brownian motions through
their drift
We now compute the four-form dQ ∧ dQ to obtain
dQ ∧ dQ = diT volg = LT volg − iT dvolg = LT volg ,

(140)

which is still equal to Γvolg with Γ = divg T (by definition of the divergence, see
eq. (4.28) in [48] and therefore
dQ ∧ dQ = Γvolg ≡ divg (T )volg .

(141)

Thus, Γ is the topological dissipation function. It expresses how the 4-volume
defined by the 4-form dQ ∧ dQ shrinks or expands in terms of the Riemannian
20 We must remark that altough the present constructions apply to an arbitrary spacetime
trace-torsion one-form, and thus includes the case of a three-dimensional fluid velocity uτ (x)
which is also is time-dependent and obeys Navier-Stokes equations, the present exterior differential has an additional time derivative component which is missing in the exterior differential
that we encountered when introducing the Navier-Stokes equations. Indeed, when there we
wrote duτ this time derivative is absent. The presentation we are giving of the topological
dimension, incorporates time as an active parameter for its definition. This is very important,
since as we shall see, the topological dimension is related to coherent structures, turbulence,
chaos, etc., in a formalization in which statistical considerations are absent completely 21 In
this respect, the topological dimension incorporating this active time parameter, coincides
with Pensinger and Paine’s idea of an active time operator in their study of severe storms
formation, which is none-other than the exterior differential in 4D written in a biquaternionic
base (see D. Paine and W. Pensinger [52]. It is important to remark that in [52] it is proved
that the Navier-Stokes equations in metereology, map into the Maxwell equation, with a limiting velocity which is not the velocity of light in vacuum; this conception was applied to yield
a superconductive model of DNA by Paine and Pensinger. Here the role of nested hierarchical
limited space-time domains play an essential role, and the probabilistic approach to quantum
mechanics is proposed to be substituted by the many-valued logics due to E. Post. We recall
that Kozyrev’s conception of time is exactly that of an active operator (this idea was elaborated contemporarily by Prigogine [74]), as we have already discussed above, so what Kiehn is
actually doing is presenting a topological theory of structures and further below, of processes,
in which time is an active operator for their formation and preservation.

46

Quantizatin in Astrophysics ...

321

volume volg . In fact, this 4-form is the Liouville form produced by the symplectic 2-form dQ, so that here spacetime adquires a symplectic structure, i.e.
a nondegenerate closed 2-form on four dimensional spacetime. In a same domain of M we can actually have different topologies in the sense of Pfaff. We
note whenever the topological dimension coincides with the spacetime dimension 4, topological torsion is related to a system whose evolution occupies the
4-dimensional domain, with the possibility that whenever in this domain T is
divergenceless, then the topological dimension of the trace-torsion Q 22 collapses
to 3, thus we have a contact Hamiltonian reversible structure defined by Q∧dQ,
corresponding to spacetime defects which are nonequilibrium long lived closed
systems, generically spacetime dislocations, or still coherent or stationary structures such as vortices, solitons, dislocations, minimal surfaces, etc. The domains
on which the topological dimension of Q is 4 correspond to thermodinamically
open irreversible systems, and in the direction of T , evolution is irreversible; according to Kiehn, these dynamics correspond to turbulent systems, in our case,
associated to the trace-torsion Q whose conjugate vector field is the drift of the
Brownian motions. In the case we have topological dimension equal to 2 or 1,
this corresponds to isolated systems in equilibrium. We we would like to remark
that we can still follow Kiehn presenting a theory of systems based upon the action of vector fields on the trace-torsion Q, which would then correspond to the
evolution of arbitrary processes on the background of the RCW Brownian motions. This description can be elaborated to establish a topological-geometrical
approach to the processes in interaction with a universal field, on which we have
the action of an active time operator, described by Kozyrev [19, 13, 20], or still
the geophysical, ionospheric and solar processes described by Korotaev, Serdyuk
and Gorohov [23].

Acknowledgements
The author would like to express his deep gratitude to Prof. L. Horwitz for
discussions on the time parameter and his kind attention to the author’s work.
Also our gratitude to Vic Christianto for pointing out the work of R. Kiehn
on the correspondence between the Schroedinger and Navier-Stokes equations.
This work was carried out while at the Mathematics Department, University of
Bı́o Bı́o, Concepción, Chile, to which we extend our gratitude.

References
[1] D. L. Rapoport, On the Unification of Geometric and Random Structures
through Torsion Fields: Brownian Motions, Viscous and Magneto-fluid-dynamics,
22 It should not be confused with Kiehn’s notation for the heat one-form which in this
formalism coincides with LV Q for V a spacetime vector field which is thought as a process
acting on the system defined by Q (noted A in [47].

47

Quantizatin in Astrophysics ...

322

Foundations of Physics 35, no. 7, 1205-1244 (2005).
[2] D. L. Rapoport, Cartan-Weyl Dirac and Laplacian Operators, Brownian Motions: The Quantum Potential and Scalar Curvature, Maxwell’s and DiracHestenes Equations, and Supersymmetric Systems, Foundations of Physics 7,
1383-1431 (2005).
[3] R. Debever, The Einstein-Cartan Correspondence, Royal Academy of Sciences,
Belgium, 1983.
[4] A. Einstein and L. Kauffman, Annals Maths., 56 (1955); Yu Obukhov, Phys.
Letts.90 A (1982),13.
[5] O. Oron and L.P. Horwitz, Relativistic Brownian Motion as an Eikonal Approximation to a Quantum Evolution Equation , Foundations of Physics 33,
1177 (2003) ; ibid. in Progress in General Relativity and Quantum Cosmology Research,V. Dvoeglazov (ed.), (Nova Science, Hauppage, 2004);ibid.,Physics Letts.
A280, (2001), 265.
[6] T. Gill, W.W. Zachary and J. Lindesay, Foundations of Physics 31, no.9,
1299-1355 (2001).
[7] E. C. Stueckelberg,Helv. Physica Acta 14(1941), 322,588 .
[8] L.P. Horwitz and C. Piron, Helv. Physics Acta 46 (1973), 316; L.P. Horwitz and
C. Piron, Helv. Physica Acta 66 (1993), 694; M. C. Land, N. Shnerb and L.P.
Horwitz, J. Math. Phys. 36 (1995), 3263; L.P. Horwitz and N. Shnerb, Found. of
Phys. 28(1998), 1509.
[9] J. Fanchi, Parametrized Relativistic Quantum Theory, (Kluwer, Dordrecht,1993).
[10] M. Trump and W. Schieve Classical Relativistic Many-Body Dynamics, (Kluwer,
Dordrecht, 1994).
[11] A. Kyprianidis,Phys. Rep. 155, no.1, (1987),1-27 and references therein.
[12] R.E. Collins and J.R. Fanchi, Nuovo Cimento A 48 (1978), 314; J. Fanchi,
Found.Phys. 30, no. 8, (2000), 1161-1189 & 31, no. 9 (2001), 1267-1285.
[13] N. Kozyrev, Sources of Stellar Energy and the Theory of the Internal Constitution
of Stars, Progress in Physics vol.3, pp.65-99 (2005); reprint of the original in
Russian.
[14] E. Nelson,Quantum Fluctuations, (Princeton Univ. Press, Princeton, New Jersey
, 1985).
[15] L. P. Hughston, Proc. Royal Soc. London 452, no. 1947, 953-979 (1996).
[16] S. Adler, and L. Horwitz, J. Math.Phys. 41, 2485 (2000); ibid. arXiv:quantph/9909026; S. Adler ,Quantum Theory as an Emergent Phenomenon, (Cambridge Univ. Press, Cambridge (U.K.), 2004).

48

Quantizatin in Astrophysics ...

323

[17] V. de Sabbata, and C. Sivaram, Spin and Torsion in Gravitation, (World Scientific, Singapore ,1994); F. Hehl, P. von der Heyde, G. D. Kerlick and J. M.
Nestor, Rev. Mod. Phys. 15, 393 (1976).
[18] T. Schilling, Geometric Quantum Mechanics,Ph.D. Thesis, Penn State University
(1996).
[19] N.A. Kozyrev, On the possibility of experimental investigation of the properties
of time, in Time in science and philosophy, Prague, 1971, p.111-132.Kozyrev
N.A., Nasonov V.V., O nekhotoryh svoistvah vremeni, obnaruzhennykh astronomicheskimi nablyudeniyami, Problemy issledovaniya Vselennoi, 9, p.76 (1980).
(russian), www.chronos.msu.edu.
[20] A.N. Kozyrev, Astronomical proofs of the reality of 4D Minkowski Geometry,
in Manifestations of Cosmic Factors on Earth and Moon, Moscow-Leningrad,
pp.85-93, (1980); www.chronos.msu.ru.
[21] M. M. Lavrentiev, V. A. Gusev, I. A. Yegonova, M. K. Lutset, S. F. Fominykh, O
registratsii istinnogo polozheniya Solntsa, Doklady Akademii Nauk SSSR, 1990,
v.315, no. 2. (Russian), On the registration of a true position of Sun; M. M.
Lavrentiev, I. A. Yeganova, V. G. Medvedev, V. K. Oleinik, S. F. Fominykh, O
skanirovanii zvyeozdnogo neba datchikom Kozyreva, in Doklady Akademii Nauk,
323,no. 4, (Russian)(1992) (On the scanning of the star sky with Kozyrev’s
detector.).
[22] A.P. Levich, Motivations and problems in the studies of time, in Constructions of
Time in Natural Science: On the Way to Understanding the Time Phenomenon;
Part 1, Interdisciplinary Studies. Moscow University Press, Moscow,pp-1-15,
(1996); ibid., Time as variability of natural systems: methods of quantitative
description of changes and creation of changes by substantial flows, in Constructions of Time in Natural Science: On the Way to Understanding the Time Phenomenon; Part 1. Interdisciplinary studies, Moscow University Press, Moscow,
1996, pp. 149-192; ibid. A.P. Levich, Gravitation and Cosmology 1, no.3, 237-242
(1995); www.chronos.msu.edu.
[23] A. N. Korotaev, On the way to understanding the time phenomenon, in The
Construction of Time in the Natural Sciences, Part 2, A. Levich (ed.), pp. 60.74,
(World Scientific, Hong-Kong, 1996); On the possibility of the causal analysis
of Geophysical Processes,Geomagnetism and Aeronomy32, no.1, pp.27-33, 1992;
and, www.chronos.msu.edu; S.M. Korotaev, V.O. Serdyuk and J.V. Gorohov,
Forecast of solar and geomagnetic activity on the macroscopic nonlocality effect,to
appear.
[24] N. Ikeda and S. Watanabe, Stochastic Differential Equations and Diffusion Processes, (North-Holland/Kodansha, Amsterdam-Tokyo,1981).
[25] P. Malliavin, Géométrie Différentielle Stochastique, (Les Presses Univ., Montreal,
1978).
[26] J.M. Bismut, Mécanique Analytique, (Springer LNM 866, 1982).

49

Quantizatin in Astrophysics ...

324

[27] E. Schroedinger Sitzunsberger Press Akad. Wiss. Math. Phys. Math.,144 (1931)
and Ann. I. H. Poincaré 11 (1932), 300.
[28] O. Manuel, M.Mozina and H. Ratcliffe, arxiv.org/astro-ph/0511/0511379.
[29] H. Muller, Raum und Zeit,
www.globalscaling.de

special issue 1,

(Ehlers Verlag,

2003),

[30] H. Muller, Raum und Zeit, vol. 76 127, pp.76-77 (2004); R. Jahn, B.
Dunne, G. Bradish, Y. Lettieri and R. Nelson, Mind/Machine Consortium:PortTEG Replication experiments, J. Scientific Explorations, 4 no.4,
pp.499-555; www.princeton.edu/ pear/publist.html; I. Marcikic et al, Longdistance quantum teleportation of qubits at telecommunications wavelengths,
Letters of Nature 412, 30, Jan. 2003; A. Furusawa et al, Unconditional Quantum Teleportation, Science, Oct. 23, 1998, 706-709, www.its.caltech.edu/ qoptics/teleport.html.
[31] L. Nottale, Fractal Space-Time and Microphysics, (World Scientific, Singapore,
1993); ibid. Astron. Astrophys. Lett.315, L9 (1996); ibid.Chaos, Solitons & Fractals 7, 877 (1996).
[32] D.L. Rapoport, Rep. Math.Phys.49, no.1, pp.1-49 (2002), 50, no.2, pp.211-250
(2002), Random Operts. Stoch.Eqts. 11, no.2, pp.2, pp.109-150 (2003) and 11,
no.4, 351-382 (2003); ibid. in Trends in Partial Differential Equations of Mathematical Physics, Obidos, Portugal (2003), J.F.Rodrigues et al (edts.), Progress
in Nonlinear Partial Differential Equations, vol.61, Birkhausser, Boston, (2005);
ibid. in Instabilities and Nonequilibrium Structures VII & VIII, O. Descalzi et al
(eds.), Kluwer Series in Nonlinear Phenomenae and Complex System, Dordrecht,
(2004); ibid. Discrete and Cont. Dynamical Systems, series B, special issue, Third
Int.Conf. Differential Eqts. and Dynamical Systems, Kennesaw Univ., May 2000,
327-336, S. Hu (ed.), (2000).
[33] R. Kiehn, http://www22.pair.com/csdc/pdf/bohmplus.pdf.
[34] L. Nottale, G. Schumacher & E.T. Lefvre, Astron. Astrophys. 361, 379 (2000).
[35] V. Fock, The Theory of Gravitation, (Pergamon Press, London-New York, 1956).
[36] M. P.Gaffney, Annals of Mathematics60, 140-145 (1953.)
[37] E.B. Davies, Heat Kernels and spectral theory, (Cambridge Univ. Press, 1989);
S. Rosenberg, The Laplacian on a Riemannian Manifold, London Mathematical
Society Students Texts, vol. 31, (Cambridge Univ. Press, 1997); R.S.Strichartz,
J.Func.Anal.52 no.1, 48-79 (1983).
[38] M. Nagasawa, Stochastic Processes in Quantum Physics (Monographs in Mathematics),(Birkhauser, Boston-Basel , 2000).
[39] Schechter, Operator Methods in Quantum Mechanics, (North-Holland, New York,
1981).

50

Quantizatin in Astrophysics ...

325

[40] R.M. Santilli, Elements of Hadronic Mechanics III, ( Hadronic Press-Naukova,
Palm Harbor-Kiev, 1993); ibid.Isodual Theory of Antimatter, with Applications
to Antigravity, Spacetime Machine and Grand Unification, (Springer, New York
,2006).
[41] D. Miller, Rev.Mod.Phys.,Vol.5(2), p.203-242, July 1933.
[42] M. Allais, Comptes Rendus de L’Acadmie des Sciences, Paris, t. 327, Série II
b, p.1411-1419 (2000) and t. 1, Série IV, p.1205-1210 (2000).
[43] R. Cahill, Progress in Physics 4, pp.73-92 (2006) and references therein .
[44] M. Pavsic, The Landscape of Theoretical Physics: A Global View, (Kluwer, Dordrecht, 2001).
[45] L. Horwitz and Y. Rabin, quant-ph/0507044, Lett. Nuovo Cimento 17 , p.501
(1976). L. Horwitz, On the significance of a recent experiment demostrating quantum interference in time, quant-ph/0507044; and contribution to the Proceedings
of IARD 2006.
[46] F. Lindner et al, quant-phi/0503165 and Phys. Rev.Letts. 95, 040401, (2005)
[47] R. Kiehn, Wakes, Coherence and Turbulent Structures, (Lulu Publs., 2004). ibid.
Non-equilibrium Thermodynamics, Vol 1 Non-Equilibrium Systems and Irreversible Processes. ibid.Falaco Solitons, Cosmology and the Arrow of Time ...
Vol2. Non-Equilibrium Systems and Irreversible Processes.ibid. Non Equilibrium
Systems and Irreversible Processes Vol. 7 Selected Publications , (Lulu Publs. ,
2004).
[48] T. Frankel, The Geometry of Physics, An Introduction, (Cambridge Univ. Press,
2000).
[49] D.L. Rapoport, in Group XXI, Physical Applications of Aspects of Geometry,
Groups and Algebras, Proceedings of the XXI International Conference on Group
Theoretical Methods in Physics, Goslar, Germany, June 1995, H.Doebner et al
(eds.), 446-450, World Scientific, Singapore (1996); ibid.Advances in Applied Clifford Algebras8, no.1, 126-149 (1998).
[50] L.C. Horwitz, private communication.
[51] D. Rapoport, Int. J.Theor. Phys. 30no. 11 (1991), 1497. D. Rapoport and S.
Sternberg, Annals of Phys. 158, 447 (1984); ibid, Lett. N. Cimento80A, 371
(1984).
[52] D. Paine and W. Pensinger [52] Toward a General Theory of Process (circa May
of 1977), http://www.geocities.com/moonhoabinh/ithapapers/generalpr.html.
[53] A G Bershadskii, Sov. Phys. Uspekhi 33 (12), 1073-1075, (1990); Turbulence and
Magnetic Fields in Astrophysics (Lecture Notes in Physics), E. Falgarone & T.
Passot (edts.), (Springer, New York, 2003); Y. Fujita, T. Matsumoto, and K.
Wada, The Astrophysical Journal, volume 612, part 2 (2004), pages L9L12.

51

Quantizatin in Astrophysics ...

326

[54] http://www.sciencedaily.com/releases/2002/01/020123075605.htm.turb1.
[55] C.Gibson, astro-ph/0304441; http:astro-ph/0304107;astro-ph/0210583; astroph/0110248 .
[56] D. Rapoport, Algebras, Groups and Geometries8, 1-61 (1980); ibid.
[57] I. Bialnicky-Birula and J. Mycielsky, Annals of Physics 100, 1611, (1976); see
also for numerical solutions J. Kamersberg and A. Zeilinger, Physica B 151, 193,
(1988).
[58] D. Rapoport and M. Tilli, Scale Fields as a simplicity principle,in Proceedings of
the Third Workshop on Hadronic Mechanics, vol. II (Patras, 1986), Hadronic J.
Suppl. 6,no.4, 682-778 (1986).
[59] C. Castro, J. Mahecha and B. Rodriguez, Nonlinear Quantum Mechanics
as a fractal Brownian motion with complex diffusion constant, arXiv:quantph/0202026.
[60] C. Castro and Jorge Mahecha, On Nonlinear Quantum Mechanics, Brownian
motions and Fisher Information, Progress in Physics, 1, 38-45 (2006).
[61] L. Nottale, Fractality field in the Theory of Scale Relativity, Progress in Physics,
1, , 12-16, April (2005).
[62] D. Rapoport and M. Tilli, Hadronic Journal 10, no.1, 25-34 (1987).
[63] C.Castro, On the coupling constants , Geometric Probability and Complex Domains, Progress in Physics,2, 46-53 (2006).
[64] R.M. Santilli, Foundations of Hadronic Chemistry,( Kluwer Series in the Fundamental Theories of Physics, Dordrecht-Boston, 2001).
[65] R.M. Santilli, arxiv.org/abs/physics/0610263.
[66] W. Feller, Introduction to Probability, (Addison-Wesley, 1961).
[67] P. Noyes, Bit-String Physics, A Finite and Discrete Approach to Natural Philosophy, J. C. van den Berg (edit.), (World Scientific,Singapore,(2001).
[68] P.R. Holland, The quantum theory of motion, (Cambridge Univestity Press, Cambridge (U.K.), 1994). D.Bohm, Phys. Rev. 85, 166 and 180 (1952); D. Bohm and
J.P.Vigier, Phys. Rev. 96 (1953).
[69] A. Kaivarainen, A unified theory of bivacuum, Corpuscle-Wave Duality, Fields
and Time, http://www.karelia.ru/ alexk/newarticles/index.html(2003).
[70] A.N.Kolmogorov, Zur Umkehrbarkeit der statistichen Naturgesetze, Math. Annalen 113 (1937), 776-772.
[71] B. de Witt, Quantum Field Theory in Curved Space-Time, Phys. Rep. C 19, no.6
(1975), 295-357. G.W. Gibbons, in General Relativity, An Einstein Centennary
Survey, S.W. Hawking and W. Israel, (Cambridge Univ. Press, Cambridge, 1979);
N.D. Birrell and P.C.W.Davies, Quantum Field Theories in Curved Space.

52

Quantizatin in Astrophysics ...

327

[72] F. Langouche, D. Roenkarts and E. Tirapegui, Functional Integration and Semiclassical Expansions; (Reidel, Dordrecht 1981).
[73] S. Albeverio et al., J.Math.Phys. 18, 907 (1977) and Stochastic Methods in
Physics, Math. Phys.Rep. 77, in K.D. Elworthy and de C. Witt-Morette, (edts.),
no.3, (1977). See also F.Guerra contribution in the last reference. Kleinert H.
Kleinert, Path integrals in Quantum Mechanics, Statistics and Polymer Physics,
(World Scientific, Singapore, 1991).
[74] I. Prigogine, From Being to Becoming, (Holden Day, San Francisco, 1991).
[75] M.Pitkanen, see the first chapter of the present volume and references therein.
[76] F. Smarandache, see references in the contributions to this volume.
[77] W. Rodrigues and E. Capelas de Oliveira, The Many Faces of the Maxwell, Dirac
and Einstein equations, (Springer,Berlin, in press, 2007).
[78] D. Rapoport, W. Rodrigues, Q. de Souza and J.Vaz, Algebras, Groups and Geometries 11, 25-35 (1995).
[79] J. Vaz Jr. and W.A.Rodrigues, Int.. J.Theor.Phys. 32 945-949 (1995); W. Rodrigues and J. Vaz, in R.Delanghe (ed.), Clifford Algebras and their Applications
in Mathematical Physics, Proceedings of the III Workshop, (Kluwer,Dordrecht,
1993).
[80] D. Rapoport, Torsion Fields, Cartan-Weyl Space-Time and State-Space Quantum
Geometries and their Brownian Motions, to appear , Foundations of Physics 37
(2007).

53

Quantizatin in Astrophysics ...

328

VISCOUS AND MAGNETO FLUID-DYNAMICS, TORSION FIELDS,
AND BROWNIAN MOTIONS REPRESENTATIONS ON COMPACT MANIFOLDS
AND THE RANDOM SYMPLECTIC INVARIANTS
Diego L. Rapoport
Dep. Sciences and Technology, Univ. Nac. de Quilmes
Bernal, Buenos Aires, Argentina
Abstract: We reintroduce the Riemann-Cartan-Weyl geometries with trace
torsion and their associated Brownian motions on spacetime to extend them to
Brownian motions on the tangent bundle and exterior powers of them. We
characterize the diffusion of differential forms, for the case of manifolds without
boundaries and the smooth boundary case. We present implicit representations
for the Navier-Stokes equations (NS) for an incompressible fluid in a smooth
compact manifold without boundary as well as for the kinematic dynamo equation (KDE, for short) of magnetohydrodynamics. We derive these representations from stochastic differential geometry, unifying gauge theoretical structures
and the stochastic analysis on manifolds (the Ito-Elworthy formula for differential forms. From the diffeomorphism property of the random flow given by
the scalar lagrangian representations for the viscous and magnetized fluids, we
derive the representations for NS and KDE, using the generalized Hamilton and
Ricci random flows (for arbitrary compact manifolds without boundary), and
the gradient diffusion processes (for isometric immersions on Euclidean space of
these manifolds). We solve implicitly this equations in 2D and 3D. Continuing
with this method, we prove that NS and KDE in any dimension other than 1,
can be represented as purely (geometrical) noise processes, with diffusion tensor
depending on the fluid’s velocity, and we represent the solutions of NS and KDE
in terms of these processes. We discuss the relations between these representations and the problem of infinite-time existance of solutions of NS and KDE.
We finally discuss the relations between this approach with the low dimensional
chaotic dynamics describing the asymptotic regime of the solutions of NS. We
present the random symplectic theory for the Brownian motions generated by
these Riemann-Cartan-Weyl geometries, and the associated random PoincaréCartan invariants. We apply this to the Navier-Stokes and kinematic dynamo
equations. In the case of 2D and 3D, we solve the Hamiltonian equations.
MSC numbers 60J60, 60H10, 35Q30, 64H30, 58G03, 58G32, 76M35, 53Z05.

1

Quantization in Astrophysics ...

329

1

Introduction.

In a separate chapter of this book, we have presented the relations between
certain spacetime geometries, Brownian motions and non-relativistic quantum
mechanics. We also touched briefly on the Navier-Stokes equations for viscous
fluid-dynamics and the associated problem of describing turbulence in topological terms in analyzing the Pfaffian (topological dimension) of the trace-torsion
one-form. But in that article, we stopped short of giving analytical representations for the Navier-Stokes equations. In that chapter it appeared that contrary to common belief, torsion fields are all pervasive, in the sense that the
Schroedinger wave function is a torsion field generator through the logarithmic differential of it. Furthermore, another remarkable torsion field appeared
to be the velocity one-form for a viscous fluid obeying the Navier-Stokes equations. We also discussed shortly the relations between the Schroedinger and
Navier-Stokes equations, and that the Brownian motions which will constitute
the virtual paths for the fluids random particles, sustain a probability measure
which is none other than the enstrophy of the fluid, and thus the square of the
vorticity function (for the case of spatial 2D case) becomes the Born probability
amplitude of the Navier-Stokes equations.
The purpose of this chapter is two-fold: To start with, to give implicit random representations for the solutions of the Navier-Stokes equation for an incompressible fluid (NS, for short in the following) and for the kinematic dynamo
problem of magnetohydrodynamics, in several instances; firstly, on an arbitrary
compact orientable smooth manifold (without boundary), following our presentation in [58,59]; further in the case in which the manifold is isometrically
embedded in Euclidean space [58,66], from which we deduce the expressions for
Euclidean space itself, and finally representations of NS as purely noise equations.
Secondly, to present as a basis for these representations, the gauge-theoretical
structures of Brownian motion theory and the stochastic analysis associated to
them. Thus, the method of integration we shall apply for our objectives stems
from stochastic differential geometry, i.e. the gauge theory of Brownian processes in smooth manifolds and Euclidean space developed in the pioneering
works by Ito [15], Eells and Elworthy [13], P. Malliavin [11] and further elaborated by Elworthy [12], Ikeda and Watanabe [14], P. Meyer [40], and Rogers and
Williams [37]. Associated to these geometrical structures which can be written
in terms of the Cartan calculus on manifolds of classical differential geometry, we
shall present the rules of stochastic analysis which describe the transformation of
differential forms along the paths of generalized Brownian motion generated by
these geometries, setting thus the method for the integration of evolution equations for differential forms; this is the well known martingale problem approach
for the integration of partial differential equations on manifolds [30].
While classical Hamiltonean systems with finite degrees of freedom may
appear to have a random behavior, in fluid dynamics it is known that the
2

Quantization in Astrophysics ...

330

2D Euler equation for an inviscid fluid is a hamiltonean system with infinite
degrees of freedom supporting as well infinite conserved quantities; such a system
appears to be non-random and the approach, pioneered by Arnold and further
elaborated by Ebin and Marsden, is a blending of global analysis and symplectic
geometry [9,56,70]. The situation is radically different in the case of a viscous
fluid described by the NS. In this case, there is a second-order partial derivative
associated to the kinematical viscosity, which points out to the fact that there
is a diffusion term, which can be thougth as related to a Brownian motion.
Thus in the viscous case, there is from the very beginning a random element.
While in the Euler case the group of interest is the group of (Riemannian)
volume preserving diffeomorphisms, it will turn out in the course of these studies,
that there is an active group of random diffeomorphisms which represent the
Lagrangian random flow of the viscous fluidparticles. In this case, when there
is a non-constant diffusion tensor describing the local amplification of noise,
these diffeomorphisms do not preserve the volume measure, contributing at a
dynamical level -as it will turn out- to the complicated topology of turbulent
and magnetized flows [56].
The essential role of randomness in Fluid Dynamics appears already at an
experimental level. The analysis of the velocimetry signal of a turbulent fluid
shows that its velocity is a random variable, even though that the dynamics
is ruled by NS [8]. The concept of a turbulent fluid as a stochastic process
was first proposed by Reynolds [16], who decomposed the velocity into mean
velocity plus fluctuations. The Reynolds approach is currently used in most
numerical simulations of turbulent fluids in spite of the fact that it leads to
unsurmountable non-closure problems of the transport equations.; see Lumley
[18], Mollo and Christiansen [38]. Furthermore, the Reynolds decomposition is
non invariant alike the usual decomposition into drift and white noise perturbation in the non-invariant treatment of diffusion processes. Other treatments
of stochasticity in turbulence were advanced from the point of view of Feynman path integrals, as initiated by Monin and Yaglom [17]. From the point of
view of diffusion processes, invariant measures for stochastic modifications of
the Navier-Stokes equations on euclidean domains, have been constructed by
Vishik and Fursikov [36] and Cruzeiro and Albeverio [42]. (It is important to
remark that the existance of an invariant measure for NS as a classical dynamical system is the starting point of the classical dynamical systems approach to
turbulence; see Ruelle [48].) Contemporary investigations develop the relations
between randomness and the many-scale structure of turbulence which stems
from the Kolmogorov theory as presented by Fritsch[8] and Lesieur [3], and
apply the renormalization group method; see Orzsag [19].
A completely new line of research followed from the understanding of the
fundamental importance of the vorticity (already stressed by Leonardo da Vinci)
in the self-organization of turbulent fluids, which was assessed by numerical
simulations by Lesieur [3,4], and theoretically by Majda [39] and Chorin [1].
It was found that NS on Euclidean domains yields a diffusion equation for the
3

Quantization in Astrophysics ...

331

vorticity which becomes a source for the velocity through the Poisson equation:
Solving the latter equation, we can obtain an expression for the fluids velocity
in 2D. This observation was the starting point for the random vortex method
in Computational Fluid Mechanics largely pioneered by Chorin, and lead to
momentous sucess in numerical implementations for viscous fluids [1,2,6]. This
conception lead to apply methods of statistical mechanics (as originally proposed
by Onsager [20]) to study the complex topology of vortex dynamics and to relate
this to polymer dynamics [1]. In the random vortex method a random lagrangian
representation for the position of the incompressible fluid particles was proposed.
Consequently, NS (heat equation) for the vorticity was integrated only for fluids
in 2D flat space (implicit to this is the martingale problem approach quoted
above); see Chorin [1,2]. The difficulty for the exact integration in the general
case apparently stems from the fact that while in dimension 2 the vorticity 2form can be identified with a density and then the integration of NS for the
vorticity follows from the application of the well known Ito formula for scalar
fields, in the case of higher dimension this identification is no longer valid and
an Ito formula for 2-forms is required to carry out the integration in the case
of manifolds. This formula only recently became available in the works by
Elworthy [27] and Kunita [24], in the context of the theory of random flows on
smooth manifolds, largely developed by Baxendale [55,77,78,79,80].
The importance of a Stochastic Differential Geometry treatment of the NavierStokes equation on a smooth n-manifold M or in Euclidean space, stems from
several fundamental facts which are keenly interwoven. For a start, it provides
an intrinsic geometrical characterization of diffusion processes of differential
forms which follows from the characterizations of the laplacians associated to
non-Riemannian geometries with torsion of the trace type, as the infinitesimal generators of the diffusions. In particular, this will allow to obtain a new
way of writing NS in terms of these laplacians acting on differential one-forms
(velocities) and two-forms (vorticities). Furthermore, these diffusion processes
of differential forms, are constructed starting from the scalar diffusion process
which under Hoelder continuity or Sobolev regularity conditions, yields a timedependant random diffeomorphism of M which will represent the Lagrangian
trajectories for the fluid particles position. This diffeomorphic property will
allow us to use the Ito formula for differential forms (following the presentation
due to Elworthy) as the key instrument for the integration of NS. Thus, it is the
knowledge of the rules of stochastic analysis what sets the martingale problem
approach to the solution of NS when he have transformed it to an equivalent
system which is essentially the heat equation for the vorticity and the Poisson-de
Rham equation for the velocity with source derived from the velocity, the latter
admitting a random integration which generalizes the well known Biot-Savart
formula. 1 To keep the methodology that steers this article, as clear as possi1 The geometrical structures on which the gauge-theoretical foundations of Brownian motion are introduced, were originally found in gauge theories of gravitation, including not only

4

Quantization in Astrophysics ...

332

ble, we shall present rather extensively the relations between gauge-theoretical
structures and stochastic analysis, keeping in mind that they are unknown to the
mathematical-physics community, at large. Thus, Sections II to V will present
the linear connections of Riemann-Cartan-Weyl, their laplacians and the random flows generated by them. Then, in Sections VI to XI we shall deal with
NS and the kinematic dynamo equations, giving the random representations for
their solutions, for arbitrary compact manifolds and for manifolds isometrically
embedded in Euclidean space, from which we shall deduce the solutions in 2D
and 3D for both equations. In Section XII we shall return to the method, discussing again the relation between connections and stochastic analysis, to give
most remarkable driftless representations for diffusion processes on any manifold
with dimension other than 1, i.e. the representation of the invariant diffusion
operator in terms of a purely (geometrical) noise operator, which is a reduction
of a more general recent result due to Elworthy-LeJan-Li [71]. This article closes
with the application of this, in Section XIII, to yield the driftless representations
for NS and the kinematic dynamo equations, thus proving that the non-linearity
of NS can be incorporated to the diffusion tensor, yielding a non-linear purely
noise equation. This might be conceived as the completion of the theoretical
treatment of an historical sequence, starting from coherent geometrical vortex
structures (whose dynamics is described by a RCW connection, i.e. by a geometry) reaching to the empirically verified noise of viscous fluids in turbulent
regime, in which the noise admits a geometrical representation itself.

2

Riemann-Cartan-Weyl Geometry of Diffusions

The objective of this section is to show that the invariant definition of
a ”heat” (Fokker-Planck) operator requires the introduction of linear connections of a certain type. This gauge-theoretical characterization, together with
stochastic analyisis, turns out to be the very the basis for the construction of
diffusion processes, either on smooth manifolds or in Euclidean space.
Let us consider for a start, a smooth n-dimensional compact orientable manifold M (without boundary), on which we shall consider a second-order smooth
differential operator L. On a local coordinate system, (xα ), α = 1, . . . , n, L is
written as
L=

1 αβ
g (x)∂α ∂β + B α (x)∂α + c(x).
2

(1)

From now on, we shall fix this coordinate system, and all local expressions shall
be written in it.
translational degrees of freedom, but additionally spinor fields [46]; furthermore, it appeared
in the formulation of the classical mechanics of spinning particles submitted to exterior gravitational fields [57], which spares the use of hamiltonean and lagrangean formalisms.

5

Quantization in Astrophysics ...

333

Although formally, there is no restriction as to the nature of M , we are
really thinking on a n-dimensional space (or space-time) manifold, and not in
a phase-space manifold of a dynamical system.
The principal symbol σ of L, is the section of the bundle of real bilinear
symmetric maps on T ∗ M , defined as follows: for x ∈ M , pi ∈ Tx∗ M , take C 2
functions, fi : M → R with fi (x) = 0 and dfi (x) = pi , i = 1, 2; then,
σ(x)(p1 , p2 ) = L(f1 f2 )(x).

(2)

Note that σ is well defined, i.e., it is independent of the choice of the functions
fi , i = 1, 2. T ∗ M is, of course, the cotangent manifold, Tx∗ M a fiber on x ∈ M .
If L is locally as in (1), then σ is locally represented by the matrix (g αβ ).
We can also view σ as a section of the bundle of linear maps L(T ∗ M, T M ), or
as a section of the bundle T M ⊗ T M , or still as a bundle morphism from T ∗ M
to T M . If σ is a bundle isomorphism, it induces a Riemannian structure g on
M , g : M → L(T M, T M ):
g(x)(v1 , v2 ) :=< σ(x)−1 v1 , v2 >x ,
for x ∈ M , v1 , v2 ∈ Tx∗ M . Here, < ., . >x denotes the natural duality between Tx∗ M and Tx M . Locally, g(x) is represented by the matrix 21 (g αβ (x))−1 .
Consider the quadratic forms over M associated to L, defined as
Qx (px ) =

1
< px , σx (px ) >x ,
2

for x ∈ M , px ∈ Tx∗ M . Then, with the local representation (1) for L, Qx is
represented as 12 (g αβ (x)). Then, L is an elliptic (semi-elliptic) operator whenever for all x ∈ M , Qx is positive-definite (non-negative definite). We shall
assume in the following that L is an elliptic operator. In this case, σ is a bundle
isomorphism and the metric g is actually a Riemannian metric. Notice, as well,
that σ(df ) = grad f , for any f : M → R of class C 2 , where grad denotes the
Riemannian gradient.
We wish to give an intrinsic description of L, i.e. a description independent
of the local coordinate system. This is the essential prerequisite of covariance.
For this, we shall introduce for the general setting of the discussion, an
arbitrary connection on M , whose covariant derivative we shall denote as ∇.
We remark here that ∇ will not be the Levi-Civita connection associated to g;
we shall precise this below. Let σ(∇) denote the second-order part of L, and
let us denote by X0 (∇) the vector field on M given by the first-order part of
L. Finally, the zero-th order part of L is given by L(1), where 1 denotes the
constant function on M equal to 1. We shall assume in the following, that L(1)
vanishes identically.
Then, for f : M → R of class C 2 , we have
σ(∇)(x) =

1
1
trace(∇2 f )(x) = (∇df )(x)),
2
2
6

Quantization in Astrophysics ...

334

(3)

where the trace is taken in terms of g, and ∇df is thought as a section of
L(T ∗ M, T ∗ M ). Also, X0 (∇) = L − σ(∇). If Γα
βγ is the local representation for
the Christoffel symbols of the connection, then the local representation of σ(∇)
is:
1 αβ
g (x)(∂α ∂β + Γγαβ (x)∂γ ),
2

(4)

1
X0 (∇)(x) = B α (x)∂α − g αβ (x)Γγαβ ∂γ .
2

(5)

σ(∇)(x) =
and

If ∇ is the Levi-Civita connection associated to g, which we shall denote as ∇g ,
then for any f : M → R of class C 2 :
σ(∇g )(df ) =

1
1
trace((∇g )2 f ) = trace(∇g df ) = −1/2divg grad f = 1/24g f. (6)
2
2

Here, 4g is the Levi-Civita laplacian operator on functions; locally it is written
as
4g = g −1/2 ∂α ((g 1/2 g αβ ∂β ); g = det(gαβ ),

(7)

and divg is the Riemannian divergence operator on vector fields X = X α (x)∂α :
divg (X) = −g −1/2 ∂α (g 1/2 X α ).

(8)

Note the relation we already have used in eqt. (6) and will be used repeatedly;
namely:
divg (X) = −δ X̃,

(9)

where δ is the co-differential operator (see (23) below), and X̃ is the one-form
conjugate to the vector field X, i.e. X̃α = gαβ X β .
We now take ∇ to be a Cartan connection with torsion [10,46], which we
additionally assume to be compatible with g, i.e. ∇g = 0. Then σ(∇) =
1
2
2 trace(∇ ). Let us compute this. Denote the Christoffel coefficients of ∇ as
α
Γβγ ; then,
Γα
βγ


=

α
βγ



α
+ 1/2Kβγ
,

(10)

where the first term in (10) stands for the Christoffel Levi-Civita coefficients of
the metric g, and
α
α
α
α
Kβγ
= Tβγ
+ Sβγ
+ Sγβ
,

7

Quantization in Astrophysics ...

335

(11)

α
κ
α
α
is the cotorsion tensor, with Sβγ
= g αν gβκ Tνγ
, and Tβγ
= Γα
βγ − Γγβ the skewsymmetric torsion tensor.
Let us consider the Laplacian operator associated with this Cartan connection, defined -in extending the usual definition- by

H(∇) = 1/2trace∇2 = 1/2g αβ ∇α ∇β ,

(12)

where ∇ stands for the covariant derivative operator with respect to Γ; then,
σ(∇) = H(∇). A straightforward computation shows that that H(∇) only
depends in the trace of the torsion tensor and g:
H(∇) = 1/24g + g αβ Qβ ∂α ≡ H0 (g, Q),

(13)

ν
with Q = Tνβ
dxβ , the trace-torsion one-form.
Therefore, for the Cartan connection ∇ defined in (10), we have that

σ(∇) =

1
1
trace(∇2 ) = 4g + Q̂,
2
2

(14)

with Q̂ the vector-field conjugate to the 1-form Q: Q̂(f ) =< Q, grad f >,
f : M → R. In local coordinates,
Q̂α = g αβ Qβ .
We further have:
 
1 αβ γ
∂γ − Q̂,
X0 (∇) = B − g
αβ
2

(15)

Therefore, the invariant decomposition of L is
1
1
trace(∇2 ) + X0 (∇) = 4g + b,
2
2

(16)

 
1 αβ γ
b=B− g
∂γ .
2
αβ

(17)

with

Notice that (15) can be thought as arising from a gauge transformation: b̃ →
b̃ − Q, with b̃ the 1-form conjugate to b.
If we take for a start ∇ with Christoffel symbols of the form
 
 α
α
2
α
δ Qγ − gβγ Qα
(18)
Γβγ =
+
βγ
(n − 1) β
with
Q = b̃,

i.e. Q̂ = b,
8

Quantization in Astrophysics ...

336

(19)

we have
X0 (∇) = 0
and
H0 (g, Q) = σ(∇) =

1
1
1
trace(∇2 ) = trace((∇g )2 ) + Q̂ = 4g + b.
2
2
2

(20)

Therefore, for ∇ as in (18) we obtain a gauge theoretical invariant representation
for L given by an operator whose first order term is incorporated into its symbol:
1
1 2
∇ = σ(∇) = trace((∇g )2 ) + Q̂ = H0 (g, Q).
2
2
or more shortly, the expression we shall follow along this article
L = H(∇) =

H0 (g, Q) =

1
trace((∇g )2 ) + Q̂.
2

(21)

where Q̂ is the vectorfield given by the g-conjugate of the one-form Q = Qβ dxβ ,
i.e. with components Q̂α = g αβ Qβ .
The restriction we have placed in ∇ to be as in (18), i.e. only the trace
component of the irreducible decomposition of the torsion tensor is taken, is
due to the fact that all other components of this tensor do not appear at all
in the laplacian of (the otherwise too general) ∇. In the particular case of
dimension 2, this is automatically satisfied. In the case we actually have assumed that g is Riemannian, the expression (21) is the most general invariant
laplacian (with zero potential term) acting on functions defined on a smooth
manifold. This restriction, will allow us to establish a one-to-one correspondance between Riemann-Cartan connections of the form (18) with (generalized
Brownian) diffusion processes. These metric compatible connections we shall
call RCW geometries (short for Riemann-Cartan-Weyl), since the trace-torsion
is a Weyl 1-form [10]. Thus, these geometries do not have the historicity problem which lead to Einstein’s rejection of the first gauge theory ever proposed by
Weyl.
To obtain the most general form of the RCW laplacian, we only need to apply
to the trace-torsion one-form the most general decomposition of one-forms on
a smooth compact manifold. This amounts to give the constitutive equations
of the particular theory of fluctuations under consideration on the manifold M ;
see [22,26,49]. The answer to this problem, is given by the well known de RhamKodaira-Hodge theorem of global analysis (in Fluid Mechanics it is known with
the acronym Helmholtz-Hodge [5]), which we present now.
We consider the Hilbert space of square summable ω of smooth differential
forms of degree k on M , with respect to volg . We shall denote this space as
L2,k . The inner product is
Z
<< ω, φ >>:=
< ω(x), φ(x) > volg (x),
(22)
M

9

Quantization in Astrophysics ...

337

where the integrand is given by the multiplication between the components
ωα1 ...αk of ω and the conjugate tensor:g α1 β1 . . . g αk βk φβ1 ...βk ; alternatively, we
can write in a coordinate independent way: < ω(x), φ(x) > volg = ω(x) ∧ ∗φ(x),
with ∗ the Hodge star operator, for any ω, φ ∈ L2,k .
The de Rham-Kodaira-Hodge operator on L2,k is defined as
4k = −(d + δ)2 = −(dδ + δd),

(23)

where δ is the formal adjoint defined on L2,k+1 of the exterior differential operator d defined on L2,k :
<< δφ, ω >>=<< φ, dω >>,
for φ ∈ L2,k+1 and ω ∈ L2,k . Then, δ 2 = 0 follows from d2 = 0.
Let R : (T M ⊕ T M ) ⊕ T M → T M be the (metric) curvature tensor defined
by: (∇g )2 Y (v1 , v2 ) = (∇g )2 Y (v2 , v1 ) + R(v1 , v2 )Y (x). From the Weitzenbock
formula [14] we have the decomposition of 41 into the Laplace-Beltrami term
and a Weitzenbock term
41 φ(v) = trace (∇g )2 φ(−, −) − Ricx (v, φ̂x ),
for v ∈ Tx M and Ricx (v1 , v2 ) = trace < R(−, v1 )v2 , − >x . Then, 40 =
(∇g )2 = 4g so that in the case of k = 0, the de Rham-Kodaira operator
coincides with the Laplace-Beltrami operator on functions.
The de Rham-Kodaira-Hodge theorem states that L2,1 admits the following
invariant decomposition. Let ω ∈ L2,1 ; then,
ω = d f + A1 + A2 ,

(24)

where f : M → R is a smooth function on M , A1 is a co-closed smooth 1-form:
δA1 = −divg Â1 = 0, and A2 is a co-closed and closed smooth 1-form:
δA2 = 0, dA2 = 0.

(25)

Otherwise stated, A2 is an harmonic one-form, i.e.
41 A2 = 0.

(26)

Furthermore, this decomposition is orthogonal in L2,1 , i.e.:
<< df, A1 >>=<< df, A2 >>=<< A1 , A2 >>= 0.

(27)

Remark 1. Note that A1 + A2 is itself a co-closed one-form. If we consider an augmented configuration space R × M for an incompressible fluid,
the de Rham-Kodaira-Hodge is applied to the fluid’s velocity described by a
time-dependant 1-form satisfying NS. If we consider instead a four-dimensional
Lorentzian manifold M provided with a Dirac-Hestenes spinor operator field
10

Quantization in Astrophysics ...

338

(i.e. a section of the spinor bundle over M ), one needs the whole decomposition
(24) associated to an invariant density ρ of the diffusion (i.e. a solution of the
equation H0 (g, Q)† ρ = 0) to describe two electromagnetic potentials such that
when restricted to the spin-plane of the DHSOF, they enforce the equivalence
between the Dirac-Hestenes equation for the DHSOF on a manifold provided
with a RCW connection, and the free Maxwell equation on the Lorentzian spacetime [26,60].

3

Generalized Laplacians on Differential Forms

Consider the family of zero-th order differential operators acting on smooth
k-forms, i.e. differential forms of degree k (k = 0, . . . , n) defined on M :
Hk (g, Q) := 1/24k + LQ̂ ,

(28)

Furthermore, the second term in (28) denotes the Lie-derivative with respect to
the vectorfield Q̂. Recall that the Lie-derivative is independant of the metric:for
any smooth vectorfield X on M
LX = iX d + diX , ,

(29)

where iX is the interior product with respect to X: for arbitrary vectorfields
X1 , . . . , Xk−1 and φ a k-form defined on M , we have (iX φ)(X1 , . . . , Xk−1 ) =
φ(X, X1 , . . . , Xk−1 ). Then, for f a scalar field, iX f = 0 and
LX f = (iX d + diX )f = iX df = g(X̃, df ) = X(f ).

(30)

where X̃ denotes the 1-form associated to a vectorfield X on M via g. We shall
need later the following identities between operators acting on smooth k-forms,
which follow easily from algebraic manipulation of the definitions:
d4k = 4k+1 d, k = 0, . . . , n,

(31)

δ4k = 4k−1 δ, k = 1, . . . , n,

(32)

and

and finally, for any vectorfield X on M we have that dLX = LX d and therefore
dHk (g, Q) = Hk+1 (g, Q)d, k = 0, . . . , n.

(33)

In (28) we retrieve for scalar fields (k = 0) the operator H0 (g, Q) defined in
(21).
Proposition 1. Assume that g is non-degenerate. There is a one-to-one
mapping
∇ ; Hk (g, Q) = 1/24k + LQ̂
11

Quantization in Astrophysics ...

339

between the space of g-compatible affine connections ∇ with Christoffel coefficients of the form
 
 α
α
2
δ Qγ − gβγ Qα
(34)
Γα
=
+
βγ
βγ
(n − 1) β
and the space of elliptic second order differential operators on k-forms (k =
0, . . . , n) with zero potential term (other than the Weitzenbock term [14]).

4

Riemann-Cartan-Weyl Connections and the
Laplacians for Differential Forms

In this section we shall construct the diffusion processes for scalar fields.
In the following we shall further assume that Q = Q(τ, x) is a time-dependant
1-form, so that we have a time-dependant RCW connection on M , which we
think of as a space manifold. The stochastic flow associated to the diffusion
generated by H0 (g, Q) has for sample paths the continuous curves τ 7→ xτ ∈ M
satisfying the Ito invariant non-degenerate s.d.e. (stochastic differential equation)
dxτ = X(xτ )dWτ + Q̂(τ, xτ )dτ.

(35)

In this expression, Q̂ is the g-conjugate of Q, the diffusion tensor X = (Xβα (x)) is
a linear surjection X(x) : Rm → Tx M satisfying Xνα Xνβ = g αβ , and {W (τ ), τ ≥
0} is a standard Wiener process on Rn . Thus < Wτ >= 0 and < Wτi Wτj >=
δij τ , where < − > denotes expectation with respect to the zero-mean standard
Gaussian function on Rm (m ≥ n). Here τ denotes the time-evolution parameter
of the diffusion (in a relativistic setting it should not be confused with the time
variable), and for simplicity we shall assume always that τ ≥ 0. Consider the
canonical Wiener space Ω of continuous maps ω : R → Rn , ω(0) = 0, with the
canonical realization of the Wiener process W (τ )(ω) = ω(τ ). The (stochastic)
flow of the s.d.e. (35) is a mapping
Fτ : M × Ω → M, τ ≥ 0,

(36)

such that for each ω ∈ Ω, the mapping F. (. , ω) : [0, ∞) × M → M, is continuous
and such that {Fτ (x) : τ ≥ 0} is a solution of equation (35) with F0 (x) = x, for
any x ∈ M .
Let us assume in the following that the components Xβα , Q̂α , α, β = 1, . . . , n
of the vectorfields X and Q̂ on M in (35) are predictable functions which further
belong to Cbm, (0 ≤  ≤ 1, m a non-negative integer), the space of Hoelder
bounded continuous functions of degree m ≥ 1 and exponent , and also that
Q̂α (τ ) ∈ L1 (R), for any α = 1, . . . , n. With these regularity conditions, if we
further assume that {x(τ ) : τ ≥ 0} is a semimartingale on a probability space
12

Quantization in Astrophysics ...

340

(Ω, F, P ), then it follows from Kunita [24] that equation (35) has a modification
(which with abuse of notation we denote as)
Fτ (ω) : M → M,

Fτ (ω)(x) = Fτ (x, ω),

(37)

which is a diffeomorphism of class C m , almost surely for τ ≥ 0 and ω ∈ Ω. We
can obtain an identical result if we assume instead Sobolev regularity conditions.
Indeed, assume that the components of σ and Q̂, σiβ ∈ H s+2 (T ∗ M ) and Q̂β ∈
H s+1 (T ∗ M ), 1 ≤ i ≤ m, 1 ≤ β ≤ n, where the Sobolev space H s (T ∗ M ) =
W 2,s (T ∗ M ) with s > n2 + m (c.f. [55]). Then, the flow of equation (35) for fixed
ω defines a diffeomorphism in H s (M, M ) (see [55]), and hence by the Sobolev
embedding theorem, a diffeomorphism in C m (M, M ) (i.e. a mapping from M
to M which is m-times continuously differentiable as well as its inverse.) In any
case, for 1 ≤ m we can consider the Jacobian (velocity, or ”derived”) ) flow of
{xτ : τ ≥ 0}. It is a random diffusion process on T M , the tangent bundle of
M.
Remarks 2. In the differential geometric approach due to V. Arnold [70]
and Ebin-Marsden [9], for integrating NS on a smooth manifold as a perturbation (due to the diffusion term we shall present below) of the geodesic flow
in the group of volume preserving diffeomorphisms of M (as the solution of
the Euler equation), it was proved that under the above regularity conditions
on the initial velocity, the solution flow of NS defines a diffeomorphism in M
of class C m . The difference of the Arnold-Ebin-Marsden classical differential
geometry approach with the one presented here, is to integrate NS through a
time-dependant random diffeomorphism 2 associated with a RCW connection.
3

Let us describe now the jacobian flow. We can describe it as the stochastic
process on the tangent bundle, T M , given by {vτ := Tx0 Fτ (v0 ) ∈ TFτ (x0 ) M,
v0 ∈ Tx0 M }; here Tz M denotes the tangent space to M at z and Tx0 Fτ is the
linear derivative of Fτ at x0 . The process {vτ , τ ≥ 0} can be described (see [27])
as the solution of the invariant Ito s.d.e. on T M :
dvτ = ∇g Q̂(τ, vτ )dτ + ∇g X(vτ )dWτ

(38)

2 As we shall see, this result which follows straightforwardly from the most remarkable results due to Baxendale and Elworthy [55], makes apparent a stochastic extension of Einstein’s
Principle of General Covariance by which all the equations of Physics have to be invariant
(covariant in the language of physicists) under transformations by classical diffeomorphisms.
So underlying a gauge theory constructed in terms of RCW connections (say the equivalence
between the non-linear massive Dirac-Hestenes invariant equation for a Dirac-Hestenes spinor
operator field and the sourceless invariant Maxwell equations on a Lorentzian manifold [26,60],
for which we stress that analytical continuation in the time variable of Lorentzian manifolds
to yield Riemannian metrics is necessary), we have an associated active family of random
diffeomorphisms.
3 As wellknown, Hoelder continuity regularity conditions are basic in the usual functional
analytical treatment of NS pioneered by Leray [45] (see also Temam [7]), and they are further
related to the multifractal structure of turbulence [41]. Furthermore, this diffeomorphism
property of random flows is fundamental for the construction of their ergodic theory [72,76,77].

13

Quantization in Astrophysics ...

341

If we take U to be an open neighborhood in Rn so that T U = U × Rn , then
vτ = (xτ , ṽτ ) is described by the system given by integrating equation (35) and
the invariant Ito s.d.e.
dṽτ (xτ ) = ∇g X(xτ )(ṽτ )dWτ + ∇g Q̂(τ, xτ )(ṽτ )dτ,

(39)

with initial condition ṽ0 = v0 ∈ Tx(0) . Thus, {vτ = (xτ , ṽτ ), τ ≥ 0} defines a
random flow on T M .
Theorem 2 : For any differential 1-form φ of class C 1,2 (R × M ) (i.e. in a
local coordinate system φ = aα (τ, x)dxα , with aα (τ, .) ∈ C 2 (M ) and aα (., x) ∈
C 1 (R)) we have the Ito formula (Corollary 3E1 in [27]):
τ
∂
= φ(v0 ) +
φ(∇ X(vs )dWs +
[
+ H1 (g, Q)]φ(vs )ds
∂s
0
0
Z τ
∇g φ(X(x)dWs )(vs )
+
0
Z τ
+
trace dφ(X(xs )−, ∇g X(vs ))(−)ds
(40)

Z

φ(vτ

τ

Z

g

0

In the last term in (40) the trace is taken in the argument − of the bilinear
form and further we have the mappings
∇g Y : T M → T M ; ∇g φ : T M → T ∗ M.
Remarks 3. From (40) we conclude that the infinitesimal generators (i.g.,
for short in the following) of the derived stochastic process is not ∂τ + H1 (g, Q),
due to the last term in (40). This term vanishes identically in the case we
shall present in the following section, that of gradient diffusions. An alternative
method which bypasses the velocity process although is related to it, is the
construction of the generalized Hessian flow further below. Both methods will
provide for the setting for the implicit integration of NS and the kinematic
dynamo.

5

Riemann-Cartan-Weyl Gradient Diffusions

Suppose that there is an isometric embedding of an n-dimensional compact orientable manifold M into a Euclidean space Rm :f : M → Rm , f (x) =
(f 1 (x), . . . , f m (x)). Suppose further that X(x) : Rm → Tx M , is the orthogonal
projection of Rm onto Tx M the tangent space at x to M , considered as a subset
of Rm . Then, if e1 , . . . , em denotes the standard basis of Rm , we have
X = X i ei , with X i = grad f i , i = 1, . . . , m.

(41)

The second fundamental form [25] is a bilinear symmetric map
αx : Tx M × Tx M → νx M, x ∈ M,
14

Quantization in Astrophysics ...

342

(42)

with νx M = (Tx M )⊥ the space of normal vectors at x to M . We then have the
associated mapping
Ax : Tx M × νx M → Tx M, < Ax (u, ζ), v >Rm =< αx (u, v), ζ >Rm ,

(43)

for all ζ ∈ νx M , u, v ∈ Tx M . Let Y (x) be the orthogonal projection onto νx M
Y (x) = e − X(x)(e), x ∈ M, e ∈ Rm .

(44)

∇g X(v)(e) = Ax (v, Y (x)e), v ∈ Tx M, x ∈ M.

(45)

Then:

For any x ∈ M , if we take e1 , . . . , em to be an orthonormal base for Rm such
that e1 , . . . , em ∈ Tx M , then for any v ∈ Tx M ,we have
either ∇g X(v)ei = 0, or X(x)ei = 0.

(46)

We shall consider next the RCW gradient diffusion processes, i.e. for which
in equation (35) we have specialized taking X = gradf . Let {vτ : τ ≥ 0} be the
associated derived velocity process. We shall now give the Ito-Elworthy formula
for 1-forms.
Theorem 3. Let f : M → Rm be an isometric embedding. For any differential form φ of degree 1 in C 1,2 (R × M ), the Ito formula is
Z τ
Z τ
φ(vτ ) = φ(v0 ) +
∇g φ(X(xs )dWs )vs +
φ(Ax (vs , Y (xs )dWs )
0
0
Z τ
∂
+ H1 (g, Q)]φ(vs )ds,
(47)
+
[
0 ∂s
i.e. ∂τ + H1 (g, Q), is the i.g. (with domain the differential 1-forms belonging to
C 1,2 (R × M )) of {vτ : τ ≥ 0}.
Proof: It follows immediately from the facts that the last term in the r.h.s.
of (40) vanishes due to (46), while the second term in the r.h.s. of (40) coincides
with the third term in (47) due to (45).
Consider the value Φx of a k-form at x ∈ M as a linear map: Φx : Λk Tx M →
R. In general, if E is a vector space and A : E → E is a linear map, we have
the induced maps
Λk A : Λk E → Λk E, Λk (v 1 ∧ . . . ∧ v k ) := Av 1 ∧ . . . ∧ Av k ;
and
(dΛk )A : Λk E → Λk E,

(dΛk )A(v 1 ∧ . . . ∧ v k ) :=

k
X

v 1 ∧ . . . ∧ v j−1 ∧ Av j ∧ v j+1 ∧ . . . ∧ v k .

j=1

15

Quantization in Astrophysics ...

343

For k = 1, (dΛ)A = ΛA. The Ito formula for k-forms, 1 ≤ k ≤ n, is due to
Elworthy (Prop. 4B [27]).
Theorem 4. Let M be isometrically embedded in Rm . Let V0 ∈ Λk Tx0 M .
Set Vτ = Λk (T Fτ )(V0 ) Then for any differential form φ of degree k in C 1,2 (R ×
M ), 1 ≤ k ≤ n,
Z τ
φ(Vτ ) = φ(V0 ) +
∇g φ(X(xs )dW s)(Vs )
0
Z τ
φ((dΛ)k Axs (−, Y (xs )dWs )(Vs ))
+
0
Z τ
∂
+ Hk (g, Q̂)]φ(Vs )ds
(48)
+
[
∂s
0
i.e., ∂τ + Hk (g, Q̂) is the i.g. (with domain of definition the differential forms of
degree k in C 1,2 (R × M )) of {Vτ : τ ≥ 0}.
Remarks 4. Therefore, starting from the flow {Fτ : τ ≥ 0} of the s.d.e.
(35) with i.g. given by ∂τ + H0 (g, Q) , we obtained that the derived velocity
process {v(τ ) : τ ≥ 0} given by (38) (or (35) and (39)) has H1 (g, Q) as i.g.;
finally, if we consider the diffusion of differential forms of degree k ≥ 1, we get
that ∂τ + Hk (g, Q) is the i.g. of the process Λk v(τ ), i.e. the exterior product of
degree k (k = 1, . . . , n) of the velocity process. In particular, ∂τ + H2 (g, Q) is
the i.g. of the stochastic process {v(τ ) ∧ v(τ ) : τ ≥ 0}.
Note that consistently with the notation we have that {Λ0 vτ : τ ≥ 0} is the
position process {xτ : τ ≥ 0} untop of which {Λk vτ : τ ≥ 0}, (1 ≤ k ≤ n)
is fibered (recall, Λ0 (M ) = M ). We can resume our results in the following
theorem.
Theorem 5. Assume M is isometrically embedded in Rm . There is a one to
one correspondance between RCW connections ∇ determined by a Riemannian
metric g and trace-torsion Q with the family of gradient diffusion processes
{Λk vτ : τ ≥ 0} generated by Hk (g, Q), k = 0, . . . , n
Finally, for isometrically embedded manifolds, f : M → Rm , we are now in
a situation for presenting the solution of the Cauchy problem
∂φ
= Hk (g, Qτ )(x)φ, τ ∈ [0, T ]
∂τ
with the given initial condition
φ(0, x) = φ0 (x),

(49)

(50)

for φ and φ0 k-forms on a smooth compact orientable manifold isometrically
embedded in Rm , φ being time-dependant. From the Ito-Elworthy formula
follows that the formal solution of this problem is as follows. Consider the
diffusion process on M generated by H0 (g, Q): For each τ ∈ [0, T ] consider the
s.d.e. (with s ∈ [0, τ ]):
τ,x
τ,x
dxτ,x
s = X(xs )dWs + Q̂(τ − s, xs )ds, where X = grad f,

16

Quantization in Astrophysics ...

344

(51)

with initial condition
xτ,x
0 = x,

(52)

τ,v(x)
{vsτ,v(x) = (xτ,x
), 0 ≤ s ≤ τ },
s , ṽs

(53)

τ,v(x)
τ,v(x)
dṽsτ,v(x) = ∇g X(xτ,x
)dWs + ∇g Q̂(τ − s, xτ,x
)ds,
s )(ṽs
s )(ṽs

(54)

and the derived velocity process

satisfying further

with initial condition
τ,v(x)

ṽ0

= v(x).

(55)

Then, the C 1,2 (formal) solution of the Cauchy problem defined in [0, T ] × M is
k τ,v(x)
φ(τ, x)(Λk v(x)) = Ex [φ0 (xτ,x
],
τ )(Λ ṽτ

(56)

where Λk v(x) is a shorthand notation for the exterior product of k linearly
τ,v(x)
denotes the
independant vectors on Tx M and in the r.h.s. of (56), Λk ṽτ
exterior product of the k flows having them as initial conditions.

6

The Navier-Stokes Equations and RiemannCartan-Weyl Diffusions

In the sequel, M is a compact orientable ( without boundary) n-manifold
with a Riemannian metric g. We provide M with a 1-form u(τ, x) = uτ (x)
such that δuτ = 0 and satisfying the invariant Navier-Stokes equations (NS)
[9,53,74],
∂u
+ P [∇gûτ uτ ] − ν41 uτ = 0,
∂τ

(57)

where P is the projection operator to the co-closed term in the de RhamKodaira-Hodge decomposition of 1-forms. We have proved in [66], that we
can rewrite NS in the form of a non-linear diffusion equation4
−1
∂u
= P H1 (2νg,
uτ )uτ ,
∂τ
2ν

(58)

4 For a detailed proof see the accompanying article by the author in the representations for
NS in the smooth boundary case. While in the boundaryless case P commutes with 41 , in
the case of M with smooth boundary this is no longer true so that we have to take P 41 uτ
instead of the viscosity term in (57) (c.f. page 144, [9]) , and we are left with the non-linear
diffusion equation (58) in any case.

17

Quantization in Astrophysics ...

345

which means that NS for the velocity of an incompressible fluid is a a nonlinear diffusion process determined by a RCW connection. This connection has
2νg for the metric, and the time-dependant trace-torsion of this connection is
−u/2ν. Then, the drift of this process does not depend explicitly on ν, as it
coincides with the vectorfield associated via g to −uτ , i.e.−ûτ . Let us introduce
the vorticity two-form
Ωτ = duτ , τ ≥ 0.

(59)

Now, if we know Ωτ for any τ ≥ 0, we can obtain uτ by inverting the definition
(59). Namely, applying δ to (59) and taking in account (23) and (28), we obtain
the Poisson-de Rham equation
H1 (g, 0)uτ = −δΩτ , τ ≥ 0.

(60)

Thus, the vorticity Ωτ is a source for the velocity one-form uτ , for all τ ; in the
case that M is a compact euclidean domain, equation (60) is integrated to give
the Biot-Savart law of Fluid Mechanics [1,39]. Now, apply d to (58); we obtain
the evolution equation (c.f. [66]):
−1
∂Ωτ
= H2 (2νg,
uτ )Ωτ .
∂τ
2ν

(61)

Theorem 6. Given a compact orientable Riemannian manifold with metric
g, the Navier-Stokes equation (57) for an incompressible fluid with velocity oneform u = u(τ, x) such that δuτ = 0, assuming sufficiently regular conditions,
are equivalent to a diffusion equation for the vorticity given by (61) with uτ
satisfying the Poisson-de Rham equation (60). The RCW connection on M
generating this process is determined by the metric 2νg and a trace-torsion
1-form given by −u/2ν.
Observations This characterization of NS in terms of a gauge structure,
will determine all the random representations for NS which we shall present in
this article. 5 We would like to recall that in the gauge theory of gravitation
[46,57] the torsion is related to the translational degrees of freedom present in
the Poincaré group, i.e. to the gauging of momentum. Here we find a similar,
yet dynamical situation, in which the trace-torsion is related to the velocity.

7

Random Diffeomorphisms and the Navier-Stokes
Equations

As an immediate corollary of Theorem 6, we have (on assuming that M is
boundaryless) the following fundamental result
5 As explained in detail in [66], C. Peskin has actually derived NS on R3 from an ad-hoc
s.d.e., which actually does not depend in its noise term, which is taken as defined on the
two-sphere, for reasons of isotropicity [68].

18

Quantization in Astrophysics ...

346

Theorem 7. The lagrangian random flow associated to NS is given by
integrating the Ito s.d.e.
1

dxν,τ,x = [2ν] 2 X(xν,τ,x )dW (τ ) − û(τ, xν,τ,x )dτ, xν,0,x = x, τ ∈ [0, T ],

(62)

where we have assumed that the diffusion tensor X and the drift −ûτ (the gconjugate to uτ ) have the regularity conditions stated above, so that the random
flow of equation (62) is a diffeomorphism of M of class C m , m ≥ 1.
Remarks 5. This is obvious from our Theorem 6 and previous construc1
uτ
tions, since the projection on M of the flow with i.e. given by H2 (2νg, − 2ν
1
(τ ≥ 0) has for i.g. the operator H0 (2νg, − 2ν uτ ), and thus coincides with equation (62). Note that the effect of curvature is already incorporated in this flow,
which is in fact a diffusion of scalars; in our formalism, it is clear that at an
operator level, the effect of the (Riemannian) Ricci curvature of M is dealt by
lifting the diffusion of scalars to that of differential forms of degree higher than
1, and not by incorporating it at the level of scalar diffusions.

8

Cauchy Problem for the Vorticity in the Case
of Isometric Immersion of M

In the following we assume additional conditions on M , namely that f :
M → Rm is an isometric embedding 6
Let us solve the Cauchy problem for Ω(τ, x) of class C m in [0, T ] × M satisfying (61) with initial condition Ω0 (x). For each τ ∈ [0, T ] consider the s.d.e.
(with s ∈ [0, τ ]) (obtained by time-reversing Lagrangian representation (62)
above):
1

)dWs − û(τ − s, xν,τ,x
)ds,
dxν,τ,x
= (2ν) 2 X(xν,τ,x
s
s
s

(63)

X = ∇f,

(64)

where

is the diffusion tensor, and further, with initial condition
xν,τ,x
= x,
0
ν,τ,v(x)

and the derived velocity process {vs

(65)
ν,τ,v(x)

, ṽs
= (xν,τ,x
s

), 0 ≤ s ≤ τ }:

1

dṽsν,τ,v(x) = (2ν) 2 ∇g X(xν,τ,x
)(ṽsν,τ,v(x) )dWs − ∇g û(τ − s, xν,τ,x
)(ṽsν,τ,v(x) )ds, (66)
s
s
with initial condition
ν,τ,v(x)

ṽ0

= v(x).

6 Although

(67)

this section has been elaborated in [66], we have chosen to present briefly its
main results to allow for an immediate discussion of other results to appear below.

19

Quantization in Astrophysics ...

347

Let Ωτ (x) be a bounded C 1,2 solution of the Cauchy problem; then it follows
from the Ito formula-Elworthy (48) (with k = 2) that it is given by the expression
Ω̃τ (Λ2 v(x)) = Ex [Ω0 (xν,τ,x
)(Λ2 ṽτν,τ,v(x) )]
τ

(68)

where the expectation value at x is taken with respect to the measure on the
process {xν,τ,x
: τ ∈ [0, T ]} (whenever it exists):
τ
Remarks 6. Thus, to compute the vorticity at time τ on a bivector
Λ2 v(x) ∈ Λ2 T ∗ M (x ∈ M ), we contract the time-zero vorticity with the runned
back derived process starting at v(x) and further take expectation values for all
runned backwards paths that at time τ start at x. Note that in these representations, both for dimensions 2 and 3, we have a coupling of the deformation tensor
(and furthermore of the Riemannian covariant derivative of the noise term) to
the original vorticity along the runned backwards paths of fluid’s particles; both
these terms contribute to the complicated topology of turbulent flows. Furthermore, these representations account for the Riemannian curvature of M in spite
that the curvature does not appear explicitly in them. When considering flat
Euclidean space, R2 , where there is no longer curvature and the vorticity equation becomes an equation for a (pseudo) scalar field, this deformation due to
noise and stress of the time-zero vorticity does no longer appear; it is simply
a transport of the time-zero vorticity along the lagrangian random flow (see
section 11 below).

8.1

Integration of the Poisson-de Rham equation

In (62) we have that uτ verifies (60), for every τ ≥ 0, Consider the autonomous s.d.e. generated by H0 (g, 0) = 12 4g :
dxg,x
= X(xgs )dWs , xg,x
s
0 = x.

(69)

We shall solve the Dirichlet problem for equation (60) in an open set U (of a
partition of unity) of M with the boundary condition uτ ≡ φ on ∂U , with φ
a given 1-form. Then one can ”glue” the solutions and use the strong Markov
property to obtain a global solution (cf. [31]). Consider the derived velocity
process v g (s) = (xg (s), ṽ g (s)) on T M , with ṽ g (s) ∈ Txg (s) M , whose i.g. is
H1 (g, 0), i.e. from (35) we have:
g
g,x
dṽsg,v(x) (xg,x
(s))(ṽ g,v(x) (s))dW (s),
s ) = ∇ X(x
g,v(x)

(70)

with initial velocity ṽ
(0) = v(x). Notice that equations (69&70) are obtained by taking u ≡ 0 in equation (62) and its derived process, respectively,
1
and further rescaling by (2ν)− 2 . Then if uτ is a solution of equation (60) for
any fixed τ ∈ [0, T ], applying to it the Ito-Elworthy formula and assuming further that δΩτ is bounded, we then obtain that the formal C 1,2 solution of the
Dirichlet problem is given by (see [66]):
Z τe
1
g,v(x)
g,v(x)
ũτ (x)(v(x)) = ExB [φ(xg,x
)(v
)
+
δΩτ (xg,x
)ds]
τe
τe
s )(vs
2
0
20

Quantization in Astrophysics ...

348

Z
=

g,v(x)
[φ(xg,x
) + 1/2
τe )(vτe

Z

τe

δΩτ (y)(vsg,v(x) (y))ds]pg (s, x, y)volg (y),

(71)

0

where τe = inf{s : xg,x
∈
/ U }, the first-exit time of U of the process {xg,x
s
s },
B
and E denotes the expectation value with respect to pg (s, x, y), the transition
density of the s.d.e. (69), i.e.the fundamental solution of the heat equation on
M:
∂τ p(y) = H0 (g, 0)(y)p(y) ≡ 1/24g p(y)

(72)

with p(s, x, −) = δx as s ↓ 0.
Theorem 9. Assume that g is uniformly elliptic, U has a C 2, -boundary,
αβ
g and δΩτ are Hoelder-continuous of order  on U and uτ is uniformly Hoeldercontinuous of order , for τ ∈ [0, T ]. Then the solution of the Dirichlet problem
above has a unique solution belonging to C 2, (U ) for each τ ∈ [0, T ]. Assume
instead that uτ ∈ H 1 (T ∗ U ) for each τ ∈ [0, T ], i.e. belongs to the Sobolev space
of order 1. If δΩτ ∈ H k−1 (Λ1 (T ∗ U )), then uτ ∈ H k+1 (Λ1 (T ∗ U )), for k ≥ 1 and
τ ∈ [0, T ].
Proof. It follows from an extension of the maximum principle [31,47] to
differential forms (cf. Prop. 1.5 and comments in page 307 in [53]).
We remark as we did in Remarks 6, that these representations (68−71) have
a built-in treatment of the Riemannian curvature. This dependance might be
exhibited through the scalar curvature term in the Onsager-Machlup lagrangian
(see [35,44]) appearing in the path-integral representation of the fundamental
solution of the transition densities of the representations for the vorticity and for
the velocity. There is further a dependance of the solution on the global geometry and topology of M appearing through the Riemannian spectral invariants
of M in the short-time asymptotics of these transition densities [28,29,43,82].

9

Kinematic Dynamo Problem of Magnetohydrodynamics

The kinematic dynamo equation for a passive magnetic field transported
by an incompressible fluid, is the system of equations (c.f.[56]) for the timedependant magnetic vectorfield B(τ, x) = Bτ (x) on M defined by iBτ µ(x) =
ωτ (x) (for τ ≥ 0), satisfying
∂τ ω + (Lûτ − ν m 4n−1 )ωt = 0, ω(0, x) = ω(x), 0 ≤ t,

(73)
1

where ν m is the magnetic diffusivity, and we recall that µ = vol(g) = det(g) 2 dx1 ∧
... ∧ dxn is the Riemannian volume density ((x1 , . . . , xn ) a local coordinate system on M ), and ω ∈ Λn−1 (R × T ∗ M ). In equation (73) u is assumed given,
and it may either be a solution of NS, or of the Euler equation given by setting

21

Quantization in Astrophysics ...

349

ν = 0 in (57). From the definition follows that divBτ ≡ 0, for any τ ≥ 0. Now
we note that from (28) we can rewrite this problem as
∂τ ω = Hn−1 (2ν m g, −

1
uτ )ωτ , ω(0, x) = ω(x), 0 ≤ τ,
2ν m

(74)

as a linear evolution equation for a (n − 1)-form, similar to the evolution
Navier-Stokes equation for the vorticity. Now if we assume that there is an
isometric embedding f : M → Rm , so that the diffusion tensor X = ∇f , we
can take the Lagrangian representation for the scalar diffusion generated by
H0 (2ν m g, − 2ν1m uτ ), i.e. the Ito s.d.e. given by substituting ν m instead of ν
in the random lagrangian equations (63) and its derived process (65), then the
formal C 1,2 solution of (73) defined on [0, T ] × M for some T > 0, is given by
ω̃τ (Λn−1 v(x)) = Ex [ω0 (xντ

m

,τ,x

)(Λn−1 ṽτν

m

,τ,v(x)

)]

(75)

Remarks 6. We note that similarly to the representation for the vorticity, instead of the initial vorticity now it is the initial magnetic field which is
transported backwards along the scalar magnetic diffusion, and along its way it
is deformed by the fluid‘s deformation tensor and the gradient of the diffusion
tensor noise term (this accurately represents the actual macroscopical physical phenomena), and finally we take the average for all those paths starting
at x. For both the vorticity and the kinematic dynamo equations as well as
the Poisson-de Rham equation, we have a mesoscopic description which clearly
evokes the Feynman approach to Quantum Mechanics through a summation
of the classical action of the mechanical system along non-differentiable paths.
In distinction with the usual Feynman approach, these Brownian integrals are
well defined and they additionally have a clear physical interpretation which
coincides with actual experience.

10

Random Implicit Integration Of The NavierStokes Equations For Compact Manifolds

Up to this point, all our constructions have stemmed from the fact that for
gradient diffusion processes, the Ito-Elworthy formula shows that the random
process on Λ2 T M given by {Λ2 vτ : τ ≥ 0} with {vτ : τ ≥ 0} the jacobian
process fibered on the diffusion process {Λ0 vτ ≡ xτ : τ ≥ 0} on M given by
equation (62) , is a random Lagrangian flow for the Navier-Stokes equation. Our
previous constructions have depended on the form of the isometric embedding
of M . This construction is very general; indeed from a well known theorem
due to J. Nash (1951) further elaborated by de Georgi and Moser, such an
embedding exists of class C ∞ for any smooth manifold (cf. [53]). (Furthermore,
our assumption of compactness is for the obtention of a random flow which is
defined for all times, and gives a global diffeomorphism of M . The removal of
22

Quantization in Astrophysics ...

350

this condition, requires to consider the random flow up to its explosion time, so
that in this case we have a local diffeomorphism of M .)
There is an alternative construction of diffusions of differential forms which
does not depend on the embedding of M in Euclidean space, being thus the
objective of the following section its presentation. A fortiori, we shall apply
these constructions to integrate NS and the kinematic dynamo problem.

10.1

The Generalized Hessian Flow

In the following M is a complete compact orientable smooth manifold
without boundary. We shall construct another flow in distinction of the derived
flow of the previous sections, which depends explicitly of the curvature of the
manifold, and also of the drift of the diffusion of scalars. We start by considering
an autonomous drift vector field Q̂ (further below we shall lift this condition)
and we define a flow Wτk,Q̂ on Λk T M (1 ≤ k ≤ n) over the flow of equation (35),
{Fτ (x0 ) : τ ≥ 0}, by the invariant equation with initial condition V0 ∈ Λk T M :
Dg Wτk,Q̂
(V0 ) = −1/2Rk (Wτk,Q̂ (V0 )) + (dΛk )(∇g Q̂(.))(Wτk,Q (V0 )),
∂τ

(76)

g

where D
∂τ denotes the Riemannian covariant derivative along the paths defined
by equation (35). These processes are the generalized Hessian processes.
Proposition 1 (Elworthy [27]). Assume that 21 Rk − (dΛ)k (∇g Q̂)(.)) is
bounded below. Define Pτk : L∞ Λk T ∗ M → L∞ Λk T ∗ M by
Pτk (φ)(V ) = E(φ(Wτk,Q (V ))

(77)

for V ∈ Λk Tx M, φ ∈ L∞ Λk T ∗ M . Then {Pτk : τ ≥ 0} is a contraction semigroup
of bounded continuous forms and is strongly continuous there with i.g. agreeing
with Hk (g, Q) on C 2 (M ).
Under the above conditions we can integrate the heat equation for bounded
twice differentiable k-forms of class C 2 (0 ≤ k ≤ n) and in the general case of a
non-autonomous drift vector field Q̂ = Q̂τ (x). Indeed, for every τ ≥ 0 consider
k,Q̂
over the flow of {xτs : 0 ≤ s ≤ τ }, given by the equation
the flow Vsτ = Wτ,s
x,τ
x,τ
dxx,τ
= X(xx,τ
= x,
s
s )dWs + Q̂τ −s (xs )ds, x0

obtained by integration of the equation
Dg Vsτ
(v0 ) = −1/2Rk (Vsτ (v0 )) + (dΛk )(∇g Q̂τ −s (.))(Vsτ (v0 )),
∂s

(78)

with v0 = V0τ ≡ v(x) ∈ Λk Tx M . Then, applying the Ito-Elworthy formula we
prove as before that if α̃τ is a bounded C 1,2 solution of the Cauchy problem for
the heat equation for k forms:
∂
ατ = Hk (g, Qτ )ατ
∂τ
23

Quantization in Astrophysics ...

351

(79)

with initial condition α0 (x) = α(x) a given k-form of class C 2 , then the solution
of the heat equation is
ατ (v(x)) = Ex [α0 (Vττ (v(x))],

(80)

with Vττ (x) the generalized Hessian flow over the flow {Fτ (x) : τ ≥ 0} of {xττ :
τ ≥ 0} with the initial condition v(x) ∈ Λk Tx M .
To integrate the Poisson-de Rham equation we shall need to consider the socalled Ricci-flow WτR ≡ Wτ1,0 (ω) : T M → T M over the random flow generated
by H0 (g, 0), obtained by integrating the covariant equation (so we fix the drift
to zero and further take k = 1 in (78))
1
Dg WτR
(v0 ) = − R̃ic(WτR (v0 ), −), v0 ∈ Tx0 M
∂τ
2

(81)

where Ric : T M ⊕ T M → R is the Ricci curvature and R̃ic(v, −) ∈ Tx M is the
conjugate vector field defined by < R̃ic(v, −), w >= Ric(v, w), w ∈ Tx M .

10.2

Integration of the Cauchy problem for the Vorticity
on Compact Manifolds

Theorem 10: The integration of the equation (61) with initial condition
Ω(0, ) = Ω0 yields
Ωτ (v(x)) = Ex [Ω0 (Vττ (v(x))]

(82)

where {Vττ : τ ≥ 0} is the solution flow over the flow of {xν,τ,x
: τ ≥ 0} (see
τ
equation (63)) of the covariant equation
2,−û0
Dg Wτ,s
(v(x))
∂s

k,−û0
(v(x)))
= −νR2 (Wτ,s
2,−û0
(v(x))), s ∈ [0, τ ]
− (dΛ2 )(∇g û0 (.))(Wτ,s

(83)

with the initial condition v(x) ∈ Λ2 Tx M [58]. In this expression, ∇g û0 (.) is a
linear transformation, A, between Tx∗ M and Tx M , and
dΛ2 (A) : Tx M ∧ Tx M → Tx M ∧ Tx M
is given by
dΛ2 (A)(v1 ∧ v2 ) = Av1 ∧ v2 + v1 ∧ Av2 ,
for any v1 , v2 ∈ Tx M , x ∈ M .

24

Quantization in Astrophysics ...

352

(84)

10.3

Integration of the Kinematic Dynamo for Compact
Manifolds

Substituting the magnetic diffusivity ν m instead of the kinematic viscosity
in (63) and we further consider {Vsτ : s ∈ [0, τ ]} given by the solution flow over
m
the flow of {xνs ,τ,x : s ∈ [0, τ ]} of the invariant equation
Dg Vsτ
(v(x)) = −ν m Rn−1 (Vsτ (v(x))) − (dΛn−1 )(∇g ûτ −s (.))(Vsτ (v(x))), (85)
∂s
with v(x) = V0τ ∈ Λn−1 Tx M . Then, the formal C 1,2 solution of (73) is
ωτ (v(x)) = Ex [ω0 (Vττ (v(x))].
with Ex denoting the expectation valued with respect to the measure on {xντ
(whenever it exists).

10.4

(86)
m

,τ,x

}

Integration of the Poisson-de Rham Equation for the
Velocity

With the same notations as in the case of isometrically embedded manifolds, we
have a martingale problem with a bounded solution given by
Z τe
uτ (v(x)) = ExB [φ(WτRe (v(x))) + 1/2
δΩτ (WsR (v(x)))ds],
(87)
0

where v(x) ∈ Tx M is the initial condition for WτR .

11

Solutions of NS on euclidean space

In the case that M is euclidean space, the solution of NS is easily obtained
from the solution in the general case [58,66]. In this case the isometric embedding f of M is realized by the identity mapping, i.e. f (x) = x, ∀x ∈ M .
Hence the diffusion tensor X = I, so that the metric g is also the identity. For
this case we shall assume that the velocity vanishes at infinity, i.e. ut → 0 as
|x| → ∞. (This allows us to carry out the application of the general solution,
in spite of the non-compacity of space). Furthermore, τe = ∞. The solution for
the vorticity equation results as follows. We have the s.d.e. (see equation (63)
where we omit the superscript for the kinematical viscosity, for simplicity)
1

τ,x
τ,x
2
dxτ,x
s = −u(τ − s, xs )ds + (2ν) dWs , x0 = x, s ∈ [0, τ ].

(88)

The derived process is given by the solution of the o.d.e. (since in equation (66)
we have ∇X ≡ 0)
τ,v(x)

τ,x,v(x)
dṽsτ,x,v(x) = −∇u(τ − s, xτ,x
)ds, v0
s )(ṽs

25

Quantization in Astrophysics ...

353

= v(x) ∈ Rn , s ∈ [0, τ ], (89)

Now for n = 3 we have that the vorticity Ω(τ, x) is a 2-form on R3 , or still
by duality has an adjoint 1-form, or still a R3 -valued function, which with abuse
of notation we still write as Ω̃(τ, .) : R3 → R3 , which from (68) we can write as
Ω̃(τ, x) = Ex [ṽττ,x,I Ω0 (xτ,x
τ )],

(90)

where Ex denotes the expectation value with respect to the measure (if it exists)
on {xτ,x
: τ ≥ 0}, for all x ∈ R3 , and in the r.h.s. of expression (90) we have
τ
matrix multiplication Thus, in this case, we have that the deformation tensor
acts on the initial vorticity along the random paths, and there is no action of
the gradient noise term; a fortiori, this produces that the random lagrangian
flow preserves the Lebesgue measure on R3 , as it can be easily verified. This
action is the one that for 3D might produce the singularity of the solution.
In the case of R2 , the vorticity can be thought as a pseudoscalar, since
Ωτ (x) = Ω̃τ (x)dx1 ∧ dx2 , with Ω̃τ : R2 → R, and being the curvature identically
equal to zero, the vorticity equation is equivalent to a scalar diffusion equation:
−1
∂ Ω̃τ
= H0 (2νI,
uτ )Ω̃τ
∂τ
2ν

(91)

so that for Ω̃0 = Ω̃ given, the solution of the initial value problem is
Ω̃(τ, x) = Ex [Ω̃(xτ,x
τ )]

(92)

This solution is qualitatively different from the previous case. Due to a geometrical duality argument, for 2D we have factored out completely the derived
process in which the action of the deformation tensor on the initial vorticity is
present.
Furthermore, the solution of equation (70) is (recall that X = I)
xg,x
= x + Wτ ,
τ

(93)

and since ∇X = 0, the derived process (see equation (71)) is constant
vτg,x,v(x) = v(x), ∀τ ∈ [0, T ].

(94)

so that its influence on the velocity of the fluid can be factored out in the
representation (71). Indeed, we have (see equation [66])
Z ∞
1
Ωτ (x + Ws )ds].
(95)
ũτ (x) = δExB [
2
0
In this expression we know from equation (72) that the expectation value is taken
2
−n
with respect to the standard Gaussian function, pg (s, x, y) = (4πs) 2 exp(− |x−y|
4s ).
Let us describe in further detail this solution separately for each dimension; for
the details see [66]. For n = 2 we have
Z τe
1 B
uτ (x) = −
E [Ω̃τ (x + Ws )Ws⊥ ]ds
(96)
2s x
0
26

Quantization in Astrophysics ...

354

where Ws⊥ = (Ws1 , Ws2 )⊥ = (Ws2 , −Ws1 ). Instead, for n = 3 we have,
Z τe
1 B
uτ (x) = −
Ex [Ω̃τ (x + Ws ) × Ws ]ds
2s
0

(97)

where × denotes the vector product and W = (W 1 , W 2 , W 3 ) ∈ R3 . These representations were obtained as well in a rather involved non-invariant analytical
approach by Busnello [54].

11.1

Integration of the Kinematic Dynamo Problem in
Euclidean space

With the notations in this section, the kinematic dynamo problem in 3D can be
solved as follows. As for the vorticity, the magnetic field is for n = 3 is a 2-form
on R3 , or still by duality has an adjoint 1-form (so the argument turns to work
out as well for 2D), or still a R3 -valued function, which with abuse of notation
we still write as ω̃(τ, .) : R3 → R3 , which from equation (92) we can write as
ω̃(τ, x) = Ex [ṽττ,x,I ω0 (xτ,x
τ )],

(98)

where Ex denotes the expectation value with respect to the measure (if it exists)
on {xτ,x
: τ ≥ 0}, for all x ∈ R3 , and in the r.h.s. of expression (98) we have
τ
matrix multiplication Thus, in this case, we have that the deformation tensor
acts on the initial magnetic field along the random paths of the magnetized fluid
particles. This action is the one that for 3D produces the complicated topology
of transported magnetic fields. This solution was obtained independently by
Molchanov et al [50] and further applied in numerical simulations (see Ghill and
Childress [51] and references therein). The important problems of the dynamo
effect and of intermittency in magnetohydrodynamics, has been discussed in
terms of random lagrangian flows and their Lyapunov stability, by Baxendale
and Rozovskii [72].

12
12.1

The Navier-Stokes Equation is Purely Noise
For Any Dimension Other than 1
Motivations

We have given up to now a derivation of diffusion processes starting from
gauge theoretical structures, and applied this to give implicit representations
for the invariant Navier-Stokes equations. These constructions were possible as
they stemmed from the extremely tight relation existing between the metriccompatible Riemann-Cartan-Weyl connections, and the diffusion processes for
differential forms, built untop of the diffusions for scalar fields. As we saw already this stemmed from the fact that there is a one-to-one correspondance
27

Quantization in Astrophysics ...

355

between said RCW connections and the scalar diffusion processes {xτ : τ ≥ 0}
with drift given by Q̂ and diffusion tensor X. As we can easily check from
expression (34), this construction is valid for n 6= 1. This could lead to conjecture that in a gauge theoretical setting and further applying stochastic analysis,
one could do away with the drift, in any dimension other than 1. This is the
case , as proved in a more general context of diffusions on a vector sub-bundle
of the tangent space, by Elworthy-LeJan-Li [71]7 Being this the case, then we
can apply this construction to the Navier- Stokes equation, which thus in any
dimension other than 1 would turn to be representable by random lagrangian
paths which do not depend explicitly on the velocity of the fluid, since they
would be purely noise processes.
Retaking the chain of structures previously described, we start by presenting
an alternative representation for the RCW connections, to further describe the
representation of the laplacians on scalars and its extension to differential forms,
and the driftless random lagrangian flows.

12.2

The push-forward LeJan-Watanabe connection

Consider the surjection map K : M × Rm → T M , linear in the second
variable, which we assume that it has a right inverse Y : T M → M × Rm . Here,
Y is the adjoint of K with respect to the Riemannian metric on T M induced by
K, Y = K ∗ . Write K(x) = K(x, .) : Rm → T M . For u ∈ T M , let Z u ∈ Γ(T M )
(the space of sections of the tangent bundle) defined by
Z u (x) = K(x)Y (π(u))u.

(99)

˜ on M such that for
Proposition 3. There is a unique linear connection ∇
all u0 ∈ Tx M, x ∈ M , we have that
˜ v Z u0 = 0.
∇
0

(100)

It is the pushforward connection defined as
˜ v Z := K(x0 )d(Y (.)Z(.))(v0 ), v0 ∈ Tx M, Z ∈ Γ(T M ),
∇
0
0

(101)

7 It is interesting to remark that in the monograph [71], the same chain of ideas developed
here were persued to give a general mathematical elaboration. As the reader might have
noticed, the sequence is: linear connections with torsion (albeit skewsymmetric torsion in
[71]) , laplacians on scalars defined from these linear connections as generators of diffusion
processes for scalar fields, the extension of these laplacians to generate diffusions of differential
forms, and in Elworthy-LeJan-Li goes further to study the decomposition of noise and, finally
the stability profile. (This last slab of this chain was applied, as stated above, to characterize
the dynamo effect and intermittency in MHD [72].) While the line of research presented
in this article was elaborated independently [10,59], having the Ito-Elworthy formula as the
connecting thread just as in [71], the coincidence underlines the naturality of this chain of ideas.
We would like to remark that the skew-symmetric torsion considered in [71] and references
therein, in a context of studies in gravitation, is related to spin [57, 69].

28

Quantization in Astrophysics ...

356

where d is the usual derivative of the map Y (.)Z(.) : M → Rm .
ˆ be any linear
Proof. The above definition defines a connection. Let ∇
connection on T M . We have
Z(.) = K(.)Y (.)Z(.).

(102)

Then, for v ∈ Tx0 M ,
ˆ v Z = K(x0 )d(Y (.)Z(.))(v) + ∇
ˆ v [K(.)(Y (x0 )Z(x0 ))] = ∇
˜ vZ + ∇
ˆ v Z Z(x0 ) .
∇
(103)
ˆ is a connection by assumption, and since the map
Since ∇
ˆ vZ
T M × T M → T M, (v, u) 7−→ ∇

(104)

ˆ is a smooth
gives a smooth section of the bundle Bil(T M × T M ; T M ), then ∇
ˆ
˜
connection on M . Taking ∇ = ∇ we obtain a connection with the desired
property.
Theorem 11. Let Y be the adjoint of K with respect to the induced metric
˜ is metric compatible, where the metric is the one induced
on T M by K. Then, ∇
by X on T M , which we denote by g̃. Moreover, since M is finite-dimensional,
any metric-compatible connection for any metric on T M can be obtained this
way from such K and Rm .
Proof: We have
2g̃(∇v U, U ) = 2g̃(K(x0 )(d(Y (.)U (.))(v), U (x0 ))
= 2g̃(d(Y (.)U (.))(v), Y (x0 )U (x0 )) = d(g̃(U, U ))(v),
(105)
˜
so that ∇ is indeed metric compatible. By the Narasimhan-Ramanan theorem
on universal connections [87], any metric compatible connection arises likes this.
˜ is the pull-back of the universal connection over the Grassmanian
Indeed, ∇
G(m, n) of n-planes in Rm by the map x 7→ [ImageY (x) : Tx M → Rm ]. In
particular, the RCW connections arise from such a construction. c.q.d.
Remarks 7. Thus, any metric compatible connection, and in particular any
RCW connection (recall that in Section 2 we imposed the condition of metric
compatibility from the very beginning, or still, the last term in expression (34)
ensures the metric compatibility of the RCW connections) can be introduced as
a push-forward (also called LeJan-Watanabe) connection defined in terms of a
defining map K for the connection.
Two connections, ∇a and ∇b on T M give rise to a bilinear map
ab
D : T M × T M → T M such that
∇aV U = ∇bV U + Dab (V, U ), U, V ∈ Γ(T M ).

(106)

Choose ∇b = ∇g , the Levi-Civita connection of a certain Riemannian metric g.
˜ of above:
Consider instead of expression (106) for ∇a = ∇
˜ v U = ∇g U + D̃(V, U ),
∇
V
29

Quantization in Astrophysics ...

357

(107)

where we decompose D̃ into
D̃(u, v) = A(u, v) + S(u, v),

(108)

where
A(u, v) = −A(v, u), S(u, v) = S(v, u).
a

b

(109)
a

b

Since the torsion tensors T and T of any two connections ∇ and ∇ respectively, are connected through the expression
T a (u, v) + T b (u, v) = Dab (u, v) − Dab (v, u).

(110)

which for the case of ∇b = ∇g as T b = 0, we can write for Dab = D̃, the identity
T̃ (u, v) = D̃(u, v) − D̃(v, u) + 2A(u, v), T̃ ≡ T a .

(111)

Thus,
1
T̃ (u, v), u, v, ∈ Γ(T M ), T̃ ≡ T a .
2
Therefore, the decomposition in (107) is written in the form
A(u, v) =

˜ v U = ∇g U + 1 T̃ (u, v) + S(u, v), u, v ∈ Γ(T M ).
∇
v
2

(112)

(113)

which for a metric compatible connection is nothing else than the original decomposition of the Cartan connection given in (10&11).
˜ is metric-compatible if and only if the map
Lemma 1. A connection ∇
D̃(v, .) : T M → T M is skew-symmetric for each v ∈ T M , i.e.
g(D̃(v, u1 ), u2 ) + g(D̃(v, u2 ), u1 ) = 0, u1 , u2 ∈ Γ(T M ).

(114)

Equivalently,
g(S(u1 , u2 ), v) =

1
1
g(T̃ (v, u1 ), u2 ) + g(T̃ (v, u2 ), u2 ).
2
2

(115)

Consequently, for U1 , U2 ,V in Γ(T M ), we have
1
1
1
g(T̃ (V, U1 ), U2 ) + g(T̃ (U2 , V ), U1 ) + g(T̃ (U2 , U1 ), V ),
2
2
2
(116)
which is decomposition (10).
Proof. Take V, U1 , U2 ∈ Γ(T M ). Then,
g(D̃(V, U1 ), U2 ) =

d(g(U1 , U2 ))(V ) = g(∇gV U1 , U2 ) + g(U1 , ∇gV U2 )
= g(∇gV U1 , U2 ) + g(U1 , ∇gV U2 ) − g(D̃(V, U1 ), U2 ) − g(U1 , D̃(V, U2 ).

30

Quantization in Astrophysics ...

358

(117)

˜ is metric compatible if and only if
So, ∇
g(D̃(V, U1 ), U2 ) + g(U1 , D̃(V, U2 )) = 0.

(118)

Now, writing D̃ = A + S we get
g(A(V, U1 ), U2 ) + g(A(V, U2 ), U1 ) = −g(S(V, U1 ), U2 ) − g(S(V, U2 ), U1 ). (119)
We now observe that for an alternating bilinear map L : T M × T M → T M ,
Cyl[g(L(v, u1 ), u2 ) + g(L(v, u2 ), u1 )] = 0,

(120)

where Cyl denotes cyclic sum. Taking the cyclic sum in equation (119) and
apply (120) to A, we thus obtain Cyl[g(S(V, U1 ), U2 )] = 0 which on further
substituting in (119) we obtain
g(A(V, U1 ), U2 ) + g(A(V, U2 ), U1 ) = g(S(U1 , U2 ), V ).

12.3

(121)

The Trace-Torsion Is Dynamically Redundant in Any
Dimension Other Than 1

Let us return to our original setting of Section 1. We assume a metric-compatible
˜ with torsion tensor T̃ . The followCartan connection, which we now write as ∇
ing result is a reduction of a more general result in [71], valid for sub-bundles
of T M .
Theorem 12. Assume M has dimension bigger than 1. Consider the lapla˜
cian on 0-forms H0 (g, Q) where Q is the trace-torsion 1-form of ∇,
Q(u) = trace g(T̃ (−, u), −).

(122)

Assume further that we can write the laplacian H0 (g, Q) in the Hormander
form:
m
1X
LV LV + LZ |Λp T ∗ M
(123)
2 i=1 i i
where Z is a vectorfield on M , V : M × Rm → T M is a smooth surjection,
linear in the second variable, and Vi is defined by
V (x, e) = V (x)e =

m
X

V i (x) < e, ei >,

(124)

i=1

whith e1 , . . . , em the standard orthonormal basis for Rm . (Since ∇g is metric
compatible, from Theorem 11 and the transformation rules between Stratonovich
and Ito calculi, we can always introduce a defining map V for ∇g that gives such
decomposition (c.f. [27])). Then, there exists a map K : M × Rm → T M linear
on the second variable, such that the solution to the Stratonovich equation
dxτ = K(xτ ) ◦ dWτ ,
31

Quantization in Astrophysics ...

359

(125)

has H0 (g, Q) for infinitesimal generator, i.e.
m

H0 (g, Q) =

1X
LKi LKi |C ∞ (M ) ,
2 i=1

(126)

where C ∞ (M ) denotes the real-valued smooth functions defined on M . In other
words, the Ito s.d.e. given by (35) admits a driftless representation given by
equation (125).
Proof. Set for the original drift vectorfield Q̂ (the g-conjugate of Q), the
decomposition
m
1X g i
∇ i V − Z,
(127)
Q̂ =
2 i=1 V
˜ suitable for this is such that
A connection ∇
2A(u, v) = T̃ (v, u) =

2
(u ∧ v)Q(x).
n−1

(128)

Consider a bundle map K : M × Rm → T M which gives rise to the metric
˜ (theorem 11). Consider the s.d.e.
compatible connection ∇
dxτ = K(xτ ) ◦ dWτ .

(129)

Its generator is (c.f. [27])
m

1X g i i
1
∇ K (K )
trace(∇g )2 +
2
2 i=1

(130)

while by assumption we have
m

H0 (g, Q) =

1
1
1X g i
trace(∇g )2 + (
∇ i V − Z) = trace(∇g )2 + Q̂
2
2 i=1 V
2

(131)

The required result follows after we show
m
X

∇g K i (K i ) = −

i=1

m
X

D̃(K i , K i ) = traceD̃(−, −)

(132)

i=1

equals 2Q̂. For this we note that for all v ∈ T M ,
m
m
m
X
X
X
g(
D̃(K i , K i ), v)) = g(
S(K i , K i ), v)) = −
g(T̃ (v, K i ), K i ) = 2g(Q̂, v)
i=1

i=1

i=1

(133)
Consequently
m

trace(∇g )2 +

1X g i
∇ (K )(K i ) = trace(∇g )2 + Q̂ = H0 (g, Q)
2 i=1
32

Quantization in Astrophysics ...

360

(134)

and the K so constructed is the required map.c.q.d.
If we further assume that there is an isometric embedding f : M → Rm ,
then we have the remarkable fact that the driftless representation of the scalar
laplacian is also valid for forms of arbitrary degree (c.f. section 2.4 in [71].), i.e.
our original operators in (28) can be written as
m

Hp (g, Q) =

1X
LKi LKi |Λp T ∗ M , ∀p ∈ {0, . . . , n},
2 i=1

where K is a defining map for the RCW connection determined by g and Q.
Remarks 8. Of course in the above construction, it is unnecessary to start
with an arbitrary metric-compatible Cartan connection, only matters the tracetorsion as already proved.

12.4

Navier-Stokes Equations Is Purely Geometrical Noise
In Any Dimension Other Than 1

We recall that for any dimension other than 1, the Navier-Stokes equation is
a diffusion process which arises from a RCW connection of the form (34) with
metric given by 2νg, where g is the original metric defined on T M , and torsion restricted to the trace-torsion given by −1
2ν uτ , where τ ≥ 0: we shall call
this connection the Navier-Stokes connection with parameter ν, which we shall
denote as ∇N S;ν . Thus the Christoffel coefficients of this connection are (c.f.
expression (34))


 
−1 α
α
2
1
α
α
+
δ u(τ )γ +
gβγ u(τ )
(135)
Γβγ = 2ν
βγ
(n − 1) 2ν β
2ν
Let us then consider a defining map K(τ ) : M × Rm → T M , for τ ≥ 0 for
such a connection; from theorem 11 we know it exists. Then, we can write
m

H0 (2νg, −

1
1X
LK(τ )i LK(τ )i |C ∞ (M ) .
uτ ) =
2ν
2 i=1

(136)

In particular, for isometrically embedding of M , it follows from our discussion
above that we can rewrite the spatial term of the Navier-Stokes operator for the
vorticity as
m

H2 (2νg, −

1
1X
LK(τ )i LK(τ )i |Λ2 T ∗ M ,
uτ ) =
2ν
2 i=1

and NS (61) takes the form of a purely geometrical- noise equation for the
vorticity
m

1X
∂Ω
=
LK(τ )i LK(τ )i Ωτ , τ ≥ 0.
∂τ
2 i=1
33

Quantization in Astrophysics ...

361

Therefore, from Theorem 12 we conclude that:
Theorem 13. For smooth compact orientable n-manifolds without boundary, if we consider an isometric immersion f : M → Rm , with n 6= 1, the
random lagrangian representation given by (61) admits a representations as a
Stratonovich s.d.e. without drift −ûτ term
dxτ = Kτ (x(τ )) ◦ dWτ .

(137)

Remarks 9. Thus, we have gauged away in the velocity in the dynamical
representation for the fluid particles. Of course, the new diffusion tensor K(τ )
(τ ≥ 0) depends implicitly on the velocity of the fluid, on the kinematical
viscosity and the metric g. Indeed, K(τ ) (τ ≥ 0) can be computed from the
knowledge of the Navier-Stokes connection with parameter ν, by solving the
equation
S;ν
∇N
Z = Kτ (x)d(Kτ∗ (.)Z(.))(v), v ∈ Tx M, Z ∈ Γ(T M ).
v

(138)

We can now return to give the random representations for NS for the vorticity, in terms of this driftless representation.
Theorem 14. Let f : M → Rm be an isometric embedding of M (provided
1
1
with a Riemannian metric g) in Euclidean space, so that (2ν) 2 X = (2ν) 2 grad f
is the diffusion tensor of a Riemann-Cartan- Weyl gradient diffusion generated
by
m

H0 (2νg, −

1
1X
uτ ) ≡
LK(τ )i LK(τ )i .
2ν
2 i=1

(139)

Consider
dxν,τ,x
= Kτ −s (xν,τ,x
) ◦ dWs , xν,τ,x
= x,
s
s
0

(140)

and the derived process
ν,τ,x,v(x)

dvsν,τ,x,v(x) = ∇g Kτ −s (xν,τ,x
)(vsν,τ,x,v(x) ) ◦ dWs , v0
s

= v(x) ∈ Tx M.
(141)

Then, NS for the vorticity admits the representation
Ωτ (x)(Λ2 v(x)) = Ex [Ω0 (xν,τ,x
)(Λ2 vτν,τ,x,v(x) ],
τ

(142)

for given initial vorticity Ω(0, x) = Ω0 (x), x ∈ M .
Remark 10. This representation is none other than expression (68) rewritten in terms of the driftless representation for the random Lagrangian fluid flow
and its Jacobian process given in (63 − 67), so there is actually no abuse of
notation.
Remarks 11. While the fusion of stochastic calculus with gauge theoretical structures has set the method to derive in a rather simple way all the

34

Quantization in Astrophysics ...

362

representations for NS introduced in this article, Theorems 13&14 are most remarkable for reasons we would like to discuss in the following. It is well known
that the existance of infinite-time and smooth solutions of NS is related to the
fact that the energy dissipation (represented at the dynamical level by the term
1
(2ν) 2 X(x(τ ))dW (τ ) appearing in the lagrangian particle paths (equation (62)),
and further, by its covariant derivative coupling to the original vorticity along
these paths in the representation for the vorticity) (see the first term in equation
(66)) competes with the non-linearity of the equation. The physical effect of
the nonlinearity is precisely the creation of energy due to the viscosity, which
is described by the drift −ûτ at the fluid particle paths level, which further
in the representations obtained in equation (68), appears as a coupling of the
original vorticity with the fluid deformation tensor transported along the fluid
particle paths which is further averaged over all the paths (see the second term
in equation (66)). The physical description of this is that would the diffusion of
energy prevail on the non-linear creation of energy, the infinite-time existance of
the solutions would be ensured, and this would still entail the regularity of the
solutions. In the other hand we have seen that the solutions in flat euclidean
space in 2D differs from the one in 3D (both for which the gradient noise term
vanishes and then the random lagrangian representation for NS yields a volume
preserving diffeomorphism), in that in the former the coupling of the vorticity
to the deformation tensor can be cancelled out while for the latter, this coupling
appears clearly in the solution. Thus these representations run parallel to the established knowledge in which the existance of infinite-time and smooth solutions
of NS for 2D are known, while for the 3D case remains open [61]. Returning to
Theorems 13&14, it was proved that for NS the velocity can be incorporated into
the diffusion tensor in this special representation and thus the coupling of the
deformation tensor to the original vorticity disappears. Thus, for n-manifolds
(n 6= 1) only the gradient of the generalized noise term (which incorporates the
velocity, naturally in a non-linear way) couples to the original vorticity, and NS
turns out to be a purely noise process. Thus, theorems 13 and 14 show that in
any dimension other than 1, in spite of the complexity that appears in 3D in
contrast with 2D, their actual behavior in terms of their infinite-time existance
and regularity of their solutions from this geometric-stochastic approach may
not be regarded as radically different cases 8 .
Theorems 13&14 cannot be regarded as actual proofs of the infinite-time life
of the solution flows for dimension 3, they rather point out to an original method
that deserves further investigation regarding this ellusive and difficult problem
8 Perhaps

this result should not be surprising, since after all for n-manifolds (in contrast
with flat euclidean space) there is a coupling of the original vorticity to the deformation
tensor, both for n = 2 and 3, which is due to the curvature term built-in in the laplacian,
and thus this common coupling originates in the fact that in this case, the Navier-Stokes
operator incorporates the Ricci scalar curvature. Of course, curvature is a source for inertia
and in fluid-dynamics, inertia is believed to be the source for the non-linearity of NS (c.f. the
discussion in page 23, [81].

35

Quantization in Astrophysics ...

363

(which has only be dealt with through mainly analytical considerations), and in
principle is valid, 9 independently of the dimension of the manifold (other than
1).
Remarks 12. Similarly to theorems 13&14, and further replacing the magnetic diffusivity instead of the kinematical viscosity, we can represent the lagrangian random paths for the kinematic dynamo problem as a purely diffusive
process, and integrate the kinematic dynamo in terms of the jacobian of this
process. Similarly to NS for the vorticity, it appears that the coherent complex structures associated to the coupling of the fluid deformation tensor to the
original magnetic field, can be accounted by a representation in which this field
couples to a generalized diffusion tensor which depends on the fluid’s velocity.
Of course, these results are valid for arbitrary passive fields transported by the
either perfect or viscous fluid.

13

Realization of the RCW Diffusions by ODE’s

To realize the s.d.e’s by o.d.e’s it is mandatory to pass to the Stratonovich
pre-prescription, which are well known to have the same transformation rules
in stochastic analysis that those of classical flows [1,2]. The need for such
approximations is obvious whenever the noise tensor is not trivial, and thus
the random integration may be extremely difficult; in the trivial noise case it
becomes superfluous, as we shall see when dealing with the Euclidean space
case further below of this article. Thus, instead of eq. (35) we consider the
Stratonovich s.d.e. (here denoted, as usual, by the symbol ◦) for it given by :
dx(τ )

= X(x(τ )) ◦ dW (τ ) + bQ,X (τ, x(τ ))dτ,
where bQ,X (τ, x(τ )) = Q̂(τ, x(τ )) + S(∇g , X)(x(τ )),

(143)

where the drift now contains an additional term, the Stratonovich correction
term, given by S(∇g , X) = 21 tr(∇gX X), where ∇gX X , the Levi-Civita covariant
derivative of X in the same direction and thus it is an element of T M , so that
in local coordinates we have S(∇g , X)β = 21 Xiβ ∇g ∂ Xiα . Now we also represent
∂xα
the jacobian flow using the Stratonovich prescription
dṽ(τ ) = ∇g X(x(τ ))(ṽ(τ )) ◦ dW (τ ) + ∇g bQ,X (τ, x(τ ))(ṽ(τ ))dτ.

(144)

Now we shall construct classical flows to approximate the random flow {x(τ ) :
τ ≥ 0}. We start by constructing a piecewise linear approximation of the Wiener
9 This driftless representation is akin to the idea of a static Universe - a Form- as in the
pre-Socratic philosophers Xhenophanes and Parmenides proposals [62], while the lagrangian
representation presented here represents Heraclitus ideas with whom we associate the idea of
dynamics. Remarkably, both conceptions appear to be equivalent in regards of a stochasticgeometrical representation of the Universe as long as it is not one-dimensional

36

Quantization in Astrophysics ...

364

process. Thus, we set for each k = 1, 2, . . .,
Wk (τ )

j+1
j
j
j+1
− τ )W ( ) + (τ − )W (
)],
k
k
k
k
j
j+1
if ≤ τ ≤
, j = 0, 1, . . .
k
k

= k[(

(145)

and we further consider the sequence {xk (τ )}k∈N satisfying
dWk
dxk (τ )
= X(xk (τ ))
(τ ) + bQ,X (τ, xk (τ )),
(146)
dτ
dτ
dṽk (τ )
dWk
= ∇g X(xk (τ ))(ṽk (τ ))
(τ ) + ∇g bQ,X (τ, xk (τ ))(ṽk (τ )),(147)
dτ
dτ
j+1
j
j
j+1
dWk
(τ ) = k[W (
) − W ( )] for < τ <
,
(148)
dτ
k
k
k
k
k
(otherwise, it is undefined ,) so that dW
dτ (τ ) exists for almost all values of τ
(a.e., in short in the following). Since {Wk (τ )}k∈N is differentiable a.e., thus
{xk (τ ) : xk (0) = x(0)}k∈N is a sequence of flows obtained by integration of
well defined o.d.e’s on M a.e., for all W ∈ Ω. We remark that {xk (τ )}k∈n
depends on the (here chosen canonical) realization of W ∈ Ω so that in rigour,
we should write {xk (τ, W, x0 )}k∈N to describe the flow; the same observation is
valid for the approximation of the derivative flow below. With the additional
assumption that X and Q are smooth, then the previous sequence defines for
almost all τ and for all W ∈ Ω, a flow of smooth diffeomorphisms of M , and
thus, the flow {vk (τ ) = (xk (τ ), ṽk (τ )) : vk (0) = (x(0), v(0))} defines a flow of
smooth diffeomorphisms of T M . In this case, this flow converges uniformly in
probability, in the group of smooth diffeomorphisms of T M , to the the flow of
random diffeomorphisms on T M defined by eqs. (35) and (39) [1,2,11].
Returning to KDE (and NSV), we can approximate eqs. (35) and (39) by
τ,s,v(x)
taking the jacobian flow {(xτ,s,x
, ṽk
)}k∈N on T M given by
k

dxτ,s,x
1
dWk (s)
k
(s) = [2ν m ] 2 X(xτ,s,x
)
+ b−u,X (τ − s, xτ,s,x
), xτ,0,x
= x,
(149)
k
k
k
ds
ds
τ,s,v(x)
dṽk
1
τ,s,v(x)
τ,s,v(x) dWk (s)
(s) = [2ν m ] 2 ∇g X(xτ,s,x
)(ṽk
)
+ ∇g b−u,X (τ − s, xτ,s,x
)(ṽk
)ds,
k
k
ds
ds
τ,0,v(x)
ṽk
= v(x) ∈ Tx M
(150)
dWk (s)
ds

=

2k {W (

[2k s/τ ] + 1
[2k s/τ ]
) − W(
)}, s ∈ [0, τ ], (τ > 0),
k
2
2k

(151)

with [z] the integer part of z ∈ (0, 1], is the Stroock & Varadhan polygonal
approximation [11]. Thus, we can write the expression:
τ,τ,v 1 (x)

ω̃τ (v 1 (x) ∧ . . . v n−1 (x)) = limk→∞ Ex [ω0 (xτ,τ,x
)(ṽk
k
37

Quantization in Astrophysics ...

365

τ,τ,v n−1 (x)

∧ . . . ∧ ṽk

)].(152)

By replacing ν m by ν we have the approximations of the representations of
NSV. We can proceed identically for the Poisson-de Rham equation, for which
in account of eqs. (69) and (70) we have to substitute 2ν m X by X and b−u,X ≡
b0,X , the latter being the Stratonovich correction term.
Remarks 1.There is not an unique construction for the approximation of
these random diffeomorphisms by o.d.e’s; indeed, the noise term can be alternatively presented in terms of the extension of the Cartan development method,
as a sequence of polygonal geodesic paths [89]. Furthermore, in the case of
manifolds being immersed in Euclidean space (which will be the case further
below) and complete (autoparallels exist for any τ ), the latter construction can
be extended to a unified setting in which the random diffeomorphisms of a
RCW diffusion can be realized (with convergence in probability) by sequences
2
)
= 0,
of polygonal autoparallel paths, i.e. smooth a.e. curves of the form ∇∂τx(τ
2
where ∇ is a RCW connection. These approximations are irreversible per se in
distinction with the above ones, since autoparallels just like geodesics can focus
in a point; they can be constructed through the image of the exponential map of
∇ as the image of the parallel random transport by ∇ of a family of linear frames
in T M ; the presentation of these constructions would increase greatly the length
of this article, and can be found in a somewhat long and intricate presentation
in Chapter 8, of the masterpiece due to Bismut [89]. This is of great importance,
as it allows to establish an original understanding of the role of the autoparallels
of ∇ as we shall argue next. Firstly, autoparallels are not the paths followed
by spinless particles submitted to an exterior gravitational field described by
a linear connection with torsion (the latter a common mistake as in [94]) , or
more restricted, a RCW connection, which is the geodesic flow as proved independantly of any lagrangian nor Hamiltonian dynamics [57]. This resulted from
applying the ideas of E. Cartan’s classical developing method and symplectic geometry, to derive the dynamics of relativistic spinning test-particles on exterior
gravitational fields turned out to be an outstanding success of this approach,
yielding extensions of the well known Papapetrou-Dixon-Souriau equations [57].
So RCW autoparallel polygonal a.e. smooth paths provide approximations of
the random continuous of RCW diffusions (or still, of the Feynman path integral
representation of their transition density), which as we already remarked, not
necessarily should be thought as spinless particles, furthermore, vis a vis the
construction of a theory of supersymmetric systems which have these motions
as their support for the motions of arbitrary degree differential forms; we shall
address the latter problem in the next Section. 10
10 Most remarkably, in the path integral representation due to Kleinert of the classical action for a scalar path on a time-sliced Euclidean space which through anholonomic coordinate
transformation adquires both torsion and curvature, the classical motions appear to be autoparallels and by applying discretization on them, a short-time-t Feynman propagator has
been built for arbitrary Q which yields the non-relativistic Schroedinger equation where the
Schroedinger operator is the non-relativistic version of our present H0 (g, Q). Yet, in this work,

38

Quantization in Astrophysics ...

366

14

KDE and Random Symplectic Diffusions

Starting with a general RCW diffusion of 1-forms generated by H1 (g, Q),
we introduce a family of Hamiltonian functions, Hk (k ∈ N ) defined on the
cotangent manifold T ∗ M = {(x, p)/p : Tx M → R linear} by
Hk = HX,k + HQ ,

(153)

with (in the following h−, −i denotes the natural pairing between vectors and
covectors)
HX,k (x, p) = hhp, X(x)i,

dWk
i,
dτ

(154)

where the derivatives of Wk are given in eq. (151), and
HQ̂ (x, p) = hp, bQ,X (x)i.

(155)

Now, we have a sequence of a.a. classical Hamiltonian flow, defined by integrating for each k ∈ N the a.a. system of o.d.e.’s
dxk (τ )
dτ
dpk (τ )
dτ

∂Hk
dWk
+ bQ,X (xk (τ )),
= X(xk (τ ))
∂pk
dτ
∂Hk
dWk (τ )
= −
i
= −hhpk (τ ), ∇g X(xk (τ ))i,
∂xk
dτ
− hpk (τ ), ∇g bQ,X (τ, xk (τ ))i.

≡

(156)

(157)

the rule for discretization is the Hanggi-Klimontovich (post-point) rule and thus it is not Ito’s
(middle point) nor the Stratonovich (pre-point) rules; see chapters 10 & 11 [94]. Now, the
appearance in the present article of H0 (g, Q) as the differential generator of a diffusion process
in terms of which the whole theory is constructed, has to do with the need of a diffeomorphism
invariant description of a diffusion process and its generator, which requires the introduction
of a linear connection [89], here a RCW connection whose laplacian is H0 (g, Q). Such an
approach fixes the discretization rule to be Ito’s, and thus the Brownian integral of the theory is given by the random integral flow of Ito’s eq. (35), and thus the Feynman integral
which corresponds by analytical continuation on τ of the flow of eq. (35) still corresponds
to a medium-point rule. In the remarkable computational work due to Kleinert (which has a
number of intriguing postulates for the definition of the Feynman measure such as a so-called
principle of democracy between differentials and increments; see page 335 in [94]), no connection is made between diffusion processes, the Schroedinger wave function and the exact term
of Q, as it shall appear in the accompanying article to the present one. Another result of this
approach is that it will yield a modification of the (controversial) coefficient affecting the metric scalar curvature term (see [94] and references therein), which in the accompanying chapter
of this book due to the author, will be associated with a generalization of Bohm’s quantum
potential in a relativistic setting. We would like to remark that in a recent formulation of a
1 + 1-dimensional relativistic theory of Brownian motion in phase space, it is claimed that
when studying the equilibrium distribution of a free Brownian particle submitted to a heat
bath, the post-point rule is the one that leads to the relativistic Maxwell distribution for the
velocities; see J. Dunkel and P. Hanggi, arXiv:cond-mat/0411011.

39

Quantization in Astrophysics ...

367

which preserves the canonical 1-form pk dxk = (pk )α d(xk )α (no summation on
k!), and then preserves its exterior differential, the canonical symplectic form
Sk = dpk ∧ dxk . We shall denote this flow as φk. (ω, .); thus φkτ (ω, .) : Tx∗k (0) M →
Tx∗k (τ ) M , is a symplectic diffeomorphism, for any τ ∈ R+ and ω ∈ Ω. Furthermore, if we consider the contact 1-form [95] on R × T ∗ M given by γk :=
pk dxk − HX,k dτ − HQ̂ dτ, ∀k ∈ N , we obtain a classical Poincaré-Cartan integral
invariant: Let two smooth closed curves σ1 and σ2 in T ∗ M × {τ = constant}
encircle the same tube of trajectories
R
R of the Hamiltonian equations for Hk , i.e.
eqs. (156) and (157); then σ1 γk = σ2 γk . Furthermore, if σ1 − σ2 = ∂ρ, where
ρ is a piece of the vortex tube determined by the trajectories of the classical
Hamilton’s equations, then it follows from the Stokes theorem [95] that
Z
Z
Z
Z
Z
γk −
γk =
pk dxk −
pk dxk = dγk = 0.
(158)
σ1

σ2

σ1

σ2

ρ

Returning to our construction of the random Hamiltonian system, we know
already that for X and Q̂ smooth, the Hamiltonian sequence of flows described
by eqs. (156) and (157) converges uniformly in probability in the group of
diffeomorphisms of T ∗ M , to the random flow of the system given by eqs. (143)
and (144) and
dp(τ )

= −hhp(τ ), ∇g X(x(τ ))i, ◦dW (τ )i − hp(τ ), ∇g bQ,X (τ, x(τ ))dτ i.
(159)

Furthermore this flow of diffeomorphisms is the mapping: φτ (ω, ., .)(x, p) =
(Fτ (ω, x), Fτ∗ (ω, x)p), where Fτ∗ (ω, x) is the adjoint mapping of the jacobian
transformation. This map preserves the canonical 1-form pdx, and consequently
preserves the canonical symplectic 2-form S = d(pdx) = dp ∧ dx, and thus
∗
∗
∗
φτ (ω, .) : Tx(0)
M → Tx(τ
) M is a flow of symplectic diffeomorphisms on T M
for each ω ∈ Ω [89]. Consequently, Λn S is preserved by this flow, and thus we
have obtained the Liouville measure invariant by a random symplectic diffeomorphism. We shall write onwards, the formal Hamiltonean function on T ∗ M
defined by this approximation scheme as
H(x, p) := hhp, X(x)i,

dWτ
i + HQ̂ (x, p).
dτ

(160)

We proceed now to introduce the random Poincaré-Cartan integral invariant for
this flow. Define the formal 1-form by the expression
γ := pdx − HQ̂ dτ − hp, Xi ◦ dW (τ ),

(161)

and its formal exterior differential (with respect to the N = T ∗ M variables
only)
dN γ = dp ∧ dx − dN HQ̂ ∧ dτ − dN hp, Xi ◦ dW (τ ).
40

Quantization in Astrophysics ...

368

(162)

Clearly, we have a random differential form whose definition was given by Bismut
[89]. Let a smooth r-simplex with values in R+ × T ∗ M be given as
σ : s ∈ Sr → (τs , xs , ps ), where Sr = {s = (s1 , . . . , sr ) ∈ [0, ∞)r , s1 + . . . + sr ≤ 1},(163)
Pr+1
with boundary ∂σ the (r − 1)-chain ∂σ = i=1 (−1)i−1 σ i , where σ i are the (r −
1) singular simplexes given by the faces of σ. σ can be extended by linearity to
any smooth singular r-chains. We shall now consider the random continuous rsimplex, c, the image of σ by the flow of symplectic diffeomorphisms φ, i.e. the
image in R × T ∗ M
φ(τs , ω, xs , ps ) = (τs , Fτ (ω, xs ), Fτ∗ (ω, xs )ps ), for fixed ω ∈ Ω,

(164)

where Fτ (ω, x) and Fτ∗ (ω, x)p are defined by eqs. (35), (37) and (159), respectively.
Then, given α0 a time-dependant 1-form on N , β0 , . . . , βm functions defined
on R × N , the meaning of a random differential 1-form
γ = α0 + β0 dτ + βi ◦ dW i (τ ), i = 1, . . . , m,

(165)

is expressed by its integration on a continuous 1-simplex
c : s → (τs , φτs (ω, ns )), where ns = (xs , ps ) ∈ T ∗ M,

(166)

the image by φ. (ω, .), (ω ∈ Ω) the random flow of symplectomorphisms
on T ∗ M ,
R
of the smooth 1-simplex σ : s ∈ S1 → (τs , (xs , ps )). Then, c γ is a measurable
real-valued function defined on the probability space Ω in [89]. Now we shall
review the random differential 2-forms. Let now α̃0 be a time-dependant 2-form
on N , thus α̃0 (τ, n) which we further assume to be smooth. Furthermore, let
β̃0 (τ, n), . . . , β̃m (τ, n) be smooth time-dependant 1 forms on N and we wish to
give a meaning to the random differential 2-form
γ = α̃0 + dτ ∧ β̃0 + dW 1 (τ ) ∧ β̃1 + . . . + dW m (τ ) ∧ β̃m .

(167)

on integrating it on a continuous 2-simplex c : s → (τs , φτs (ω, ns )), or which we
define it as a measurable real valued function on Ω in [89]. To obtain the random
Poincaré-Cartan invariant we need the following results on the approximations
of random differential 1 and 2-forms by classical differential forms. Given as
before α̃0 a time dependant smooth 2-form on N and time-dependant smooth
1-forms β̃1 , . . . , βm on N , there exists a subsequence ki and a zero-measure
Ω̂ subset of Ω dependant on α̃0 , β̃1 , . . . , β̃m such that for all ω ∈
/ Ω̂, φk. i (ω, .)
converges uniformly on any compact subset of R+ × R2n to φ. (ω, .) as well as
l ki
∂ l φ.
φ
all its derivatives ∂∂n
l (ω, .) with |l| ≤ m, converges to ∂nl (ω, .), and for any
smooth 2-simplex, σ : s → (τs , ns ) valued on R+ × N , if
γk = α̃0 + dτ ∧ (β̃0 + β̃1

dWk1
dWkm
+ . . . β̃m
)
dτ
dτ

41

Quantization in Astrophysics ...

369

(168)

and if ck is the 2-simplex given by the image of a smooth 2-chain by the a.a.
smooth diffeomorphism φk. (ω, .) defined by integration of eqs. (156) and (157):
cRk : s → (τs , φkτs (ω, nsR)), and c is the continuous 2-chain s → (φτs (ω, ns )) , then
γ ki converges to c γ. If instead we take a time-dependant 1-forms α0 and
cki
time-dependant functions β0 , . . . , βm on N and consider the time-dependant
1-form on N given by
γk = α0 + (β0 + β1

dWk1
dWkm
+ . . . βm
)dτ
dτ
dτ

(169)

and for any a.e. smooth 1-simplex ck : s → (τs , φkτs (ω, ns )) then there exists
a subsequence ki and a zero-measure set Ω̂, dependant of α0 , β0 , . . . , βm , such
that for all ω ∈
/ Ω̂, φk. i (ω, .) converges uniformly over all compacts of R+ × R2n
with all its derivatives of order up to m to thoseR of φ. (ω, .), and if c isR the
continuous 1-simplex c : s → (τs , φs (ω, ns )), then cki γ ki converges to c γ,
with γ defined in eq. (60).
Then, we can state the fundamental theorem of Stokes for this random setting, which is due to Bismut ([89], Theorem 3.4). Let c be a random continuous
2-simplex image of an arbitrary smooth 2-simplex by the flow φR. (ω, .). There
R
exists a zero-measure set Ω̃ ⊂ Ω such that for any ω ∈
/ Ω, then c dγ = ∂c γ,
for any differential random 1-form γ.
In the following in the case defined by KDE, for which Q̂ = −û with u a
solution of NS or Euler equations, so that we set
1

α0 = pdx, β0 = −H−û ≡ Hû , βi = −(2ν m ) 2 hp, Xi i ≡ pα Xiα , i = 1, . . . , m, (170)
where X : Rm → T M with X(x) = gradf with f : M → Rd is an isometric
immersion of M , then
γKDE

1

= pdx + Hû dτ − (2ν m ) 2 hp, Xii ◦ dW i (τ )
1

= pα (dxα + (bu,X )α dτ − (2ν m ) 2 Xiα ◦ dW i (τ )),

(171)

is the random Poincaré-Cartan 1-form defined on R+ × N for KDE. The Hamiltonian function for KDE is
1
dWτ
H(x, p) := [2ν m ] 2 hhp, X(x)i,
+ H−û (x, p),
(172)
dτ
with
H−û (x, p) = pα (b−u,X )α = g αβ pα (−uβ + ν m Xiα ∇g ∂ Xiβ )

(173)

∂xβ

so that the Hamiltonian system is given by the system
1

dx(τ ) = [2ν] 2 X(x(τ )) ◦ dW (τ ) + b−u,X (τ, x(τ ))dτ, (174)
−u,X
with b
(τ, x(τ )) = ν∇gX X(x(τ )) − û(τ, x(τ )))
(175)
dp(τ )

1

= −(2ν m ) 2 )hhp(τ ), ∇g X(x(τ ))i, ◦dW (τ )i
− hp(τ ), ∇g b−u,X (τ, x(τ ))dτ i.
42

Quantization in Astrophysics ...

370

(176)

As in the general case, we then obtain a Liouville invariant measure produced
from the n-th exterior product of the canonical symplectic form. Substituting ν m by ν we obtain the random Poincaré-Cartan invariant γNSV for NSV
[91,92,96,97].
To obtain the invariants of the full Navier-Stokes equations [91,92,96,97], we
have to consider in addition, the random Hamiltonian flow corresponding to the
invariant Poisson-de Rham equation, i.e. eq.(60) which we rewrite here
dx̃(τ ) = X(x̃(τ )) ◦ dW (τ ) + S(X(x̃), g) ◦ dW (τ ), and
dp̃(τ ) = −hhp̃(τ ), ∇g X(x̃(τ ))i, ◦dW (τ )i
− hp̃(τ ), ∇g S(∇g , X)(τ, x̃(τ ))dτ i.

(177)
(178)

Furthermore this flow of diffeomorphisms preserves the canonical 1-form p̃dx̃,
and consequently preserves the canonical symplectic 2-form S = d(pdx) = dp ∧
∗
∗
dx, and thus φτ (ω, .) : Tx(0)
M → Tx(τ
) M is a flow of symplectic diffeomorphisms
∗
on T M for each ω ∈ Ω. Consequently, Λn S is preserved by this flow, and
thus we have obtained the Liouville measure invariant by a random symplectic
diffeomorphism. We shall write onwards, the formal Hamiltonian function on
T ∗ M defined by the approximation scheme for the formal Hamiltonian function
H(x̃, p̃) := hhp̃, X(x̃)i,

dWτ
i + hp, S(∇g , X)(x)i.
dτ

(179)

We now proceed to introduce the random Poincaré-Cartan integral invariant for
this flow. It is the 1-form
γPoisson := p̃dx̃ − S(∇g , X)(x̃)dτ − hp̃, Xi ◦ dW (τ ).

(180)

This completes the construction of the random invariants for NS.

15

The Euclidean Case

To illustrate with an example, consider M = Rn , f (x) = x, ∀x ∈ M , and
then X = ∇f ≡ I, the identity matrix, as well as g = XX † = I the Euclidean
metric, and ∇g = ∇, is the gradient operator acting on the components of
differential forms. Consequently, the Stratonovich correction term vanishes since
∇X X = 0 and thus the drift in the Stratonovich s.d.e’s. is the vector field
b−u,X = −û = −u (we recall that û is the g-conjugate of the 1-form u, but here
g = I).
We shall write distinctly the cases n = 2 and n = 3. In the latter case we
have that both the vorticity and the magnetic form, say Ω(τ, x) are a 2-form
on R3 , or still by duality has an adjoint 1-form, or still a R3 -valued function,
which with abuse of notation we still write as Ω̃(τ, .) : R3 → R3 . Consider the
43

Quantization in Astrophysics ...

371

flows which integrates KDE (for NSV we simply substitute ν m by ν) is given
by integrating the system of equations (s ∈ [0, τ ])
dxτ,s,x
dṽ

τ,s,v(x))

1

= [2ν m ] 2 ◦ dW (s) − u(τ − s, xτ,s,x )ds, xτ,0,x = x,
= −∇u(τ − s, xτ,s,x )(ṽ τ,s,v(x) )ds, ṽ τ,0,v(x) = v(x)

(181)
(182)

the second being an ordinary differential equation (here, in the canonical basis of
∂ui
R3 provided with Cartesian coordinates (x1 , x2 , x3 ), ∇u is the matrix
( ∂x
j ) for
R
τ
u(τ, x) = (u1 (τ, x), u2 (τ, x), u3 (τ, x)), which in account that since 0 ◦dW (s) =
W (τ ) − W (0) = W (τ ), we integrate
Z s
τ,s,x
m 12
u(τ − r, xτ,r,x )dr, s ∈ [0, τ ],
(183)
x
= x + [2ν ] W (s) −
0

and
ṽ τ,s,v(x) = e−s∇u(τ −s,x

τ,s,x

)

v(x)

(184)

so that the analytical representation for KDE (and alternatively for NSV) in
R3 is
Ω̃(τ, x) = Ex [ṽ τ,τ,I Ω0 (xτ,τ,x )],

(185)

where Ex denotes the expectation value with respect to the measure (if it exists)
: τ ≥ 0}, for all x ∈ R3 , which is a Gaussian function albeit not centered
on {xτ,x
τ
in the origin of R3 due to the last term in eq. (183) and in the r.h.s. of eq. (185)
we have matrix multiplication Thus, in this case, we have that the deformation
tensor acts on the initial vorticity along the random paths. This action is the
one that for 3D might produce the singularity of the solution of NS for 3D.
We finally proceed to present the random symplectic theory for KDE (and
alternatively, NSV) on R3 . In account of eq. (55) with the above choices, the
formal random Hamiltonian function is
1

H(x, p) := [2ν m ] 2 hp,

dW (τ )
i + H−û (x, p),
dτ

(186)

with
H−û (x, p) = −hp, ui.

(187)

The Hamiltonian system is described by specializing eqs. (174 − 176), so that
we obtain the Stratonovich s.d.e. for x(τ ) ∈ R3 , ∀τ ≥ 0:
1

dx(τ ) = [2ν m ] 2 ◦ dW (τ ) − u(τ, x(τ ))dτ,

(188)

dp(τ ) = −hp(τ ), ∇u(τ, x(τ ))idτ.

(189)

and the o.d.e

44

Quantization in Astrophysics ...

372

If we further set x(0) = x and p(0) = p, the Hamiltonian flow preserving the
canonical symplectic form S = dp ∧ dx on R6 is given by
Z τ
m 12
u(r, x(r))dr, e−τ ∇u(τ,x(τ )) p).(190)
φτ (., .)(x, p) = (x(τ ), p(τ )) = (x + [2ν ] W (τ ) −
0

Finally, the Poincaré -Cartan 1-form takes the form
1

γKDE = hp, dx − udτ − (2ν m ) 2 ◦ dW (τ )i,

(191)

and the Liouville invariant is S ∧ S ∧ S. This, completes the implementation of
the general construction on 3D, for KDE (alternatively, for NSV).
To complete our symplectic representations for NS, we still have to give the
symplectic structure associated to eq. (60) (Poisson-de Rham) for both R2 and
R3 . This structure is the same in both cases, the only difference is in the form
of the random Liouville invariant. Indeed, the random Hamiltonean system for
Poisson-de Rham is given by eqs. (177) and (178), which in the Euclidean case
the former yields eq. (93), while the latter is dp̃(τ ) = 0, so that if p̃(0) = p,
then the random symplectic flow for Poisson-de Rham equation is given by
φτ (., .)(x, p) ≡ (x̃(τ ), p̃(τ )) = (x + W (τ ), p),

(192)

and the Liouville invariant is S̃ ∧ S̃ for n = 2, and S̃ ∧ S̃ ∧ S̃ for n = 3, where
S̃ = dp̃ ∧ dx̃ is the canonical symplectic form for both cases, for the Poisson-de
Rham equation. In distinction with the random symplectic invariants for NSV,
here the momentum is constant, and of course, the position variable does no
longer depend manifestly on u.
Remarks 13. Geometrical-topological invariants in magnetohydrodynamics and hydrodynamics have been widely studied [9,56,70]. We have followed
the presentation in [91,92,96] which lead to the random symplectic invariants
of NS, hitherto unkown. The present approach applies as well to the random
quantization of quantum mechanics through stochastic differential equations, as
we shall present in the accompanying article, and thus we shall have random
phase invariants which have been unnoticed till today.

16

Derivation of the Symplectic Structure for
Perfect Fluids

We have seen that NS has an associated Hamiltonian function and a Liouville invariant, and thus we have in principle the basic elements to develop a
statistical mechanics approach to NS. The purpose of this section, is to obtain
the symplectic structure for the Euler equations from our perspective. Indeed,
note that if we set ν = 0 we have a classical limit whose dynamics is described
45

Quantization in Astrophysics ...

373

by the characteristics curves defined by the integral curves of −û, i.e. (minus)
the velocity vector-field. Indeed, if we set the kinematical viscosity ν to zero in
eqs. (174 − 176) we obtain
dx(τ )
dτ
dp(τ )
dτ

= −û(τ, x(τ )),

(193)

= − < p(τ ), ∇g u(τ, x(τ ) > .

(194)

Now, on integrating eq. (193) with some given initial condition x(0), we obtain
a family (indexed by time) of classical diffeomorphisms of M which to x(0)
associates the position x(τ ) of the fluid particles with velocity vector field given
by −û(τ, x(τ )); in fact for each τ this diffeomorphisms preserves the Riemannian
volume since û is divergenceless. Thus, it follows from our particular case for a
perfect incompressible fluid obeying the Euler equations (set ν = 0 in eq. (57)),
that the configuration space is given by the volume preserving diffeomorphisms
of M , which we denote by SDiff(M) which is nothing else than the starting point
the AEM theory; by contrast in the present approach the configuration space
for NS are the random diffeomorphisms defined by the lagrangian flow described
above, which is not volume preserving but in the special case of Euclidean space
for which X = Id.
Now SDiff(M) is an infinite-dimensional Lie group, and we are interested -in
following Arnold- in its Lie algebra, which is the set of divergenceless vector
fields on M , SVect(M ) provided with the usual commutator. Arnold further
considered the orbits of the coadjoint action of this group on the dual of the Lie
algebra, as a Hamiltonian system whose Hamiltonian function is (c.f. definition
7.20 and Lemma-definition 7.21 in [56]) (following eq. (22) above)
Z
 1

1
[uτ ], [uτ ] =
g [ûτ ], [ûτ ] volg ,
(195)
2
2 M
where [uτ ] denotes the equivalence class of all 1-forms on M of the type uτ + df ,
with δuτ = 0 and some function f : M → R, which is nothing else than
Z
Z
1
1
−
H(x, [uτ ])volg :=
H[ûτ ] (x, [uτ ])volg ,
(196)
2 M
2 M
∗

which coincides with Arnold’s energy function on SVect(M) , the dual Lie algebra of SVect(M). From the minimal action principle Arnold obtained finally
the geodesic equation in SDiff(M). But we can obtain these equations directly
in our setting if we further set p ≡ u in eq. (194) , so that eqs. (193) and (194)
turn to be the geodesic equation on SDiff(M):
d2 x(τ )
+ ∇gûτ (x) uτ (x(τ )) = 0,
dτ

46

Quantization in Astrophysics ...

374

(197)

which in account of the identity
1
∇gûτ (x) uτ (x) = Lûτ (x) uτ (x) − d(|uτ |2 ),
2

(198)

we get the Euler equation (see pages 37, 38 in [56])
1
∂u
+ Lûτ (x) uτ (x) = d(|uτ |2 )
∂τ
2

(199)

identically to set ν = 0 in NS. Note here that the pressure function p̃ reduces
to be (modulo an additive constant) − 12 |uτ |2 , minus the kinetic energy term of
uτ , and the non-appearance of itself the −dp̃ term in the r.h.s. of eq. (199)
is produced by the fact that our random flows for NS have been constructed
for the vorticity equation, for which there is no pressure term since d2 p̃ = 0;
otherwise stated, to obtain the Euler equation we have taken uτ ∈ [uτ ] such
that f ≡ 0, and thus the total pressure is
1
1
f − (|uτ |2 ) = − |uτ |2
2
2
(see comments in first paragraph after Remark 7.22 in [56]).
Remarks 13. Thus, we have proved that the random symplectic approach
to NS yields the classical symplectic approach to the Euler equation, in the case
of null viscosity, as a particular result of the kinematics of the random viscous
flow. We may remark that Arnold’s approach stops short of discussing analytical representations for NS, yet his symplectic approach has been extended by
the addition of Wiener processes, to give the representations of NS for the flat
torus, by Gliklikh [63].Probably the present work could be seen as a natural
addendum to the joint work by Arnold and Khesin [56], in which prior to the
introduction of the (random) symplectic geometry, one has to introduce first
the stochastic differential geometry from which it stems, both aspects being absent in this beautiful treatise. We have derived through the association between
RCW connections and generalized Brownian motions, the most general implicit
analytical representations for NS, in the case of manifolds without boundaries.
The case with smooth boundaries and Euclidean semi-space has been treated
completely in [ ]. Furthermore, in the case without boundary, we have proved
that the interaction representation of the solutions of NS, and in general of
diffusion processes, in which the trace-torsion plays the major role of describing the average motions, can be gauged away (for any dimension other then 1)
and transformed into an equivalent representations in which the trace-torsion
enters in the definition of the noise-tensor, as if the random motion would be
completely free [5]! Yet, concerning NS the present treatment is still unsatisfactory, since the representations are implicit, since we have not presented a theory
in which we would decouple the velocity 1-form (the gauge potential) and the
vorticity 2-form (the ’curvature’ field strength). We would like to suggest that
47

Quantization in Astrophysics ...

375

if this problem might have a solution, then it should be approached through
the application of Clifford algebras and Clifford analysis, in which through the
Dirac operator whose square is the NS laplacian, we could integrate the theory
in terms of the vorticity alone. This would be similar to the Maxwell equation
as a single equation for the electromagnetic field strength (a 2-form, and not
in terms of the electromagnetic potential 1-form), as we shall describe in the
accompanying article that follows the present one. We have discussed in a previous chapter in this book, that the Schroedinger equation in spatial 2D, can
be transformed into the Navier-Stokes equations, and that the Born probability
density maps into the enstrophy. In forthcoming articles, we shall present the
relations between fluid-dynamics and turbulence, electrodynamics and quantum
mechanics.
The method of integration applied in this article is the extension to differential forms of the method of integration (the so-called martingale problems)
of elliptic and parabolic partial differential equations for scalar fields [24,31,73].
The remarkable key for this method is, as we have shown for all given representations, the Ito-Elworthy formula of stochastic analysis for differential forms in
its various expressions.11
In distinction with the Reynolds approach in Fluid Mechanics 12 which has
the feature of being non-invariant, in the present approach, the invariance by
the group of space-diffeomorphims has been the key to integrate the equations,
in separating covariantly the fluctuations and drift terms and thus setting the
integration in terms of covariant martingale problems. The role of the RCW
connection is precisely to yield this separation for the diffusion of scalars and
differential forms, and thus the role of the differential geometrical structure is
essential. Yet, as we have shown, we can introduce a push-forward description
of the RCW connection, such that via stochastic analysis we can gauge away
the trace-torsion in any dimension other than 1, to obtain equivalent purely
noise representations for NS. The noise is purely geometrical, incorporating the
parameters which characterize the random lagrangian flow for scalars of NS in
its definition.
A new approach to NS as a (random) dynamical system appears. Given
a stationary measure for the random diffeomorphic flow of NS given by the
stationary flow of NS, one can construct the state space of this flow and further,
its random Lyapunov spectra [72,76,77]. Consequently, assuming ergodicity
of this measure, one can conclude that the moment instability of the flow is
related to a cohomological property of M , namely the existance of non-trivial
harmonic one-forms φ, which are preserved by the vectorfield û of class C 2 , i.e.
Lû φ = diû φ = 0; see page 61 in [27]. We also have the random flows {vτ ∧ vτ :
τ ≥ 0} and Wτ2,−û0 on T M ∧ T M of Theorem 7 and Theorem 10 respectively,
11 Furthermore, this approach and can be extended to the case of smooth manifolds with
smooth boundaries, yet due to length limitations, we shall discuss it elsewhere [67].
12 A generalization of the Reynolds approach to random lagrangian flows, completed with
the k −  theory, is the basis for the studies on turbulence by Pope [75].

48

Quantization in Astrophysics ...

376

which integrate the heat equation for the vorticity. Concerning these flows, the
stability theory of NS (57) requires an invariant measure on a suitable subspace
of T M ∧ T M and further, the knowledge of the spectrum of the one-parameter
1
uτ ); alternatively, one could use
family of operators depending on ν, H2 (2νg, − 2ν
the expression for the Navier-Stokes operator provided by Theorem 13. These
operators may play the role of the Schroedinger operators in Ruelle’s theory
of turbulence, which were introduced by linearising NS for the velocity as the
starting point for the discussion of the instability theory; see article in pages
295 − 310 in Ruelle [48].
As well known, the stability theory is usually encountered in the framework
of a major approach to turbulence in fluids, that of classical dynamical systems
presenting chaotic behavior. In this approach, the aim is to construct of a lowdimensional non-random model that describes the asymptotic expression of typical solutions of NS under adequate boundary and regularity conditions. Bounds
on the dimension of these attractors for NS have been computed [48,74,83,84],
yet the actual construction of the dynamics of these attractors from first principles is an open problem under study for NS [81]. More generally, little is known
about these low dimensional (inertial manifolds) which describe this asymptotic
dynamics for infinite–dimensional dynamical systems [74]. Of course, ad-hoc
reductions of NS such as the one that leads to the Lorenz ordinary differential
equations [84] have been useful to give a qualitative description. Remarkably
enough, the random approach to NS that we have presented, can also be encountered in low dimensional chaotic classical dynamics. Indeed, it has been
proved that the solution of o.d.e’s of the form of the Lorenz system, converges
(in the sense of weak convergence of processes) to the solution of stochastic
differential equations, and thus representable by a RCW connection. This has
been proved for the case of inertial manifolds defined on compact manifolds
with partial dynamics described by complete vector fields generating an axiom
A flow, having further a non-periodic attractor and with initial conditions distributed according to the Sinai-Ruelle-Bowen measure [85]. Thus, also the low
dimensional chaotic classical dynamics approach to turbulence has the same
type of structure as the full infinite-dimensional random flows presented in this
article. 13
Finally, it has been established numerically that turbulent fluids resemble
the random motion of dislocations [4]. In the differential geometric gauge theory
of crystal dislocations, the torsion tensor is the dislocation tensor [12,86], and
our presentation suggests that this analogy might be established rigorously from
the perspective presented here.
We would like to remark that averaged extensions of the equations of Euler
and Navier-Stokes are being currently investigated. In these works the averaging is in the sense of an ensemble of initial conditions taken in an appropiate
13 In fact our representations can be realized by rather standard methods in terms of ordinary
differential equations on M , thus guarranteeing effectively low dimensional realizations [88].

49

Quantization in Astrophysics ...

377

analytical space. Remarkably as well, the extension of the Euler equations can
be derived as the geodesic flow of an extended metric on the manifold of volume preserving diffeomorphims of M ; thus this program is an extension of the
Arnold-Ebin-Marsden approach (see [64,65]). As well in the past years, Gliklikh
has integrated NS for a viscous fluid in the flat torus, as a stochastic perturbation of the geodesic Euler-flow [63], thus extending the classical approach of
Arnold-Ebin-Marsden.

References
[1] A. Chorin, Turbulence and Vorticity, Springer,New York, (1994);
[2] K. Gustafson & J. Sethian (edts.), Vortex Methods and Vortex Motions,
SIAM, Philadelphia, (1991).
[3] M. Lesieur, Turbulence in Fluids, 3rd.ed., Kluwer, Dordrecht, (1997)
[4] M.Lesieur, La Turbulence, Presses Univs. de Grenoble, (1994).
[5] A. Chorin & J. Marsden, A Mathematical Introduction to Fluid Mechanics,
Springer, New York/Berlin, (1993).
[6] C. Marchioro & M. Pulvirenti, Mathematical Theory of Incompressible Nonviscous Fluids, Springer, New York/Berlin, (1994).
[7] R. Temam, Navier-Stokes Equations, North-Holland, Amsterdam, (1977).
[8] U.Frisch, Turbulence. The legacy of A.N.Kolmogorov, Cambridge Univ.
Press, (1996).
[9] D. Ebin & J. Marsden, Ann. Math. 92, 102-163 (1971)
[10] a. D. Rapoport, Int. J.Theor. Physics 35 No.10 (1997), 2127-2152; b.30,
(1)1, 1497 (1991); c. 35,(2),287 (1996)
[11] P. Malliavin, Géométrie Differentielle Stochastique, Les Presses Univ. Montreal (1978).
[12] K.D. Elworthy, Stochastic Differential Equations on Manifolds, Cambridge
Univ. Press, Cambridge, (1982).
[13] J. Eells & K.D.Elworthy, Stochastic dynamical systems, in Control Theory and topics in Functional Analysis, v.III, ICTP- Trieste, International
Atomic Energy Agency, Vienna, 1976.
[14] N. Ikeda & S. Watanabe, Stochastic Differential Equations on Manifolds,
North-Holland/Kodansha, Amsterdam/Tokyo, (1981).

50

Quantization in Astrophysics ...

378

[15] K. Ito, The Brownian motion and tensor fields on Riemannian manifolds,
in Proc. the Intern. Congress of Mathematics, Stockholm, 536-539, 1963.
[16] O.Reynolds, On the dynamical theory of turbulent incompressible fluids
and the determination of the criterion, Philosophical Transactions of the
Royal Society of London A, 186, 123-161, (1894)
[17] A. S. Monin & A. M. Yaglom, Statistical Fluid Mechanics, vol. II, J. Lumley
(ed.), M.I.T. Press, Cambridge (MA) (1975).
[18] J. Lumley, Stochastic Tools in Turbulence, Academic Press, New York,
(1970).
[19] S.A. Orszag, Statistical Theory of Turbulence, in Fluid Dynamics, Les
Houches 1973, 237-374, Eds. R. Balian & J.Peube, Gordon and Breach,
New York, (1977).
[20] L.Onsager, Statistical Hydrodynamics, Nuovo Cimento 6(2), 279-287,
(Suppl.Serie IX), 1949.
[21] D. Rapoport, The Geometry of Quantum Fluctuations, the Quantum Lyapounov Exponents and the Perron-Frobenius Stochastic Semigroups, in
Dynamical Systems and Chaos, Proceedings (Tokyo, 1994), Y.Aizawa (ed.),
World Sc. Publs., Singapore, 73-77,(1995).
[22] D. Rapoport, Covariant Non-linear Non-equilibrium Thermodynamics and
the Ergodic theory of stochastic and quantum flows, in Instabilities and
Non-Equilibrium Structures, vol. VI, p.259-270, Proceedings, E. Tirapegui
and W. Zeller (eds.), Kluwer, (2000).
[23] H. Kleinert, The Gauge Theory of Defects, vols. I and II, World Scientific
Publs., Singapore,(1989).
[24] H. Kunita, Stochastic Flows and Stochastic Differential Equations, Cambridge Univ. Press, (1994).
[25] S. Kobayashi & K.Nomizu, Foundations of Differentiable Geometry I, Interscience, New York, (1963).
[26] D. Rapoport, Torsion and non-linear quantum mechanics, in Group XXI,
Physical Applications and Mathematical Aspects of Algebras, Groups and
Geometries, Proceedings (Clausthal, 1996), H.D. Doebner et al (edts.),
World Scientific, Singapore, (1997). ibidem, Riemann-Cartan-Weyl Geometries, Quantum Diffusions and the Equivalence of the free Maxwell and
Dirac-Hestenes Equations, Advances in Applied Clifford Algebras, vol. 8,
No.1, p. 129-146, (1998).

51

Quantization in Astrophysics ...

379

[27] K.D. Elworthy, Stochastic Flows on Riemannian Manifolds, in Diffussion
Processes and Related Problems in Analysis, M.A. Pinsky et al (edts.),
vol.II, Birkhauser,(1992).
[28] Fulling, S.A., Aspects of Quantum Field Theory in Curved Space-Time,
Cambridge U.P., (1989)
[29] M. Berger, P. Gauduchon & E. Mazet, Le spectre d’une variété riemanniene, Springer LNM 170, (1971).
[30] R. Durrett, Brownian Motion and Martingales in Analysis, Wadsworth,
Belmont,(1984).
[31] R. Pinsky, Positive Harmonic Functions and Diffusions, Cambridge University Press, (1993).
[32] B. Simon, Schroedinger Semigroups, Bull. AMS (new series) 7, 47-526,
1982.
[33] M. Nagasawa, Schroedinger Equations and Diffusion Theory, Birkhauser,
Basel, (1994).
[34] E. Nelson, Quantum Fluctuations, Princeton Univ. Press, Princeton, New
Jersey, (1985).
[35] Y. Takahashi & S. Watanabe, The probability functionals (OnsagerMachlup functions) of diffusion processes, in Durham Symposium on
Stochastic Integrals, Springer LNM No. 851, D. Williams (ed.), (1981).
[36] M.I.Vishik & A. Fursikov, Mathematical Problems of Statistical Hydrodynamics, Kluwer Academic Press, 1989.
[37] L.C. Rogers & D.Williams, Diffusions, Markov Processes and Martingales,
vol. II, John Wiley, New York, 1989.
[38] M.T. Landhal & E. Mollo-Christensen, Turbulence and Random Processes
in Fluid Mechanics, Cambridge Univ. Press, 1994.
[39] A. Majda, Incompressible Fluid Flow, Communications in Pure and Applied Mathematics Vol.XXXIX, S-187-220, 1986.
[40] P. Meyer, Géometrie stochastique san larmes, in Séminaire des Probabilites XVI, Supplement,Lecture Notes in Mathematics 921, SpringerVerlag, Berlin, 165-207, 1982.
[41] T. Bohr, M. Jensen, G.Paladini & A.Vulpiani, Dynamical Systems Approach to Turbulence, Cambridge Non-linear Series No.7, Cambridge Univ.
Press, Cambridge,1998.

52

Quantization in Astrophysics ...

380

[42] A.B. Cruzeiro & S. Albeverio, Global flows with invariant (Gibbs) measures
for Euler and Navier-Stokes two dimensional fluids, Communications in
Mathematical Physics 129 (1990), 431-444; ibidem Solutions et mesures
invariantes pour des equations d evolution du type de Navier-Stokes,Expo.
Math. 7 (1989), p.73-82.
[43] S.A. Molchanov, Diffusion Processes and Riemannian Geometry, Russian
Mathematical Surveys 30 (1975), 1-63.
[44] F. Langouche, D. Roenkarts and E. Tirapegui, Functional Integration and
Semiclassical Expansions, Reidel Publs. Co., Dordrecht (1981).
[45] J. Leray, Selected Works, vol. II, Societé Mathematique de France and
Springer-Verlag, Berlin, 1998.
[46] F. Hehl, J. Dermott McCrea, E. Mielke & Y. Ne’eman, Physics Reports
vol. 258, 1-157, 1995.
[47] A. Friedman, Stochastic Differential Equations and Applications, vol. I,
Academic Press, New York, (1975).
[48] D. Ruelle (editor), Turbulence, Strange Attractors and Chaos, Series A on
Nonlinear Science vol. 16, World Scientific (1995).
[49] D. Rapoport, On the Geometry of Fluctuations, I,& II, in New Frontiers
of Algebras, Groups and Geometries, Proceedings (Monteroduni, 1995), G.
Tsagas (ed), Hadronic Press, 1996.
[50] S.A. Molchanov, A.A. Ruzmaikin and D.D. Sokoloff, ”A Dynamo Theorem”, Geophs. Astrophys. Fluid Dynamics30 (1984), 242.
[51] M. Ghill and S.Childress, Stretch, Twist and Fold, Springer-Verlag, New
York, 1995.
[52] D. Elworthy and X.M. Li, Formulae for the Derivatives of Heat Semigroups,
J. of Functional Analysis 125 (1994), 252-286.
[53] M. Taylor, Partial Differential Equations vol.III , Springer Verlag, 1995.
[54] B.Busnello, Ph.D. Thesis, Dipartimento di Matematica, Univ.di Pisa,
February 2000, & A Probabilistic approach to the 2-dimensional NavierStokes equations, to appear in Annals of Probability.
[55] P.Baxendale and K.D.Elworthy,Flows of Stochastic Dynamical Systems,
Z.Wahrschein.verw.Gebiete 65, 245-267 (1983).
[56] V.I. Arnold and B. Khesin, Topological Methods in Hydrodynamics,
Springer Verlag, 1999.

53

Quantization in Astrophysics ...

381

[57] D. Rapoport and S. Sternberg, On the interactions of spin with torsion,
Annals of Physics, vol. 154, no. 11, (1984), p.447-475
[58] D. Rapoport, Random representations for viscous fluids and the magnetic
fields transported on them, in Proceedings, Year 2000 International Conference on Dynamical Systems and Differential Equations, Atlanta, May
2000, pag. 239-247, special issue of Journal of Discrete and Continuous
Dynamical Systems, vol. 6, 2000.
[59] D. Rapoport, Torsion and Quantum, thermodynamical and hydrodynamical fluctuations, in The Eighth Marcel Grossmann Meeting in General Relativity, Gravitation and Field Theory, Proceedings , Jerusalem, June 1997,p.
73-76, vol. A, T.Piran and R.Ruffini (edts.), World Scientific, Singapore,
1999.
[60] D.Rapoport, Torsion, Brownian Motion, Quantum Mechanics and Fluiddynamics, in press in Proceedings of the Ninth Marcel Grossmann Meeting in Relativity, Gravitation and Field Theories, June 2000, Rome.
, R. Ruffini et al. (edts.), World Scientific Publs., Singapore, and
www.icra.it/MG/mg9/Proceedings/Proceedings.html.
[61] M. Ben Artzi, Global solutions to two-dimensional Navier-Stokes and Euler
equations, Archiv for Analysis and Rational Mechanics, (1984), 329-358.
[62] R. Mondolfo, Il pensiero Antico (1936), Spanish translation El pensamiento
antiguo, ed. Losada, Buenos Aires, 1958; W. Jaeger, The theology of the
early Greek philosophers, 1947.
[63] Yu. Gliklikh, Global Analysis in Mathematical Physics, Applied Mathematics Sciences 122, Springer Verlag, Berlin, 1997; Ordinary and Stochastic Differential Geometry as a Tool for Mathematical- Physics, Kluwer, Dordrecht,
1996; Viscous Hydrodynamics through stochastic perturbations of flow of
perfect fluids on groups of diffeomorphims, Proceedings of the Voronezh
State University (Russia), No. 1, p.83-91, 2001.
[64] S. Shkoller, Geometry and curvature of diffeomorphims groups with H s
metric and mean hydrodynamics, J. of Functional Analysis 160 (1998),
337-365.
[65] J.Marsden & S. Shkoller, The anisotropic Lagrangian averaged Euler and
Navier-Stokes equations, Archiv Rational Mechanics and Analysis (2001),
to appear; ibid. Global well-posedness for the LANS equations on bounded
domains, Phil. Trans. Royal Soc. London A (2001), to appear.
[66] D. Rapoport, Random diffeomorphisms integration of the classical NavierStokes equations, Reports on Mathematical Physics, vol. 49, No.1, 1-27
(2002).
54

Quantization in Astrophysics ...

382

[67] D. Rapoport, Martingale problem approach to the Navier-Stokes equations on smooth manifolds with smooth boundary, Random Operators and
Stochastic Eqts., vol. 11, No.2, (2003), 109-136.
[68] C. Peskin, A random walk interpretation of the incompressible NavierStokes equations, Communications in Pure and Applied Mathematics 38,
845-852 (1985).
[69] D. Rapoport, W. Rodrigues, Q. de Souza, Q. and J. Vaz, J., ”The EinsteinCartan effective geometry associated to a Dirac-Hestenes spinor field”, Algebras, Groups and Geometries, vol 14, 27, (1994).
[70] V.I.Arnold, Sur la géometrie différentielle des groupes de Lie de dimension
infinie et ses applications a l’hydrodynamique des fluides parfaits, Ann.
Inst. Fourier 16, 319-361, (1966).
[71] K.D. Elworthy, LeJan & Xue M. Li, On the Geometry of Diffusion Operators and Stochastic Flows, Springer Verlag Lecture Notes in Mathematics
1704, New York/Berlin, 2000.
[72] P. Baxendale & B.L. Rozovskii, ”Kinematic Dynamo and Intermittency”,
Geophys. Astrophys. Fluid Dynamics, vol. 73, pp.30-60, 1993.
[73] D. Stroock and S.R.S. Varadhan, Multidimensional Diffusion Processes,
Springer-Verlag, 1984.
[74] R. Temam, Infinite Dimensional Dynamical Systems in Mechanics and
Physics, Springer Verlag, New York/Berlin, 1989.
[75] S. Pope, Turbulence, Cambridge Univ. Press, 2000.
[76] L. Arnold, E.Oeljeklaus and E.Pardoux, ”Almost sure and moment stability for linear Ito equations”, in Lyapunov Exponents (eds. L. Arnold,
V.Wihstutz), Lecture Notes in Mathematics 1186, pp.129-159, Springer,
Berlin, (1986).
[77] P. Baxendale, ”Stability and equilibrium properties for stochastic flows of
diffeomorphisms”, in Diffusion processes and related problems in analysis, vol. II:Stochastic flows, (eds. M. Pinsky and V.Wihstutz), Progress in
Probability27,3-35, Birkhauser, Boston/Basel/Berlin (1992).
[78] P.Baxendale, ”Brownian motion in the diffeomorphism group I”,
Comp.Math.53, pp. 19-50, 1984.
[79] P.Baxendale, ”Gaussian measures on function spaces”, Amer.J.Math98,
pp. 892-952, 1976.
[80] P.Baxendale,” Wiener processes on manifolds of maps”, Proc. Roy. Soc.
Edinburgh, Sect. A87, pp. 127-152, 1980/81.
55

Quantization in Astrophysics ...

383

[81] P. Holmes, J. Lumley and G. Berkooz, Turbulence, Coherent Structures,
Dynamical Systems and Symmetry, Cambridge Monograph on Mechanics,
Cambridge Univ. Press, Cambridge, 1996.
[82] R. Sowers, Recent results on the short-time geometry of random heat kernels, Math. Res.Letts.1 663-675 (1994).
[83] P. Constantin and C. Foias, Navier-Stokes Equations, Chicago Lectures in
Mathematics, 1988.
[84] C. Doering and J.D. Gibbon, Applied Analysis of the Navier-Stokes Equations, Cambridge Texts in Applied Mathematics, Cambridge Univ. Press,
Cambridge (U.K.), 1995
[85] T. Taylor, On stochastic and chaotic motion, Stochastics and Stochastic
Reports43, pp. 179-197, (1993).
[86] J. Wenzelburger, A kinematic model for continuous distributions of dislocations, J. of Geometry and Physics 24, pp.334-352 (1998).
[87] M.S. Narasimhan and S. Ramanan, ”Existance of Universal Connections”,
American Journal of Mathematics 83, 1961.
[88] D. Rapoport, Realizations of the random representations of the NavierStokes equations by ordinary differential equations by ordinary differential
equations, in Instabilities and Nonequilibrium Structures VII & VIII, Nonlinear Phen.Complex Systems , Kluwer, 313-331, (2004).
[89] J.M. Bismut, Mécanique Analytique, Lecture Notes in Mathematics 866,
(Springer, Berlin,1982).
[90] D. Rapoport, On the geometry of the random representations for viscous
fluids and a remarkable pure noise representation, Rep. Math.Phys.50, no.2,
211-250 (2002).
[91] D. Rapoport, Random representations for Navier-Stokes and their realizations by ordinary differential equations, Random Operators and Stochastic
Equations11, n.4, (2003), 371-401.
[92] D. Rapoport, Foundations of Physics35, no.7, 2005.
[93] D.Rapoport, Foundations of Physics35, no.8, 2005.
[94] H. Kleinert, Path integrals in Quantum Mechanics, Statistics and Polymer
Physics,(World Scientific, Singapore, 1991).
[95] B.Dubrovin, A. Fomenko & S.Novikov, Modern Geometry-Methods
and Applications, vol.I, (Springer Verlag, Berlin, 1995).

56

Quantization in Astrophysics ...

384

[96] D. Rapoport, Stochastic Geometry Approach to the kinematic dynamo
equation of magnetohydrodynamics, in Trends in Partial Differential Equations of Mathematical-Physics, J.F. Rodrigues et al (ed.), Birkhausser,
Progress in Nonlinear Differential Equations Appl. 61, Boston, 2005.

57

Quantization in Astrophysics ...

385

A Note on Holographic Dark Energy
Gao Shan
Institute of Electronics, Chinese Academy of Sciences
LongZeYuan 24-3-501, ChangPing District, Beijing 102208, P.R.China
E-mail: rg@mail.ie.ac.cn
The unknown constant in the holographic dark energy model is determined in terms of a recent conjecture. We
find

d ≈ π /2.

The result is consistent with the present cosmological observations. The holographic dark

energy is re-explained by considering the quantum uncertainty and discreteness of space-time. We also predict that
there may exist more dark energy between two adjacent black holes.

PACS: 98.80.Bp, 98.80.Cq

Recently one kind of holographic dark energy model is used to explain the observed dark
energy [1-4]. According to the model, the dark energy density is

ρV =

3d 2 c 4
8πGLH

2

(1)

where c is the speed of light, G is the Newton gravitational constant, LH is the size of the
event horizon of our universe, d is an undetermined constant. By comparing with the
observations, it was found that the holographic model is a viable one in describing dark energy
[5-9]. Ref [7] obtained d ≈ 0.85 for the flat universe by using the supernova Ia (SN Ia) data and
the shift parameter. In addition, the value of d smaller than one or the phantom-like holographic
dark energy is also favored by the analysis results of the angular scale of the acoustic oscillation
from the BOOMERANG and WMAP data on the cosmic microwave background (CMB) [9]. In
this paper, we will theoretically determine the constant d in the holographic dark energy model
in terms of a recent conjecture [10]. The holographic dark energy will be re-explained from a
different point of views. We will also predict a new quantum effect of black holes.
According to a recent conjecture on the origin of dark energy [10], the dark energy may
originate from the quantum fluctuations of space-time limited in our universe. By using the
uncertainty principle in quantum theory, the quantum fluctuation energy of space-time of one
degree of freedom limited in our universe is

ε≈

h/2
hc
c=
2 LH
4 LH

(2)

where is L H the event horizon of our universe. Since the quantum fluctuations of space-time are
essentially nonlocal, and one degree of freedom corresponds to two Planck area units in the two
ends of the event horizon, the whole number of degrees of freedom of such fluctuations in our
universe is

Quantization in Astrophysics ...

386

2

πL
1 A
N=
= H2
2
2 4 LP
2 LP

(3)

where A is the area of event horizon, LP is the Planck length. Then the energy density of the
quantum fluctuations of space-time in our universe is

ρΛ ≈

Nε
3c 4
=
3
2
4πLH / 3 32GLH

(4)

This formula results from the quantum uncertainty and discreteness of space-time. In comparison
with the formula (1), we can get

d ≈ π / 2 ≈ 0.886

(5)

This value is in excellent agreement with the analysis result of observational data (see, for
example, [7]). By inputting the current fraction value Ω Λ ≈ 0.73 , we can work out the equation
of state:

1
2
wΛ ( z ) = − (1 +
Ω V ) ≈ −0.98 + 0.25 z + O ( z 2 )
3
d

(6)

In addition, we can also determine the current event horizon of our universe:

LH ≈

π

−1

2 ΩV

−1

H 0 c ≈ 1.04 H 0 c

(7)

This means that the current event horizon approximately satisfies the Schwarzschild relation
3

LH = 2GM / c 2 , where M = ρ c 4πLH / 3 .
It is noted that d < 1 will lead to dark energy behaving as phantom, and seems to violate
the second law of thermodynamics during the evolution phase when the event horizon shrinks.
However, the universe inside the event horizon is not an isolated system in case of the existence of
the quantum process such as the Hawking radiation. The event horizon of a black hole can shrink
due to the Hawking radiation, the event horizon of our universe can also do. Thus d < 1 does
not violate the second law of thermodynamics when considering the whole universe system. The
universe inside the event horizon and that outside the event horizon will inevitably exchange
energy and information due to the quantum process. This may also explain the non-conservation
of dark energy inside the event horizon of our universe.
It is generally believed that the holographic form of dark energy is obtained by setting the
UV and IR cutoff to saturate the holographic bound set by formation of a black hole [1-3]. Thus
the dark energy can still come from the usual vacuum zero-point energy in quantum field theory.
However, a simple calculation shows that this may be not right. The lowest frequency of the
vacuum zero-point energy limited in our universe is

E1 =

Quantization in Astrophysics ...

hc
8 LH

387

(8)

According to the holographic principle [13-15], the whole number of degrees of freedom in our
universe is

NH =

A
4 LP

2

=

πL H 2
LP

(9)

2

Then the vacuum zero-point energy density should satisfy the following inequality:

ρ VZE ≥ N H E1 =
This requires that d ≥

3πc 4
16GLH

(10)

2

2π / 2 in the holographic form of dark energy. Since the total energy in

a region of the size L should not exceed the mass of a black hole of the same size, there should

2π / 2 is also ruled out by

exist a theoretical upper bound d ≤ 1 . In addition, the result d ≥

the cosmological observations. This can be shown more directly from the equation of state:

1
2
1
2 2
w = − (1 +
Ω V ) ≥ − (1 +
ΩV )
π
3
d
3

(11)

By inputting the current fraction value Ω V ≈ 0.73 we obtain w0 ≥ −0.59 . This has been ruled
out by the observational constraint w0 < −0.75 . Thus the observed dark energy may not come
from the vacuum zero-point energy in quantum field theory. The analysis also implies that the
usual vacuum zero-point energy may not exist [16-18]. Even a holographic number of modes with
the lowest frequency will give more vacuum zero-point energy than the observed dark energy. By
comparison, the quantum fluctuations of space-time, which energy density is described by the
equation (4) and is consistent with the observations, may be the origin of dark energy. Since the
quantum fluctuations of space-time may be also called quantum-gravitational vacuum fluctuations,
the vacuum fluctuation energy still exists. It does not come from matter, but from space-time. This
may have some deep implications for a complete theory of quantum gravity.
Lastly, we will predict a new quantum effect of black holes in terms of the above analysis of
dark energy. If the quantum fluctuations of space-time limited in the event horizon of our universe
do exist, then it will also exist between two black holes. This means that there will exist more
quantum fluctuation energy or dark energy between two black holes. Consider two black holes
with the same radius R . The distance between them is L >> R . The quantum fluctuation
energy of space-time of one degree of freedom limited between them is

number of degrees of freedom of such fluctuations is N ≈

πR 2
2 LP

2

ε≈

hc
. The whole
2L

. Then the whole quantum

fluctuation energy of space-time between the black holes is

E BHV = Nε ≈

Quantization in Astrophysics ...

πhcR 2
2

4 LP L

=

πR
2L

388

E BH

(12)

where E BH is the energy of black hole. The energy density is

ρ BHV ≈

Nε
hc
≈
2
πr L 4 LP 2 L2

(13)

It is evident that the density of the quantum fluctuation energy between the black holes is much
larger than that of the observed dark energy. Such energy can be detected in the local part of the
universe such as the center of Milky Way. For example, the repulsive acceleration of an object
near one black hole is a ≈

2πc 2
r , where r is the distance between the object and the black
3L2

hole. The repulsive force equalizes the gravitational force of the black hole when r ≈ ( RL2 )1 / 3 .
In conclusion, the unknown constant in the holographic dark energy model is determined in
theory. We find d = π / 2 ≈ 0.886 . This value is perfectly consistent with the observational
data. The holographic dark energy is re-explained by considering the quantum fluctuations of
space-time. We also predict a new quantum effect of black holes. There may exist more quantum
fluctuation energy or dark energy between two black holes.

References
[1] A. Cohen, D. Kaplan and A. Nelson, Phys. Rev. Lett. 82 (1999) 4971.
[2] P. Horava and D. Minic, Phys.Rev.Lett. 85 (2000) 1610.
[3] S. Thomas, Phys. Rev. Lett. 89 (2002) 081301.
[4] M. Li, Phys. Lett. B 603 (2004)1
[5] Q. G. Huang and Y. Gong, JCAP 0408 (2004) 006.
[6] K. Enqvist and M. S. Sloth, Phys. Lett. 93, (2004) 221302; K. Enqvist, S. Hannestad and M. S.
Sloth, astro-ph/0409275.
[7] Y. G. Gong, B. Wang and Y. Z. Zhang, hep-th/0412218
[8] J. Y. Shen, B. Wang, E. Abdalla and R. K. Su, hep-th/0412227
[9] H. C. Kao, W. L. Lee and F. L. Lin, astro-ph/0501487
[10] S. Gao, Chin. Phys. Lett. 22 (2005) 783
[11] L. J. Garay, Int. J. Mod. Phys A10 (1995) 145-166.
[12] R. J. Adler and D. I. Santiago, Mod. Phys. Lett. A 14 (1999) 1371
[13] J. D. Bekenstein, Phys. Rev. D 23 (1981) 287
[14] G. ’t Hooft, Salamfest 1993:0284-296, gr-qc/9310026
[15] L. Susskind, J. Math. Phys. 36 (1995) 6377.
[16] S. Gao, Quantum Motion and Superluminal Communication (Chinese Broadcasting &
Television Publishing House, Beijing, 2000)
[17] S. Gao, Quantum Motion: Unveiling the Mysterious Quantum World (abramis academic,
London, 2006).
[18] S. E. Rugh and H. Zinkernagel, Studies in History and Philosophy of Modern Physics 33
(2002) 663

Quantization in Astrophysics ...

389

Hypergeometrical Universe
*Marco A. Pereira, ny2292000@yahoo.com
Citigroup, 390 Greenwich Street, New York, NY 10013, USA
This paper presents a simple and purely geometrical Grand Unification Theory. Quantum Gravity,
Electrostatic and Magnetic interactions are shown in a unified framework. Biot-Savart Law is
derived from first principles. Unification symmetry is defined for all the existing forces. A 4D
Shock-Wave Hyperspherical topology is proposed for the Universe together with a Quantum
Lagrangian Principle resulting in a quantized stepwise expansion for the whole Universe along a
radial direction in a 4D spatial manifold. The hypergeometrical standard model for matter is
presented.

1 Introduction
Grand Unification Theories are the subject of intense research. Among current theories,
Superstring, M-Theory, Kaluza-Klein based 5D Gauge Theories have shown diverse degrees of
success. All theories try to keep the current conceptual framework of science. Kaluza-Klein
melded both Electromagnetism and Einstein Gravitational equations in a 5D metric.
Here is presented a theory that departs radically from other theories and tries to bridge the
conceptual gap as opposed to explore the formalism gap. Most research is concerned on how to
express some view of Nature in a mathematically elegant formalism while keeping what we
already know. It has been said that for a theory to be correct, it has to be beautiful.
This work concentrates on what to say, the conceptual framework of Nature instead. All the
constructs of science, matter, charge, and energy are dropped in favor of just dilator positions and
dilaton fields, which are metric modulators and traveling modulations, respectively. There is no
concept of charges or mass. Mass is modeled a quantity proportional to the 4D displacement
volume at precise phases de Broglie cycles. Charge sign is modeled by dilaton phase (sign) on
those specific phases. This mapping is not necessary for calibration; there are no calibration
parameters in this theory. The mapping is needed to show that the geometrical framework
replicates current scientific knowledge.
We propose that dilators are the basic model of matter. They are coherences between two states
in a rotating four-dimensional double well potential. A single coherence between two 4D-space
deformation states or fundamental dilator is considered to account of all the constituents of nonexotic matter (elements, neutrons, electrons and protons and their antimatter counterparties). This
coherence is between two deformation states with 4D volumes corresponding to the electron and
proton, or electron-proton coherence. Here the proton and the electron are considered to be
the same particle or the fundamental dilator, just two faces of the same coin.
The equation that describes these states is not the subject of this work. In section 2.9, a detailed
description of the fundamental dilator is given, as well as the origin of the spin quantization.
Dilaton are the 5D spacetime waves, traveling metric modulations, created by the alternating
(back and forth) motion of the fundamental dilator from one state of the double potential well to
the other. Since these two states have different displacement volumes, spacetime waves are
created. Displacement volumes are the missing (extra) volume due to spacetime contraction
(dilation). Let’s say that one has two points separated by a distance L in a 4D space with a dilator
in the middle. The distance between those two points will change depending upon the phase of
dilaton. If one considers this maximum distance change along the four dimensions for each of the
two states, would be able to determine the dilator volume on each state and thus fully characterize
it.

* ny2292000@yahoo.com
Quantization in Astrophysics ...

1
391

In addition to tunneling back and forth, the proton-electron dilator is considered to be tumbling
(spinning) as it propagates radially (along the radial expansion direction) and that poses a
constraint on the spinning frequency. Spin half particles are modeled as having a spinning
frequency equal to half the electron-proton dilator tunneling frequency. Similarly higher spin
particle coherences, e.g. spin N, are modeled as having a spinning frequency equal to N times the
electron-proton dilator tunneling frequency.
Whenever the word dilator is mentioned within this paper, it will refer to the fundamental dilator
or fundamental coherence, although there are other coherences in nature and similarly associated
particle pairs.
A 3D projection of this volume corresponds to the perceived 3D mass, a familiar concept.
A logical framework is proposed on the Hypergeometrical Universe1 section. This model
conceptualize the 3D universe manifold as being a 3D shock wave universe traveling at the speed
of light in a direction perpendicular to itself, along the radial direction.
Absolute time, absolute 4D Cartesian space manifold are proposed without loss of time and space
relativism. Thus there are both preferential direction in space and preferential time, but they are
both non-observables.
On the cosmological coherence section, the consequences of the topology of the
hypergeometrical universe and the homogeneity proposed in the Hypergeometrical Standard
Model is shown to result in a cosmological coherence, that is, the whole 3D universe expands
radially at light speed and in de Broglie (Compton) steps.
When cosmological coherence is mentioned it is within the framework of absolute time and
absolute 4D space.
A new Quantum Lagrangian Principle (QLP) is created to describe the interaction of dilators and
dilatons. Quantum gravity, electrostatics and magnetism laws are derived subsequently as the
result of simple constructive interference of five-dimensional spacetime waves2 overlaid on an
expanding hyperspherical universe described in section 3. In the electrostatics and magnetism
derivation, a one atomic mass unit (a.m.u) electron or fat electron is used. This means that the
dilatons being 5D spacetime waves driven by coherent metric modulations are sensitive to both
sides of the dilator coherence.
Since 3D mass – the mass of an electron or proton from the 3D universe manifold perspective - is
sensitive only to one side of the dilator coherence, the side of the dilator in phase with the 3D
shock wave universe, a pseudo time-quantization is proposed in section 2.9.
Appendix A contains a brief description of the Hypergeometrical Standard Model. It shows that
hyperons and the elements are modeled as longer coherences of tumbling 4D deformations.
Nuclear energy is proposed to be stored on sub-coherence local twisting of the fabric of space. A
more detailed description of the model will be presented elsewhere.
A grand unification theory is a far-reaching theory and touches many areas of knowledge.
Arguments supporting this kind of theory have by definition to be equally scattered. Many
arguments will be presented with little discussion when they are immediate conclusions of the
topology or simple logic.

* ny2292000@yahoo.com
Quantization in Astrophysics ...

2
392

2 Hyperspherical Universe
2.1 Quantum Lagrangian Principle
A new Quantum Lagrangian Principle (QLP) is defined in terms of dilator and dilaton fields. It
proposes that the dilator is always in phase with the surrounding dilatons at multiples of 2π
wavelength. This simply means that a dilator, trying to change the metric in a specific region of
4D space, will always do that in phase with all the other dilators. The fundamental dilaton
wavelength will be called de Broglie wavelength and will be shown in the section 2.9 to
correspond to the Compton wavelength, since motion along the radial direction is at lightspeed, of
a one atomic mass unit particle.

2.2 Topology
The picture shown in Figure 1 represents a cross section of the hyperspherical light speed
expanding universe. The universe is considered to be created by an explosion, but not by a threedimensional explosion. Instead, it is considered the result of a four-dimensional explosion. The
evolution of a three-dimensional explosion is an expanding two-dimensional surface. The
evolution of a four-dimensional explosion is an expanding three-dimensional hypersurface
on quantized de Broglie steps. The steps have length equal to the Compton wavelength
associated with the fundamental dilator (one atomic mass unit). All times are made
dimensional by the multiplication by the speed of light.

Figure 1. Shows the cross-section Xτ and XR for the expanding universe. The universe length along X is represented
by the band. X (or Y or Z) is displayed along the perimeter of the circle. Also shown in the diagram is Φ (cosmological
time) and radial time projection R.

Definitions:
•

Cosmological time Φ represents an absolute time frame, as envisioned by Newton and
Mach - it is a fifth dimension in the hypergeometrical universe model.

* ny2292000@yahoo.com
Quantization in Astrophysics ...

3
393

•

The radial direction is a preferential direction in 4D space. It is the radial expansion
direction. This direction doubles as a direction on 4D Space and a projection of the
cosmological time. Since they are related by the expansion speed (light speed), one can
think about the radial direction as the radial time – an absolute time projection.

•

Similarly, τ is any other propagation direction and also a projection of proper time, here
called dimensional time. For small velocities with respect to the fabric of space (see
description below), the dimensional time approximately matches the 4D direction of
propagation (artan (v/c)~arctanh(v/c)~v/c). This maps our local frame proper time to a
4D direction of propagation and it is the source of the relativism in the theory of
relativity. Different angles of propagation reflect different relative velocities. Notice that
although this argument made use of a preferential 4D direction, it could be done using
any possible referential frame. Within the 3D space, one can only observe the relative
angle and relative velocity.

•

This mapping done because of the consideration that a Lorentz transformation can be
thought as a rotation around the directions perpendicular to proper time and velocity by
an imaginary angle of arctanh (v/c). On a 4D spacetime, when one considers a proper
frame of reference, one only travels in time. The addition of a fourth spatial dimension
means also that when one is in one’s proper 3D frame of reference one is also
propagating along either the directions R/Φ or r/τ.

•

R keeps a simple relationship with the dimensionalized cosmological time Φ (identical
module relationship).

•

The fabric of space (FS) is just the region of 4D space – a traveling boundary- where the
3D hypersurface (shock wave universe) stands at any given time. It is different from the
rest of the 4D space because it contains imprinted in local deformations, all particles in
the universe.

•

Fabric of space is used in two manners: a) as the locally non-twisted 4D space – pointing
to this traveling boundary-, where local proper time projection τ and direction of 4D
propagation points in the radial direction Φ/ R and b) the subject of deformation.

•

Under these conditions one can define a referential frame that is standing still with
respect to the FS while traveling at the speed of light outwards radially. This is a
preferred referential frame. Two preferred referential frames far apart in 3D space will
recede from each other at the Hubble’s speed (see section 2.5).

•

After the shock wave universe passes through, the 4D space returns to it relaxed
condition.

•

There are two kinds of deformations in 4D space: compression and torsion. The
compression is what happens in dilators or particles. They represent coherence between
two compression deformation states.

•

Torsional states are related to absolute state of motion of neutral matter and are defined
by the local tilt of the perpendicular to the FS region inhabited by it. The FS can be
under torsional forces in the region near dilators. The region where a dilator exists will
persist under stress (tilted) as the dilator moves towards a region where that stress can be
relaxed.

•

Far from matter, there should be only residual torsional deformation due to the
evanescing dilatons. On the other hand, the region of space where a “zero spin particle”
or neutral matter exists, the local environment is permanently deformed through

* ny2292000@yahoo.com
Quantization in Astrophysics ...

4
394

interactions with other bodies’ dilatons. Deformation will last until the all the interacting
bodies reach regions where their relative velocity matches the Hubble velocity of that
part of the Fabric of Space.
•

The angle between R (Φ) and r (τ) defines the local FS deformation.

•

The angle between τ’ (r’) and τ (r) defines the relative degree of local FS deformation.

•

“Volumetric” and “superficial” dilatons are 5D and 4D spacetime waves define in
analogy to volumetric and superficial sound waves. Instead of having pressure or density
modulations as in sound waves, one has metric (or 4D space) modulations.

•

Since the hypersurface is our three-dimensional universe, a “superficial” dilaton is a
spacetime disturbance that propagates along the FS. Associated zero spin dilators will
propagate always in perpendicular to the FS, although they might move sidewise between
de Broglie expansion steps.

•

A “volumetric” spacetime dilaton is free to redirect its k-vector on any direction.
Associated non-zero spin dilators will be able to freely change their propagation direction
in addition to the sidewise motions at each de Broglie expansion step.

•

Dilatons and dilators are used interchangeably in certain situations since the QLP requires
the dilatons to always be in phase (surf) the surrounding dilaton field.

Figure 1 displays one time dimension (Φ) and three time projections (R, τ and τ’). Each reference
frame has its own proper time projection. This figure also shows that the four-dimensional
spacetime is curved, being the radius of curvature given by the dimensional age of the universe.
This simple figure eliminates the need for cosmological constant questions, considerations about
gravitational collapse or anti-gravitational acceleration of the expansion of the three-dimensional
universe, since the universe is proposed to be four-dimensional plus a cosmological time Φ. In
this model, the shock wave hyperspherical universe is clearly finite, circular (radius of curvature
equal to the dimensional age of the universe, that is, the speed of light times the age of the
universe). It is also impossible to traverse, since it is expanding at the speed of light. The
Cosmic Microwave Background is assigned to a Doppler shifted view of the initial Gamma
Radiation Burst3.

2.3 Origins of the Hyperspherical Expansion
The clues for the creation of this models lies on relativity and quantum mechanics. Relativity
states that the energy of a particle with rest mass m0 and momentum p is given by:

E = mc2 = c p 2 + m 2c 2
0

(2.1)

where m is the mass in motion. This equation has implicit assumptions which can be brought into
light by considering it a momentum conservation equation instead:

P 2 =  mc 2 = p 2 + m 2c2
0

(2.2)

Where P is the four-momentum of the particle in motion (at the speed of light) traveling such that
its τparticle makes angle α with the static reference frame τObserver. Implicit in equation (2.2) is that
the particle is actually traveling along a four-dimensional space (timed by a fifth time dimension)
and has two linear momentum components:
a) Three-dimensional momentum p

* ny2292000@yahoo.com
Quantization in Astrophysics ...

5
395

b) Perpendicular momentum m0c in the direction of radial time.
In addition, the particle travels at the speed of light in along a hypotenuse with an inertial mass m.
Now it starts to become clear that the motion of the particle is actually in a five dimensional space
(four physical dimensions and a time) and at the speed of light, being the three dimension motion
just a drift. The trigonometric functions associated with a relativistic Lorentz transformation are
given in terms of velocity by:

cosh(α ) ==

sinh(α ) ==

1

(2.3)

v2
1− 2
c
v
c

(2.4)

v2
1− 2
c

tanh(α ) = β =

v
c

(2.5)

Manipulating equation (2.2) and using m=m cosh(
α) one obtains:
0
(2.6)
 mc 2 = (mv )2 + m 2c2


0
(2.7)
 m cosh(α )c 2 = (m v cosh(α ))2 + m 2c2
0
 0

0
(2.8)
 m cosh(α )c 2 = (m sinh(α )c )2 + m 2c2
0
 0

0
2
2
2
(2.9)
 1  1   1 
  = 
 − 

 λτ   λ x Pr me   λτ Pr ime 
With
1 m0 c de Broglie wavelength for the particle on its own reference frame, traveling at the
=
speed of light in the proper time projection τ direction.

λτ

h

1

=

1

λτ Pr ime λτ
1

λ x Pr ime

=

1

λτ

cosh(α )

Projection on the τprime direction.

sinh(α )

Projection on the xprime direction.

Equation (2.9) is the basic equation for the quantization of relativity. It describes the motion of a
particle as the interaction of two waves along proper time projection and three-dimensional space.
The λτ Pr ime , that is, the projection on the τ’ axis of the wave propagating along the τ axis (resting
reference frame) is given by:

λτ

λτ Pr ime

= cosh(α )

(2.10)

* ny2292000@yahoo.com
Quantization in Astrophysics ...

6
396

λτ

λx Pr ime

= sinh(α )

(2.11)

This means that the projected de Broglie time-traveling wavelength is zero when the relative
velocity reaches the speed of light. Zero wavelengths means infinite energy is required to twist
spacetime further. The rate of spacetime twisting with respect to proper time relates to the power
needed to accelerate the particle to a given speed. From equation (2.5), acceleration in the
moving reference frame can be calculated to be:

AccelerationPr ime = c 2

d tanh(α )
dτ Pr ime

(2.12)

In the particle reference frame the acceleration has to be given by Newton’s second law

Force = M 0 AccelerationPr ime = M 0c 2

d tanh(α )
dτ Pr ime

(2.13)

This means that any force locally twists spacetime, and not only gravitation as it is considered in
general relativity. It also shows that as the relative speed between the two reference frames
increases towards the speed of light, the required force to accelerate the particle approaches
infinite. The same reasoning can be done for the concomitant rotation perpendicular to RX,
resulting in the replacing the minus sign by a plus sign on equation (2.9) and the recasting
equations (2.10) and (2.11) in terms of trigonometric functions as opposed to hyperbolic
functions. Rotations around τX or RX result in a real angle α= arctan(v/c).
Figure 2 below displays the particle as a de Broglie wave oscillating as a function of
cosmological time Φ, propagating along R. This is a proper reference frame plot, that is, the
particle is at rest at the origin with respect to the fabric of space and only travels along the radial
time direction R.
Figure 2. This model shows a de Broglie oscillation as a
function of Cosmological Time Φ using the proxy of time R.

The diagram below represents the same observation
from a moving frame of reference (relative velocity
c times tan(α)):

* ny2292000@yahoo.com
Quantization in Astrophysics ...

7
397

Figure 3. Projection of de Broglie Wave in the moving frame of reference.

Energy Conservation of de Broglie Waves:
The total kinetic energy, calculated in terms of de Broglie momenta, is equal to the Relativistic
Total Energy value of a free particle. The total energy is M0c2 in the proper reference frame and
equal to:
1
E=
M0

 h  2  h  2  h 2  cosh(α )  2  sinh(α )  2 
h2
  =
 − 
  =
 − 


= M 0c2
2
 λ x Pr ime   λt Pr ime   M 0  λt   λt   M 0 λ τ

(2.14)

in the moving referential frame.

Phase Matched de Broglie Wave Interpretation of a Particle
Let consider a particle as a de Broglie wave. In its own referential, it just propagates in the
direction of radial time R, as in figure 2. On a moving reference frame, shown in figure 3, the de
Broglie wave is decomposed in two:

1

•

One with wavelength

•

A second with wavelength

λ x Pr ime

=

cosh(α )

1

λτ Pr ime

λτ
=

propagating along x

sinh(α )

λτ

propagating along τ.

Their nonlinear interaction results in:

ψ 1 ( x,τ ) = cos(

2π

λe

2π
x cosh(α )) cosh( τ sinh(α ))

(2.15)

λe

* ny2292000@yahoo.com
Quantization in Astrophysics ...

8
398

1
2π
1
2π
ψ 1 ( x,τ ) = cosh( ( x cosh(α ) −τ sinh(α ))) + cos( ( x cosh(α ) +τ sinh(α )))
λe
λe
2
2

or two waves propagating in the direction of α and –α with wavelength equal to

(2.16)

λe
cosh(α )

. Thus

a particle can be described as a phase matched wave propagating along its dimensional time
direction as the hyperspherical universe expands as a function of cosmological time.

2.4 The Meaning of Inertia
From equation (2.12) it is clear that inertia is a measure of the spring constant of spacetime, that
is, how difficult it is to twist spacetime. Any change in the state of motion also changes the
direction as referred to the absolute referential frame RΦ, which means that inertia is also a
measure of how difficult it is to locally twist the fabric of space.
Notice that Newton’s first equation (equation 2.13) can be recast as an equation stating that the
strain on the fabric of space is the same on both projections shown in figure 1.

d tanh(α )

dτ

τ =

d tan(α )
R since tanh(α ) = tan(α ) = v and r and τ are numerically
R
τ
dr
c

identical.
To obtain the force (stress) needed to create such a strain, one needs to multiply the strain by the
area subject to it. In the 4D hyperspherical paradigm, this means that the mass is proportional to
the 3D projection of the 4D displacement volume associated with the objects (particles).
This identity is used thoroughly during the grand unification calculations on section 3.

2.5 Why do things move?
The relaxation of a fabric of space deformation is considered within this theory to be the cause of
inertial motion. Two objects would act upon each other and then distance themselves until their
interaction is vanishingly small. Under those conditions their distance would grow until they
reach their Hubble equilibrium position, that is:

v = C Hubble * L HubbleEqui librium
LHubbleEquilibrium =

(2.17)

v * 4 DRadiusOfTheUniverse
c

(2.18)

Where it is clear that the Hubble constant is given by:

C Hubble =

c
4 DRadiusOfTheUniverse

(2.19)

The 4D radius of the universe is shown in Figure 1, and it is equal to the age of the universe times
c. At that point, 4D space would be relaxed and their distance would grow governed by the
universe expansion. Even though matter would be standing still with respect to the FS, their
relative motion would continue at the Hubble speed forever. The fraction of the universe that is
relaxed at any given time and direction can be measured from the distribution of Hubble
constants. The narrower the distribution of Hubble constants from a given region of the
Universe, the more relaxed that region is. Needless to say, this is the underlying reason for
Newton’s first law. The proposed topology implies that the Big Bang occurred on all points of
the shock wave universe (or the currently known 3D Universe) at the same time. Since matter is
considered to have rushed away from each and every point of the 3D universe in a spherically

* ny2292000@yahoo.com
Quantization in Astrophysics ...

9
399

symmetric manner, the Hubble constant has to be a constant for the average. Other cosmological
implications will be discussed in a companion paper.
Equation (2.16) might seem obvious but it is not. There are questions about why the Hubble
constant is not constant. In this theory it is clear that the Hubble constant relates to the average
velocity in a given region of space and thus it should not be a constant applicable to each and
every observation.

2.6 Why is the Speed of Light the Limiting Speed
In this model, in a de Broglie universe expansion step, the furthest a dilator can move is one de
Broglie wavelength sidewise, that is, along the spatial direction (see Figure 1). That would result
in a 45 degrees angle with respect with R.
The proposition of this theory is that this is the real reason for Lorentz transformations asymptotic
behavior and that inertia is really a measure of the difficulty to bend local 4D space. In section 2.3
it became clear that they are the same rotation, driven by the change in velocity.
From Hubble considerations and from examining Figure 1, it is clear that the maximum absolute
speed is π*c, but cannot be measured because one can never see or reach anything beyond one
radian in the shock wave universe.

2.7 Hypergeometrical Standard Model
A new model for matter is proposed. In this initial model, the elements, protons, electrons and
neutrons and their antimatter counterparties are recast as being derived from a single particle.
This particle is expressed in geometrical terms as being a coherence between two 4D deformation
stationary states from a rotating 4D double potential well. This coherence is called a dilator. As
the dilator oscillates between sides of the potential well, it creates a traveling modulation of the
metric or 5D spacetime waves or dilatons. Spin is modeled not as an intrinsic degree of freedom,
but as an extrinsic tumbling or rotation of the dilator. Since the dilaton frequency is defined
just by the gap between the fundamental dilator states, it frequency does not depends upon
the mass of the dilator. Dilatons travel through the 4D space. 3D projections are known as de
Broglie matter waves. This is an important concept since a corollary is that a monochromatic
(same velocity) flow of electrons will produce a coherence dilaton field, superimposed on the
dilaton random black body background. This will be used to explain the double slit experiment in
the Conclusions section. Planck’s constant is the connection between the 3D dilaton projection
wavelength and the particle 3D linear momentum. Planck’s constant ensures that for the 3D
observed mass and velocity, the de Broglie wavelength will match it fundamental dilaton 3D
projection. Mass is considered to be proportional to dilator maximum 4D volume. Calibration is
made to replicate Gauss’ electrostatics law, Newton’s gravitational law and Biot-Savart law of
magnetism. Since mass is proportional to a 4D volume and volume depends upon lengths, which
are Lorentz invariant , the 4D-mass volume representation is also Lorentz invariant. 4D-mass is
defined as being the total mass or 4D volume displaced in an oscillation cycle. Since the dilator
oscillates between states corresponding to an electron and a proton, its 4D mass will be one
atomic mass unit. 3D-mass is the mass or 4D displacement volume perceived in the 3D Space at
given phases of the de Broglie expansion.
Dilators with spin zero are modeled to couple with superficial wave, and thus their position
changes from one de Broglie cycle to the next just by the displacement governed by a new
quantum Lagrangian principle. Its propagation direction continues to be perpendicular to the 3D
universe hypersurface. Dilators with non-zero spin are modeled to couple with volumetric wave,
and their position changes from one de Broglie cycle to the next just by the displacement
governed by the Lagrangian principle. In addition, its propagation direction is redirected by the

* ny2292000@yahoo.com
Quantization in Astrophysics ...

10
400

angle covered by this transition. Since the change in angle is defined with respect to the last step
k-vector, charged particles are to sense a much higher acceleration than zero spin particles
(matter). This is the proposed reason behind the strength difference between gravitation
and electromagnetic Forces.

2.8 Cosmological Coherence
The coordinated actions of dilators implicit in the proposed Lagrangian principle mean that even
though the dilator is a 5D spacetime wave generator it behaves as a wave, thus implicitly
replicating wave behavior. Its position is determined at each de Broglie step according to the
local dilaton environment.
The concept of 4D spacetime deformation coherences generating waves is created in analogy
with electromagnetic waves being created by electronic coherences. In the case of spacetime
coherences, the coherences for the fundamental dilator (proton-electron dilator) are never
dephased. Dephasing would result in proton or electron decay or disappearance. The states
corresponding to the proton and to the electron are considered to be the ground states for each one
of the two wells, thus they cannot decay further, only dephase.
All matter in gravitational and electromagnetism studies here modeled are composed of protons,
electrons and neutrons, thus are composed of this fundamental dilator. Although current
understanding of charged particles associates with them a gravitational mass, their gravitational
field could never be measured. If it were to exist, their electric field would be 1036 times larger
than their gravitational field.
In this model, charged particles have no gravitational field, since in this model there is only one
kind of interaction and two kinds of responses.
The Quantum Lagrangian Principle means that all matter, charged or not, is synchronized with
the surrounding dilatons, thus generating a cosmological coherence.
This idea of a cosmological or macroscopic coherence might seem unexpected but it is built-in in
the concept of field. Fields are constructs derived from electromagnetism and gravitation
equations. In a purely geometrical theory, which has been the goal of many scientists and
philosophers for thousands of years, there should be only a few constructs: space, space wave
(metric modulations), and local and global symmetry rules (angular and linear momentum
conservation) adapted to the appropriate constructs. There shouldn’t be mass or charge in a purely
geometrical theory, only displacement volume and phase. Returning to the concept of fields,
when one consider gravitation/electrostatics to be an extensive properties of mass/charge, one is
implicitly adding the corresponding wave amplitudes within an implicit geometrical theory
without regard to their phases, that is, fields imply coherences. This is a fine point that has been
missed since nobody planned to eliminate the concept of mass and charge to build a geometrical
theory. Einstein’s gravitation theory used mass to deform spacetime. Kaluza-Klein also used
mass to deform spacetime and created a compact dimension to store the electromagnetic fields. In
this theory, coherent dilatons controls dilators motions in a mutually consistent cosmic
symphony.
Figure 3 displays two inertial systems with the same origin. System with distinct origins would
have an additional phase-shift due to the retarded potential interaction. This is the reason why all
the waves in a multi-particle body can have their amplitudes added together, as opposed to having
their amplitudes averaged out to zero due to a random phase relationship. It shows that a particle
state of motion does not modify its phase relationship with the expanding hypersurface (3D
Universe). The particle is always phase matched to the rest of the universe. This is the meaning
of physical existence. Our concept of existence is based on interaction. If a particle had a de

* ny2292000@yahoo.com
Quantization in Astrophysics ...

11
401

Broglie wavelength different from the one of the fundamental dilator, its interaction would
average out to nothing. No interaction means no material existence. A neutrino is an almost
perfect example of this pattern – it still interacts a little. The phase matching condition implies
that the entire universe is in phase (lived the same number of de Broglie cycles) as it propagates
along the radial direction R. This also means that the universe is thin along the radial direction of
propagation (much less than one de Broglie wavelength thin).
The number of de Broglie cycles a particle passes through is independent upon the angle α
(relative velocity). This means that any dilator of a given type is always in phase with
another of the same type, irrespectively of its trajectory through the universe. It also means
that protons, electrons, neutrons created in the dawn of the universe kept the same phase
relationship with all the other protons, electrons and neutron of the universe throughout the
ages. The same is true for any particle created at any time. De Broglie wavelength, phase
and intensity are properties shared by particle classes.
This coherence is essential in creating a quantum gravity theory and it is essential to the
hypergeometrical theory. In fact, cosmological coherence is a hypothesis and a corollary of the
hypergeometrical theory, because one could not construct a geometrical theory without a
cosmological coherence due to the extensive property of gravitational, electrostatic and magnetic
fields.

2.9 Quantization of Time and the Fat Electron
The theory makes use of a fat electron, that is, a one atomic mass unit electron in the derivations
on sections 3.1 and 3.2.
The derivations are done in the 5D spacetime and yield the acceleration for a single particle
subject to one kilogram mass or to one kilogram of charge. Notice that acceleration is not force.
To obtain a force, which is a 3D concept, one has to multiply by a 3D mass. To understand why
one would use a one atomic mass unit electron, and what are the 3D and 4D masses, one has to
see the process in 4D. First one needs to understand neutron decay to have some representation of
the electron and proton 4-D deformational states.
The hypergeometrical standard model for the neutron decay process is shown next:
neutron -> proton + electron + anti-neutrino
Where the 4D deformation states are given:
(2/3,-1/3,-1/3) -> (2/3, 2/3,-1/3) + (0,-2/3,-1/3) + (0,-1/3, 1/3)

(2.20)

respectively.
Conversely:
proton + electron -> neutron + neutrino
(2/3, 2/3,-1/3) + (0,-2/3,-1/3) -> (2/3,-1/3,-1/3) + (0,1/3, -1/3)

(2.21)

The representation of the neutron decay is presented here just to showcase how one thinks about
nuclear chemistry in the hypergeometrical universe framework. The “quark” numbers are not
meant to be considered the quark composition of the particles. It is an equation of 4D volume
conservation and the numbers represent the three axis lengths of a 4D ellipsoid of revolution.
Negative numbers just means that they have opposing phases. The total 4D volume of all
particles in the universe should add up to zero. Any particle can be described through these types
of equations and that will be discussed elsewhere. Notice that no number was given to the fourth
dimension. There is no mentioning of the residual length of the fourth coordinate for simplicity,

* ny2292000@yahoo.com
Quantization in Astrophysics ...

12
402

but it is certainly smaller than the others, thus the resulting skinny profile when the dilator is
rotated by 90 degrees. This assignment was done considering the lowest 4D volumes or lowest
numbers, the number ONE can be decomposed for representing the nuclear reaction (neutron
decay).
This is clearly unorthodox, since the electron is not supposed to have a quark composition.

Figure 4. The figure above show an electron-proton dilator as it tumbles during two de Broglie wavelength universe
expansion, with the two possible initial phases. The left (right) scheme corresponds to an electron (proton).

The red dot indicates that the coherence is on the proton side (2/3, 2/3,-1/3), while the green
rectangle indicates that the coherence is on the electron side (0,-2/3,-1/3). Spin has been modeled
as an extrinsic rotation perpendicular to RX. Spin half (N) means that the dilator performs half
(N) rotational cycle for each de Broglie expansion step. Notice that the representation of spin as a
4D rotation is distinct from orbital momentum L and total Momentum J. This is a fourdimensional space theory and one has to have angular momentum conservation in four
dimensions, thus the rules for total angular momentum conservation are valid.
Orbital momentum L and total angular momentum J are 3D concepts and will result from the
projection of the equations of motion solution on the 3D hypersurface. Quantum mechanics
replication is outside the scope of this paper. The behavior required by the quantum Lagrangian
principle has the similar traits to the Bohr model. If one considers that in the prescribed QLP 4D
trajectories, the electron riding the 4D dilaton wave will also ride its 3D projection -the
corresponding de Broglie matter wave- then it becomes clear that QLP will immediately
reproduce the Bohr hydrogen model and more.

2.9.1 3D and 4D Masses
Now one can define the 3D and 4D masses. From Figure 4, it is clear that what distinguishes an
electron from a proton is a rotational (spin) phase. This means that our 3D interactions (material
existence) support a pseudo time-quantization or intermittent interaction on quantized time steps.
Thus 3D masses are the masses one observes at de Broglie expansion phases 0, 2π, 4π…. It is
worthwhile to notice that on the de Broglie expansion phases π, 3π, 5π.. (when the dilator
character changed totally and the 4D volume reaches a maximum) the 3D projection is minimal.
In the case of an electron, the de Broglie expansion phase π corresponds to a skinny or laying
down proton (nothing to grab). In this work, we are not presenting the equations of motion of the
4D tunneling rotor, since they are not necessary for the understanding of the physical model.
They are not needed either for the proposed grand unification theory. One only needs to know
that 4D volumes are associated with electrons and protons and that electrons and protons are the
two sides of the tunneling system. One also has to keep in mind that the 3D projection of this 4D
volume is proportional to the corresponding 3D mass or simply particle mass. The derivation of

* ny2292000@yahoo.com
Quantization in Astrophysics ...

13
403

3D volumes from 4D volumes is done through simple local Cartesian projections, thus are trivial.
The detailed shape of these states also doesn’t matter for this discussion. One only needs to know
that the thickness (radial dimension thickness) of these states is much smaller than the de Broglie
(Compton) wavelength of the dilator (one a.m.u. particle) to understand that when the tunneling
rotor (dilator) is rotated by 90 degrees, it should show a much smaller volume from the
perspective of the 3D universe.
What would be the meaning of a 4D Mass?
Remember that in a geometrical theory, mass has to be related to a 4D displacement volume.
From the point of view of four-dimensional waves being generated by coherently located dilators,
it doesn’t matter if the proton or electron are standing up or laying down, the 4D volume is the
same. This means that the 4D mass of an electron is equal to the 4D mass of a proton approximately one atomic mass unit. One atomic mass unit corresponds to a standing-up proton
and a standing-up electron which is the exactly the mass of a hydrogen atom. This is an
approximation because of the relativistic shrinking of volume as a function of relative motion.
The correction factor should be, in terms of 4D volumes, equal to (standing-up electron + layingdown proton)/(standing-up electron + standing-up proton). In other words, a standing-up electron
(proton) has a different perceived 4D volume than a laying-down electron (proton). This
correction factor should be related to the electron gyromagnetic ratio. From there, one should be
able to derive an instantaneous tangential speed. For sake of completeness, I will present briefly
the other relevant particles:

Figure 5. The left (right) scheme corresponds to a spin-plus half electron (spin-zero neutron).

A zero spin neutron is just a combination of the two dilator rotational states such that the total
angular momentum and charge is zero. Its physical meaning is a transition state in the nuclear
chemistry reaction described in equation (2.21). It is also, the closest representation to the
archetypical gravitational particle. In this theory, electrons and protons join to form neutral
matter. Neutral, zero spin matter is the matter that shows only gravitation. Figure 5 shows how
the rotating dilator would interact to create this state. It is shown to showcase reasoning behind
Hund’s law. Hund’s law states that particles should form pairs of zero spin, that is, one electron
of spin +0.5 joins another with spin -0.5 in and electronic orbital. This is supposed to have a
lower energy than if you put each electron by itself in each orbital. Figure 6 shows a spin -0.5

* ny2292000@yahoo.com
Quantization in Astrophysics ...

14
404

neutron, the only one observed in nature. It seems that the neutron as a whole is rotating, but this
corresponds to a combination of a spin +0.5 proton with a spin +0.5 electron. Since this
combination is the only one observed in nature, it should mean that it has higher energy or higher
4D volume. Remember that one is considering the reaction in reverse, that is, one is considering
adding one electron to a proton to yield a neutron and a neutrino- an endothermic reaction.
The difference in energy or conversely 4D volume is the emitted neutrino described in equation
(2.21). The neutrino also carries the extra angular momentum.
Figure 6. The figure above shows an electron-proton dilator
dimmer (neutron) as it tumbles during a two-de-Brogliewavelength universe expansion. The scheme corresponds to
spin-minus half neutron.

If one adds a Spin -0.5 proton to a Spin +0.5 electron
you get a Spin 0 neutron, which subsequently decays
and emits a neutrino. The neutrino corresponds to
the transition from spin-zero to a spin-half neutron.
Since one would expect that the change in volume
between the two states to be minimal, the neutrino is
expected to have a lower tunneling frequency and
thus being non-interacting, in fact, very little
interaction, massless, chargeless and thus
immaterial.
Normally, one only speaks of spin projections along
the 3D space directions. Since one has a physical model of a tumbling 4D dilator in a 4D space
one has to define a direction of motion. Negative and positive spins are assigned to clockwise and
counterclockwise rotations to keep angular momentum conservation in the 4D space.
Antiparticles differ from their counterparties just by a 180 degrees dilation phase shift. Other
nuclear chemistry reactions should have similar representation being the only difference the
dilator coherence.

2.10 The Meaning of a Charge
From section 2.9 it becomes obvious what is the meaning of a charge. It is only the in-phase sign
of the dilation. A proton is positive because it is dilated as a proton – it has proton 3D mass or
proton 4D Volume, when observed by the shock wave universe. An anti-proton would have the
same 3D mass but the 4D displacement volume would be negative, that is, the modulation in
metric had the opposite effect on 4D Space. The difference in 4D Volume on specific phases is
why a proton and an electron do not annihilate each other, as do a proton and an anti-proton.

3 Force Unification
3.1 Quantum Gravity and Electrostatic Interaction
Let’s consider a body and a particle interacting through their four-dimensional waves. The body
will always have a kilogram (of mass or charge) and the particle will always be a one a.m.u.
(atomic mass unit) particle (~neutron). For the gravitational interaction, this particle will have
zero spin, while it will have spin half for the electrostatic interaction. Although the fourdimensional wave interaction is taking place on the hypersurface of a four-dimensional expanding
hypersphere, one will make use of cross-sections to calculate interference patterns. Interference
is considered on each de Broglie expansion of the hyperspherical universe. Notice that spacetime
waves and their sources will be described in detail in a paper of this series. One can briefly
describe the source of waving as a four-dimensional particle (four-dimensional ellipsoid of
* ny2292000@yahoo.com
Quantization in Astrophysics ...

15
405

revolution or particle X for simplicity). The X particles are characterized by four axes lengths.
Three axes lengths correlate with the quarks composition of matter. The fourth-axis always points
in the radial time direction. Needless to say, different quarks (axis lengths) and different
rotational states around the four axis will be sufficient to maps all known particles (photons,
mesons, neutrinos, etc). Volume (mass) tunnels in an out of the three-dimensional space for
spinning particles (particles with non-zero spin) and out and in towards the radial time dimension.
Spin is considered to be a special rotation, since the rotation axis is perpendicular to radial time
and one of the spatial coordinates. That gives spinning a different effect; it brings the particles in
and out of the fabric of space, thus allowing for a realignment of the k-vector of associated

spacetime waves. Let’s consider the interaction through a two-dimensional cross-section (X x τ).
Particle one (one a.m.u “zero spin neutron” or fat electron) sits on x=0, while particle two (the
body of 1 Kg) sits on x=R0. The four-dimensional dilatons are embedded in a fifth dimension
(cosmological time). A position in this space is defined by the following vector:
 r.α 


 r.β 
v
r =  r.γ  using director cosines α, β and γ.


 τ 
Φ



(3.1)

At time zero, the positions for particles 1 and 2 are given by:
 0
 
 0
v  
r0 = 0
 
 0
 0
 

and

 R
 
 0
v  
R= 0
 
 0
 0
 

(3.2)

After a de Broglie cycle, one has these three vectors:
0
r
R
 
 
 
0
0
0
v
v   and v  


and
(3.3)
r0 (λ1 ) = 0
r= 0
R= 0
 
 
 
 λ1 
 λ1 
 λ1 
 
 
 
 λ1 
 λ1 
 λ1 
v
r0 ( λ1 ) is the unperturbed crest of the four-dimensional wave of particle 1 after a de Broglie cycle.
v
r is the position of the same crest under the influence of particle 2.

The k-vector is given by:
1

0
v
j 2π 
k = g .k =
0
ij
λ 
0
0


1
0  1 
 
 0
0
0  
 0  2π  0 

0 1 0 0  =

λ  −1
0 0 −1 0  1 
1
1
 
0 0 0 1  
 2
2
0 0
1 0

0
0

(3.4)

* ny2292000@yahoo.com
Quantization in Astrophysics ...

16
406

Where g

ij

is the local metric of the five-dimensional space. Again, cosmological distances

would require a further refinement and the usage of a non-local metric. This is not required in the
calculation of near-proximity forces. In the derivation of the Biot-Savart law, g will be
ij

rewritten with regard the corresponding non-zero relative speeds. Notice the ½ phase dependence
on k-vector, corresponding to the fifth dimension for a half-spin fat electron.
And
1

0
v 2π 
k =
0
λ 
0
0


0  1 
1
 
 
0  0 
0
2π  



0 1 0 0 0 =
0
  λ  
0 0 − 1 0  1 
 − 1
0
0 0 0 1  0 
 
0 0
1 0

0
0

(3.5)

for a “static zero spin neutron ” forward time traveling wave. Notice that the dilaton and the
dilator are treated as one due to the QLP. Where
•

N=1Kg of Matter ≅1000 Avogrado’s Number=6.0221367360E+26 particles of type 1.

•

λ1 ≅ h*1000*Avogrado/(1 Kg x c)= 1.3310E-15 meters ( in the MKS system).

•

λ2=λ1Kg≅ h/(1Kg x c)= 2.2102E-42 meters ( in the MKS system).

•

GGravitational is the gravitational constant = 6.6720E-11 m3.Kg-1.s-2

•

Single electric charge ( 1.6022 E − 19 Coulomb).

•

qe is the effective value of the single electric charge= charge divided by a corrective
factor of 1.004145342= 1.59556231E-19 Coulomb

•

ε0=permittivity of the vacuum = 8.8542E-12 C2.N-1.m-2 (MKS)

Starting with the standard MKS equation for electrostatic force between two one Kg bodies of
electrons (one a.m.u. “electrons” or “protons”) = x Coulombs, one obtains:

 N


1Kg 
 1Kg


2

(qe * Coulombs * per * particle ) 2

FElectrostatic

( xCoulomb ) 2
1
=
=
2
4πε 0 1meter
4πε 0

FElectrostatic

2
1  N

=
(qe * Coulombs * per * particle ) (1Kg ) 2
4πε 0  1Kg
 1meter

1

1meter 2
2

G Electrostatic =

1
4πε 0

(N .qe )2 = 8.29795214E + 25

GElectrostatic 8.29795214E + 25
=
= 1.24369786E + 36
6.672 E − 11
GGravitational

(3.5)

(3.6)

The dilaton for a single particle can be represented by:

v v
cos(k1.r )
v v v
ψ 1 ( x, y, z ,τ , Φ ) =
1+ P. f (k1 , r − r0 )

(3.7)

where

* ny2292000@yahoo.com
Quantization in Astrophysics ...

17
407

•

|| means absolute value

(

)

v v
v v
v v
f (k1 , r ) = θ k1.r − 2π k1 .r

•
•

Where θ is the Heaviside function.

•

P (absolute value of the phase volume) is 3.5 for a particle with spin half and 3 for
neutral matter. The meaning of P is that for one de Broglie wavelength traversed path by
the hyperspherical universe, a propagating spacetime wave spread along by a factor of
P2π (7π for charged particles and 6π for neutral-zero spin matter).

v
Similarly, for a 1 Kg body located at position R :

v v v
M .N . cos(k 2 .( R − r ))
v v v
ψ 2 ( x, y, z,τ , Φ ) =
1 + P. f (k 2 , R − r )

(3.8)

where the effect of the 1 kg mass is implicit in the k2-vector and expressed by the factor N. The
wave intensity scales up with the number of particles (N). One kilogram of mass has 1000 moles
of 1 a.m.u. “zero-spin neutrons”, or |k2| = 1000.Avogadro. |k1|=N. |k1|, where
•

M=1 for neutral matter-matter or antimatter-antimatter interactions or opposite charge
interactions

•

M=-1 for neutral matter-antimatter interactions or same charge interactions

To calculate the effect of gravitational/electrostatic attraction, one needs to calculate the
displacement on the crest of each particle or body wave due to interaction with the dilatons
generated by the other body.
This is done for the lighter particle, by calculating the derivative of the waveform and considering
the extremely fast varying gravitational wave from the macroscopic body always equal to one,
since the maxima of these oscillations are too close to each other and can be considered a
continuum.
The total waveform is given by:

v v
cos(k1.r )
M *N
v v v +
v v v
ψ
( x , y , z ,τ , Φ ) =
(3.9)
total
1+ P. f (k1 , r − r0 ) 1+ P. f (k 2 , R − r )
v v v
The term f (k , R − r ) contains the treatment for retarded potentials, but for simplicity we will
2
v
v
neglect differences in dimensional time between R and r . Equation (3.9) is the one and only
unification equation, that is, it is the four-dimensional wave equation that yields all the forces,
when one consider four-dimensional wave constructive interference. It shows that anti-matter will
have gravitational repulsion or anti-gravity with respect to normal matter. The derivative for ψ1 is
given by:
∂ψ1( x, y, z ,τ ,Φ)
∂x

τ =λ1

(3.10)

~
= −k 2r
1

v v v
v v v
∇ P. f (k1 , r − r0 ) = 0 due to k .(r − r0 ) << 2π.
1

(

)

Similarly

* ny2292000@yahoo.com
Quantization in Astrophysics ...

18
408

∂ψ 2 ( x, y, z,τ ,φ )
∂x

N *M
~
=
2
τ = λ1 Pk 2 .R

(3.11)

Solving for x:

λ λ N *M
N
= 1 2 3 2
x=
2
2
Pk1 k 2 .R
P (2π ) R
2

(3.12)

There are two regimen of spacetime travel and they are depicted in Figure 7 below:
Figure 7. This figure shows the geometry of a surface bound particle. This
is a X versus τ cross-section of the hyperspherical expanding universe.
Notice that the two circles represent a one de Broglie expansion of the
hyperspherical universe.

At each de Broglie step both types of particles (zero and nonzero) change position by the same amount x and that defines a
change in k-vector direction. The difference is with which
referential that change in angle occurs. In the case of
volumetric waves (non-zero spin particles), the k-vector is
allowed to change by the angle α1, while in the case of
superficial waves (zero spin particles), the k-vector changes
just by the amount given by α0 since its k-vector has to remain
perpendicular to the fabric of space. Tan(α) is given by tan(α)
=x/λ1 or by tan(α0) =x/λ1*( λ1/R0) depending upon if the interaction is such that the particle kvector shifts as in α1 or it just acquires the radial pointing direction as in α0. A further refinement
introduced by equation (3.13) below introduces a level of local deformation of the de Broglie
hypersurface or fabric of space. A change in angle α0 corresponds to a much smaller angle
change between the radial directions (by a factor λ1/R0 = 9.385E-42, with R0 (circa 15 billion
light-years) as the dimensional age of the Universe). The experimental spacetime torsion due to
gravitational interaction lies someplace in between 1 and 10-41, thus showcasing a level of local
deformation of the fabric of space. From figure 7, one calculates tan(α) as:
tan(α ) =

x

λ1

δ=

λ1 λ 2 N
δ
3
P(2π ) R 2

(3.13)

λ1

≤ δ ≤ 1 and M=1. It will be shown that the upper limit is valid for
R0
charged particle interaction, while the lower limit modified by a slight deformation of the fabric
of space will be associated with gravitational interaction. For the case of light, one has the
following equation:

Where 9.385.10 -42 =

tan(α 0 ) = 1

(3.14)

That is, light propagates with proper time projection/propagation direction τ at 450 with respect to
the radial time/direction. To calculate the derivative of tan (α) with respect to τ, one can use the
following relationship:

* ny2292000@yahoo.com
Quantization in Astrophysics ...

19
409

λ2 N
tan(α 0 )
∂
tan(α 0 ) =
=
δ
3
∂τ
λ1
P(2π ) R 2

(3.15)

Acceleration is given by:

a = c2

c 2 λ2 N
∂
tan(α 0 ) =
δ
3
∂τ
P(2π ) R 2

(3.16)

To calculate the force between two 1 Kg masses (1000 moles of 1 a.m.u. particles) separated by
one meter distance one needs to multiply equation (3.15) by 1Kg (N particles/Kg* 1Kg):
2
(δ ) (1 Kg )

F = G Calculated

 N
c λ 2 * 
 1 Kg
= −
3
P (2 π )
2

(1 meter ) 2

2



 δ

(1 Kg )2
(1 meter ) 2

(3.17)

For δ=1 and P=3.5 one obtains the GElectrostatic (3.5).
 N 
λ1
c 2 
1 Kg 

(3.18)
= 8.29795214E + 25 = GElectrostatic
G Calculated (δ = 1 ) =
3
P (2 π )
Where one made use of λ1=Nλ2 and considered the absolute value. It is important to notice that
the derivation of the GCalculated never made use of any electrostatic property of vacuum, charge etc.
It only mattered the mass (spacetime volumetric deformation) and spin. Of course, one used the
Planck constant and the speed of light and Avogadro’s number. By setting δ=1 one recovers
the electrostatic value of G!

To analyze gravitational interaction, let’s consider that Hubble coefficient measurements estimate
the universe as being around 15 Billion Years old or 1.418E26 meters radius. To obtain the
elasticity coefficient of spacetime, let’s rewrite δ= (λ1/R0)ξ on equation (3.17) and equate the
GCalculated to GGravitational for two bodies of 1 Kg separated by 1 meter.

(1 Kg )

2

F = G Gravitaion

al

= − 6.6720 E - 11

(1 meter ) 2

 N 
 λ1
c 2 
2
1 Kg 
λ1
1 Kg )
(

= −
ξ
3
R 0 (1 meter ) 2
P (2 π )

(3.19)

Where P =3 since we are considering a spin-zero interaction. Solving for ξ:

ξ =

P (2 π

)3 R 0 G Gravitaion

 N
c 
 1 Kg
2

 2
 λ 1


al

= 8.567 × 10

(3.20)

4

If we consider that the force is given by mass times acceleration:
F = m Mass a x = mMass c 2
F=

∂ tan(θ ) m Mass c 2 λ1
=
ξ .x
∂λ
λ 2 1 R0

(3.21)

2
mMass c 2
ξ .x = mMass  2π .Ω GUniverse  .x


λ1 R0

(3.22)

The natural frequency of spacetime oscillations is:

* ny2292000@yahoo.com
Quantization in Astrophysics ...

20
410

ΩGUniverse =

c2ξ
= 32.14 KHz
λ1R0

1
2π

(3.23)

Notice that this is not dependent upon any masses. That should be the best frequency to look for
or to create gravitational waves. Of course, Hubble red shift considerations should be used to
determine the precise frequency from a specific region of the universe. At last one can calculate
the value of the vacuum permittivity from equations (3.5) and (3.18) as:

ε0 =

7 π 2 Nq e
c 2 λ1

2

= 8.85418782

(3.24)

E - 12

Not surprisingly, there is a perfect match between theoretical and experimental (8.85418782E-12
C2.N-1.m-2 ) values. The correction factor used to calculate the effective charge per particle is due
to the effect of non-zero spin on matter, thus related to the particle gyromagnetic ratio. It is
important to notice that this derivation don’t use any parameterization. The “gyromagnetic ratio”
and the “FS elasticity” are predictions of the theory, which uses only electron charge, speed of
light, Avogadro’s number and Planck’s constant to relate it to non-hypergeometrical physics.
The complete gravitation equation is given by:

FGravitaional

 2 N 

λ1
 c 

 1Kg  λ1  m1 m2

=
ξ
 P(2π )3 R0  R 2





(3.25)

Quantum aspects can be recovered by not using fast oscillation approximations. It is also
important to notice that equations (3.8) and (3.9) can be used to calculate the interaction between
any particles (matter or anti-matter) or to perform quantum mechanical calculations in a manner
similar to molecular dynamic simulations. The quantum character is implicit in the de Broglie
wavelength stepwise quantization. It is also relativistic in essence, as it will become clear when
one analyzes magnetism next.

3.2 Magnetic Interaction
The Derivation of the Biot-Savart Law
Let’s consider two wires with currents i1 and i2 separated by a distance R. Let’s consider i2 on the
element of length dl2 as the result of a moving charge of mass of 1Kg of fat electrons (one a.m.u.
electrons). This is done to obtain the correct scaling factor.

Without loss of generality, let’s consider that the distance between the two elements of current is
given by:
1
 0
 
 
1
 0
v R  
v
and
(3.26)
R=
r0 =  0 
1 = RIˆ




3
 0
 0
 0
 0
 
 
The velocities are:

* ny2292000@yahoo.com
Quantization in Astrophysics ...

21
411

α 1 
 
 β1 
v
V1 = v1  γ 1 
 
0
0
 

α 2 
 
β2 
v
V2 = v 2  γ 2 
 
 0 
 0 
 

and

(3.27)

Due to the spin half, one has after a two de Broglie cycles:
v
 r

 R 
.(1 + 2 α 2 3 ) 



c
 0 
 3

 3


v2
 r

 R 
 0 
β
.(
1
+
3
)
2

v
v  3
c
and Rv =  3  and
r0 =  0 
r =

 R 


v
r



.(1 + 2 γ 2 3 ) 
 2λ1 
 3
c

 3




 2λ1 
2λ1
 2λ1 




2λ1
 2λ1 



(3.28)

Since one expects that the motion of particle 2 will produce a drag on the particle 1 along particle
2 direction of motion.
The figure below showcase the geometry associated with these two currents.

1

v1

z

γ1

R

β1 y

α1
x

R0

2
z'

v2

γ2
α2

β2 y'

x'

Figure 8. Derivation of Biot-Savart law using spacetime waves.

Notice also that the effect of the ½ spin is to slow down the rate of phase variation along the
dimensional time τ in half.
In the case of currents, the velocities are not relativistic and one can make the following
approximations to the five-dimensional rotation matrix or metric: cosh(α) ≅1 and sinh(αι) ≅vi/c
where vi is the velocity along the axis i.
The k-vectors for the two electrons on the static reference frame are given by:

* ny2292000@yahoo.com
Quantization in Astrophysics ...

22
412

v 2π
~
k =
1 λ
1



1




0




0



v

1
−α1

c

0


0

0

1

0

0

1

v
− β1 1
c
0

v
−γ 1 1
c
0

v
−α1 1
c
v
− β1 1
c
v1
−γ 1
c
−1
0




0  1 


3 


 1 
0  
 3 


 1 

0  

3

  −1 

0  1 



1  2 

2π
~
=
λ1














 
 
− α1
 







v
1

+ α1 1

c

3

v

1

+ β1 1

c
3

v

1
1

+γ1

c
3

v
v
v  
1 + β 1 + γ 1  −1

1
1
c
c
c  


1


2


(3.29)

Similarly:






v
2
π

k ~
=
2 λ 
2

 −α v 2
 2 c




v

+α 2 2

c
3

v
1

+β2 2

c
3

v
1

+γ 2 2

c
3

v
v
− β 2 2 −γ 2 2 −1
c
c 

1


2
1

(3.30)

v

The wave intensities at r are:

v v
cos(k1.r )
v v v
ψ 1 ( x, y, z ,τ , Φ ) =
1+ P. f (k1 , r − r0 )
v v v
M .cos(k 2 .( R − r ))
v v v
ψ 2 ( x, y, z,τ , Φ ) =
1 + P. f (k 2 , R − r )

(3.31)

(3.32)

Where N= 1000 Avogadro, λ1= de Broglie wavelength of a one a.m.u (atomic mass unit) particle,
λ2=de Broglie wavelength of a 1Kg particle= λ1/N.
Now one can calculate:


v
(3.33)
1


1

v v v 2π
k .(r − r0 )~
=
1
λ1














 −α 1







+α 1




c

v

1

+ β1

c
3


v
1
1

+γ 1

c
3


v
v
v  
1 − β 1 −γ 1  −1
1
1
c
c
c  

1


2


3
1


v

.(1+ 2 α 2 3) 

c
3


v
r
v
v
v v
2

.(1+ β 2 3)

V .Rˆ V .Rˆ V .V 
c
3
 = 2πr 1+ 1 + 2
+ 1 2 


v
c
c
λ
r


c 2 
1
.(1+ 2 γ 2 3) 

c
3

0



0


r

v
v
v v 

v v 2πr  V1.Rˆ V2.Rˆ V1.V2 
~
1+
 + 2π
k .r =
+
+
1
2 
c
c
λ1 
c


v
v
v v

ˆ
ˆ V .V 

V
.
R
V
.
R
v v v
v v 2π
∇ k .(r − r0 ) = ∇ k .r ~
= 1+ 1 + 2 + 1 2  Rˆ
1
1
c
c
λ1 
c 2 


(

(3.34)

(3.35)

) ( )

* ny2292000@yahoo.com
Quantization in Astrophysics ...

23
413

v v v
v
~ 0 due to k .(rv − rv ) << 2π.
∇ P. f (k1,r − r0 ) =
0
1

(

)

Similarly:
v


1
+α 2 2


c
3


v
1


2
+
β


c
3
2


v v v
v
2
π
1


k .( R − r ) ~
=
+γ 2 2
2

λ2 
c
3

v2
v2
v2  
 −α 2 − β 2 −γ 2  −1
c
c
c  



1



2


(

)

v v v
2π
∇ f (k2 , R −r ) ~
=
λ2

R
−
3
R
−
3
R
−
3



v
1


+α 2




3 2 c

 −
v


1


+β 2


3 2 c

 −
v


1
2


+γ


2
c
3



  −
v
v
v
 

2
2
2
 −1
−β
−γ
 −α
2 c
2 c 2 c  

 



1




2



v

r
.(1+ 2 α 2 3 ) 
c
3

v

r
v
.(1+ 2 β 2 3 ) 
2πR  V2 .Rˆ 
c
3
~
=
1+
v
r
 λ2 
c 
.(1+ 2 γ 2 3 ) 
c
3

0


0


(3.36)

(3.37)

v

1
.(1+ 2 α 2 3) 

c
3

v

1
v 

.(1+ 2 β 2 3)
ˆ
2π  V2.R  ˆ
c
3
=−
1
+
R

v
c 
1
λ2 
.(1+ 2 γ 2 3) 


c

3

0





0

Hence:

v v
v v
∇(k1..r )
~
v v v sin(k1.r )
∇ψ ( x, y, z ,τ , Φ ) = −
1
1+ P. f (k1,r −r0 )
 2π 
∇ψ ( x, y, z, τ , Φ ) ~
= − 
λ 
1
 1

(3.38)

2  v ˆ v ˆ v v 2
V .R V .R V .V
1+ 1 + 2 + 1 2  rRˆ

c
c
c 2 


(3.39)

And

(3.40)

v 
v v v

 λ2  V2.Rˆ  Rˆ
ˆ
v
∇
−
P
(
f
(
k
,
R
r
))
1
R
~−
2
~ −

1−
∇ψ (r ,τ , Φ) =
=−
=
v
v v v 2
2
ˆ  R 2  2πP 
c  R 2


V
.
R
2π


1+ P. f (k 2 , R − r )



P .1+ 2 
c 
λ2 


Thus,
v

~−
r=

r ~
=
ee

 λ 
V .Rˆ  Rˆ
 2 


1− 2

 2πP 
c  R 2




v
v
v v 2

2 
V .Rˆ V .Rˆ V .V 
 2π  
1
2

 1+
+
+ 1 2 
λ  
c
c

 
c 2 
 1 

v

v

v

v v



V .Rˆ 
V .Rˆ V .Rˆ V .V
1− 2
 R
ˆ.Rˆ − 2 1 − 2 2 − 2 1 2


c 
c
c
c2

~ − λ 2λ  
=
 1
2 

2π 3 P

( )

v
v
v v
v
v
v
v
v
v v


V .Rˆ V .Rˆ V .V
V .Rˆ  V .Rˆ  V .Rˆ   V .Rˆ  V .Rˆ   V .V
R
ˆ .Rˆ − 2 1 − 2 2 − 2 1 2 − 2 2 + 2 1  2  + 2 2  2  + 2 1 2








c
c
c
 c2
 c  c 
 c  c 
c2









− λ 2λ  
1 2
3

2π P

( )

* ny2292000@yahoo.com
Quantization in Astrophysics ...








(3.41)
Rˆ
R2

v 

ˆ
 V .R  

 2 
 c  





Rˆ
R2

(3.42a)

24
414

v



V .Rˆ 
ˆ.Rˆ − 2 1 
R

c 



+ λ 2λ  
1 2
3

2π P

Rˆ
( )
R2
v
v
v
v


ˆ

V .R V .Rˆ  V .Rˆ  V .Rˆ  
R
ˆ.Rˆ − 2 2 − 2 2 + 2 2  2  




c
c
 c  c   R
ˆ





=  λ 2λ  
r ~
pe  1 2 
(2π )3 P
R2
~
r =
ep

r

 ˆ ˆ
ˆ

  R.R  R
~
=  λ 2λ  
pp  1 2  ( )3
2π P R 2

~
r
= r +r +r +r
total ee ep pe pp

v
v v
  v
ˆ  V .Rˆ   V .V
  V .R
  1  2 

−2 1 2
 2
c  c   c 2

 2
  


= − λ λ 
1 2

2π 3 P

( )

since v2=0

(3.42b)

since v1=0

(3.42c)

since v1=v2=0

(3.42d)

v  


ˆ  
 V .R

1− 2   

 


 c  


 

(3.42f)
Rˆ
R2

Where p stands for proton and e for electron.

r

v v


V .V
1 2

− 2

c2
~ − λ 2λ  
=
 1
2 


v  v  

ˆ  V .Rˆ  
 V .R
+ 2 1  2  
 c  c  




(2π )3 P


ˆ .Rˆ 
 V ⊗V ⊗ R
 2
   1  2
 
r~
= 2 λ λ 
1 2

2π 3 Pc 2

( )

(3.43)

Rˆ
R2

Rˆ
R2

(3.44)

Where non-velocity dependent and single velocity dependent contributions where neglected due
to the counterbalancing wave contributions from static positively charged centers.
The force is given by:
v
v
v
λ2
∂ tan(α )
r
R λ v .v
R
(3.45)
F = c2
= c2
=
V1 ⊗ V2 ⊗ Rˆ .Rˆ 3 = 2 1 2 dlˆ1 ⊗ dlˆ2 ⊗ Rˆ .Rˆ 3
∂τ
R
R
(2π )3 P
2λ21 (2π )3 P

([ (

)] )

([

(

)] )

Where one took into consideration that a particle with spin half has a cycle of 2 λ1 instead of λ1.
The Biot-Savart law can be written as:
v v
v µ 0 I 1 .I 2 ( dl1 .dl 2 ) xv12
(3.46)
dF =
v
4π
| x12 |3
Comparing the two equations one obtains:

λ2
µ0
=
4π (2π )3 q 2 P
e

Thus

µ0 =

(3.47)

λ2

(3.48)

2π 2 qe 2 P

From equation (3.24)
2
2 P π 2 Nq e
ε0 =
c 2 λ1

* ny2292000@yahoo.com
Quantization in Astrophysics ...

25
415

Thus

λ2

2 P π 2 Nq e
1
= 2
2
2
c λ1
c
2 P π qe2
Thus one recovers the relationship between µ0 and ε0.

µ 0 .ε 0 =

2

(3.49)

3.3 Grand Unification Supersymmetry
As the dimensional age of the universe becomes smaller, the relative strength of gravitation
interaction increases. Conversely, one expects that as the universe expands gravity will become
weaker and weaker. This and the four-dimensional light speed expanding hyperspherical
universe topology explain the acceleration of expansion without the need of anti-gravitational
dark matter.
For gravitation the spring coefficient is given by:

F =m

∗a =m
c2
neutron
x
neutron

2
4
∂ tan(θ ) mneutron c 8.56610 λ1
=
x =κ x
g
∂λ
R0
λ21

(3.50)

Similarly for electrostatic interaction, one has:
2
∂ tan(θ ) mneutron c
2
F =m
∗a = m
c
=
x =κ x
neutron
x
neutron
e
∂λ
λ21
Thus

κg
κe

=

8.56610 4 λ1

(3.51)

(3.52)

R0

Thus when R0 was smaller than 8.566104 times λ1 (3.8E-19s), gravitational and electromagnetic
interactions had equal strength. They were certainly indistinguishable when the radius of the
universe was one de Broglie wavelength long. This section is called Grand unification
supersymmetry, because condition (3.52) plays the role of the envisioned group theoretical
supersymmetry of the grand unification force in future theories. Of course, it has a geometrical
interpretation. At that exact radius, an elastic spring constant of the fabric of space allows for a
change in the local normal such that it is parallel to the redirection of k-vector of a freely moving
dilator. This is not what most scientists in this field expected but science is not about
expectations.

4 Conclusions:
The hypergeometrical theory, a model that considers the interference of four-dimensional wave
on the hypersurface of a hyperspherical expanding universe was introduced.
The complexity of the present description of the universe in our sciences4-6 is assigned to the fact
that one is dealing with four-dimensional projections of a five dimensional process. Our inability
to realize that made the description unnecessarily complex.
These are the ingredients for a new and simple formulation of Physics:
•

A new quantum Lagrangian principle (QLP) was proposed.

* ny2292000@yahoo.com
Quantization in Astrophysics ...

26
416

•

Quantum gravity, electrostatics and electromagnetism were derived using the same
equations (QLP), same framework. The theory is inherently quantum mechanical.

•

The quantum version of this theory is readily achieved just by eliminating the high mass
or short wavelength approximation on equation (3.9). It is outside the scope of this paper
to implement hypergeometrical universe quantum algorithms. In a fully geometric
theory, there are no energy or mass quanta. Motion is quantized by the QLP. All the
other quantizations can be recovered from that.

•

Two fundamental parameters of the universe were calculated from the first principles
(permittivity and magnetic susceptibility of vacuum).

•

Biot-Savart law was derived from the first principles.

•

Grand unification supersymmetry conditions for the time when all forces were equal were
derived from simple geometrical considerations.

•

The fabric of space can be considered to be the regions of the hypersphere where the
normal to its local space is pointing in the radial direction. Any region where that
happens has a distinct and yet undistinguishable character. It is distinct because it is
pointing in the direction of the universe expansion, but it is indistinguishable within the
four-dimensional (relativistic) perspective. All reference frames are equivalent within a
four-dimensional perspective. They become distinct but not distinguishable under a fivedimensional analysis.

•

The natural frequency of spacetime oscillations is derived to be 32.14 KHz.

•

Mach’s non-local gravitational interaction explanation for inertia is replaced by a
hypergeometrical local fabric of space distortion argument.

•

Mach’s and Newton’s absolute times are assignable to the cosmological time. That time
is absolute but can only be measured by observing the expansion of the four-dimensional
hyperspherical universe.

•

3D and 4D masses were defined in terms of 3D projections of a 4D volume at specific
phases of the hypergeometrical universe expansion 4D masses were the corresponding
mass within a de Broglie expansion cycle

•

Pseudo time-quantization was proposed.

•

A fundamental dilator corresponding to both the proton and the electron was proposed.
Particles were modeled as coherences between two 4D deformation states of a rotating
4D double potential well.

•

Dilatons from the fundamental dilator were proposed to be light speed traveling metric
modulations generated as the dilator tunnels from one state to the other, thus changing
character from electron to proton and vice-versa. Anti-matter was proposed to be the
same dilator just with a negative phase.

•

Since all non-exotic matter (elements, electrons, neutrons, protons, anti-elements, antielectrons, anti-neutrons and anti-protons) were proposed to be composed of the same
dilator, a cosmological coherence is derived.

•

Exotic matter (hyperons) is proposed to be the more complex coherences shown in
Appendix A. Nuclear energy is proposed to be stored in deformations of the fabric of
space resulting from mismatch of tunneling and tumbling processes within a complex
coherence period. The mismatching would result in a tilted state at the de Broglie phases

* ny2292000@yahoo.com
Quantization in Astrophysics ...

27
417

of the Cosmological Coherence. It is proposed that interaction of these particles with the
Universe through the QLP, requires that the beginning and final states to be flat on the
3D hypersurface and that any distortion to be distributed among sub-coherences. The
amount of tilting on the individual sub-coherences is recovered at the moment of decay.
•

Higher degrees of internal tilting can be achieved by non-fundamental sub-coherences.
The higher the degree of internal tilting the lower the element or isotope lifetime.

•

The only “force” is due to dilaton-dilator interactions subject to the quantum Lagrangian
principle. There is no need for intermediating virtual particles to convey different forces.

•

Particle decay, as opposed to collisional reactions, can be explained by nonlinear optics
methods or standard barrier tunneling methods – quantum chemistry methodology. Of
course, to create quantum chemistry methodology one has to have the Schrodinger
equation for the 4D deformation rotating double well potential. This is outside the scope
of this paper.

•

There is a dilaton bath from which one can envision virtual dilators popping into
existence, but it is not clear they are needed at all. Current science does not have the
dilaton field, thus under those condition, virtual particles are need to explain nuclear
chemistry. Notice that a dilator field is a matter field, that is, it is a function of the
proximity of matter and not a property of empty space. It decays as one goes away from
matter and thus it doesn’t blow up as vacuum zero point fluctuations would. This is at
the heart of the solution to the action-at-distance paradox. The photon decay is due to
dephasing of the electronic coherence due to interaction with the dilaton black body field
from the detectors themselves. Since the radiation arriving from the detector on the
emitting molecule is polarized (by the polarizers), the outgoing photon will know its
polarization at the moment of emission and not at the moment of interaction with the
polarizers. This eliminates the need of infinite velocity and thus eliminates the actionat-distance paradox.

•

The black body radiation due to dilators thermal fluctuations is not polarized and
normally average to nothing. Thermal fluctuations are uncorrelated and isotropic. Any
coherent motion will have a corresponding dilaton coherence along their 4D trajectory
and a de Broglie projection in the 3D universe. This 3D de Broglie projection is real, that
is, it is independent upon a single electron and at the same time it is dependent upon each
and every electron in the coherent flow. The double slit experiment is done with a
monochromatic flow of electrons passing through two slits. Due to the QLP, electrons
will travel or surf the 4D dilaton field. That will have a 3D projection, which means that
the electron will also surf the 3D projection of this dilaton field. We propose that the
electron does not pass the two slits at the same time. It surfs a de Broglie dilaton
projection that will create an interferometric pattern after the slits. Since the electron
follows the dilaton field before and after the slits, it will follow the interferometric pattern
and deposit accordingly. Thus the electron in the double slit experiment does not need
to pass through both slits at the same time.

•

Dilatons and standard collisional excitation should suffice in this theory. In the same way
that electronic transitions can be created by collisions, dilator collisions can create 4D
deformation transitions. These transitions, if accompanied with the creation of new
coherences will interact with the existing universe otherwise they would just disappear.
The appropriate description of the 4D deformational rotating double potential well and
the dilator rotational dynamics will be described elsewhere.

* ny2292000@yahoo.com
Quantization in Astrophysics ...

28
418

•

A refinement on the fundamental dilator model is to consider it a four-dimensional
ellipsoid of revolution with a 3D projection of the 4D volume proportional to the particle
mass and three axes’ length quantum numbers equal to the corresponding quark
composition. This is a zero 4D Volume sum rule for all the particles in the universe.
Matter is energy and energy cannot be destroyed. 4D displacement volumes can! They
have signs and any cosmogenesis theory basic on them will be able to reduce the whole
universe to a fluctuation of zero. A simple hypergeometrical universe cosmogenesis
theory will be presented in a companion paper

•

Since quarks are modeled as quantum numbers (axis lengths) of a volume, they cannot be
separated in the same way one cannot separate the X dimension from a three-dimensional
object. Structured scattering, which has been used as an indication of the existence of
quarks, can be easily understood as an indication of the existence of a form or shape, that
is, particles are not spheres. Other dimensions of the standard model are modeled as
rotations. Spin is modeled as a rotation perpendicular to radial direction and one spatial
coordinate (x, y or z). Three/two additional dimensions are captured as rotational degrees
of freedom for rotation along the three/two spatial axes.

•

Matter and anti-matter should present anti-gravitational interaction, that is, they should
repel each other with the corresponding gravitation strength.

•

Planck’s constant has a new meaning within this theory. It is the proportionality constant
that ensures that the de Broglie wavelength, relating the observed 3D mass and 3D
velocities, matches the 3D projection of the 4D dilaton. Notice that the 4D dilaton
wavelength (frequency) depends only upon the gap between the two states of the
fundamental dilator. This mapping is done through the linear momentum equation
h=m.v.λ .

Cosmological Conclusions:
The hyperspherical expanding universe has profound cosmological implications:
•

The expanding hypersphere clearly shows in geometrical terms that any position
(cosmological angle) in the hypersurface (3-D universe) has a Hubble receding velocity.

•

The HubbleVel, the Hubble cosmological expansion velocity at a cosmological angle θ
(see Figure 1) is given by
o

HubbleVel = cθ

o

This means that the three-dimensional space is expanding at the Hubble
cosmological expansion velocity (speed of light per radian) as the hypersphere
moves outwards along the radial time direction.

o

The corresponding elicited motions to all interactions in the universe are just
side-drifts from a light-speed travel along the radial time direction. This explains
why the speed of light is the limiting speed in our Universe. It is the only
velocity anything can move.

Conclusions about Time
•

This model contains one absolute time, the Cosmological Time and time projections for
each inertial frame of reference.

•

Although absolute, one cannot measure time using the Cosmological Time, unless one
observes directly the Hyperspherical Expansion of The Universe.

* ny2292000@yahoo.com
Quantization in Astrophysics ...

29
419

•

Our universe corresponds to the Xτ cross-section shown in figure 1. There one can only
measure the relative angle between τ and τ’, and thus only the relative passage of time.

Hence time can be both Absolute and Relative and both Einstein and Newton were right.

Astronomical Conclusions:
•

The entire Universe is contained in a very thin three-dimensional hypersurface of a fourdimensional hypersphere of radius c*[Age of The Universe].

•

The thickness of this hypersurface varies depending upon which dilator state is in phase with
the 3D Universe. Electrons thickness is about 2000 times higher than the one for the proton.

•

The average radius of curvature of this hypersurface is exactly the speed of light times the
age of the Universe, or R=15 billion light-years or so.

•

The visible Universe volume is given by: VisibleUni verseVolum e =

•

The

whole

(Visible

UniverseVo lume =

plus

Invisible)

Universe

should

4πR 3
.
3

have

a

volume

of

4π (πR )
. The actual radius of the Universe is πR or around 47 billion
3
3

light-years.
•

Beyond the visible Universe lies the Never-to-be-Seen-Universe, whose linear dimension is
actually (2π-2) times the dimensional time radius of the hypersphere. 3π/2R of the Universe
linear dimension can never be reached

•

Of course, the four-dimensional light speed expanding hypersurface topology also explains
why the Big Bang radiation comes from all directions and why one cannot ever locate a
simple point where the Big Bang occurred. The Big Bang will always seem to have occurred
in any direction if one looks far enough (the dimensional age of the Universe) and that is the
result of four-dimensional explosion dynamics.

•

The other topology derived conclusion is that if one could “see and measure velocity using
Cosmological Time” farther than the dimensional time radius of the Universe, galaxies would
be traveling at speeds faster than the speed of light with respect to us.
This wouldn’t be the case if we measure any velocity using cross-reference time τ. Under
those circumstances the maximum velocity is always c.

•

The fact that it is impossible to “see” any farther than the dimensional radius of the Universe
means that the postulate of Relativity remains semi-solid. If one travels far enough but not as
far as the age of dimensional radius of the Universe, one still could travel at absolute speeds
faster than the speed of light.

•

The highest absolute receding speed of this Universe is πc, which is the real speed bump in
the whole Universe. Absolute receding speeds are measure with respect to the Cosmological
Time Φ.

•

Since the receding speed of the Big Bang is equal to the speed of light, all its electromagnetic
energy is Doppler shifted by the time they arrive at us, thus one cannot ever observe the Big
Bang with a telescope. On the other hand, one can probe the initial dynamics by looking as
far as one can with a large telescope.

* ny2292000@yahoo.com
Quantization in Astrophysics ...

30
420

•

The Cosmic Microwave Background is likely to be Doppler Shifted Gamma Radiation and
not Blackbody Equilibrium Radiation.

•

Another corollary of this theory is the Hubble conclusion about an expanding hyperspherical
Universe. The speed of light divided by the average numerical value for the Hubble constant
is the inverse of the Age of the Universe (e.g. 16.4 Billion years, 55 Km/s per megaparsec
with one megaparsec = 3 million light years). The averaging is necessary since if one looks
at any direction, there will be debris from the Big Bang (Galaxies) of different sizes traveling
towards and from your direction.

•

The topology offers the revolutionary perception that while we see ourselves at rest we are
actually traveling at the speed of light in a direction perpendicular to all the three dimensions
we can perceive in our daily life. General Relativity and present Cosmology has no qualms
associating a Black Hole with a disturbance of spacetime continuum. Since we could easily
fall into a Black Hole, it is not surprising that we should be modeled as a disturbance of the
spacetime continuum in a similar manner. Like any disturbance, there is a natural
propagation velocity, in our case that velocity is c (the speed of light).

•

One can easily see that the Big Bang occurred when the Universe was an infinitesimally
small circle across each one of the three dimensions, thus it spanned the whole Universe. It
occurred on all places at the same time. This is the basis for the non-locality of the Big Bang
in a three-dimensional Universe projection. This means that in our Universe, the Big Bang
occurred exactly where we are no matter where we are. The heat, horrendous explosion and
debris has long since left this region and now one only can see the beginning of the Universe
if one looks very far away to see the debris that traveled the age of the Universe and are only
now reaching us. This is a quite surprising and elegant conclusion.

•

Due to the topology of a four-dimensional Big Bang, the center of the Universe is a location
in the radial direction and not in 3D space.

•

Unlike motions along other directions of the four dimensional space, travel along the radial
time occurs only at the speed of light.

•

The visible Universe corresponds to a hyper-cap in this hypersphere. The hyper-cap radius is
also the age of the Universe, which is also the average radius of curvature of the hypersphere.
Thus the Universe is not only finite but also curved: a perfect circle.

•

Despite of that one cannot travel around it (due to its expansion at the speed of light) and due
to the limit imposed on the highest traveling speed in this Universe. Finite, circular but
impossible to traverse.

•

In addition, the hypersphere model makes any point in the Universe equivalent to another; in
the same way that no point on the surface of an expanding balloon is closer to the origin of
times (its center or the point in space defined by the balloon when it was very small).

•

The fact that we cannot see the past or travel there is because it does not exist any longer, due
to the extremely thin character of the hyperspherical Universe. It is only a de Broglie
wavelength thick. Needless to say, one cannot either travel to the future because it doesn’t
exist yet. We can only reach the future when it is the present, since we are traveling there
even as we speak.

•

Beyond the Big Bang lies more of the same (Universe), albeit invisible Universe. The
furthest visible part of the Universe is the Big Bang, that doesn’t mean that one could
traveling faster than the speed of light go there and see it first hand. It only means that if we
travel at the speed of light in any direction, the cosmic microwave background will Doppler

* ny2292000@yahoo.com
Quantization in Astrophysics ...

31
421

shift into gamma rays (a possible tremendous inconvenience for light speed travelers) and one
will be able to actually see the beginning. From Figure 1, it is clear that the hypersphere is
uniform and that traveling in any direction wouldn’t bring us into the past. The hypersphere
travels inexorably into the future.
•

It becomes clear that the Hubble expansion theory has to be modified to accommodate a fourdimensional Big Bang. The change is that in a four-dimensional explosion the Big Bang
occurred in each and every point of the initial circumference, that is, the Big Bang occurred
in each and every point of the Universe at the same time. From each and every point, energy
and matter were ejected by tremendous forces. This means, that at any given point of the
Universe there is a three dimensional isotropic expansion and thus the average Hubble
constant is equal to the inverse of the dimensional age of the Universe times the speed of
light. In a three-dimensional Big Bang, matter would expand radially from a single point,
thus the Universe would be highly anisotropic and the Hubble constant would be a constant.

•

Finally, the relativistic effects and inertia are due to local distortions of the curvature of this
hyperspherical surface. The highest distortion one can create is to travel at the speed of light.
That corresponds to having one’s proper dimensional time vector τ at 45 degrees with the
three-dimensional space. Different regions of the hypersurface have different tangents with
respect to an originating point, thus flow of observed time will depend upon how fast and
how far you travel. One does have receding velocities that are larger than the speed of light,
indicating the Relativity is a local approximation of Universe dynamics.

•

Appendix B showcase modifications to Relativity that allows for the higher than the speed of
light receding speeds expected in a hyperspherical expanding Universe. It also shows the
correct way to add receding speeds over Cosmological distances.

Grand Unification Conclusions
•

The meaning of physical existence is being phase-matched along the radial direction.

•

Appendix B shows that one can see all the way up to the Big Bang (or thereabouts), but one
can only reach a Cosmological angle of π/4 due to the Universe expansion.

•

Quarks are modeled as positive and negative axes’ length of the ellipsoid of revolution. A
negative axis length means that the four-dimensional wave generated along that axis direction
has a negative phase (180 degrees phase shift). The directionality of waves will only play a
role when one discusses polarized matter (see DeltaPlus and SigmaPlus hyperons in
Appendix A). From this description it becomes evident that antimatter should produce antigravity. This is supported by the grand unification equations presented in section 3.

•

Appendix A indicates that the conversion of matter to antimatter is done through halfneutrinos interaction with matter. Cross-section for neutrino splitting might be low, thus
explaining why there is an asymmetry in the proportions of matter and antimatter in the
Universe.

•

The light speed, fast expanding hypersphere model of the Universe allows for the existence of
an infinite number of other hyperspherical expanding Universes, separated by dimensional
time intervals. The source of “matter and energy” will be explained in the Cosmogenesis
paper of this series. Although there is an allowance, it will be described that the Big Bang
occurred simultaneously with Dimensional Transitions. This seems to preclude the
coexistence of Hyperspherical Universes.

* ny2292000@yahoo.com
Quantization in Astrophysics ...

32
422

•

The fate of the Universe is continuous expansion. It will become clear how the Universe
recycles itself and what is the meaning of recycling in the Cosmogenesis paper.

Fundamental Conclusion:
A last conclusion worth mentioning is a modification of Newton’s first law:
In the absence of interactions, a body (locally deformed FS region) will drift within the
hypersurface (3-D universe) until τ and R are parallel again or conversely until it reaches a
point where its drift velocity equals the Hubble velocity of that region of space.
Notice that the apparent motion will still exist since the fabric of space is expanding and any
place in the 3D universe has a Hubble expansion velocity. Although moving relatively to its
original position, the body remains static with respect to the fabric of space (τ parallel to R). At
that point, the local deformation ceases to exist and the body drifts with the expansion at the
Hubble velocity. In other words, motion is a way for 4D space to relax; in the same way a
tsunami is the means for the sea to regain a common level.

Appendix A- Hypergeometrical Standard Model
Here is a brief description of the Hypergeometrical Standard Model. A full description will be
published elsewhere.
The Fundamental Dilator is modeled as a coherence between two 4D deformational stationary
states of a double potential.
The quantum numbers, associated with the 4D deformational states, are modeled as axes’ lengths
of a 4D ellipsoid of revolution. Negative values correspond to 180 degrees in phase with respect
to a dilator with a positive axis. This means that when the positive dilator is expanding the 4D
space, the negative dilator is shrinking 4D space.
Below is a diagram showing the states involved with the fundamental dilator.

Electron Model
E-State
e*
e
p

0,-1/3,-2/3

e*
e

0,1/3,-1/3

p

0,-2/3,-1/3

e*
e

0,-1/3,1/3
0,-1/3,1/3

e*
e

p
0,-1/3,1/3

p

Electron
2/3,-1/3,2/3
2/3,0,1/3
2/3,1/3,0
2/3,2/3,-1/3

* ny2292000@yahoo.com
Quantization in Astrophysics ...

33
423

e*
e
p

Where p=(2/3,2/3,-1/3),e=(0,-2/3,-1/3),e*=(0,-1/3,-2/3) are a subset of states involved in the three
most common “particles”= proton, electron and neutron. Below is another representation of the
electron and positron. Notice that the first and last elements of the coherence chain are the same
and that the coherence repeats itself for its lifetime. In the case of a proton/electron, that lifetime
is infinite, since that coherence is between two ground states.
This is an effort to represent a tumbling 4D object which changes shape as it tumbles. Notice that
the sidewise states have no 3D projection. Since in the theory, there is an absolute time, one can
define an absolute phase and that is what distinguishes an electron from a positron. Later it will
be clear that more complex coherences involving the e* state (neutrino) will result in a phase shift
of the tunneling process with respect to the tumbling process, thus modifying which state is in
phase with the shock-wave universe.
The colors are shown only for states that have both a 3D projection and the same frequency as the
fundamental dilator.
Another important element of the model is the bolden of the first axis length (e.g. p=(2/3,2/3,1/3)). This means that the spin is a tumbling process around and rotational axis perpendicular to
both the radial direction (perpendicular to all three spatial coordinates and the x coordinate). This
defines a 4D angular momentum which has to be conserved. More complex coherences like the
ones associated with Delta and Sigma particles differs just by the final spin and thus by how the
sub-coherences tumbles to make up the final amount of spinning.

* ny2292000@yahoo.com
Quantization in Astrophysics ...

34
424

Proton Model
Similarly for a proton:
Proton State
e*
e
p
0,- 1/3,-2/3
0,1/3,-1/3

e*
e

0,-2/3,-1/3

p
p

Pr oton

e*
e

p

0,- 1/3,1/3
0,- 1/3,1/3
0,-1/3,1/3

e*
e

2/3,-1/3,2/3
2/3,0,1/3
2/3,1/3,0
2/3,2/3,-1/3

e*
e
p

Here is the representation of a proton and an antiproton.

* ny2292000@yahoo.com
Quantization in Astrophysics ...

35
425

NeutrinoElectron Model
Here is the electron neutrino model. Notice that there are no color associated with the neutrino
states since they have zero 3D volume (they are 2D objects spinning around the a

Neutrino State
e*
e
p
0,-1/3,-2/3
0,- 2/3,-1/3

0,1/3,-1/3

Neutr inoElectron

e*
e
p
p
e*
e
p

0,-1/3,1/3
0,-1/3,1/3
0,-1/3,1/3

e*
e

2/3,-1/3,2/3
2/3,0,1/3
2/3,1/3,0
2/3,2/3,- 1/3

e*
e
p

Where p=(2/3,2/3,-1/3),p*=(2/3,-1/3,2/3),e=(0,-2/3,-1/3),e*=(0,-1/3,-2/3)

* ny2292000@yahoo.com
Quantization in Astrophysics ...

36
426

Neutron Model
Below is the Neutron model. It is worthwhile to notice that the Electron-Proton and ProtonElectron transitions (transmutation coherences) are not in phase with the tumbling process and
thus lead to a mismatch between the Neutron overall tumbling and a number of full rotations.
This means that due to those sub-coherences, there is kinetic energy stored in the form of a local
fabric of space twisting. The angle error at the end of the coherence is the sum of those two
contributions. The electron and the proton coherences are by definition in phase with the
tumbling process.
The shift in phase is such that the electron/proton fabric of space twisting is 42.77/-0.07294
degrees for a neutron at rest, respectively. This is the fabric of space twisting that would result in
the observed relative velocities after neutron decaying. Notice that twisting the fabric of space
results in an increase in the mass or 3D projection of the 4D volume displacement associated with
different states, and thus explains the extra mass involved in the neutron formation. The same
reasoning is applicable to all particles and elements. The elements and isotopes are modeled as
simple coherences involving only the fundamental dilator (electron and proton) and these two
transmutation coherences.

0,-1/3,-2/3
0,-2/3,-1/3

0,1/3,-1/3
Neutron

2/3,-1/3,2/3
2/3,0,1/3
2/3,1/3,0
2/3,2/3,-1/3

0,-1/3,1/3
0,-1/3,1/3
0,-1/3,1/3

* ny2292000@yahoo.com
Quantization in Astrophysics ...

37
427

E-State

ElectronProton
Transition

e*
e

Proton
State

e*
e
p

Proton Electron
Transition
e*
e

p

e*
e
p

p*
p

e*
e

p

e*
e

p

p

e*
e
e*
e

p

e*
e

p

p

e*
e

e*
e
p

p*
p

e*
e

e*
e
p

e*
e

p

Where p=(2/3,2/3,-1/3),p*=(2/3,-1/3,2/3),e=(0,-2/3,-1/3),e*=(0,-1/3,-2/3).

The extra energy or mass associated with the neutron is due to the dephasing created by the
Electron-Proton Transition and vice-versa. The total angle is balanced between the two 3D
footprints (electron and proton masses) to be 42.77/-0.07294 degrees, thus resulting in a
dephasing angle by Electron-Proton Transmutation of around 21.4 degrees.
Thus the available kinetic energy after neutron decay is the difference in twisting between these
two coherences.

* ny2292000@yahoo.com
Quantization in Astrophysics ...

38
428

ElectronProtonElectron
Transition

e*
e

0,- 1/3,-2/3
0,- 2/3,-1/3

0,1/3,- 1/3

Neutrino State

NeutrinoElectron

e*
e
p

e*
e
p

p*
p

e*
e

p
p

e*
e

0,- 1/3,1/3
0,-1/3,1/3
0,-1/3,1/3

e*
e

2/3,-1/3,2/3
2/3,0,1/3
2/3,1/3,0
2/3,2/3,-1/3

p

0,1/3,- 1/3

e*
e

0,-2/3,-1/3

p

0,- 1/3,-2/3
ElectronPr otonElectr on Tr ansition

e*
e

p*
p

2/3,-1/3,2/3
2/3,0,1/3
2/3,1/3,0
2/3,2/3,- 1/3

e*
e

0,- 1/3,1/3
0,- 1/3,1/3
0,1/3,1/3

e*
e
p

p

Spin Differences between Hyperons
The hyperons(9) below differs only by the spinning direction of their sub-coherences(7).
Hyperon
Name
DeltaPlus

∆+

Mass
(MeV/c2)
1232

Decay
Process
π+ + n

3/2

Coherence
Lifetime
6×10-24

DeltaPlus

∆+

1232

π0 + p

3/2

6×10-24

DeltaZero

∆0

1232

π0 + n

3/2

6×10-24

DeltaZero

∆0

1232

π- + p

3/2

6×10-24

DeltaMinus

∆-

1232

π- + n

3/2

6×10-24

LambdaZero

Λ0

1115.7

π- + p

1/2

2.60×10-10

Symbol

* ny2292000@yahoo.com
Quantization in Astrophysics ...

Spin

Coherence
Decomposition
(0,2/3,1/3).
(0,-1/3,1/3).
(2/3,2/3,-1/3).
(0,-2/3,-1/3).
(0,-1/3,1/3)
(2/3,2/3,-1/3)
.(0,1/3,2/3) .
(0,-1/3,-2/3)
(2/3,2/3,-1/3).
(0,-2/3,-1/3).
(0,-1/3,1/3).
(0,1/3,2/3) .
(0,-1/3,-2/3)
(0,-2/3,-1/3).
(0,1/3,-1/3).
(2/3,2/3,-1/3)
(0,-2/3,-1/3).
(0,1/3,-1/3).
(2/3,2/3,-1/3).
(0,-2/3,-1/3).
(0,-1/3,1/3)
(0,-2/3,-1/3).

39
429

LambdaZero

Λ0

1115.7

πo + n

1/2

2.60×10-10

SigmaPlus

Σ+

1189.4

π0 + p

1/2

0.8×10-10

SigmaPlus

Σ+

1189.4

π+ + n

1/2

0.8×10-10

(0,1/3,-1/3).
(2/3,2/3,-1/3)
(0,1/3,2/3) .
(0,-1/3,-2/3).
(2/3,2/3,-1/3).
(0,-2/3,-1/3).
(0,-1/3,1/3)
(2/3,2/3,-1/3).
(0,1/3,2/3).
(0,-1/3,-2/3)
(2/3,2/3,-1/3).
(0,-2/3,-1/3).
(0,-1/3,1/3).
(0,2/3,1/3).
(0,-1/3,1/3)

This means that in the case of ∆+ , four sub-coherences are tumbling in one direction while the
one left is tumbling in the opposite direction. Since each sub-coherence has spin half angular
momentum, the resulting spin is 3/2.
In the case of the Σ+, three sub-coherences tumble in one direction and two sub-coherences
tumble on the opposite direction, resulting in spin ½. One expects that the amount of strain on
the fabric of space will correlate with the coherence lifetime or life of the particle and thus that Σ+
would have a lower amount of accumulated dephasing with the fundamental dilator tumbling than
∆+.
Hyperon
Name
DeltaPlus

Symbol
∆+

Mass
(MeV/c2)
1232

Decay
Process
π+ + n

SigmaPlus

3/2

Coherence
Lifetime
6×10-24

Σ+

1189.4

π+ + n

1/2

0.8×10-10

* ny2292000@yahoo.com
Quantization in Astrophysics ...

Spin

Coherence
Decomposition
(0,2/3,1/3).
(0,-1/3,1/3).
(2/3,2/3,-1/3).
(0,-2/3,-1/3).
(0,-1/3,1/3)
(2/3,2/3,-1/3).
(0,-2/3,-1/3).
(0,-1/3,1/3).
(0,2/3,1/3).
(0,-1/3,1/3)

40
430

Appendix B- HU Corrections to Relativity
Let’s consider velocity addition as a function of cosmological angle θ.

τ'
α12

R

R

α12

α2
180−α2

180−α1

τ

α1

θ

x

φ

Figure B1. Hyperspherical Universe Model displaying two reference frames.
The two velocities are given by their angles with the Cosmological Radial direction. From simple
trigonometry, one obtains:

α12 + (180 0 − α1 ) + (180 0 − α 2 ) + θ = 360 0 Or α12 = α1 + α 2 − θ
This means that when θ = α 1 + α 2 the two parties will never meet. There are two special case of
interest:
•

The two bodies are traveling at the speed of light (α1=α2=π/4). Under those conditions
θ=π/2. This means that two traveling parties departing up to a cosmological angle θ=π/2,
can meet half-way if they travel at the speed of light.

•

The other case is when one is deciding to explore some of the Universe and travel at the
speed of light (α1=π/4, α2=0). This means that one can only explore one quarter of the
Universe length in any direction.

* ny2292000@yahoo.com
Quantization in Astrophysics ...

41
431

•

The correct relativistic velocity addition rule can be written as:

tan(α 1 ) + tan(α 2 )
− tan(θ )
v12
1 + tan(α 1 ) tan(α 2 )
= tan(α 1 + α 2 − θ ) =
tan(α 12 ) =
tan(α 1 ) + tan(α 2 )
c
1−
tan(θ )
1 + tan(α 1 ) tan(α 2 )

Or
v12 =

v1 + v 2 − c tan(θ )(1 +
1+

v1 v2
)
c2

v1v 2 tan(θ )
−
(v1 + v 2 )
c
c2

Relativity fails for cosmological distances. It is worth emphasizing that for tan(θ)=1 (θ=45o),
independently upon the local velocities v1 and v2, the perceived velocity v12 is always -c.
v1v 2
)
c 2 = −c
v12 =
vv
1
1 + 1 2 2 − (v1 + v 2 )
c
c
v1 + v 2 − c (1 +

Thus for θ=45o, anything at that cosmological angle will be rushing away at the speed of light.
Beyond that cosmological angle, relative time references and relative velocities are meaningless
since there can not ever be communication or energy exchange between these two sites. There is
a subtle difference between communication and travel and seeing the cosmological past, which
has to do with the nature of light.
It is important to distinguish that the above derivation has to do with places one can travel or
reach in terms of cosmological angles and not places one can see. One can see all the way to the
beginning of times (with Doppler Shifted Vision – by upconverting the cosmic microwave
background through fast traveling or other photonic means). The beginning of the Universe will
always stare us in the eye, sitting at one radian or at the Beginning of Time. Gamma Radiation
Doppler Shifted from the Big Bang is proposed to be the pervasive Cosmic Microwave
Background.
REFERENCES
[1] Pereira, Marco – Hypergeometrical Universe – http://Hypergeometricaluniverse.blogspot.com
[2] Homepage “Acoustics and Vibration Animations” -Dan Russell, Ph.D., Associate Professor of
Applied Physics at Kettering University in Flint, MI http://www.kettering.edu/~drussell/demos.html
[3] Schwarzschild, Bertram. "WMAP Spacecraft Maps the Entire Cosmic Microwave Sky
With Unprecedented Precision.", Physics Today. Vol. 56, No. 4 (April 2003): 21.
[4] Pauli W 1958 Theory of Relativity, London: Pergamon Press
[5] Landau L D and Lifshitz E M 1975, The Classical Theory of Fields, Oxford: Pergamon Press.
[6] Classical Electrodynamics – J.D. Jackson (1975-John Wiley & Sons).
[7] Pereira, Marco - Hypergeometrical Standard Model, to be published elsewhere.
[8] Pereira, Marco - Hypergeometrical Cosmogenesis, to be published elsewhere.
[9] Hyperon Polarization and Magnetic Moments, Joseph Lach – Fermilab-Conf-93/381

* ny2292000@yahoo.com
Quantization in Astrophysics ...

42
432

Three Solar System Anomalies Indicating the
Presence of Macroscopically Quantum Coherent
Dark Matter in Solar System
M. Pitkänen
Dept. of Physics, University of Helsinki, Helsinki, Finland.
Email: matpitka@rock.helsinki.fi.
http://www.helsinki.fi/∼matpitka/.

1

Introduction

Three anomalies associated with the solar system, namely Pioneer anomaly
[3], the evidence for shrinking of planetary orbits [7, 8, 9], and flyby anomaly
[4] are discussed. The first anomaly is explained by a universal 1/r distribution of dark matter, second anomaly finds a trivial explanation in TGD
based quantum model for planetary orbits as Bohr orbits with Bohr quantization reflecting macroscopically quantum coherent character of dark matter
with a gigantic value of Planck constant [11]. Fly-by anomaly can be understood if planetary orbits are surrounded by a flux tube containing quantum
coherent dark matter. Also spherical shells can be considered.

2

Explanation of the Pioneer anomaly

The data gathered during one quarter of century ([2, 3]) seem to suggest
that spacecrafts do not obey the laws of Newtonian gravitation. What has
been observed is anomalous constant acceleration of order (8 ± 3) × 10−11 g
(g = 9.81 m/s2 is gravitational acceleration at the surface of Earth) for the
Pioneer/10/11, Galileo and Ulysses [3]. The acceleration is directed towards
Sun and could have an explanation in terms of 1/r2 long range force if the
density of charge carriers of the force has 1/r dependence on distance from
the Sun. From the data in [2, 3], the anomalous acceleration of the spacecraft
is of order

1

Quantization in Astrophysics ...

433

δa ∼ .8 × 10−10 g ,

(1)

where g ' 9.81 m/s2 is gravitational acceleration at the surface of Earth.
Using the values of Jupiter distance RJ ' .8 × 1012 meters, radius of Earth
RE ' 6 × 106 meters and the value Sun to Earth mass ratio MS /ME '
.3 ∗ 106 , one can relate the gravitational acceleration

a(R) =

2
GMS
MS R E
=
R2
ME R 2

(2)

of the spacecraft at distance R = RJ from the Sun to g, getting roughly
a ' 1.6 × 10−5 g. One has also
δa
' 1.3 × 10−4 .
a

(3)

The value of the anomalous acceleration has been found to be aF =
(8.744±1.33)×10−8 cm/s2 and given by Hubble constant: aF = cH. H = 82
km/s/Mpc gives aF = 8 × 10−8 cm/s2 . It is very difficult to believe that this
could be an accident. There are also diurnal and annual variations in the
acceleration anomaly [4]. These variations should be due to the physics of
Earth-Sun system. I do not know whether they can be understood in terms
of a temporal variation of the Doppler shift due to the spinning and orbital
motion of Earth with respect to Sun.
One model for the acceleration anomaly relies on the presence of dark
matter increasing the effective solar mass. Since acceleration anomaly is
constant, a dark matter density ρd = (3/4π)(H/Gr), where H is Hubble
constant giving M (r) ∝ r2 , is required. For instance, at the radius RJ of
Jupiter the dark mass would be about (δa/a)M (Sun) ' 1.3 × 10−4 M (Sun)
and would become comparable to MSun at about 100RJ = 520 AU. Note
that the standard theory for the formation of planetary system assumes a
solar nebula of radius of order 100AU having 2-3 solar masses. For Pluto
at distance of 38 AU the dark mass would be about one per cent of solar
mass. This model would suggest that planetary systems are formed around
dark matter system with a universal mass density. The dependence of the
primordial dark matter density on only Hubble constant is very natural if
mass density perturbations are universal.
In [4] the possibility that the acceleration anomaly for Pioneer 10 (11)
emerged only after the encounter with Jupiter (Saturn) is raised. The model
2

Quantization in Astrophysics ...

434

explaining Hubble constant as being due to a radial contraction compensating cosmic expansion would predict that the anomalous acceleration should
be observed everywhere, not only outside Saturn. The model in which universal dark matter density produces the same effect would allow the required
dark matter density ρd = (3/4π)(H/Gr) be present only as a primordial
density. The formation of dark matter structures could have modified this
primordial density and visible matter would have condensed around these
structures so that only the region outside Jupiter would contain this density.

3

Shrinking radii of planetary orbits and Bohr
quantization

There are two means of determining the positions of planets in the solar
system [7, 8, 9, 10]. The first method is based on optical measurements and
determines the position of planets with respect to the distant stars. Already
thirty years ago [10] came the first indications that the planetary positions
determined in this manner drift from their predicted values as if planets
were in accelerated motion. The second method determines the relative
positions of planets using radar ranging: this method does not reveal any
such acceleration.
C. J. Masreliez [8] has proposed that this acceleration could be due
to a gradual scaling of the planetary system so that the sizes L of the
planetary orbits are reduced by an over-all scale factor L → L/λ, which
implies the acceleration ω → λ3/2 ω in accordance with the Kepler’s law
ω ∝ 1/L3/2 . This scaling would exactly compensate the cosmological scaling
L → (R(t)/R0 ) × L of the solar system size L, where R(t) the curvature
parameter of
cosmology having the line element ds2 =
 Robertson-Walker

2
dr
2
2
dt2 − R2 (t) 1+r
.
2 + r dΩ
According to Masreliez, the model explains also some other anomalies in
the solar system, such as angular momentum discrepancy between the lunar
motion and the spin-down of the Earth [8]. The model also changes the rate
for the estimated drift of the Moon away from the Earth so that the Moon
could have very well formed together with Earth some five billion years ago.
The Bohr quantization for planetary orbits predicts that the orbital radii
measured in terms of M 4 radial coordinate rM are constant. This means that
planetary system does not participate cosmic expansion so that the orbital
radii expressed in terms of the coordinate r = rM /a shrinking. Therefore the
stars accelerate with respect to the Robertson-Walker coordinates (t, r, Ω))
defined by the distant stars since in this case the radii correspond naturally
3

Quantization in Astrophysics ...

435

to the coordinate r = rM /a giving dr/dt = −HrM so that cosmic expansion
is exactly compensated. This model for the anomaly involves no additional
assumptions besides Bohr quantization and is favored by Occam’s razor.

4

Fly-by anomaly

The so called flyby anomaly [4] might relate to the Pioneer anomaly. Flyby mechanism used to accelerate space-crafts is a genuine three body effect
involving Sun, planet, and the space-craft. Planets are rotating around sun
in an anticlockwise manner and when the space-craft arrives from the right
hand side, it is attracted by a planet and is deflected in an anticlockwise
manner and planet gains energy as measured with respect to solar center of
mass system. The energy originates from the rotational motion of the planet.
If the space-craft arrives from the left, it loses energy. What happens is
analyzed in [4] using an approximately conserved quantity known as Jacobi’s
integral J = E − ωez · r × v. Here E is total energy per mass for the spacecraft, ω is the angular velocity of the planet, ez is a unit vector normal to
the planet’s rotational plane, and various quantities are with respect to solar
cm system.
This as such is not anomalous and flyby effect is used to accelerate spacecrafts. For instance, Pioneer 11 was accelerated in the gravitational field of
Jupiter to a more energetic elliptic orbit directed to Saturn ad the encounter
with Saturn led to a hyperbolic orbit leading out from solar system.
Consider now the anomaly. The energy of the space-craft in planet-spacecraft cm system is predicted to be conserved in the encounter. Intuitively
this seems obvious since the time and length scales of the collision are so
short as compared to those associated with the interaction with Sun that the
gravitational field of Sun does not vary appreciably in the collision region.
Surprisingly, it turned out that this conservation law does not hold true in
Earth flybys. Furthermore, irrespective of whether the total energy with
respect to solar cm system increases or decreases, the energy in cm system
increases during flyby in the cases considered.
Five Earth flybys have been studied: Galileo-I, NEAR, Rosetta, Cassina,
and Messenger and the article of Anderson and collaborators [4] gives a nice
quantitative summary of the findings and of the basic theoretical notions.
Among other things the tables of the article give the deviation δEg,S of
the energy gain per mass in the solar cm system from the predicted gain.
The anomalous energy gain in rest Earth cm system is ∆EE ' v · ∆v and
allows to deduce the change in velocity. The general order of magnitude is

4

Quantization in Astrophysics ...

436

∆v/v ' 10−6 for Galileo-I, NEAR and Rosetta but consistent with zero for
Cassini and Messenger. For instance, for Galileo I one has v∞,S = 8.949
km/s and ∆v∞,S = 3.92 ± .08 mm/s in solar cm system.
Many explanations for the effect can be imagined but dark matter is
the most obvious candidate in TGD framework. The model for the Bohr
quantization of planetary orbits assumes that planets are concentrations of
the visible matter around dark matter structures. These structures could
be tubular structures around the orbit or a nearly spherical shell containing
the orbit. The contribution of the dark matter to the gravitational potential
increases the effective solar mass Mef f,S . This of course cannot explain the
acceleration anomaly which has constant value.
For instance, if the space-craft traverses shell structure, its kinetic energy
per mass in Earth cm system changes by a constant amount not depending
on the mass of the space-craft:
G∆Mef f,S
∆E
' v∞,E ∆v = ∆Vgr =
.
m
R

(4)

Here R is the outer radius of the shell and v∞,E is the magnitude of asymptotic velocity in Earth cm system. This very simple prediction should be
testable. If the space-craft arrives from the direction of Sun the energy increases. If the space-craft returns back to the sunny side, the net anomalous
energy gain vanishes. This has been observed in the case of Pioneer 11
encounter with Jupiter [4].
The mechanism would make it possible to deduce the total dark mass
of, say, spherical shell of dark matter. One has
∆M
MS

'

K =

∆v 2K
,
v∞,E V
2
v∞,E
GMS
, V =
.
2
R

(5)

For the case considered ∆M/MS ≥ 2×10−6 is obtained. One might consider
the possibility that the primordial dark matter has concentrated in spherical
shells in the case of inner planets as indeed suggested by the model for
quantization of radii of planetary orbits. Note that the amount of dark mass
within sphere of 1 AU implied by the explanation of Pioneer anomaly would
be about 6.2 × 10−6 MS from Pioneer anomaly whereas the mass of Earth
is ME ' 5 × 10−6 MS . Since the orders of magnitude are same, one might
consider the possibility that the primordial dark matter has concentrated
5

Quantization in Astrophysics ...

437

in spherical shells in the case of inner planets as indeed suggested by the
model for quantization of radii of planetary orbits. Of course, the total
mass associated with 1/r density quite too small to explain entire mass of
the solar system.
In the solar cm system the energy gain is not constant. Denote by v i,E
and v f,E the initial and final velocities of the space-craft in Earth cm. Let
∆v be the anomalous change of velocity in the encounter and denote by θ
the angle between the asymptotic final velocity v f,S of planet in solar cm.
One obtains for the corrected Eg,S the expression

Eg,S =

i
1h
(v f,E + v P + ∆v)2 − (v i,E + v P )2 .
2

(6)

This gives for the change δEg,S
δEg,S ' (v f,E + v P ) · ∆v ' vf,S ∆v × cos(θS )
= v∞,S ∆v × cos(θS ) .

(7)

Here v∞,S is the asymptotic velocity in solar cm system and in excellent
approximation predicted by the theory.
Using spherical shell as a model for dark matter one can write this as

δEg,S =

v∞,S G∆M
cos(θS ) .
v∞,E R

(8)

The proportionality of δEg,S to cos(θS ) should explain the variation of the
anomalous energy gain.
For a spherical shell ∆v is in the first approximation orthogonal to vP
since it is produced by a radial acceleration so that one has in good approximation

δEg,S ' v f,S · ∆v ' v f,E · ∆v ' vf,S ∆v × cos(θS )
= v∞,E ∆v × cos(θE ) .

(9)

For Cassini and Messenger cos(θS ) should be rather near to zero so that
v∞,E and v∞,S should be nearly orthogonal to the radial vector from Sun in
these cases. This provides a clear cut qualitative test for the spherical shell
model.
6

Quantization in Astrophysics ...

438

References
[1] M. Pitkänen (2006), Physics in Many-Sheeted Space-Time.
http://www.helsinki.fi/∼matpitka/tgdclass/tgdclass.html.
[2] C. Seife in New Scientist, No 2151, Sept. 12, 1998, p. 4.
[3] J. D. Anderson et al (1998), Phys. Rev.Lett. Vol. 81, No 14,p. 2858.
[4] J. D. Anderson et al (2006), The energy Transfer Process in Planetary
Flybys, astro-ph/0608087.
[5] D. Da Roacha and L. Nottale (2003), Gravitational Structure Formation
in Scale Relativity, astro-ph/0310036.
[6] A. Rubric and J. Rubric (1998), The Quantization of the Solar-like
Gravitational Systems, Fizika B7, 1, 1-13.
Idid (1999), Square Law for Orbits in Extra-solar Planetary Systems,
Fizika A 8, 2, 45-50.
[7] C. J. Masreliez (2001), Do the planets accelerate.
http://www.estfound.org.
[8] C. J. Masreliez (2001), Expanding Space-Time Theory,
http://www.estfound.org.
[9] Y. B. Kolesnik (2000), Applied Historical Astronomy, 24th meeting of
the IAU, Joint Discussion 6, Manchester, England.
Ibid (2001a), Journees 2000 Systemes de reference spatio-temporels,
J2000, a fundamental epoch for origins of reference systems and astronomomical models, Paris.
[10] C. Oesterwinter and C. J. Cohen (1972), Cel. Mech. 5, 317.
[11] The chapter TGD and Astrophysics of [1].
http://www.helsinki.fi/∼matpitka/tgdclass/tgdclass.html#astro.

7

Quantization in Astrophysics ...

439

RELATIVISTIC OSCILLATOR
IN QUATERNION RELATIVITY
A.P.Yefremov
Institute of Gravitation and Cosmology,
Peoples Friendship University of Russia
Moscow, Russia.
E-mail: a.yefremov@rudn.ru
www.rudn.ru
Abstract
In the framework of Quaternion (Q-) Theory of Relativity implying invariance of the 6D-space-time vector “interval” the kinematics of two frames is considered under condition that one frame
is inertial and the other is subject to action of harmonic force. Using mathematical tools of Q-relativity the cinematic problem is
completely solved from the viewpoint of each frame, i.e. distance,
velocity and acceleration are found as functions of observers’ time.
Majority of cinematic relations are revealed to be represented by
exact expressions: elementary functions and series; some relations
though are found only approximately. Observed motions are of
course not harmonic functions. Clock paradox is discussed.
I. Introduction: Q-relativity in short
There are many types of physics theories based on more than
three space-time dimensions, but the only one, EinsteinMinkowski 4D theory, has comprehensive reasons for number of
its dimensions. All others, beginning from 5D Kaluza-Klien theory
up to 21D supergravity or to 2nD Calabi-Yau string theory spaces,
are heuristically postulated. It is worth mentioning that several attempts to build “symmetric” 6D-relativities (3D-space + 3D-time)
were made by Cole, Starr, Pavshic, Recami and others (see e.g. [1]
and ref. therein). But the symmetry introduced also “ad hoc” together with abelian character of multiplication inherited from Ein1

Quantization in Astrophysics ...

440

stein’s relativity lead in these theories to a series of interpretational
difficulties.
Differently from these patterns 6D-theory of Q-relativity (or
Rotational Relativity) suggested in 1996 [2,3] does not result from
phenomenological considerations but is extracted from quaternion
mathematics as its modest but quite natural part. The extraction
goes through following six steps. First, basic multiplication rule for
Q-numbers is discovered to be form-invariant under Q-units
transformations composing rotational group SO(3,C). Second, and
this was pointed out by W.Hamilton, three “imaginary” Q-units,
behave exactly as a Cartesian vector triad. Fourth, it is shown that
real rotations from SO(3,C) save form of Q-vector (with real
components) defined in a Q-triad. Fifth, similar form-invariance
property is observed for biquaternionic (BQ) vectors under mixed
real-imaginary rotations reducing the initial group to SO(1,2); this
distinguishes the set of BQ-vectors with definable norm. All these
facts have purely mathematical nature with no evident relevance to
physics. But knowledge that SO(3,C) and its subgroup SO(1,2) are
closely related to Lorents group hints to make the sixth “physical”
step: the BQ-vector components are taken for space and time “displacements”, space-time acquires 3+3-symmetric geometry, and
the basic BQ-vector turns out nothing else but a specific 6D Qsquare root of the interval of Einstein’s relativity. Since no limitations are found for rotation parameters one is free to operate with
inertial as well with non-inertial Q-frames.
So, Q-Relativity exploits the fact of SO(1,2)-invariance of 6Dspace-time biquaternionic vector “interval”
dz = (idt k + dx k ) q k

with definable real norm
dz 2 = dt 2 − dx 2 .

If a Q-frame composed of “imaginary” vector Q-units

2

Quantization in Astrophysics ...

441

Σ ′ ≡ q k ′ = {q 1′ , q 2′ , q 3′ } ∗

is observed from another analogous Q-frame Σ , then the dz invariance results in a simple relation for time and space “displacement” vectors
idt ′q 1′ = idt q 1 + dx q 2 ,

vectors dt and dr obviously orthogonal to each other. The last
condition naturally distinguishes scalar time out of 6dimensionality, and allows regarding physical situations. The Qframes may depend in general on 6 real parameters representing
spatial rotations and boosts; in their turn the parameters are not
banned to be variable e.g. dependent on time of observers hiding at
the origins of the frames which are in this case non-inertial but
nevertheless well described in the Q-approach. Technological tool
of the theory is a Rotational Equation (RE) of the type
Σ′ = O Σ

where O is a combination of real R and hyperbolic H rotations
from SO(1,2) “converting” the frame of the observer Σ into the
observed frame Σ ′ . From the RE cinematic effects of Q-frames
relative motion are easily calculated, among them all effects of
Einstein’s Special Relativity and a number of non-inertial motion
effects, e.g. hyperbolic motion and Thomas precession [4].
The Q-relativity also represents a good mean to study noninertial clock behavior once largely discussed [5]. A desirable
model to illustrate the problem is a “fast” linear harmonic oscillator. In Sect.2 definition of a relativistic harmonic oscillator is given
and full cinematic problem is solved from the viewpoint of inertial
and oscillating observers. Sect.3 is devoted to discussion of twin
paradox issues associated with the solution.

∗

The multiplication rule for Q-units is q k q n = −δ kn + ε knj q j where δ kn , ε knj
are Kroneker and Levi-Civita 3D symbols, summation convention is assumed.

3

Quantization in Astrophysics ...

442

II. Linear harmonic oscillator problem in Q-relativity
Mathematical tool of Q-relativity allows studying behavior of
non-inertially moving clock. A natural model of such a clock is a
harmonic oscillator, “spring pendulum”, arranged so that initial
and final positions of its «massive body» (too, a body of reference
of non-inertial harmonically moving observer) precisely coincide
with position of immobile inertial observer, and relative velocity of
the two observers at these moments is zero.
Let Σ be inertial frame and Σ′ represent non-inertial frame
whose body of reference is subject to action of a periodical harmonic force along a straight line. Since kinematics of the system is
the focus of this study nature of the force here is of no importance.
CASE A. Σ′ IS OBSERVED FROM Σ
If inertial frame Σ is modeled by a constant Q-triad q k whose
vector q 2 is aligned with frames relative velocity, then rotational
equation interconnecting two frames in question has the form
Σ ′ = H 3ψ (t ′) Σ

(1)

′
with H 3ψ (t ) being 3 × 3 -matrix of simple hyperbolic rotation (about
axis parallel to q 3 ) and variable parameter ψ (t ′) depending on
time of moving observer

ψ

H3

⎛ cosh ψ
⎜
= ⎜ i sinh ψ
⎜ 0
⎝

− i sinh ψ

0⎞
⎟
0⎟ .
1 ⎟⎠

cosh ψ
0

The first row of Eq.1
q 1′ = cosh ψ q 1 − i sinh ψ q 2 ,

under standard conditions
cosh ψ =

dt ′
,
dt

(2)

V = tanhψ

(3)
4

Quantization in Astrophysics ...

443

is equivalent to biquaternionic vector basic in Q-theory of relativity (fundamental velocity equals unity: с = 1 )
dz = idt ′q 1′ = idt q 1 + dx q 2

(4)

so that Σ-time and Σ′-time are aligned respectively with q1 , and
q1′ . Eq.4 yields main cinematic vector characteristics, i.e. for Σ′observer one readily finds relative Q-vector velocity
v′ ≡

dz
= q 1′
idt ′

and Q-vector acceleration
a′ ≡

dv ′ dq 1′
=
= −iω 1′ 2′ q 2′ ≡ a ′ q 2′ .
idt ′ idt ′

(5)

where the only non-vanishing component of Q-connection [3]
ω 1′ 2′ = i

dψ
dt ′

is computed as
ω′ =

dH −1
H .
dt ′

Thus value of Q-acceleration (5) aligned with q 2′ is simply expressed through velocity parameter
a ′(Σ ′) =

dψ
.
dt ′

It is natural to attribute to Q-frame a type of motion corresponding
to the type of acceleration “felt” by observer in this frame. Well
known example is the hyperbolic motion where non-inertial observer subject to constant acceleration feels constant force acting
on him [6]. Similarly motion of Σ′ is represented as harmonic one

5

Quantization in Astrophysics ...

444

if Σ′-acceleration, force per unit mass, obeys harmonic, e.g. cosine,
law measured in its own time∗∗
a ′(Σ ′) =

dψ
= Ω ′β cos Ω ′t ′ ;
dt ′

(6)

here β is a real constant (amplitude is chosen in this form for future commodity reasons), acceleration is maximal for t ′ = 0 . Integration of Eq.6 gives dependence of hyperbolic parameter on
proper Σ′-time
ψ (t ′) = β sin Ω ′t ′ .

(7)

Constant of integration, initial phase, is chosen zero so that at the
beginning and the end of oscillation period relative velocity vanish.
It is worth noting here that the hyperbolic parameter, not velocity
itself, has to be a harmonic function.
Σ′(Σ) time ratio
Now complete cinematic problem for the regarded mechanical
system can be solved, i.e. coordinate, velocity and acceleration of
Σ′ are to be found as functions of Σ-observer’s time. But preliminary integration of the time-correlation equation resulting from
Eq.2
dt ′ = dt cosh ψ (t ′) ,

is necessary to determine Σ′-Σ observers’ times interdependence
t = ∫ cosh ψ (t ′) dt ′ = ∫ cosh( β sin Ω ′t ′) dt ′ .

(8)

An easy analysis shows that the integral can be computed exactly,
not in elementary functions but as series. First, one applies wellknown development of hyperbolic cosine
∞

1
u 2n ,
n =1 ( 2 n )!

cosh u = 1 + ∑

u <∞,

∗∗

Phase of the oscillation is chosen so that at initial and final moments of oscillation period velocity vanishes.

6

Quantization in Astrophysics ...

445

last condition being always fulfilled since u ≡ β sin Ω ′t ′ < ∞ . Second, one uses the following table integral
2n
∫ sin y dy =

(−1) n
1 ⎛ 2n ⎞
⎜
⎟
y
+
2 2n ⎜⎝ n ⎟⎠
2 2n−1

n

∑ (−1)

k

k =0

⎛ 2n ⎞ sin (2n − 2k ) y
⎜⎜ ⎟⎟
.
⎝ k ⎠ 2n − 2k

(9)

And third, the substitution y ≡ Ω ′t ′ in Eq.9 gives the sought for
result of integration in Eq.8
β 2n ⎡ 1 ⎛ 2n⎞

(−1) n
⎢ 2n ⎜⎜ ⎟⎟ t ′ + 2n−1
2
n=1 (2n)! ⎢
⎣2 ⎝ n ⎠
∞

t = t′ + ∑

n−1

∑ (−1)

k

k =0

⎛ 2n⎞ sin(2n − 2k) Ω′t ′ ⎤
⎜⎜ ⎟⎟
⎥.
⎝ k ⎠ (2n − 2k) Ω′ ⎥⎦

(10)

This result compels to recall that obtaining exact solutions in
framework of relativity theory is a remarkable feature of simple
and correctly formulated physical problems such as hyperbolic or
circular motion. The relativistic oscillator problem seems to belong
to the distinguished set.
One oscillation is completed when Ω ′T ′ = 2π , T ′ being oscillation period measured in Σ′. Corresponding Σ-time interval, “period” T , is straightforwardly found from Eq.10
∞
⎛
β 2 n 1 ⎛ 2n ⎞ ⎞⎟
⎜ ⎟⎟ ,
T = T ′ ⎜⎜1 + ∑
2n ⎜
⎟
n =1 ( 2 n )! 2
⎝ n ⎠⎠
⎝

(11)

as well as respective cycle frequencies ratio
∞
⎛
β 2 n 1 ⎛ 2n ⎞ ⎞⎟
⎜ ⎟⎟ .
Ω ′ = Ω ⎜⎜1 + ∑
2n ⎜
⎟
n =1 ( 2 n )! 2
⎝ n ⎠⎠
⎝

(12)

Eq.12 tells that Σ-observed periodic motion possesses less frequency than oscillations felt by Σ′-observer; the fact sounds conventionally in relativity: moving clock is apparently slow.
Inversion of Eq.10 is evidently hardly possible, so expression
of Σ′-time as a function of Σ-time t ′(t ) is looked for in approximation. First several terms of series in Eq.10 are written as

7

Quantization in Astrophysics ...

446

t = t′ +

β2 ⎛
1
⎞
sin 2Ω ′t ′ ⎟ +
⎜t′ −

4 ⎝
2Ω ′
⎠
β ⎡
1 ⎛1
⎞⎤
+
3t′ −
⎜ sin 4Ω ′t ′ − 2 sin 2Ω ′t ′ ⎟⎥ + ...
⎢
′
Ω ⎝4
8 ⋅ 4! ⎣
⎠⎦
4

(10a)

Dimensionless factor β may be given in the form
β = V0 / c << 1 ,

where V0 can be any characteristic value of relative velocity, e.g.
mean value for ½ of period. Then the following ratios connecting
2
frequencies and times are written up to the terms including β

Ω=

Ω′
1+ β 2 / 4

t′ = t −

,

(13a)

β2 ⎛

1
⎞
sin 2Ω t ⎟ .
⎜t −
4 ⎝
2Ω
⎠

(13b)

Substitution of Eqs.13 into expression for velocity parameter
(Eq.3) allows to find approximate solution of the cinematic problem for Σ-observer.
Σ′(Σ) Velocity
Velocity value of the frame Σ′ observed from Σ is (fundamental
velocity c is explicitly shown)
7
⎛ 1
⎞
V (t ) = c tanh (β sin Ω′t ′) ≅ V0 sin Ω t ⎜1 − β 2 + β 2 cos 2 Ω t ⎟ .
12
⎝ 3
⎠

At the beginning and at the end of period velocity acquires minimal value V (T ) = 0 , i.e. at these moments the two frames are really
immobile relative to each other. Maximal value of the velocity
⎞
⎛ 1
V (T / 4 ) = V0 ⎜1 − β 2 ⎟
⎝ 3 ⎠
may be regarded as “V-amplitude”
8

Quantization in Astrophysics ...

447

~
⎛ 1 ⎞
V = V0 ⎜1 − β 2 ⎟ ,
⎝ 3 ⎠

and final expression has the form
7 2
~
⎛
⎞
β cos 2 Ω t ⎟
V (t ) ≅ V sin Ω t ⎜1 +
12
⎝
⎠

(14)

meaning that the periodic process observed from Σ definitely has
no harmonic character.
Σ′(Σ) Acceleration
Value of Σ-observed acceleration of Σ′ is found as
a (t ) =

dV (t )
7
7
~
⎛
⎞
≅ ΩV cos Ω t ⎜1 − β 2 + β 2 cos 2 Ω t ⎟ .
6
4
dt
⎝
⎠

Minimal value acceleration acquires at the middle of period

a (T / 4) = 0 ,
and maximal value, “a-amplitude”, at its end
~⎛
7 2⎞ ~
a (T ) = ΩV ⎜1 −
β ⎟= A;
⎝ 12
⎠

the final expression is
~
7
⎛
⎞
a (t ) ≅ A cos Ω t ⎜1 − β 2 sin 2 Ω t ⎟ .
4
⎝
⎠

(15)

Σ′(Σ) Coordinate
Σ-coordinate of Σ′ is computed as result of integration
~
V
7 2
⎛
⎞
x(t ) = ∫ V (t )dt ≅ x 0 − cos Ω t ⎜1 +
β cos 2 Ω t ⎟ ,
36
Ω
⎝
⎠

the integration constant
x0 =

~
V ⎛
7 2⎞
β ⎟
⎜1 +
36
Ω⎝
⎠

is chosen to satisfy the following initial conditions
9

Quantization in Astrophysics ...

448

(16)

x(0) = x(T ) = 0 ,

x(T / 4) = x 0 ,

x(T / 2) = 2 x 0 ,

meaning, that at the initial and final moments of period the two
frames are not only relatively immobile but too are found at the
same point in space.
Thus the Σ′(Σ)-cinematic problem is solved in approximation
β << 1 ; the result is given in Eqs.10, 14, 15, 16.
CASE B. Σ IS OBSERVED FROM Σ′
Rotational equation for this case
Σ = H 3−ψ ( t ′) Σ ′ ,

(17)

has the same parameter

ψ (t ′) = β sin Ω′t ′
describing harmonic oscillations of Σ′. The first row of Eq.17 represent space-time vector “interval”
idtq 1 = idt ′ q 1′ − dx ′ q 2′

(18)

where dx ′ is space displacement of Σ and dt ′ is respective time
interval, both measured by non-inertially moving Σ′-observer.
Calculated from Eq.18 proper Q-vector acceleration of Σ is naturally zero
a≡

dq 1
=0,
idt

so study of this case considers cinematic magnitudes only as they
are seen from genuinely accelerated frame Σ′.
Σ (Σ′)-time ratio
Following from Eq.18 standard relativistic expression
dt ′ = dt cosh ψ (t ′)

leads to integral determining t (t ′) functional dependence
10

Quantization in Astrophysics ...

449

t=∫

dt ′
= sech ( β sin Ω ′t ′) dt ′ .
cosh ψ (t ′) ∫

(19)

This integration also can be performed exactly due to existence of
development
∞

sech u = 1 + ∑ (−1) n
n =1

E n 2n
u ,
( 2n)!

u <π /2

(20)

and table integral already used in the Case A and given by Eq.9.
Series in Eq.20 includes Euler numbers
En ≡

2 2 n + 2 ( 2n)!

π 2 n +1

( −1) k −1
∑
2 n +1 ,
k =1 ( 2 k − 1)
∞

and its convergence condition
u ≡ β sin Ω ′t ′ < π / 2 ,

is always satisfied since
sin Ω ′t ′ < 1 .
β = V0 / c < 1 ,
Resulting formula of integration in Eq.20 has the form
(−1) n En β 2n ⎡ 1 ⎛ 2n⎞
(−1) n
⎢ 2n ⎜⎜ ⎟⎟ t ′ + 2n−1
(2n)!
2
⎢⎣ 2 ⎝ n ⎠
n=1
∞

t = t′ + ∑

n−1

∑ (−1)

k

k =0

⎛ 2n⎞ sin(2n − 2k) Ω′t ′ ⎤
⎜⎜ ⎟⎟
⎥ . (21)
⎝ k ⎠ (2n − 2k) Ω′ ⎥⎦

Eq.21 permits to Σ′-observer to measure real period of T ′ and cycle frequency Ω ′ and also to calculate similar characteristics T , Ω
theoretically attributed to the frame Σ and to find respective correlations
∞ ( −1) n E β 2 n
⎛
n
⎜
′
T =T 1+ ∑
⎜
( 2n)!2 2 n
n =1
⎝

∞ ( −1) n E β 2 n
⎛
n
⎜
Ω′ = Ω 1 + ∑
⎜
( 2n)!2 2 n
n =1
⎝

⎛ 2n ⎞
⎜⎜ ⎟⎟
⎝n⎠

⎞
⎟,
⎟
⎠

(22)

⎛ 2n ⎞
⎜⎜ ⎟⎟
⎝n⎠

⎞
⎟
⎟
⎠

(23)

11

Quantization in Astrophysics ...

450

of course different from analogous Eqs.11, 12 of the Case A due to
different inertiality properties of the observers. But computing frequency ratio in Eq.23 up to first approximation in β 2
⎛
− E1 β 2
Ω ′ ≅ Ω⎜⎜1 +
2 ⋅ 2!
⎝

⎛ 2⎞⎞
⎜⎜ ⎟⎟ ⎟⎟ ,
⎝ 1⎠⎠

where
E1 =

25 ⎛
1
1
1
1
⎞
1 − 3 + 3 − 3 + 3 − ... ⎟ ≅ 1 ,
3 ⎜
5
7
9
π ⎝ 3
⎠

gives result similar to Case A analog Eq.13a
⎛
β2
Ω ′ ≅ Ω ⎜⎜1 −
4
⎝

⎞
⎟⎟ ,
⎠

(24)

i.e. Σ′-frequency, normally for relativity, is smaller than that of Σ
from the viewpoint of Σ′-observer.
All cinematic functions in the Case B are to depend on time t ′ ,
hence there is no need to inverse variables in Eq.21, giving relation
of time-lines length: proper one, t ′ , and “observed” one, t . Nonetheless it seems useful to put down several first terms of the development
t = t′ −

β2 ⎛

4
1
1
⎞ 5β ⎡
⎤
sin2Ω′t ′⎟ +
t ′ + (2sin4Ω′t ′ − 4sin2Ω′t ′)⎥ + ... (21a)
⎜t ′ −
⎢
4 ⎝ 2Ω′
⎠ 16⋅ 4! ⎣ Ω′
⎦

Comparing Eq.21a with its analogue Eq.10a one notes symmetry
of time-functions in the least β -approximation for Cases A and
B; this is too an expected relativistic result of exchanging observation bases.
2

In the Case B whole of cinematic problem has exact solution.
Σ (Σ′) Velocity
V ′(t ′) = c tanh (β sin Ω ′t ′) .

(24)

12

Quantization in Astrophysics ...

451

For practical purposes it is useful to consider approximation up to
small β 2 = (V0 / c) 2
⎛ β2
⎞
V ′(t ′) ≅ V0 sin Ω′ t ′ ⎜⎜1 −
sin 2 Ω′ t ′ ⎟⎟ .
3
⎝
⎠
Minimal value velocity V (T ′) = 0 acquires at the beginning and the
end of each oscillation, at these moments the two frames are immobile to each other. Maximal value (V-amplitude) velocity has at
quarter of period
V
~
V ′(T ′ / 4 ) = c tanh 0 .
c

Σ (Σ′) Acceleration
a ′(t ′) =

V0 Ω′ cos Ω′ t ′
dV (t ′)
;
=
dt ′
cosh 2 (β sin Ω′t ′)

(25)

2
its β -approximation is

a ′(t ′) =

dV (t ′)
~
≅ ΩV cos Ω t 1 − β 2 sin 2 Ω ′ t ′ .
dt ′

(

)

Minimal and maximal values respectively are

a ′ (T ′ / 4) = 0 ,

a′(T ′) = V0 Ω′ .

Σ (Σ′) Coordinate
x ′(t ′) = ∫ V ′(t ′)dt ′ = ∫ c tanh (β sin Ω ′t ′) dt ′ .

This function too can be integrated exactly due to (i) existence of
divergent series
2 2 n (2 2 n − 1) Bn 2 n −1
u
,
( 2n)!
n =1
∞

tanh u = ∑

u ≡ β sin Ω ′t ′ < π / 2 ,

whose coefficients are Bernoulli numbers tied by recurrent formula
B (2n)(2n − 1)(2n − 2)...(2n − k + 2) ⎤
⎡ 1
1 n −1
− + ∑ (−1) k −1 k
Bn ≡ (−1) n ⎢
⎥
(2k )!
⎦
⎣ 2n + 1 2 k =1
13

Quantization in Astrophysics ...

452

so that B1 = 1 / 6 , B2 = −1 / 30 , and (ii) existence of table integral

∫ sin

2 n −1

(−1) n
y dy = 2 n − 2
2

n −1

∑ (−1)
k =0

k

⎛ 2n − 1⎞ cos ( 2n − 1 − 2k ) y
⎟⎟
⎜⎜
;
2n − 1 − 2k
⎝ k ⎠

here y ≡ Ω ′t ′ . Resulting coordinate function of time has the form
x′(t ′) = x0 +

c ∞ 22n (22n − 1)Bn 2n−1 (−1) n
∑ (2n)! β 22n−2
Ω′ n=1

n−1

⎛ 2n − 1⎞ cos(2n − 1 − 2k)Ω′t ′
⎟⎟
2n − 1 − 2k
⎝ k ⎠

∑(−1) ⎜⎜
k

k =0

, (26)

its β -approximation is
2

x ′(t ′) = x0′ −

V0
⎡ 1 ⎛ 1
⎞⎤
cos Ω′ t ′ ⎢1 − β 2 ⎜1 − cos 2 Ω t ⎟⎥ .
Ω′
⎠⎦
⎣ 3 ⎝ 3

Integration constant
x0′ =

V0 ⎛ 2 2 ⎞
⎜1 − β ⎟
Ω′ ⎝ 3 ⎠

is chosen so that the following conditions are satisfied
x ′(0) = x ′(T ′) = 0 ,

x ′(T ′ / 4) = x 0′ , x ′(T ′ / 2) = 2 x 0′

meaning that at the beginning and the end of oscillation relatively
immobile frames are found at the same space point.
Thus the cinematic problem for Σ′-observer is shown to have
exact solution, it is represented by Eqs.24, 25, 26, and their weekrelativity approximations with β << 1 are given.
Clock paradox discussion
Specific features of the discussed relativistic oscillator model
make it an appropriate cinematic system for discussion of famous
clock paradox formulated in Special Relativity (SR) a centaury
ago. First, one of the two involved frames of the system is always
immobile (inertial) while the other is accelerated hence obviously
non-inertial. Second, at starting and final moments the initial
points of the frames spatially coincide while the frames are recip14

Quantization in Astrophysics ...

453

rocally at rest. Third, oscillating frame itself can serve as a clock
for both observers. And last but not least, the key formulae
describing time correlations are exact solutions.
Frequent explanation of the paradox relates clock delay to noninertiality of motion (e.g. [7,8]). But if one considers two identical
non-inertial frames moving in opposite directions the paradox
seems to arise again: alleged non-inertial (“gravitational”) delay of
the both clocks should be exactly the same, but accordingly to SR
locally, at any moment, each observer detects his/her partner’s
time slowing down. Conventional SR seems not to be able to cope
with the problem.
The clock paradox can be regarded as a result of “one-side”
measurement procedure, when two cinematically different time
intervals of the same observation are measured by a time-unit of
one observer; in this case they obviously will have different
“length”. But if each interval is measured by time-unit of its own
observer the lengths should be equal similar to the case of distance
measurement: indeed, moving ruler seems to immobile observer
shorter but it has the same “number of centimeters” as an identical
ruler at rest.
Discussed above oscillating frame seems to be a successful illustration to the explanation of paradox given further in terms of
space-traveling twins. Fig.1 shows Minkowski diagram of the oscillation process form the viewpoint of inertial (“immobile”) Σtwin. Let time-segments subject to measurement be periods T and
T ′ “observed” from Σ, hence interconnected by Eq.11, while a “Σsecond” (Σ-time-unit) is
τ ≡ T / 4 = π / 2Ω .

Then the “length” of segment T is 4 sec. Period T ′ measured in Σtwin time-units τ appears obviously shorter, so that returning
home twin-traveler is allegedly younger than his brother-observer.
This “one-side” measurement gives incorrect result since for Σtwin not only Σ′-time-segment contracts but Σ′-time-unit too, adequate change of “Σ′-seconds” (in this case all equal) extracted
from Eq.11
15

Quantization in Astrophysics ...

454

q1 : ∆ t

4

4′

Σ

Σ′
light
signal

3

3′

2

T

q1′ : ∆ t ′

projection
of τ′
1

T′

2′
q 2′ : ∆ x′

1′

τ

τ′
q2 : ∆ x

0

0′

Fig.1. Minkowski diagram for relativistic oscillator.

∞
⎛
β 2n 1
τ ′ = τ ⎜⎜1 + ∑
2n
⎝ n =1 (2n)! 2

−1

⎛ 2n ⎞ ⎞
⎜⎜ ⎟⎟ ⎟⎟ ;
⎝ n ⎠⎠

using the units one finds the length of T ′ also to be 4 seconds. If
Σ′-twin on his way sends regular (at each his second) light signals
then Σ-twin receives them irregularly, but during traveling period
Σ-twin will count exactly four such signals. Exchange of the last
16

Quantization in Astrophysics ...

455

signal occurs at the very end of mission, at the same space point, at
zero relative velocity, and the age of twins at the meeting point
remains equal.
Besides, Fig.1 shows that projections of equal in length but different Σ′-time segments onto Σ-time (straight) line are also different, but they change steadily as smooth functions, and in three
points of the twins relative immobility (0-0′, 2-2′, 4-4′) their units
smoothly became equal. This means that this model is free of “lost
time” or “unit gap” features sometimes present in discussions of
non-inertial approach to the paradox from Special Relativity position.
There are two final remarks.
1. Found relativistic solution for oscillator system frequent in
nature and in description of many physical processes remarkably
incorporates to the set of non-inertial cinematic problems already
solved in the framework of Q- relativity theory from the viewpoint
of all involved observers: hyperbolic motion, circular motion and
Thomas-like precession [9].
2. In the study an accent may be made on the method used to
endow a frame with definite non-inertial character. The method
can serve as an instructive example helpful for construction of any
non-inertial frame provided the acceleration law is given; this allows easier formulation and solution of new relativistic problems
involving non-inertially moving observers.
Acknowledgment
The author greatly appreciates kind suggestion of Victor Christianto to incorporate to this book.

17

Quantization in Astrophysics ...

456

References
1. E.A.B.Cole, J.M. Starr (1985) Six-Dimensional Relativity:
Physical Appearance of a Particle whose Path Time
Changes, Lett. Nuovo Cim., , 43 № 8, P. 388-392.
2. A.P.Yefremov (1996) Quaternionic Relativity. I. Inertial
Motion, Gravitation & Cosmology, Vol. 2, № 1, 77-83,
1996.
3. A.P.Yefremov (1996) Quaternionic Relativity. II. NonInertial Motion, Gravitation & Cosmology, Vol. 2, № 4,
335-341, 1996.
4. A.P.Yefremov (2000) Six dimensional Rotational relativity,
Acta Phys. Hung., Series – Heavy Ions, Vol. 11 (1-2), 147153, 2000.
5. A.P.Yefremov (1997) On Twin Paradox in Quaternionic
Relativity, Gravitation & Cosmology, Vol. 3, № 3, 200203, 1997.
6. L.Landau, E.Lifshits (1973), Field Theory. Moscow, Nauka
publ. (in Russian).
7. H.Bondy (1967) Assumption and Mith in Physical Theory,
Cambridge Univ. Press.
8. M.G.Bowler (1976) Gravitation and Relativity, Pergamon
Press, Oxford-N.Y.-Toronto-Sydney-Paris-Frankfurt.
9. A.P.Yefremov (2004) Quaternionic Program, Generalized
Theories and Experiments, Kluwer Acad. Publ., Netherlands, 395-409.

18

Quantization in Astrophysics ...

457

Quaternionic Relativity. II Non-Inertial Motions∗.
Alexander P. Yefremov∗∗
Peoples’ Friendship University of Russia, Miklukho-Maklaya Str., 6, Moscow, 117198,
Russia.

In the framework of six-dimensional quaternionic theory of relativity (a short review is
given) non-inertial frames are reasonably described: uniformly accelerated observer on
rectilinear trajectory and arbitrary accelerated observer on circular orbit. The results are used
to derive exact Thomas precession formula and calculate change of position of Jupiter’s
satellite observed from Earth, an integral cinematic effect for frames with variable relative
velocity.
Section 1. Introduction.
As is shown in previous paper [1] relative motion of particles and frames can be noncontradictorily described within framework of six-dimensional non-Abelian scheme based
upon fundamental properties of quaternionic (Q) algebra. The key point of the scheme is Qmultiplication rule for one “real” unit, 1, and three “imaginary” units q k (k = 1,2,3)
(1)
1qk = qk1 = qk , q k q j = −δ kj 1 + ε kjn q n
where δ kj and ε kjn are Kroneker and Levi-Civita symbols, summation rule is valid. The nonAbelian units q k geometrically can be treated as unit vectors of an orthonormal triad. Such a
triad admits ordinary R-rotations with real parameters Φ ξ (e.g. the angles of subsequent
rotations Φ1 = α , Φ 2 = β , Φ 3 = γ ): R(Φ ξ ) ∈ SO(3, R) . In this case any real Q-vector
a ≡ ak q k is a SO(3,R)-invariant a k q k = a k © q k ′ with q k ′ ≡ Rk ′j (Φ ξ )q j .
Parameters of transformation leaving the multiplication rule (1) intact can be complex,
and in particular pure imaginary: Φ ξ → iΨξ . Then real rotations convert into hyperbolic ones

R(Φ ξ ) → H (Ψξ ) while the respective invariance is sought for a vector biquaternion
u ≡ ( ak + ibk )q k .
The latter can be rewritten as
u = ak q k + bk p k
where p k ≡ iq k are three unit vectors obeying the Pauli-matrices multiplication rule. This new
triad is rigidly attached to q k , but defines scales and directions in a three-space imaginary
with respect to initial one. Necessary condition for the Q-vector u to be invariant under Hrotations is the orthogonality of its vector parts ak bk = 0 . All such biquaternions posses real
norm (zero included). The orthogonality condition allows representing u in the simplest form
u = aq1 + bp 2 ,

∗

Gravitation & Cosmology, Vol. 2 (1996), No.4(8), p.p. 333-341. [Editorial note: this early paper on Qrelativity is included here to accompany more recent paper by the same author: “Relativistic Oscillator in
Quaternion Relativity.” Section 5 in particular may yield new observation.]
∗∗
e-mail: a.yefremov@rudn.ru

Quantization in Astrophysics ...

458

where a, b are lengths of the respective vectors. The complete group of transformations
preserving u invariant is SO(1,2) ⊂ SO(3, C ) , the latter being the most general group of the Qmultiplication rule (1) invariance. It means that provided one q k (e.g. q1 or p1 ) is chosen to

perform about it R-rotations, the other two ( q 2 , q 3 ) can only serve as axes of H-rotations.
Q-relativity arises when u-like vector
dz = dt k p k + dxk q k
(2)
is considered as specific space-time interval with dxk being displacement and dt k respective
change-of-time (both vectors! dxk dt k = 0 ) of a particle observed from a frame of reference
Σ ≡ {p k , q k } . Fundamental velocity is taken for a unity. SO(1,2)-invariance of the “interval”
(2) under finite R- and H-rotations (transfer from one inertial frame to another) leads to
cinematic relations all equivalent to those of the Einstein’s Special Relativity [1].
A most simple constant frame Σ is represented e.g. by Pauli-type matrices
 0 1
0 − i
1 0 
 , q 2 = −i
 , q 3 = −i
 .
(3)
q1 = −i
 1 0
i 0 
 0 − 1
The triad may be realized by a platform with three gyroscopes immobile relatively to “distant
stars”: the observer in Σ ”feels” no acceleration.
On the other hand the most general Q-frames may be functions of complex parameters
Ζξ = Φ ξ + Ψξ :

q k ′ ( Ζξ ) = O ( Ζξ ) k ′j q j .

(4)

There is no evident obstacle for parameters of the transformation O ( Ζ ξ ) ∈ SO (1,2) to be
localized; natural is a frame’s dependence upon its proper time:
Σ′(t ′) = {p k ′ [ Ζξ (t ′)], q k ′ [ Ζξ (t ′)]} .

(5)

Q-frames of the type (5) are non-inertial ones, some of them having very complicated
behaviour. Before considering the situation in general it seems reasonable to analyze
relativistic motion in simple non-inertial cases.
In Section 2 rectilinear uniformly accelerated motion is investigated in detail. Section 3 is
devoted to accelerated circular motion. In Section 3 classical example of Thomas precession
is treated from quaternionic relativity viewpoint. Computation of an integral relativistic effect
for two frames with variable relative velocity is suggested in Section 5. Discussion and
perspectives are found in Section 6.

Section 2. Uniformly accelerated rectilinear motion.
This simplest case of accelerated observer is known as hyperbolic motion [2], [3]. The
motion is usually treated from SR-positions but with the help of necessary additional
assumptions (time-dependence of four-velocity, demand of Fermi-Walker transport of the
observer’s tetrad) appropriate rather for GR. The Q-relativity approach allows treating the
motion without loss of the theory’s logic.
If Σ ′ is a frame uniformly accelerated along its q 2′ , then observer in Σ ′ must feel
acceleration ε = const as in Einstein’s elevator. This implies specific dependence of Σ ′ on its
proper time t ′ ; the dependence is found out of following considerations.
Let Σ ′ move relatively to inertial frame Σ (3) along its q 2 . If Σ ′ is observed then the
simplest form of interval (2) in this case is
dz ′ ≡ dt ′p1′ = dtp1 + drq 2 ,
(6)
what is equivalent to the H-rotation
q′ = H 3ψ q ,

Quantization in Astrophysics ...

459

or in explicit form
 q1′   coshψ − i sinhψ 0  q1 
  
 
coshψ
0  q 2  .
(7)
 q 2′  =  i sinhψ
q   0
0
1  q 3 
 3′  
with
dr
ψ (t ′) = ar tanh .
(8)
dt
Analogously to what was done in [1] for Newtonian non-inertial motion one may compute
cinematic Q-vectors of Σ ′ : its proper Q-velocity:
dz ′
(9)
v′ ≡
= p1′
dt ′
naturally containing the only unit time-like component, and Q-acceleration:
dv′ dp1′
a′ ≡
=
.
(10)
dt ′
dt ′
Computation of Q-acceleration (10) involves notion of quaternionic connection [1, 4]. For
a triad obtained from a constant one as in Eq.(4) the derivative of q k ′ in the group space is
expressed through coefficients of antisymmetric connection ωξk ′n′ = −ωξn′k ′ :

dq k ′
= ωξk ′n′q n′ .
(11)
dΖ ξ
From Eqs.(11), (4) the connection components are found as
dO
ωξk ′n′ = k′m On′m .
(12)
dΖ ξ
If group parameters depend on observer’s time then the time derivative is defined
dq k ′ dΖξ
=
ωξk ′n′q n′ ≡ ω k ′n′q n′
(13)
dt
dt
with
dΖ
ω k ′n′ ≡ ξ ωξk ′n′ .
(14)
dt
Using Eqs.(12), (14) and (7) computation of the Q-acceleration (10) is straightforward
dq
dψ
(15)
a′ = i 1′ = iω1′ 2′q 2′ + iω1′ 3′q 3′ =
q 2′ .
dt ′
dt ′
Eq.(15) states that the only component of a ′ is the acceleration of Σ ′ “felt” by its own
observer, hence
dψ
=ε ,
dt ′
or
(16)
ψ = εt ′ ,
ψ t ′ =0 = 0 .
Now general cinematic problem (i.e. functions of time, coordinate, velocity and
acceleration of the Q-frames) is easily solved.
Case (a). Frame Σ ′ is observed; interval dz ′ has the form (6) therefore
dt = dt ′ cosh(εt ′) .
After integration one obtains time-correlation equations (integration constant is assumed zero)
1
t = sinh(εt ′)

ε

Quantization in Astrophysics ...

460

or

t′ =

1

ar sinh(εt ) =

ε

1

ln[εt + 1 + (εt ) 2 ] .

(17)

ε

Velocity dependence on t is found from v = tanh(εt ′) with t substituted by t ′ from (17)
εt
.
v(t ) = tanh[ar sinh(εt )] =
1 + (εt ) 2
Integration of (18) yields the Σ′ -motion law
1
1
r (t ) =
1 + (εt ) 2 − ,

ε

(18)

(19)

ε

(integration constant is − 1 / ε ), while differentiation of (18) with respect to t gives
acceleration of Σ′ seen from Σ
a (t ) =

ε
.
[1 + (εt ) 2 ]3 / 2

(20)

Cinematic problem is solved; the results precisely repeat those of [2,3]. For small t: t ′ → t ,
a (t ) → ε = const , v(t ) → εt and r (t ) → εt 2 / 2 as it must be for non-relativistic uniformly
1
accelerated motion. If t → ∞ then t ′ → ln(2εt ) → ∞ , a → ε −2 t −3 → 0 , v → 1 and

ε
r → t → ∞ as is natural from SR viewpoint.

Case (b). Frame Σ is observed; the interval takes the form
(21)
dz ≡ dt ′p1′ − dr ′q 2′ = dtp1
where − dr ′ is apparent displacement of the origin of Σ in a time dt ′ measured in Σ ′ . Hrotation parameter is given by Eq.(16), inertial Σ -observer obviously feels no acceleration
dz ′
=0.
a≡
dt
The cinematic problem is solved analogously
dt ′
,
(22)
dt =
cosh(εt ′)
1
1
t (t ′) = arctan[sinh(εt ′)] ≡ arcsin[tanh(εt ′)] ,
(23)

ε

ε

v′(t ′) = tanh(εt ′) ,
1
r ′(t ′) = ln[cosh(εt ′)] ,

(24)
(25)

ε
ε

.
(26)
cosh 2 (εt ′)
Behaviour of the quantities in characteristic points of time-ray is the following. If t ′ → 0 then
t → t ′ , a ′(t ′) → ε = const , v′ → εt ′ and r ′(t ′) → εt ′ 2 / 2 as is normal for non-relativistic
uniformly accelerated motion. If t ′ → ∞ then a ′ → 2ε e −2εt′ → 0 , v′ → 1 , r ′ → t ′ → ∞ which
agrees with notions of SR. Asymptotic behaviour of time is specific. Eq.(23) implies that
observer in Σ ′ finds the clock of Σ more and more slow; at infinite time t ′ → ∞ the Σ πc
(c is the fundamental velocity).
clock tends to stop on the value
2ε
An important feature of time measurement must be emphasized here. The matter is that
Eqs.(17-20) and (23-26) give such values of cinematic quantities as if frames Σ ′ and Σ
a ′(t ′) =

Quantization in Astrophysics ...

461

exchange information instantly. Actually in vacuum the signal travels with velocity c = 1 , so
the information about physical status of the object reaches distant observer at a later time.
In the case (a) this time is
(27)
t r = t + r (t ) .
Substitution of Eq.(19) into (27) allows to express t as function of t r

1 
1 
.
t = t r 1 +
(28)
2  1 + εt r 
Now instant time t can be replaced in Eqs. (17-20) by retarded time t r that the observer reads
from his clock. Thus recalculated velocity of Σ ′ is

1 

εt r 1 +
1 + εt r 

.
(29)
v(t r ) =
2
 
1 

4 + εt r 1 +
  1 + εt r 
Closely to zero and asymptotically t and t r are similar: if t → 0 , then t r → t → 0 ; if t → ∞ ,
then t r → 2t → ∞ .
In the case (b) the retarded time is
(30)
t r′ = t ′ + r ′(t ′) .
Time-recalculation formula follows from Eqs.(25) and (30)
1
t′ =
ln(2e εtr′ − 1) .
(31)
2ε
Eq.(31) helps to introduce time t r′ into Eqs.(23-26); e.g. “observed” velocity of Σ for Σ ′
clock is
1

′
(32)
v′(t r′ ) = tanh  ln(2eε t r − 1) .
2

For t ′ → 0 , t ′ → ∞ behaviour of t ′ and t r′ is similar.
The retarded time problem within framework of non-inertial relativity is much wider. It
comprises experimental aspects: since position and velocity of an object are really measured
in retarded time, there might be a need for knowledge of “instant” values. Establishing of
mathematical ties between retarded quantities also will be helpful to see the whole picture on
the base of available data. Detailed analysis of these aspects will be given in a separate paper.
Section 3. Circular motion.
The simplest curvilinear accelerated motion is circular motion. In the Q-relativity
framework it needs at least two types of group parameters: a real (rotational) one and an
imaginary (hyperbolic) one.
Let the origin of Σ lie in the centre and vectors q 2 , q 3 in the plane of circular orbit (with
radius R) of the frame Σ ′ (Fig.1). There are two steps in building cinematic relativistic model.
The first step is non-relativistic construction of the object’s displacement. In the base Σ
the coordinates of Σ ′ are
x2 = R cosα (t ) , x3 = R sin α (t ) , x3 = 0 .
Make one of the triad vectors, say the third, be always parallel to the Σ ′ velocity
x& k
q3 ≡
q k = − sin α q 2 + cos α q 3 ;
x& j x& j
this as a part of the simple R-rotation

Quantization in Astrophysics ...

462

Σ = R1α (t ) Σ ,
or in explicate form
q1  1
0
  
 q 2  =  0 cosα
 q   0 − sin α
 3 

0  q1 
 
sin α  q 2  .
cosα  q 3 

(33)

From non-relativistic viewpoint interpretation of Σ is two-folded. On one hand it can be
treated as a frame rotating in the orbit’s centre with angular velocity ω (t ) ≡ α& ; its q 2 is
constantly chasing the origin of the frame on orbit. On the other hand Σ represents a frame
revolving on the orbit with speed v = ωR its displacement being dr = ωRdt .

The second step is to “switch” relativity, i.e. to “H-rotate” Σ at “angle” ψ = ar tanh(ωR)
about q 2 , so that change-of-time vector becomes aligned with q1 = q 1 not involved into
description of space cinematic quantities

Σ′ = H ψ2 Σ ,
or
 q1′   coshψ
  
 q 2′  =  0
 q   i sinhψ
 3′  

0 − i sinhψ  q 1 
 
1
0
 q 2  .
0 coshψ  q 3 

(34)

Altogether Σ ′ is combination of two subsequent rotations (20) and (21) subject to SO(1,2)
symmetry
Σ′ = H ψ2 R1α Σ .
(35)
Q-acceleration felt by Σ ′ -observer
dq
a′ = i 1′ = iω1′ 2′q 2′ + iω1′ 3′q 3′
dt ′

Quantization in Astrophysics ...

463

is found from (35) with the help of Eqs.(12, 14)
dα
dψ
a′ = −
sinhψ q 2′ +
q 3′ .
dt ′
dt ′
Here are normal (centripetal)
dα
a norm = −
sinhψ
dt ′
and tangent (angular)
dψ
a tan =
dt ′
components of the acceleration.
For simple cases the components are readily found. Uniform motion implies
ψ = ar tanh ω ′R′ = const ( ω ′ , R′ are angular velocity, and the orbit’s radius for Σ ′ -observer),
therefore a tan = 0 , α = ω ′t ′ , a norm = ω ′ 2 R ′ cosh ψ = const . For uniformly accelerated motion

ψ = ar tanh ω ′R ′ = λt ′ with λ = const , then a tan = λ , a norm = ω ′ 2 (t ′) R ′ cosh(λt ′) . These are
quite expected results.
Farther analysis of circular motion is made for general form of the hyperbolic parameter
ψ = ψ (t ′) that is assumed given.
Case (aa). Frame Σ ′ is observed.
The interval expression is read from the first row of matrix Eq.(34) for rotating (noninertial) base Σ , or from the first row of Eq.(35) for the inertial base Σ
(36)
dz ′ ≡ idt ′q1′ = idt q 1 + dtRω q 3 = idt q1 + dtRω (− sin α q 2 + cos α q 3 ) .
Procedure of solving the cinematic problem is analogous to that of Section 2. From
Eq.(36) it follows
(37)
dt = dt ′ coshψ (t ′) ,
t = ∫ dt ′ coshψ (t ′) ,

(38)

and the inverse function t ′ = t ′(t ) is possibly found. The latter is used in expression for
angular velocity
1
ω (t ) = tanhψ [t ′(t )] .
(39)
R
Eq.(38) yields Σ -time dependence of rotation angle
α (t ) = ∫ ω (t )dt
(40)
and tangent acceleration
dω (t )
1
dψ
.
(41)
a tan (t ) = R
=
3
dt
cosh ψ dt ′
These are all quantities available for Σ -observer. Observer in Σ additionally makes
conclusion about normal part of the acceleration
2

 dα (t ) 
a norm (t ) = R
(42)
 .
 dt 
Eqs.(38-42) represent solution of the cinematic problem.
An essential note must be made here. An arc segment collinear to relative velocity is
relativistically contracted as in SR
dl = dl ′ coshψ ,
while the orbit’s radius perpendicular to velocity and not involved into transformations
remains the same for Σ and Σ ′
(43)
R′ = R .

Quantization in Astrophysics ...

464

Hence in the case (a) the angle measures are related as
dl dl ′
dα = =
coshψ = dα ′ coshψ .
(44)
R R′
The last ratio together with Eqs.(43, 37) gives unique numerical value of the frames’ angular
velocities for respective observers
dα dα ′
(45)
ω (Σ) =
=
= ω ′(Σ′) .
dt
dt ′
This result agrees with the axiomatic fact that for Σ and Σ ′ value of relative velocity (or
hyperbolic parameter) is the same
v = tanh ψ = ωR = ω ′R′ .
Case (bb). Frame Σ is observed.
The interval for Σ (and Σ ) is equivalent to the first row of matrix equation
Σ = R1−α H 2−′ψ Σ′
inverse to Eq.(35)
dz = dtp1 = dtp 1 = dt ′p1′ − R′ω ′dt ′q 3′
(46)
with
ω ′R ′ = ωR = tanhψ (t ′) .
Q-acceleration of Σ vanish
dz
a≡
=0
dt
since Σ -observer is considered genuinely immobile. In this case
dt ′ = dt coshψ
dt ′
.
(47)
t=∫
coshψ (t ′)
Σ ′ -observer is able to measure apparent cinematic quantities: velocity, angle and tangent
acceleration
v = tanhψ (t ′) ,
(48)
tanhψ (t ′)
α (t ′) = ∫
dt ′ ,
(49)
R
1
dψ
.
(50)
a tan =
2
cosh ψ dt ′
Eqs.(47-50) give solution of cinematic problem in the case (bb). Reasonable behaviour of
the quantities is readily verified for simple types of circular motion in non-relativistic and
ultrarelativistic limits.
In both cases (aa) and (bb) observers receive in fact retarded signals. But contrary to
situation for rectilinear motion, here constant time delay does not influence noticeably values
of cinematic quantities.

Section 4. Thomas precession.
Apparent rotation of genuinely constant “spin” vector of the top relativistically revolving
about the origin of an inertial frame (Thomas precession) is regarded in SR either when
circular trajectory is approximated by straight line segments [5] or when Fermi-Walker
transport of vectors is postulated [3]. Quaternionic relativity suggests a shorter and more
consistent way (from logical viewpoint) to describe the phenomenon.
Let Σ be an immobile Q-frame (3) in the centre of the circular orbit of another Q-frame,
Σ ′′ uniformly revolving about Σ . Respective observers find space vectors of their frames

Quantization in Astrophysics ...

465

constantly oriented. Notice that Σ ′′ can be obtained from Σ ′ determined by Eq.(35) with the
help of inverse R-rotation at appropriate angle −α ′
Σ′′ = R1−′α ′ H ψ2 R1α Σ ,
(51)
or in explicit form
0
0  coshψ 0 − i sinhψ  1
0
0  q1 
 q1′′   1
  


 
1
0
(52)
 q 2′′  =  0 cosα ′ − sinα ′ 0
 0 cosα sinα  q 2  .
 q   0 sinα ′ cosα ′  i sinhψ 0 coshψ  0 − sinα cosα  q 
 3′′  
 3 


Suppose that angles of rotation are calculated in terms of laboratory time t: α = ωt ,
α ′ = ω ′(Σ)t . For the base Σ′ proper (real) period T ′(Σ′) of its retrograde (second) rotation
measured in Σ ′ due to Eq.(47) is related to apparent period T ′(Σ) measured in Σ as
T ′(Σ′) = T ′(Σ) coshψ .
Then proper cyclic frequency of the rotation (measured in Σ ′ ) is
2π
2π
ω ′(Σ)
ω ′(Σ′) =
=
=
,
T ′(Σ′) T ′(Σ) coshψ coshψ
or, taking into account Eq.(45),
(53)
ω ′(Σ) = ω (Σ) coshψ :
cyclic frequency of Σ ′ second rotation “seen” from Σ is coshψ times bigger than that of
first rotation of Σ measured in itself and needed to chase Σ ′ .
Now compute change of the top’s spin direction seen from Σ while in Σ ′′ being
constantly pointed at a distant star, say, along q 2′′ , space unit vector of Σ ′′ . From Eq.(52)
vector q 2′′ in projections onto unit vectors of Σ is
q 2′′ = −i sinhψ sin α q1 + (cosα cosα ′ + coshψ sin α sin α ′)q 2 +
(54)
+(sin α cosα ′ − coshψ cosα sin α ′)q 3 .
Projections of q 2′′ onto spatial directions of Σ allow us determining apparent precession, e.g.
projection onto q 3
1
q 2′′ 3 = sin(α − α ′) − (coshψ − 1) sin α sin α ′ ,
(55)
2
or after some algebra
ψ
q 2′′ 3 = sin{[ω (Σ) − ω ′(Σ)]t} − sinh 2 sin ω (Σ)t sin ω ′(Σ)t .
(56)
2
Angular velocity of the first rhs term in Eq.(56) ωT ≡ ω − ω ′ corresponds to “mostly
noticeable” Thomas precession. Due to Eq. (53) it can be presented as
(59)
ω T = ω (1 − coshψ ) ;
for small relative Σ - Σ ′′ velocities it takes the known form
ω
ωT = − v 2
(60)
2
with v = ωR . The second right-hand side term of Eq.(56) describes much “less noticeable”
precession since its amplitude is v 2 c 2 less than that of the first term. For small relative

v2
sin 2ωt .
4
It is worth to note that results given by Eqs.(55, 56) precisely coincide with those found in
[3]. The only difference is that due to SO(1,2)-symmetry preserving choice of R- and Hrotation axes no time-like components of spin ever appear.
velocities the second term is −

Quantization in Astrophysics ...

466

Section 5. Jupiter’s satellite.
Presented relativistic treatment of circular motion suggests the following observational
experiment aimed to control consistence of the theory. Consider a part of Solar system (Fig.2)
~
where Σ is constant Q-frame with Sun as reference body, Σ ′ is attached to Earth, and Σ
belongs to Jupiter. In non-relativistic limit relative Jupiter-Earth velocity is
(61)
V 2 = v E2 + v J2 − 2v E v J cos(α − β )
with constant orbit velocities of the planets
v E = ω E RE , v J = ω J R J
~
and respective radius angles measured from q ~1 linearly depending on Σ time
β = ω J ~t .
α = ω E ~t ,
Picture of a satellite revolving about Jupiter and observed from Earth is similar of that
regarded in Section 3. The only difference is that both object Σ and observer Σ ′ are noninertial, their relative speed variable, and this is the crucial point. If the Earth’s observer in a
short period of time measures the satellite angular velocity ω (Σ′) and neglects influence of
relative Earth-Jupiter motion then in time t ′ he computes the rotation angle as
ϕ theor = ω (Σ′)t ′ .
In fact, according to Eq.(53), the apparent angular velocity is
ω (Σ′) = ω coshψ ,
where ω (Σ ) ≡ ω is genuine constant quantity measured on Jupiter. Since velocity of relative
motion is variable, ω (Σ ′) is variable too. Hence the rotation angle really observed from Σ ′ is
found as
ϕ real = ∫ ω (Σ′)dt ′ = ω ∫ coshψ (t ′)dt ′ .
Difference between computed and observed values of the angle is
∆ϕ ≡ ϕ real − ϕ theor = ω[coshψ (t 0′ ) − ∫ coshψ (t ′)dt ′] .
Ratio V c is small, so non-relativistic value of relative velocity (61) is sufficient
2

1 V (t ′) 
.
coshψ ≈ 1 + 
2  c 

Quantization in Astrophysics ...

467

(63)

If at the moment of initial measurement t 0′ velocities of Earth and Jupiter are parallel
r
r
v E (t 0′ ) ↑↑ v E (t 0′ ) , then computed value of apparent angular velocity is minimal and Eq.(63)
takes the form
ω v v  sin(α − β ) 
(64)
∆ϕ ≈ E2 J 1 −
t′ .
α − β 
c

The second term in brackets tends to zero with time so that final formula is
ω vE v J
(65)
t′ .
∆ϕ ≈
c2
For the closest Jupiter satellite (“Metes”) ω = 2.5 ⋅10 −4 sec , v E = 30.4km sec ,
v J = 31.1km sec in one Jupiter year t ′ = 12 Earth years = 3.7 ⋅ 10 8 sec the angle difference
∆ϕ = 4.14 ⋅10 −4 rad = 1.4′ might be observable.

Section 6. Discussion.
Examples given in paper [1] and above show that quaternionic approach to relativistic
kinematics seems to provide consistent description of inertial and non-inertial frames of
reference. In the framework of the theory all cinematic effects of Einstein’s Special Relativity
are found as well as hyperbolic motion and Thomas precession, effects whose standard
descriptions demand additional assumptions. The suggested relativistic scheme due to
vectorial character of its space-time “interval” is obviously convenient to consider frames with
curvilinear trajectories. In particular it permits to obtain plausible results of relativistic circular
motion of general type and predict integral effects for frames with variable relative speed.
Nevertheless several examples whatever successful they were represent only separate
pieces of unique theory. Kinematics of quaternionic relativity is not completed until the most
general types of frames trajectories are taken into account. This is the task of next paper.
References
[1] A.P.Yefremov, Gravitation & Cosmology, Vol. 2 (1996), No.1(5), p.p. 77-83.
[2] L.D.Landau, E.M.Lifshts, “Theory of Fields”, M., Nauka, 1973, p. 39.
[3] Ch.W.Misner, K.S.Thorn, J.A.Wheeler, “Gravitation”, San Francisco, W.N.Freeman &
Co., 1973, p.p. 166, 175.
[4] A.P.Yefremov, Izvestiya Vuzov, Fizika, (1985), 12, p.p. 14-18.
[5] E.F.Tailor, J.A.Wheeler, “Spacetime Physics”, San Francisco, W.N.Freeman & Co., 1966,
p.p. 222-228.

Quantization in Astrophysics ...

468

Relativistic Doppler Effect and Thomas Precession on
ArbitraryTrajectories
(and comment on Pioneer anomaly)
Alexander P. Yefremov∗∗
** Institute of Gravitation and Cosmology , Peoples’ Friendship University of Russia,
Miklukho-Maklaya Str., 6, Moscow, 117198, Russia. http://www.rudn.ru

THEORY
Formulation of question.
There are two relatively (and arbitrary) moving bodies A and B. In some coordinate system
r
r
coordinates of A and B are given as rA (t ) , rB (t ) , t is A-observer’s time. Body B sends light
r rr
signal (with given on B wave-vector k , k k = 0 ) towards body A.
r
Question is: what k components A-observer detects?
Solution.
r
r
~
1. Vectors of relative A-B coordinates r and relative A-B velocity V are found in Σ :
r
r
r r
r r r
r = rB − rA , V = dr / dt = r&B − r&A .

r
2. A-observer sets a 3D Q-frame Σ = {q n } with one vector (e.g. q 2 ) aligned with r , it pursuits
r
r
body B. Another vector of Σ (e.g. q 3 ) lays in the instant plane formed by vectors r and V ,
while vector q1 remains always orthogonal to the plane.
3. Before “switching on” relativity one needs first to spatially rotate Σ → Σ so that new vector
r
q 2 becomes aligned with velocity V , the rotation is done on A in the instant plane about vector
rr
q1 : Σ = O1α (t ) Σ , angle of rotation is found from scalar product: cos α (t ) = r V /( rV ) .
4. Now all is ready for relativity, and frame Σ is “transported” from A to B: Σ → Σ ′ by
hyperbolic rotation so that “time” and “velocity” axes are involved, intact remains vector q 3 :

Σ ′ = O3iψ (t ) Σ , imaginary “angle” of the rotation (velocity parameter) is tanhψ (t ) = V / c ,
c is fundamental velocity.
5. On body B “velocity aligned” frame Σ ′ may be spatially rotated Σ ′ → Σ ′ so that new vector
q 2′ becomes oriented (for technical convenience) oppositely to direction where the light signal is
sent; the rotation is again done in the instant plane about vector q 1′ : Σ′ = O1− β (t ′) Σ ′ , angle of
rotation β (t ′) is determined in B-time t ′ , tied with A-time t by ratio t ′ = ∫ coshψ (t )dt . When
rr
β (t ′) = α (t ′) , cos α (t ′) = RV /( RV ) , i.e. signal is sent form B towards A, α ′(t ) = α (t ′) ≠ α (t ) .

6. The chain of rotations results in one Rotational Equation (main tool of Q-relativity)
Σ ′ = O1− β O3iψ O1α Σ = OΣ ,

∗∗

e-mail: a.yefremov@rudn.ru

1
Quantization in Astrophysics ...

469

(1)

with O ∈ SO(1,2) ; so any bi-quaternion vector is form-invariant under transformation (1), wavevector included: k Σ = k ′ Σ ′ . From Eq. (1) one has Σ = O −1Σ ′ hence invariance equation yields
r
k Σ = k ′ O O −1Σ′ , or k = k ′O . Since all values are known ( k , α ,ψ , β ), all components of wavevector sent from B and “seen” by A-observer are found. Problem is solved.
Comment.
Eq. (1) has exactly the same form as Rotational Equation for classical Thomas precession
problem solved for circular motion of observed spin. Essentially new feature is that here we
succeeded to exhibit construction of rotational equation for the most general case: for arbitrary
trajectories of involved bodies. Thus relativistic Doppler effect and Thomas precession effect are
sister-solutions of the same relativistic model.
CALCULATIONS
r
Determine components of wave-vector k in Σ , if in Σ′ it is given as k n′ = (iω ′ / c, − ω ′ / c, 0) ,
with ω ′ being signal’s frequency. Explicit form of Eq. k = k ′O is
0
⎛1
⎜
(k1 k 2 k 3 ) = (k1′ k 2′ k 3′ ) ⎜ 0 cos β
⎜ 0 sin β
⎝

0 ⎞ ⎛ coshψ
⎟⎜
− sin β ⎟ ⎜ i sinh ψ
cos β ⎟⎠ ⎜⎝ 0

− i sinh ψ cos α
coshψ cos α
− sin α

− i sinh ψ sin α ⎞
⎟
coshψ sin α ⎟ .
⎟
cos α
⎠

First (“time”) component
k1 = k1′ coshψ + ik 2′ coshψ sin β + ik 3′ sinh ψ sin β ,
or

ω = ω ′(coshψ − sinh ψ cos β ) = ω ′ coshψ (1 − tanhψ cos β )
what is the Doppler effect formula

ω′

⎛ V
⎞
⎜1 − cos β ⎟ ,
c
⎠
1 − (V / c) ⎝

ω=

(2)

2

with completely determined functions V (t ) and β (t ) , hence the shift depends on time.
Relativistic non-equality of angles β (t ′) and β (t ) makes this formula slightly different from
that of Special Relativity.
Second (space) component
k 2 = (ω ′ / c) (sinh ψ cos α − coshψ cos β cos α − sin β sin α ) ,

or
⎡V
⎤
⎛
1 ⎞
⎟⎟ sin β sinα ⎥ .
k 2 = (ω ′ / c) coshψ ⎢ cosα − cos(α − β ) + ⎜⎜1 −
⎝ coshψ ⎠
⎣c
⎦
Third (space) component

k 3 = (ω ′ / c) (sinhψ sin α − coshψ cos β sin α + sin β cos α ) ,
or
k3 =

ω′
c

⎡V
⎛
1
sin α + sin(α − β ) − ⎜⎜1 −
⎢
2
1 − (V / c) ⎣ c
⎝ coshψ
1

2
Quantization in Astrophysics ...

470

⎤
⎞
⎟⎟ sin β cos α ⎥ .
⎠
⎦

r
The last component is a “relativistic aberration” of vector k , same as apparent (on A)
component of constantly oriented (on B) spin in Thomas precession scheme.

It is easily verified (e.g. by direct calculation) that
k12 + k 22 + k 32 = 0 .

Remarks.
1. Praising Q-relativity. Used rotational method offering straightforward computation of all
components of any Q-vector in arbitrary moving frames helps to realize that some relativistic
effects usually regarded different are in fact two solutions of unique problem.
2. Concerning Pioneer anomaly. One can use Eq. (2) with particular trajectory data to try to
explain experimentally detected anomalous frequency shift of signals sent by Pioneer
spacecrafts. We’ll try to give examples.
Consider simple case.
r
In Sun’s frame Craft travels with speed V C ( t ) along straight line inclined to ecliptic plane at
angle θ . Light signal is sent from Craft into direction of Sun.
Craft

r
VC

signal
q ~1

r
VE

q ~3

Sun
~
Σ

r
r

θ

Earth

q ~2

Frequency-shift formula is in Eq. (2)

ω=

ω′

⎛ V
⎞
⎜1 − cos β ⎟ ,
c
⎠
1 − (V / c) ⎝
2

where angle β between direction opposite Craft’s velocity and direction of signal is zero. Terms
of order (V / c) 2 and higher will be neglected. Then
⎛
⎝

ω ≅ ω ′⎜1 −

V (t ) ⎞
⎟,
c ⎠

or
Δω = ω ′ − ω = V (t ) / c .

(3)

Relative velocity modulus is determined straightforwardly
V = VC2 + VE2 − 2VCV E cosθ sin Ωt .

Substitution this to Eq. (3) gives
3
Quantization in Astrophysics ...

471

Δω =

ω

VC2 (t ) + V E2 − 2VC (t ) V E cosθ sin Ωt .

(4)
с
Time derivative of Eq. (4) determines change of the shift compared to that of Special Relativity

d
ω ε& K (t ) − [VEV&C (t ) sin Ωt + ΩVEVC (t ) cos Ωt ] cosθ
Δω =
dt
с
VC2 (t ) + VE2 − 2VC (t ) VE cosθ sin Ωt

(5)

VC2
is Craft’s kinetic energy per unit mass in Sun’s frame.
where ε K =
2
Analyzing Eq.(5) we see that dependence of involved functions on time contributes to
secular anomaly while harmonic functions cause annual “modulations”.
More complicated case. If Craft’s trajectory is bending with time (like hyperbola tail with normal
acceleration) but Craft insistently sends signal towards Sun or Earth, then the angle β (t ) ≠ 0 and
this will cause additional contribution to secular anomaly.

4
Quantization in Astrophysics ...

472

1

Less mundane explanation of Pioneer
anomaly from Q-relativity*
F. Smarandache1 & V. Christianto2
. Department of Mathematics, University of New Mexico, NM 87301, USA, email:
smarand@unm.edu
2
. http://www.sciprint.org, email: admin@sciprint.org

1

There have been various explanations of Pioneer blueshift anomaly in the past few
years; nonetheless no explanation has been offered from the viewpoint of Q-relativity
physics. In the present paper it is argued that Pioneer anomalous blueshift may be
caused by Pioneer spacecraft experiencing angular shift induced by similar Qrelativity effect which may also affect Jupiter satellites. By taking into consideration
‘aether drift’ effect, the proposed method as described herein could explain Pioneer
blueshift anomaly within ~0.26% error range, which speaks for itself. Another new
proposition of redshift quantization is also proposed from gravitational Bohr-radius
which is consistent with Bohr-Sommerfeld quantization. Further observation is of
course recommended in order to refute or verify this proposition.

Introduction
In the past few years, it is becoming well-known that Pioneer spacecraft
has exhibited an anomalous Doppler frequency blueshifting phenomenon
which cannot be explained from conventional theories, including general
relativity.[1][4] Despite the nature of such anomalous blueshift remains unknown, some people began to argue that a post-einsteinian gravitation theory
may be in sight [1b], which may be considered as further generalisation of
pseudo-Riemann metric of general relativity theory.
Nonetheless, at this point one may ask: Why do we require a generalization of pseudo-Riemann tensor, instead of using ‘patch-work’ as usual to
modify general relativity theory? A possible answer is: sometimes too much
patch-work doesn’t add up. For instance, let us begin with a thoughtexperiment which forms the theoretical motivation behind general relativity,
an elevator was put in free-falling motion.[8a] The passenger inside the elevator will not feel any gravitational pull, which then it is interpreted as formal analogue that ‘inertial acceleration equals to gravitational acceleration’
(Equivalence Principle). More recent experiments (after Eotvos) suggest,
however, that this principle only holds at certain conditions.
Further problem may arise if we ask: what if the elevator also experiences
lateral rotation around its vertical axis? Does it mean that the inertial acceleration will be slightly higher or lower than gravitational pull? Similarly we
observe that a disc rotating at high speed will exert out-of-plane field resemble an acceleration field. All of this seems to indicate that the thoughtexperiment which forms the basis of general relativity is only applicable for
some limited conditions, in particular the F = m.

dv
part (because general
dt

relativity is strictly related to Newtonian potential), but it may not be able to
represent the rotational aspects of gravitational phenomena. Einstein himself apparently recognizes this limitation [8a, p.61]:
“all bodies of reference K’ should be given preference in this sense,
and they should be exactly equivalent to K for the formation of
natural laws, provided that they are in a state of uniform rectilinear

*To appear at Progress in Physics 3 Vol. 1 (Jan. 2007) www.ptep-online.com
Quantization in Astrophysics ...

473

2

F. Smarandache & V. Christianto

and non-rotary motion with respect to K.” (Italic original by Einstein).
Therefore, it shall be clear that the restriction of non-rotary motion remains a limitation for all considerations by relativity theory, albeit the uniform rectilinear part has been relaxed by general relativity theory.
After further thought, it becomes apparent that it is required to consider a
new kind of metric which may be able to represent the rotational aspects of
gravitation phenomena, and by doing so extends the domain of validity of
general relativity theory.
In this regard, the present paper will discuss the aforementioned Pioneer
blueshift anomaly from the viewpoint of Q-relativity physics, which has
been proposed by Yefremov [2] in order to bring into application the quaternion number. Despite the use of quaternion number in physical theories is
very scarce in recent years –apart of Pauli matrix-, it has been argued elsewhere that using quaternion number one could expect to unify all known
equations in quantum mechanics into the same framework, in particular via
the known isomorphism between Dirac equation and Maxwell equations. [5]
Another problem that was often neglected in most treatises on Pioneer
spacecraft anomaly is the plausible role of aether drift effect.[6] Here it can
be shown that taking this effect into consideration along with the aforementioned Q-relativity satellite’s apparent shift could yield numerical prediction
of Pioneer blueshift within ~0.26% error range, which speaks for itself.
We also suggest a new kind of Doppler frequency shift which can be predicted using Nottale-type gravitational Bohr-radius, by taking into consideration varying G parameter as described by Moffat [7]. To our knowledge
this proposition of new type of redshift corresponding to gravitational Bohrradius has never been considered before elsewhere.
Further observation is of course recommended in order to verify or refute
the propositions outlined herein.
Some novel aspects of Q-relativity physics. Pioneer blueshift anomaly
In this section, first we will review some basic concepts of quaternion
number and then discuss its implications to quaternion relativity (Qrelativity) physics [2]. Then we discuss Yefremov’s calculation of satellite
time-shift which may be observed by precise measurement [3]. We however
introduce a new interpretation here that such a satellite Q-timeshift is already
observed in the form of Pioneer spacecraft blueshift anomaly.
Quaternion number belongs to the group of “very good” algebras: of real,
complex, quaternion, and octonion [2]. While Cayley also proposed new
terms such as quantic, it is less known than the above group. Quaternion
number can be viewed as an extension of Cauchy imaginary plane to become [2]:
Q ≡ a + bi + cj + dk .
(1)
Where a,b,c,d are real numbers, and i,j,k are imaginary quaternion units.
These Q-units can be represented either via 2x2 matrices or 4x4 matrices [2].
It is interesting to note here that there is quaternionic multiplication rule
which acquires compact form:
1q k = q k 1 = q k ,
q j q k = −δ jk + ε jkn q n ,
(2)
Where

δ kn and ε jkn represents

3-dimensional symbols of Kronecker and

Levi-Civita, respectively [2]. Therefore it could be expected that Q-algebra
may have neat link with pseudo-Riemann metric used by general relativity.
Interestingly, it has been argued in this regard that such Q-units can be generalised to become Finsler geometry, in particular with Berwald-Moor metric. It also can be shown that Finsler-Berwald-Moor metric is equivalent

Quantization in Astrophysics ...

474

Less mundane explanation of Pioneer anomaly from Q-relativity theory

3

with pseudo-Riemann metric, and an expression of Newtonian potential can
be found for this metric [2a]. And with regard to peculiar aspects of Berwald-Moor metric which uses fourth root of quartic differential, perhaps it is
worth to quote here from Riemann himself [2b]:
“in which line-element may be expressed as the fourth root of a
quartic differential expression.”
It may also be worth noting here that in 3D space Q-connectivity has
clear geometrical and physical treatment as movable Q-basis with behaviour
of Cartan 3-frame [2].
It is also possible to write the dynamics equations of classical mechanics
for an inertial observer in constant Q-basis. SO(3,R)-invariance of two vectors allow to represent these dynamics equations in Q-vector form [2]:

m

d2
(x k q k ) = Fk q k .
dt 2

(3)

Because of antisymmetry of the connection (generalised angular velocity)
the dynamics equations can be written in vector components, by conventional vector notation [2]:

(

)

r r r r r r r
r
r
m(a + 2Ω × v + Ω × r + Ω × Ω × r ) = F

(4)
Therefore, from equation (4) one recognizes known types of classical acceleration, i.e. linear, coriolis, angular, centripetal, respectively. Meanwhile
it is known that the general relativity introduces Newton potential as rigid
requirement [2a][6b]. In other words, we can expect -- using Q-relativity -to predict new effects that cannot be explained with general relativity.
From this viewpoint one may consider a generalisation of Minkowski
metric into biquaternion form [2]:
(5)
dz = (dx k + idt k )q k ,
With some novel properties, i.e.:
- temporal interval is defined by imaginary vector;
- space-time of the model appears to have six dimensions (6D);
- vector of the displacement of the particle and vector of corresponding time change must always be normal to each other, or:
dx k dt k = 0 .
(6)
It is perhaps quite interesting to note here that Einstein himself apparently
once considered similar approach, by proposing tensors with Riemann metric with Hermitian symmetry [8]. Nonetheless, there is difference with Qrelativity described above, because in Einstein’s generalised Riemann metric
it has 8-dimensions, rather than 3d-space and 3d-imaginary time.
One particularly interesting feature of this new Q-relativity (or rotational
relativity) is that there is universal character of motion of the bodies (including non-inertial motions), which can be described in unified manner (Hestenes also considers classical mechanics from similar spinor language). For
instance advanced perihelion of planets can be described in terms of such
rotational precession. [2]
Inspired by this new Q-relativity physics, it can be argued that there
should be anomalous effect in planets’ satellite motion. In this regard, Yefremov argues that there should be a deviation of the planetary satellite position, due to discrepancy between calculated and observed from the Earth
motion magnitudes characterizing cyclic processes on this planet or near it.
He proposes [2]:

Δϕ ≈

ωVeV p
c2

t,

(7)

Or

Quantization in Astrophysics ...

475

4

F. Smarandache & V. Christianto

Δϕ ' ≈ −

ωVeV p
c2

t' .

(8)

Therefore, given a satellite orbit radius r, its position shift is found in
units of length Δl = r Δϕ . His calculation for satellites of Mars and Jupiter is given in Table1. Nonetheless he gave no indication as to how to observe this anomalous effect.
Table 1. The following table gives values of the effect for five fast satellites of Mars
and Jupiter. Orbital linear velocities are: of the Earth

V E = 29.8 km/s, of Mars V P

=

24.1 km/s, of Jupiter V P = 13.1 km/s; value of light velocity is c = 299 793 km/s;
observation period is chosen 100 years. (after A. Yefremov, 2006 [3])

Satellites

Cycle frequency

ω:

Angular
shift

Δϕ :

Linear shift

Linear

Δl :

size a: km

1/s

′′/100 yrs

km/100yrs

Phobos (Mars)

0.00023

18.2

54

20

Deimos (Mars)

0.00006

4.6

34

12

Metis (Jupiter/J)

0.00025

10.6

431

40

Adrastea (J)

0.00024

10.5

429

20

Amalthea (J)

0.00015

6.3

361

189

In this regard, we introduce here an alternative interpretation of the
aforementioned Q-satellite time-shift effect by Yefremov, i.e. this effect
actually has similar effect with Pioneer spacecraft blueshift anomaly. It is
known that Pioneer spacecraft exhibits this anomalous Doppler frequency
while entering Jupiter orbit [1][4], therefore one may argue that this effect is
caused by Jupiter planetary gravitational effect, which also may cause similar effect to its satellites. (Interestingly, we may also note that Pioneer
spacecraft was intended to study more closely the various aspects of Jupiter).
Despite the apparent contradiction with Yefremov’s own intention, one
could find that the aforementioned Q-satellite time-shift could yield a natural
explanation of Pioneer spacecraft blueshift anomaly. In this regard, Taylor
[9] argues that there is possibility of a mundane explanation of anomalous
blueshift of Pioneer anomaly (5.99 x 10-9 Hz/sec). The all-angle formula for
relativistic Doppler shift is given by [9a, p.34]:

v' = v0

(1 − β cos φ )
1− β 2

Quantization in Astrophysics ...

,

(9)

476

Less mundane explanation of Pioneer anomaly from Q-relativity theory

Where β = v / c . Or using the relativistic term
the standard expression:
v' = v0 γ (1 − β cos φ ) .

γ = 1/ 1 − β 2

5

, one gets
(9a)

The derivative with respect to φ is:

dv'
= v0 γβ sin φ ,
(10)
dφ
dv'
=5.99x10-9 Hz/sec, i.e. the observed Pioneer anomaly. IntroducWhere
dφ
ing this value into equation (10), one gets requirement of an effect to explain
Pioneer anomaly:

dφ = arcsin(5.99 x10 −9 Hz )(v0 γβ ) −1 =1.4x10-12 degree/sec.

(11)
Therefore, we can conclude that to explain 5.99x10 Hz/sec blueshift
anomaly, it is required to find a shift of emission angle at the order 1.4x10-12
degree/sec only (or around 15.894” per 100 years).
Interestingly this angular shift can be explained with the same order of
magnitude from the viewpoint of Q-satellite angular shift (see Table 1), in
particular for Jupiter’s Adrastea (10.5” per 100 years). There is however, a
large discrepancy at the order of 50% from the expected angular shift.
It is proposed here that such discrepancy between Q-satellite angular shift
and expected angular shift required to explain Pioneer anomaly can be reduced if we take into consideration the ‘aether drift’ effect [6]. Interestingly
we can use experimental result of Thorndike [6, p.9], saying that the aether
drift effect implies a residual apparent Earth velocity is vobs=15+4 km/sec.
Therefore the effective Ve in equation (8) becomes:
Ve.eff = vobs + Ve = 44.8km / sec .
(12)
-9

Using this improved value for Earth velocity in equation (8), one will get
larger values than Table 1, which for Adrastea satellite yields:

Δϕ obs =

ωVe.eff V p
c

2

t=

Ve.eff
Ve

Δϕ = 15.935" / 100 yrs .

(13)

Using this improved prediction, the discrepancy with required angular
shift only (15.894” per 100 years) becomes ~ 0.26%, which speaks for itself.
Therefore one may conclude that this less mundane explanation of Pioneer
blueshift anomaly with Q-relativity may deserve further consideration.

A new type of redshift from gravitational Bohr radius. Possible observation in solar system.
In preceding paper [10][11] we argued in favour of an alternative interpretation of Tifft redshift quantization from the viewpoint of quantized distance between galaxies. A method can be proposed as further test of this
proposition both at solar system scale or galaxies scale, by using the known
quantized Tifft redshift [14][15][16]:

δr ≈

c
δz
H

(14)

In this regards, we use gravitational Bohr radius equation [10][11]:

rn = n 2

GM
vo2

Quantization in Astrophysics ...

(15)

477

6

F. Smarandache & V. Christianto

Inserting equation (15) into (14), then one gets quantized redshift expected from gravitational Bohr radius:

zn =

H 2 GM
n
c
vo2

(16)

Which can be observed either in solar system scale or galaxies scale. To our
present knowledge, this effect has never been described elsewhere before.
Therefore, it is recommended to observe such an accelerated Dopplerfreequency shift, which for big jovian planets this effect may be detected. It
is also worth noting here that according to equation (16), this new Doppler
shift is quantized.
At this point one may also take into consideration a proposition by Moffat, regarding modification of Newtonian acceleration law to become [7]:

a(r) = −

G∞ M
r

2

+K

exp( −μ φ r )
r2

(1 + μ φ r )

(17)

Where

⎡
M0 ⎤
G∞ = G ⎢1 +
⎥
M ⎥⎦
⎢⎣

(17a)

Therefore equation (16) may be rewritten to become:

zn ≈

M0 ⎤
H 2 GM ⎡
H 2 GM
1+
n
⎥ ≈ χ. n
2 ⎢
c
M ⎦⎥
c
vo ⎣⎢
vo2

(18)

where n is integer (1,2,3…) and:

⎡

χ = ⎢1 +
⎣⎢

M0 ⎤
⎥
M ⎦⎥

(18a)

To use the above equations, one may start by using Bell’s suggestion that
there is fundamental redshift z=0.62 which is typical for various galaxies
and quasars [14]. Assuming we can use equation (16), then by setting n=1,
we can expect to predict the mass of quasar centre or galaxy centre. Then the
result can be used to compute back how time-variation parameter affects
redshift pattern in equation (18). In solar system scale, time-varying radius
may be observed in the form of changing Astronomical Unit [4].
This proposition, however, deserves further theoretical considerations.
Further observation is also recommended in order to verify and explore further this proposition.

Concluding remarks
In the present paper it is argued that Pioneer anomalous blueshift may be
caused by Pioneer spacecraft experiencing angular shift induced by similar
Q-relativity effect which may also affect Jupiter satellites. By taking into
consideration aether drift effect, the proposed method as described herein
could predict Pioneer blueshift within ~0.26% error range, which speaks for
itself. Further observation is of course recommended in order to refute or
verify this proposition.
Another new proposition of redshift quantization is also proposed from
gravitational Bohr-radius which is consistent with Bohr-Sommerfeld quantization. It is recommended to conduct further observation in order to verify

Quantization in Astrophysics ...

478

Less mundane explanation of Pioneer anomaly from Q-relativity theory

7

and also to explore various implications of our propositions as described
herein.

Acknowledgment
The writers would like to thank to Profs. C. Castro, and D.L. Rapoport for
valuable discussions. Special thanks to D. Rabounski, S. Crothers, L. Borissova, and also to Prof. A. Yefremov for sending his recent calculation of
Jupiter satellite’s Q-time-shift effect.

Quantization in Astrophysics ...

479

8

F. Smarandache & V. Christianto

References
[1] Anderson, J.D., J.K. Campbell, & M.M. Nieto, arXiv:astro-ph/0608087 (2006).

[2]

[3]
[4]
[5]
[6]
[7]
[8]

[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]

[1a] M.M. Nieto & J.D. Anderson, gr-qc/0507052 (2005). [1b] Jaekel, M., & S.
Reynaud, “Radar ranging and Doppler tracking in post-Einsteinian metric
theories of gravity,” arXiv:gr-qc/0610155 (2006).
Yefremov, A., “Quaternions: Algebra, Geometry and physical theories,”
Hypercomplex numbers in Geometry and Physics 1(1) p.105 (2004); [2a]
Pavlov, D.G., arXiv:math-ph/0609009 (2006); [2b] B. Riemann, Nature Vol.
VIII Nos. 183, 184, p.14-17, 36, 37. Translated by William K. Clifford.
Yefremov, A., communication via email, a.yefremov@rudn.ru, (Oct. 2006)
Laemmerzahl, C., & H. Dittus, Clocks and Gravity, From Quantum to Cosmos,
(2006) www.physics.ucla.edu/quantum_to_cosmos/q2c06/Laemmerzahl.pdf
Christianto, V., EJTP Vol. 3 No. 12 (Oct. 2006), http://www.ejtp.com
Consoli, M., arXiv:physics/0306094 (2003) p.9, [6a] Consoli, M. et al,
arXiv:gr-qc/0306105 (2003),; [6b] arXiv:hep-ph/0109215 (2001).
Moffat, J., arXiv:astro-ph/0602607 (2006).
Einstein, A., “Generalised theory of gravitation,” Ann. Math. 46 (1945) at
JSTOR; [8a] Einstein, A., Relativity: The special and the general theory,
Crown Trade Paperback, New York, translation by R. Lawson (1951) p. 61, 6670. With regards to his Generalised Theory of Gravitation [8], it is perhaps
worth to quote here his confident tone in remarks at the end of his book [8a]:
“After long probing I believe that I have now found the most natural form for
this generalisation, but I have not yet been able to find out whether this
generalised law can stand up against the facts of experience”. (Appendix V, p.
156).
Taylor, S., “Hypothesis of a Mundane solution of the Pioneer anomaly,”
arXiv:physics/0603074 (2006); [9a] Engelhardt, W., Apeiron Vol. 10 No. 4
(2003) p.34.
Smarandache, F., & V. Christianto, ‘A note on geometric and information
fusion interpretation of Bell theorem and quantum measurement,’ Progress in
Physics Vol.2 No. 4 (2006), http://www.ptep-online.com
Smarandache, F., & V. Christianto, ‘Plausible explanation of quantization of
intrinsic redshift from Hall effect and Weyl quantization,’ Progress in Physics
Vol.2 No. 4 (2006), http://www.ptep-online.com
Smarandache, F., & V. Christianto, Progress in Physics Vol.2 No. 2 (Apr.
2006), http://www.ptep-online.com
Fischer, U., arXiv:cond-mat/9907457 (1999); [13a] arXiv:cond-mat/0004339
Bell, M.B., arXiv: astro-ph/0111123 (2001); [14a] Bell, M.B.,
http://arxiv.org/PS_cache/astro-ph/pdf/0305/0305112.pdf; [14b] Bell, M.B.,
arXiv: astro-ph/0305060 (2003).
Humphreys, R, TJ Archive Vol.16 http://answersingenesis.org/tj/v16/i2/ (2002)
http://www.answersingenesis.org/home/area/magazines/tj/docs/TJv16n2_CEN
TRE.pdf; [15a] Setterfield, B., www.journaloftheoretics.org
Munera, H., “Redshift in absolute space,” Apeiron Vol. 5 No.3-4, July. (1998)
Zurek, W. (ed.), in Proc. Euroconference in Formation and Interaction of
Topological Defects, Plenum, 1995, arXiv :cond-mat/9502119 (1995).
Volovik, G., arXiv:cond-mat/0507454 (2005).

First version: 7th Nov. 2006. 1st revision 23rd Jan. 2006

Quantization in Astrophysics ...

480

1

A note on unified statistics including
Fermi-Dirac, Bose-Einstein, and Tsallis
statistics, and plausible extension to
anisotropic effect
V. Christianto1 & F. Smarandache2
. http://www.sciprint.org, email: admin@sciprint.org
2
. Dept. of Mathematics, Univ. of New Mexico, Gallup
1

In the light of some recent hypotheses suggesting plausible unification of thermostatistics where Fermi-Dirac, Bose-Einstein and Tsallis statistics become its special
subsets, we consider further plausible extension to include non-integer Hausdorff
dimension, which becomes realization of fractal entropy concept.
In the subsequent section, we also discuss plausible extension of this unified statistics to include anisotropic effect by using quaternion oscillator, which may be observed in the context of Cosmic Microwave Background Radiation. Further observation is of course recommended in order to refute or verify this proposition.

Introduction
In recent years, there have been some hypotheses suggesting that the
spectrum and statistics of Cosmic Microwave Background Radiation has a
kind of scale invariant character [1], which may be related to non-integer
Hausdorff dimension. Interestingly, in this regard there is also proposition
sometime ago suggesting that Cantorian spacetime may have deep link with
Bose condensate with non-integer Hausdorff dimension [2]. All of these
seem to indicate that it is worth to investigate further the non-integer dimension effect of Bose-Einstein statistics, which in turn may be related to Cosmic Microwave Background Radiation spectrum.
In the meantime, some authors also consider a plausible generalization of
known statistics, i.e. Fermi-Dirac, Bose-Einstein, and Tsallis statistics, to
become more unified statistics [3][4]. This attempt can be considered as one
step forward from what is already known, i.e. to consider anyons as a generalization of bosons and fermions in two-dimensional systems.[5, p.2] Furthermore, it is known that superfluidity phenomena can also be observed in
Fermi liquid [6].
First we will review the existing procedure to generalize Fermi-Dirac,
Bose-Einstein, and Tsallis statistics, to become more unified statistics [3][4].
And then we explore its plausible generalization to include fractality of Tsallis’ non-extensive entropy parameter.
In the subsequent section, we also discuss plausible extension of this proposed unified statistics to include anisotropic effect, which may be observed
in the context of Cosmic Microwave Background Radiation. In particular we
consider possibility to introduce quaternionic momentum. To our knowledge
this proposition has never been considered before elsewhere.
Further observation is of course recommended in order to verify or refute
the propositions outlined herein.

Quantization in Astrophysics ...

481

2

V. Christianto & F. Smarandache

Unified statistics including Fermi-Dirac, Bose-Einstein, and Tsallis statistics.
In this section we consider a different theoretical framework to generalize
Fermi-Dirac and Bose-Einstein statistics, from conventional method using
anyons, [5] in particular because this conventional method cannot be generalized further to include Tsallis statistics which has attracted some attention
in recent years.
First we write down the standard expression of Bose distribution [9, p.7]:

n (∈i ) =

1
,
exp( β (∈i − μ )) − 1

(1)

Where the harmonic energy levels are given by [9, p.7]:

3
∈i = (n x + n y + n z + )hω 0 .
2

(2)

When we assume that bosons and fermions are g-ons obeying fractional
exclusion statistics, then we get a very different picture. In accordance with
[3], we consider the spectrum of fractal dimension (also called generalized
Renyi dimension [11]):

1 ln Ω q
,
q − 1 ln δ

Dq = lim δ →0

(3)

(therefore the spectrum of fractal dimension is equivalent with Hausdorff
dimension of the set A [11]).
Then the relation between the entropy and the spectrum of fractal dimension is given by: [3]
S q = − k B . lim δ →0 ln δDq ,
(4)
Where kB is the Boltzmann constant. The spectrum of fractal dimension may
be expressed in terms of p:
K

1
Dq ≈
q −1

∑p
i =1

q
i

−1
.

ln δ

(5)

Then, substituting equation (5) into (4), we get the Tsallis non-extensive
entropy [3]:
K

S q = −k B .

∑p
i =1

q
i

−1

q −1

,

(6)

After a few more assumptions, and using g-on notation [3], i.e. g=1 for
generalized Fermi-Dirac statistics and g=0 for generalised Bose-Einstein
statistics, then one gets the most probable distribution for g-ons [3]:

n k (∈i , g , q ) =

1
(1 − (q − 1) β (∈i − μ ))

1
q −1

,

(7)

+ 2g − 1

Which gives standard Planck distribution for μ =0, g=0 and q=1. [3][9] In
other words, we could expect that g-ons gas statistics could yield more generalized statistics than anyons’.
To introduce further generality of this expression (7), one may consider
the parameter q as function of another non-integer dimension, therefore:

Quantization in Astrophysics ...

482

A note on unified statistics including Fermi-Dirac, Bose-Einstein, Tsallis…

1

n k (∈i , g , q, D) =

,

1

(1 − (q D − 1) β (∈i − μ )) q

D

−1

3

(8)

+ 2g − 1

Where D=1, then equation (8) reduces to be (7).
Of course, the picture described above will be different if we introduce
non-standard momentum [5, p.7]:

p2 = −

λ
d2
+ 2
2
dx
x

(9)

In the context of Neutrosophic logic as conceived by one of these writers
[8], one may derive a proposition from the arguments presented herein, i.e.
apart from common use of anyons as a plausible generalization of fermion
and boson, perhaps an alternative method for generalization of fermion and
boson can be described as follows:
(a) If we denote fermion with (f) and boson with (b), then it follows
that there could be a mixture composed of both (f) and (b) Î (f) ∩
(b), which may be called as ‘anyons’;
(b) If we denote fermion with (f) and boson with (b), and because g=1
for generalized Fermi-Dirac statistics and g=0 for generalised BoseEinstein statistics, then it follows that the wholeness of both (f) and
(b) Î (f) ∪ (b), which may be called as ‘g-on’;
(c) Taking into consideration of possibility of ‘neitherness’, then if we
denote non-fermion with ( ¬ f) and non-boson with ( ¬ b), then it
follows that there shall be a mixture composed of both ( ¬ f) and
( ¬ b) Î ( ¬ f) ∩ ( ¬ b), which may be called as ‘feynmion’ (after physicist the late R. Feynman);
(d) Taking into consideration of possibility of ‘neitherness’, then it follows that the wholeness of both ( ¬ f) and ( ¬ b) Î ( ¬ f)
∪ ( ¬ b), which may be called as ‘anti-g-on’.
Therefore, a conjecture which may follow from this proposition is that
perhaps in the near future we can observe some new entities corresponding
to g-on condensate or feynmion condensate.

Further extension to include anisotropic effect.
At this section we consider the anisotropic effect which may be useful for
analyzing the anisotropy of CMBR spectrum, see Fig 1 [13]:

Fig 1. Anisotropy of CMBR (After Tkachev [13]).

Quantization in Astrophysics ...

483

4

V. Christianto & F. Smarandache

For anisotropic case, one cannot use again equation (2), but shall instead
use [7, p2]:

1
1
1
∈i = (n x + )hω x + (n y + )hω y + (n z + )hω z ,
2
2
2

(10)

Where nx, ny, nz are integers and >0. Or by neglecting the ½ parts and assuming a common frequency, one can re-write (10) as [7a, p1]:
∈i = (n x r + n y s + n z t )hω 0 ,
(11)
Where r,s,t is multiplying coefficient for each frequency:

r=

ωy
ωx
ω
,s =
,t = z
ω0
ω0
ω0

(12)

This proposition will yield a different spectrum compared to isotropic
spectrum by assuming isotropic harmonic oscillator (2). See Fig 2 [7a]. It is
interesting to note here that the spectrum produced by anisotropic frequencies yields number of peaks more than 1 (multiple-peaks), albeit this is not
near yet to CMBR spectrum depicted in Fig 1. Nonetheless, it seems clear
here that one can expect to predict the anisotropy of CMBR spectrum by
using of more anisotropic harmonic oscillators.

Fig. 2 Spectrum for anisotropic harmonic oscillator potential.
(After Ligare [7a])
In this regard, it is interesting to note that some authors considered half
quantum vortices in px+i.py superconductors [14], which indicates that energy of partition function may be generalized to include Cauchy plane, because:
E = p.c + i. p.c ≈ hω + ihω ,
(13)
or by generalizing this Cauchy plane to quaternion number [12], one gets
instead of (13):

Quantization in Astrophysics ...

484

A note on unified statistics including Fermi-Dirac, Bose-Einstein, Tsallis…

E qk = hω + ihω + jhω + khω ,

5

(14)

Which is similar to standard definition of quaternion number:
(15)
Q ≡ a + bi + cj + dk ,
Therefore the partition function with anisotropic harmonic potential can
be written in quaternion form. Therefore instead of (11), we get:
∈i = (n x r + n y s + n z t + in x r + jn y s + kn z t )hω 0 ,
(16)
Which can be written as:

∈i = (1 + q k )(nk rk )hω 0 ,
(17)
Where k=1,2,3 corresponding to index of quaternion number i,j,k. While we
don’t obtain numerical result here, it can be expected that this generalisation
to anisotropic quaternion harmonic potential could yield better prediction,
which perhaps may yield to exact CMBR spectrum. Numerical solution of
this problem may be presented in another paper.
This proposition, however, deserves further considerations. Further
observation is also recommended in order to verify and explore further this
proposition.
Concluding remarks
In the present paper, we review an existing method to generalize FermiDirac, Bose-Einstein, and Tsallis statistics, to become more unified statistics.
And then we explore its plausible generalization to include fractality of
Tsallis non-extensive entropy parameter .
Therefore, a conjecture which may follow this proposition is that perhaps
in the near future we can observe some new entities corresponding to g-on
condensate or feynmion condensate.
In the subsequent section, we also discuss plausible extension of this proposed unified statistics to include anisotropic effect, which may be observed
in the context of Cosmic Microwave Background Radiation. In particular we
consider possibility to introduce quaternionic momentum. To our knowledge
this proposition has never been considered before elsewhere.
It is recommended to conduct further observation in order to verify and
also to explore various implications of our propositions as described herein.

Acknowledgment
The writers would like to thank to Profs. C. Castro, A. Yefremov and D.L.
Rapoport for valuable discussions. And special thanks to D. Rabounski, S.
Crothers, L. Borissova,

Quantization in Astrophysics ...

485

6

V. Christianto & F. Smarandache

References
[1] Antoniadis, I., et al., arXiv:astro-ph/9611208 (1996).
[2] Castro, C., & A., Granik, “Why we live in 3 dimensions,” arXiv:hep[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]

th/0004152 (2000).
Buyukkilic, F., & D. Demirhan, arXiv:hep-th/0010192 (2001)
Utyuzh, O.V., et al., arXiv:hep-ph/9910355 (1999)
Leinaas, J.M., arXiv:hep-th/9611167 (1996) p.2.
Pinilla, M.R., Signatures of superfluidity in atomic Fermi gases, Diss. Helsinki
University of Tech. (Nov. 2003) 72 p.
Ligare, M., “Numerical analysis of Bose-Einstein condensation in a three
dimensional harmonic oscillator potential,” Am. J. Phys. 66 No. 3 (1998) p.2;
[7a] M. Ligare, Am. J. Phys. 70 No. 1 (2002).
Smarandache, F., & V. Christianto, Progress in Physics Vol.2 No. 4 (Oct.
2006), http://www.ptep-online.com
Griffin, A., arXiv:cond-mat/9911419 (1999) p.7.
Tuszynski, J., et al., Physica A 325 (2003) 455-476 (http://sciencedirect.com)
Batunin, F., Physics-Uspekhi 38(6) 609-623 (1995)
Yefremov, A., “Relativistic Oscillator in Quaternion Relativity,” in this volume
(Nov. 2006).
Tkachev, I., arXiv:hep-ph/0405168 (2004) p. 17-18.
Stone, M., et al., arXiv:cond-mat/0505515 (2005) p. 14-15.

First version: 8th Jan. 2007.

Quantization in Astrophysics ...

486

1

Numerical solution of Time-dependent
gravitational Schrödinger equation*
V. Christianto1 , D.L. Rapoport2 & F. Smarandache3
. http://www.sciprint.org, email: admin@sciprint.org
2
. Dept. of Sciences and Technology, Univ. Nac. de Quilmes, Bernal, Argentina
3
. Dept. of Mathematics, Univ. of New Mexico, Gallup
1

In recent years, there are attempts to describe quantization of planetary distance
based on time-independent gravitational Schrödinger equation, including Rubcic &
Rubcic’s method and also Nottale’s Scale Relativity method. Nonetheless, there is no
solution yet for time-dependent gravitational Schrödinger equation (TDGSE). In the
present paper, a numerical solution of time-dependent gravitational Schrödinger
equation is presented, apparently for the first time. This numerical solution leads to
gravitational Bohr-radius, as expected.
In the subsequent section, we also discuss plausible extension of this gravitational
Schrödinger equation to include the effect of phion condensate via Gross-Pitaevskii
equation, as described recently by Moffat. Alternatively one can consider this condensate from the viewpoint of Bogoliubov-deGennes theory, which can be approximated with coupled time-independent gravitational Schrödinger equation. Further
observation is of course recommended in order to refute or verify this proposition.

Introduction
In the past few years, there have been some hypotheses suggesting that
quantization of planetary distance can be derived from a gravitationalSchrödinger equation, such as Rubcic & Rubcic and also Nottale’s scale
relativity method [1][3]. Interestingly, the gravitational Bohr radius derived
from this gravitational Schrödinger equation yields prediction of new type of
astronomical observation in recent years, i.e. extrasolar planets, with unprecedented precision [2].
Furthermore, as we discuss in preceding paper [4], using similar assumption based on gravitational Bohr radius, one could predict new planetoids in
the outer orbits of Pluto which apparently in good agreement with recent
observational finding. Therefore one could induce from this observation that
the gravitational Schrödinger equation (and gravitational Bohr radius) deserves further consideration.
In the meantime, it is known that all present theories discussing gravitational Schrödinger equation only take its time-independent limit. Therefore
it seems worth to find out the solution and implication of time-dependent
gravitational Schrödinger equation (TDGSE). This is what we will discuss in
the present paper.
First we will find out numerical solution of time-independent gravitational Schrödinger equation which shall yield gravitational Bohr radius as
expected [1][2][3]. Then we extend our discussion to the problem of timedependent gravitational Schrödinger equation.
In the subsequent section, we also discuss plausible extension of this
gravitational Schrödinger equation to include the effect of phion condensate via
Gross-Pitaevskii equation, as described recently by Moffat [5]. Alternatively
one can consider this condensate from the viewpoint of BogoliubovdeGennes theory, which can be approximated with coupled timeindependent gravitational Schrödinger equation. To our knowledge this
proposition of coupled time-independent gravitational Schrödinger equation

*Submitted to Progress in Physics (Jan. 2007) www.ptep-online.com
Quantizatin in Astrophysics ...

487

2

V. Christianto & D. Rapoport

has never been considered before elsewhere. Further extension to timedependent case is straightforward.
Further observation is of course recommended in order to verify or refute
the propositions outlined herein.
All numerical computation was performed using Maple. Please note that
in all conditions considered here, we use only gravitational Schrödinger
equation as described in Rubcic & Rubcic [3], therefore we neglect the Scale
relativistic effect for clarity.
Numerical solution of time-independent gravitational Schrödinger
equation and time-dependent gravitational Schrödinger equation.
First we write down the time-independent gravitational Schrödinger radial
wave equation in accordance with Rubcic & Rubcic [3]:

2 4π 2 GMm 2
d 2 R 2 dR 8πm 2 E '
l(l + 1)
+
+
+
R−
R = 0,
R
2
2
2
r dr
r
dr
H
H
r2

(1)

When H, V, E’ represents gravitational Planck constant, Newtonian potential, and the energy per unit mass of the orbiting body, respectively, and [3]:

⎛
Mmn
H = h⎜⎜ 2π . f
mo2
⎝
GMm
,
V (r ) = −
r
E
.
E' =
m

⎞
⎟⎟ ,
⎠

(2)

(3)
(4)

By assuming that R takes the form:

R = e −α . r

(5)
And substituting it into equation (1), and using simplified terms only of
equation (1), one gets:

Ψ = α e e −α .r −

2α .e −α .r 8πGMm 2 e −α .r
+
.
r
r.H 2

(6)

After factoring this equation (6) and solving it by equating the factor with
zero, yields:

RR = −

(

)

2 4πGMm 2 − H 2α
=0.
α 2H 2

(7)

Or

RR = 4πGMm 2 − H 2α = 0 .

(7a)

And solving for α , one gets:

a=

4π 2 GMm 2
.
H2

(8)

Gravitational Bohr radius is defined as inverse of this solution of
one finds (in accordance with Rubcic & Rubcic [3]):

r1 =

H2
.
4π 2 GMm 2

Quantizatin in Astrophysics ...

α , then
(9)

488

Numerical solution of time-dependent gravitational Schrödinger equation

3

And by substituting back equation (2) into (9), one gets [3]:

⎛ 2π . f ⎞
r1 = ⎜
⎟ GM
⎝ α .c ⎠
2

(10)

Which is equivalent with Nottale’s result [1][2], especially when we introduce the quantization number: rn=r1.n2 [3]. For complete Maple session of
these steps, see Appendix A.1.
Solution of time-dependent gravitational Schrödinger equation is more or
less similar with the above steps, except that we shall take into consideration
the right hand side of Schrödinger equation and also assuming time dependent form of r:

R = e −α .r (t ) .

(11)
Therefore the gravitational Schrödinger equation now reads (neglecting i
number in both sides):

d 2 R 2 dR 8πm 2 E '
+
+
R+
dr 2 r dr
H2
,
2 4π 2 GMm 2
l(l + 1)
dR
R−
R=H
r
dt
H2
r2

(12)

Or by using Leibniz chain rule, we can rewrite equation (12) as:

−H

dR dr (t ) d 2 R 2 dR 8πm 2 E '
+ 2 +
+
R+
dr (t ) dt
r dr
dr
H2

2 4π 2 GMm 2
l(l + 1)
R−
R=0
2
r
H
r2

.

(13)

The remaining steps are similar with the aforementioned procedures for
time-independent case, except that now one gets an additional term for RR:

RR ' = H 3α (

d
r (t ))r (t ) − α 2 r (t ) H 2 + 8πGMm 2 − 2 H 2α = 0 . (14)
dt

d
r (t ) term, because otherwise
dt
d
r (t ) = 1 for simplicity, then
the equation cannot be solved. We choose
dt
At this point one shall assign a value for

one gets solution for (14):

a2 := { α = α, π = π, m = m, h = h, G = G, M = M,
t = RootOf ( r( _Z ) α h 3 − r( _Z ) α 2 h 2 + 8 π 2 G M m 2 − 2 α h 2 ) },
{ α = 0, t = t, m = m, h = h, G = G, M = M, π = 0 },
{ α = 0, π = π, t = t, m = m, h = h, M = M, G = 0 },
{ α = α, h = 0, t = t, m = m, G = G, M = M, π = 0 },

h3
},
4 π2 M m2
{ α = α, h = 0, π = π, t = t, m = m, M = M, G = 0 },
{ α = 0, π = π, t = t, m = m, h = h, G = G, M = 0 },
{ α = 0, π = π, t = t, h = h, G = G, M = M, m = 0 },
{ π = π, t = t, m = m, h = h, M = M, α = h, G =

Quantizatin in Astrophysics ...

489

4

V. Christianto & D. Rapoport

{ α = α, h = 0, π = π, t = t, m = m, G = G, M = 0 },
{ α = α, h = 0, π = π, t = t, G = G, M = M, m = 0 }
Therefore one can conclude that there is time-dependent modification factor to conventional gravitational Bohr radius solution. For complete Maple
session of these steps, see Appendix A.2

Gross-Pitaevskii effect. Bogoliubov-deGennes approximation and coupled time-independent gravitational Schrödinger equation.
At this point it seems worthwhile to take into consideration a proposition
by Moffat, regarding modification of Newtonian acceleration law due to
phion condensate medium, to include Yukawa type potential [5][6]:

a(r ) = −

exp(− μ φ r )
G∞ M
K
+
(1 + μφ r )
r2
r2

(15)

Therefore equation (1) can be rewritten to become:

d 2 R 2 dR 8πm 2 E '
R+
+
+
dr 2 r dr
H2
2
2
2 4π (GM − K exp(− μφ r )(1 + μ φ r ))m
H2

r
Or by assuming μ
can be rewritten as:

l(l + 1)
R−
R=0
r2

, (16)

= 2 μ o = μ o .r for the exponential term, equation (16)

d 2 R 2 dR 8πm 2 E '
+
+
R+
dr 2 r dr
H2
. (17)
2 4π 2 (GM − Ke − 2 μ0 (1 + μ o r ))m 2
l(l + 1)
R−
R=0
r
H2
r2
Then instead of equation (7a), one gets:

RR ' ' = 8πGMm 2 − 2 H 2α − 8π 2 m 2 Ke − μ 0 (1 + μ ) = 0 .

(18)
Solving this equation will yield a modified gravitational Bohr radius
which includes Yukawa effect:

r1 =

H2
.
4π 2 (GM − Ke − 2 μ0 )m 2

(19)

And the modification factor can be expressed as ratio between equation
(19) and (9):

χ=

GM
.
(GM − Ke − 2 μ0 )

(20)

For complete Maple session of these steps, see Appendix A.3.
A careful reader may note that this ‘Yukawa potential effect’ as shown in
equation (20) could be used to explain the small discrepancy (around +8%)
between the ‘observed distance’ and the computed distance based on gravitational Bohr radius [4][6a]. Nonetheless, in our opinion such an interpretation remains an open question, therefore it may be worth to explore further.
There is, however, an alternative way to consider phion condensate medium is by introducing coupled Schrödinger equation, which is known as

Quantizatin in Astrophysics ...

490

Numerical solution of time-dependent gravitational Schrödinger equation

5

Bogoliubov-deGennes theory [7]. This method can be interpreted also as
generalisation of assumption by Rubcic-Rubcic [3] of subquantum structure
composed of positive-negative Planck mass. Therefore, taking this proposition seriously, then one comes to hypothesis that there shall be coupled
Newtonian potential, instead of only equation (3).
To simplify Bogoliubov-deGennes equation, we neglect the timedependent case, therefore the wave equation can be written in matrix form
[7, p.4]:
(21)
A Ψ =0

[ ][ ]

[ ]

Where [A] is 2x2 matrix and Ψ is 2x1 matrix, respectively, which can be
represented as follows:
( −α r )

⎡ 8 π2 G M m2 e
⎢
⎢
⎢
r h2
⎢
Ar := ⎢
( −α r )
⎢
⎢ 2 ( −α r ) 2 α e
⎢α e
−
⎢
r
⎣

( −α r )

⎤
⎥
⎥
⎥
⎥
⎥
2
2 ( −α r ) ⎥
8π GMm e
⎥
⎥
−
2
⎥
rh
⎦

α2 e

( −α r )

−

2αe
r

(22)

And

f( r ) ⎤
Br := ⎡⎢⎢
⎥⎥
⎣g( r )⎦

(23)

Numerical solution of this matrix differential equation is shown in Appendix A.4. It is clear here that Bogoliubov-deGennes approximation of
gravitational Schrödinger equation, taking into consideration phion condensate medium will yield nonlinear effect. This perhaps may explain complicated structure beyond Jovian Planets, such as Kuiper Belt, inner and outer
Oort Cloud etc. which of course these structure cannot be predicted by simple gravitational Schrödinger equation [1][2][3]. In turn from the solution od
(21) one could expect that there are multitude of celestial objects not found
yet in the Oort Cloud.
This proposition, however, deserves further considerations. Further
observation is also recommended in order to verify and explore further this
proposition.

Concluding remarks
In the present paper, a numerical solution of time-dependent gravitational
Schrödinger equation is presented, apparently for the first time. This numerical solution leads to gravitational Bohr-radius, as expected.
In the subsequent section, we also discuss plausible extension of this
gravitational Schrödinger equation to include the effect of phion condensate
via Gross-Pitaevskii equation, as described recently by Moffat. Alternatively
one can consider this condensate from the viewpoint of BogoliubovdeGennes theory, which can be approximated with coupled timeindependent gravitational Schrödinger equation.
It is recommended to conduct further observation in order to verify and
also to explore various implications of our propositions as described herein.

Acknowledgment
The writers would like to thank to Profs. C. Castro, F. Smarandache, T. Love
for valuable discussions.

Quantizatin in Astrophysics ...

491

6

V. Christianto & D. Rapoport

References
[1] Nottale, L., et al., Astron. Astroph. 322 (1997) 1018.
[2] Nottale, L., G. Schumacher, E.T. Levefre, Astron. Astroph. 361 (2000) 379389, preprint at http://daec.obspm.fr/users/nottale

[3] Rubcic, A., & J. Rubcic, “The quantization of solar like gravitational systems,”
Fizika B-7 Vol. 1 No. 1-13 (1998)

[4] Smarandache, F., & V. Christianto, Progress in Physics Vol.2 No. 2 (Apr.
2006), http://www.ptep-online.com

[5] Moffat, J., arXiv:astro-ph/0602607 (2006).
[6] Smarandache, F., & V. Christianto, Progress in Physics Vol.2 No. 4 (Oct.
[7]
[8]
[9]
[10]
[11]
[12]
[13]

2006), http://www.ptep-online.com; [6a] Christianto, V., EJTP Vol. 3 No. 12
(Oct. 2006) 117-144, http://www.ejtp.com
Lundin, N.I., “Microwave induced enhancement of the Josephson DC,”
Chalmers University of Technology & Gotterborg University report ( ) p. 4-7.
Griffin, M., arXiv:cond-mat/9911419 (1999).
Tuszynski, J., et al., Physica A 325 (2003) 455-476 (http://sciencedirect.com)
Toussaint, M., arXiv:cs.SC/0105033 (2001).
Fischer, U., arXiv:cond-mat/9907457 (1999); [8a] arXiv:cond-mat/0004339
Zurek, W. (ed.), in Proc. Euroconference in Formation and Interaction of
Topological Defects, Plenum, 1995, arXiv :cond-mat/9502119 (1995).
Volovik, G., arXiv:cond-mat/0507454 (2005).

First version: 2nd Jan. 2007. 1st revision: 21st Jan 2007.

Quantizatin in Astrophysics ...

492

Numerical solution of time-dependent gravitational Schrödinger equation

7

Appendix A.1. Time-independent gravitational Schrödinger equation

> restart;
> with (linalg);
> R:=exp(-(alpha*r));

R := e

( −α r )

> D1R:=diff(R,r); D2R:=diff(D1R,r);

D1R := −α e
D2R := α 2 e

( −α r )
( −α r )

>
SCHEQ1:=D2R+D1R*2/r+8*pi^2*m*E*R/h^2+8*pi^2*G*M*m^2*R/(r*
h^2)-l*(l+1)*R/r^2=0;

SCHEQ1 :=
α e
2

( −α r )

( −α r )

2αe
−
r

8 π2 m E e
+
h2

( −α r )

8 π2 G M m2 e
+
r h2

( −α r )

l (l + 1) e
−
r2

( −α r )

> XX1:=factor(SCHEQ1);

XX1 :=

e

( −α r )

( α 2 r 2 h 2 − 2 α r h 2 + 8 π2 m E r 2 + 8 π2 G M m2 r − l 2 h 2 − l h 2 )
=0
r 2 h2

> KK:=solve(XX1,G*M); AA:=solve(XX1,alpha);

KK :=

AA :=

h + h2 + l 2 h2 + l h2 − 8 π2 m E r 2 − 8 π2 G M m2 r
,
hr

h − h 2 + l 2 h 2 + l h 2 − 8 π2 m E r 2 − 8 π2 G M m2 r
hr
> #Using simplified terms only from equation (A*8)
> SCHEQ2:=D2R+D1R*2/r+8*pi^2*G*M*m^2*R/(r*h^2)=0;

SCHEQ2 := α e
2

( −α r )

Quantizatin in Astrophysics ...

( −α r )

2αe
−
r

8 π2 G M m2 e
+
r h2

( −α r )

493

=0

=0

8

V. Christianto & D. Rapoport

> XX2:=factor(SCHEQ2);

XX2 :=

e

( −α r )

( α 2 r h 2 − 2 h 2 α + 8 π2 G M m2 )
=0
r h2

> RR:=solve(XX2,r);

RR := −

2 ( 4 π2 G M m2 − h2 α )
α2 h2

> #Then solving for RR=0, yields:
> SCHEQ3:=4*pi^2*G*M*m^2-h^2*alpha=0;

SCHEQ3 := 4 π 2 G M m 2 − h 2 α = 0
> a:=solve(SCHEQ3,alpha);

a :=

4 π2 G M m2
h2

> #Gravitational Bohr radius is defined as inverse of alpha:
> gravBohrradius:=1/a;

gravBohrradius :=

Quantizatin in Astrophysics ...

h2
4 π2 G M m2

494

Numerical solution of time-dependent gravitational Schrödinger equation

9

Appendix A.2. Time-dependent gravitational Schrödinger equation

> #Solution of gravitational Schrodinger equation (Rubcic, Fizika 1998);
> restart;
> #with time evolution (Hagendorn's paper);
> S:=r(t); R:=exp(-(alpha*S)); R1:=exp(-(alpha*r));

S := r( t )
R := e

( −α r( t ) )

R1 := e

( −α r )

> D4R:=diff(S,t); D1R:= -alpha*exp(-(alpha*S)); D2R:= alpha^2*exp(-(alpha*S)); D5R:=D1R*D4R;

D4R :=

d
r( t )
dt

D1R := −α e

( −α r( t ) )

D2R := −α 2 e
D5R := −α e

( −α r( t ) )

( −α r( t ) )

⎛ d r( t ) ⎞
⎜⎜
⎟⎟
⎝ dt
⎠

> #Using simplified terms only from equation (A*8)
> SCHEQ3:=-h*D5R+D2R+D1R*2/S+8*pi^2*G*M*m^2*R/(S*h^2);

SCHEQ3 := h α e

( −α r( t ) )

( −α r( t ) )

⎛ d r( t ) ⎞ − α 2 e ( −α r( t ) ) − 2 α e
⎜⎜
⎟⎟
r( t )
⎝ dt
⎠

8 π2 G M m2 e
+
r( t ) h 2

> XX2:=factor(SCHEQ3);

e
XX2 :=

( −α r( t ) )

⎛ h 3 α ⎛ d r( t ) ⎞ r ( t ) − α 2 r( t ) h 2 − 2 α h 2 + 8 π 2 G M m 2 ⎞
⎜⎜
⎟⎟
⎜⎜
⎟⎟
⎝
⎠
⎝ dt
⎠
r( t ) h 2

>

Quantizatin in Astrophysics ...

495

( −α r( t ) )

10

V. Christianto & D. Rapoport

> #From standard solution of gravitational Schrodinger
equation, we know (Rubcic, Fizika 1998):
> SCHEQ4:=4*pi^2*G*M*m^2-h^2*alpha;

SCHEQ4 := 4 π 2 G M m2 − α h 2
> a:=solve(SCHEQ4,alpha);

a :=

4 π2 G M m2
h2

> #Gravitational Bohr radius is defined as inverse of alpha:
> gravBohrradius:=1/a;

gravBohrradius :=

h2
4 π2 G M m2

> #Therefore time-dependent solution of Schrodinger equation may introduce new term to this gravitational Bohr
radius.
> SCHEQ5:=(XX2*(S*h^2)/(exp(-(alpha*S))))-2*SCHEQ4;

d
SCHEQ5 := h 3 α ⎛⎜⎜ r( t ) ⎟⎟⎞ r( t ) − α 2 r( t ) h 2
d
⎝ t
⎠
> #Then we shall assume for simplicity by assigning value
to d[r(t)]/dt:
> D4R:=1;

D4R := 1
> Then we can solve again SCHEQ5 similar to solution of
SCHEQ4 :
> a2:=solve((h^3*alpha*(D4R)*S-alpha^2*S*h^2)+2*SCHEQ4);

a2 := { α = α, π = π, m = m, h = h, G = G, M = M,
t = RootOf ( r( _Z ) α h 3 − r( _Z ) α 2 h 2 + 8 π 2 G M m 2 − 2 α h 2 ) },
{ α = 0, t = t, m = m, h = h, G = G, M = M, π = 0 },
{ α = 0, π = π, t = t, m = m, h = h, M = M, G = 0 },
{ α = α, h = 0, t = t, m = m, G = G, M = M, π = 0 },

Quantizatin in Astrophysics ...

496

Numerical solution of time-dependent gravitational Schrödinger equation

11

h3
},
4 π2 M m2
{ α = α, h = 0, π = π, t = t, m = m, M = M, G = 0 },
{ α = 0, π = π, t = t, m = m, h = h, G = G, M = 0 },
{ α = 0, π = π, t = t, h = h, G = G, M = M, m = 0 },
{ α = α, h = 0, π = π, t = t, m = m, G = G, M = 0 },
{ α = α, h = 0, π = π, t = t, G = G, M = M, m = 0 }
{ π = π, t = t, m = m, h = h, M = M, α = h, G =

> gravBohrradius1:=1/(a2);

⎛
gravBohrradius1 := 1/⎜ { α = α, π = π, m = m, h = h, G = G, M = M,
⎜
⎝
t = RootOf ( r( _Z ) α h 3 − r( _Z ) α 2 h 2 + 8 π 2 G M m 2 − 2 α h 2 ) },
{ α = 0, t = t, m = m, h = h, G = G, M = M, π = 0 },
{ α = 0, π = π, t = t, m = m, h = h, M = M, G = 0 } ,
{ α = α, h = 0, t = t, m = m, G = G, M = M, π = 0 },
h3
},
4 π2 M m2
{ α = α, h = 0, π = π, t = t, m = m, M = M, G = 0 },
{ α = 0, π = π, t = t, m = m, h = h, G = G, M = 0 },
{ α = 0, π = π, t = t, h = h, G = G, M = M, m = 0 },
{ α = α, h = 0, π = π, t = t, m = m, G = G, M = 0 } ,
{ π = π, t = t, m = m, h = h, M = M, α = h, G =

⎞
{ α = α, h = 0, π = π, t = t, G = G, M = M, m = 0 } ⎟
⎟
⎠
> #Comparing our gravitational Bohr radius to the standard grav Bohr radius yield modification factor, called
here chi-factor:
> chi:=gravBohrradius*(1+gravBohrradius1/gravBohrradius);

χ :=

⎛⎛
⎜ ⎜ { α = α, π = π, m = m, h = h, G = G, M = M,
⎜⎜
⎝⎝
t = RootOf ( r( _Z ) α h 3 − r( _Z ) α 2 h 2 + 8 π 2 G M m 2 − 2 α h 2 ) },
{ α = 0, t = t, m = m, h = h, G = G, M = M, π = 0 },
{ α = 0, π = π, t = t, m = m, h = h, M = M, G = 0 },
{ α = α, h = 0, t = t, m = m, G = G, M = M, π = 0 },
1 2⎛
h ⎜ 1 + 4 π2 G M m2
4 ⎜⎝

h3
},
4 π2 M m2
{ α = α, h = 0, π = π, t = t, m = m, M = M, G = 0 },

{ π = π, t = t, m = m, h = h, M = M, α = h, G =

Quantizatin in Astrophysics ...

497

12

V. Christianto & D. Rapoport

{ α = 0, π = π, t = t, m = m, h = h, G = G, M = 0 },
{ α = 0, π = π, t = t, h = h, G = G, M = M, m = 0 },
{ α = α, h = 0, π = π, t = t, m = m, G = G, M = 0 } ,
⎞ ⎞⎞
{ α = α, h = 0, π = π, t = t, G = G, M = M, m = 0 } ⎟ h 2 ⎟⎟ ⎟⎟
⎟
⎠ ⎠⎠
> #Therefore one could expect that there is timedependent change of gravitational Bohr radius.

Quantizatin in Astrophysics ...

498

( π2 G M m2 )

Numerical solution of time-dependent gravitational Schrödinger equation

13

Appendix A.3. Time-independent gravitational Schrödinger equation
with Yukawa potential [5]
> #Extension of gravitational Schrodinger equation (Rubcic, Fizika 1998);
> restart;
> #departure from newton potential;
> R:=exp(-(alpha*r));

R := e

( −α r )

> D1R:=diff(R,r); D2R:=diff(D1R,r);

D1R := −α e
D2R := α 2 e

( −α r )
( −α r )

>
> SCHEQ2:=D2R+D1R*2/r+8*pi^2*(G*M-K*exp(2*mu)*(1+mu*r))*m^2*R/(r*h^2)=0;

SCHEQ2 := α e
2

( −α r )

( −α r )

2αe
−
r

+

8 π2 ( G M − K e

( −2 μ )

( 1 + μ r ) ) m2 e
r h2

( −α r )

=0

> XX2:=factor(SCHEQ2);

XX2 := −
e

( −α r )

( −α 2 r h 2 + 2 α h 2 − 8 π 2 m 2 G M + 8 π 2 m 2 K e
r h2

( −2 μ )

=0
> RR:=solve(XX2,r);

RR :=

2 ( −α h 2 + 4 π 2 m 2 G M − 4 π 2 m 2 K e
−α 2 h 2 + 8 π 2 m 2 K e

( −2 μ )

( −2 μ )

)

μ

> #from standard gravitational Schrodinger equation we
know:
> SCHEQ3:=4*pi^2*G*M*m^2-h^2*alpha=0;

Quantizatin in Astrophysics ...

499

+ 8 π2 m2 K e

( −2 μ )

μ r)

14

V. Christianto & D. Rapoport

>

SCHEQ3 := 4 π 2 G M m 2 − h 2 α = 0
> a:=solve(SCHEQ3,alpha);

a :=

4 π2 G M m2
h2

> #Gravitational Bohr radius is defined as inverse of alpha:
> gravBohrradius:=1/a;

gravBohrradius :=

h2
4 π2 G M m2

> #Therefore we conclude that the new terms of RR shall
yield new terms (YY) into this gravitational Bohr radius:
> PI:=(RR*(alpha^2*h^2)-(-8*pi^2*G*M*m^2+2*h^2*alpha));

Π :=

2 ( −α h 2 + 4 π 2 m 2 G M − 4 π 2 m 2 K e
−α h + 8 π m K e
2

2

2

2

( −2 μ )

( −2 μ )

) α2 h2

μ

+ 8 π2 m2 G M − 2 α h2

> #This new term induced by pion condensation via GrossPitaevskii equation may be observed in the form of longrange potential effect. (see Moffat, J., arXiv:astroph/0602607, 2006; also F. Smarandache & V. Christianto,
Progress in Physics, Vol. 2 no. 2, 2006, & Vol. 3 no. 1,
2007, www.ptep-online.com)
> #We can also solve directly :
> SCHEQ5:=RR*(alpha^2*h^2)/2;

SCHEQ5 :=

( −α h 2 + 4 π 2 m 2 G M − 4 π 2 m 2 K e
−α 2 h 2 + 8 π 2 m 2 K e

( −2 μ )

( −2 μ )

μ

> a1:=solve(SCHEQ5,alpha);

4 π2 m2 ( G M − K e
a1 := 0, 0,
h2

Quantizatin in Astrophysics ...

( −2 μ )

)

500

) α2 h2

Numerical solution of time-dependent gravitational Schrödinger equation

15

> #Then one finds modified gravitational Bohr radius in
the form:

> modifgravBohrradius:=1/(4*pi^2*(G*MK*exp(-2*mu))*m^2/h^2);
1
h2
modifgravBohrradius :=
4 π 2 m 2 ( G M − K e ( −2 μ ) )
> #This modification can be expressed in
chi-factor:
> chi:=modifgravBohrradius/gravBohrradius;
GM
χ :=
( −2 μ )
GM−Ke

Quantizatin in Astrophysics ...

501

16

V. Christianto & D. Rapoport

Appendix A.4. Coupled time-independent gravitational Schrödinger
equation (Bogoliubov-deGennes)

> #Solution of gravitational Schrodinger equation (Rubcic, Fizika 1998);
> restart;
> with(DEtools):
> #without time evolution, Bogoliubov-deGennes method
(version3);
> R:=exp(-(alpha*r));

R := e

( −α r )

> D1R:=diff(R,r); D2R:=diff(D1R,r);

D1R := −α e

( −α r )
( −α r )

D2R := α 2 e

> #Using simplified terms only from equation (A*8), timeindependent Schrodinger equation with BOgoliubov deGennes
method (TISE)
> ODESCHEQ2:=D2R+D1R*2/r+8*pi^2*G*M*m^2*R/(r*h^2);

ODESCHEQ2 := α 2 e

( −α r )

( −α r )

−

2αe
r

+

8 π2 G M m2 e
r h2

( −α r )

> Ar := matrix(2,2,[8*pi^2*G*M*m^2*R/(r*h^2),D2R+D1R*2/r,+D2R+D1R*2
/r,-8*pi^2*G*M*m^2*R/(r*h^2)]);
( −α r )

⎡ 8 π2 G M m2 e
⎢
⎢
⎢
r h2
Ar := ⎢⎢
( −α r )
⎢
⎢ 2 ( −α r ) 2 α e
⎢α e
−
⎢
r
⎣

( −α r )

⎤
⎥
⎥
⎥
⎥
⎥
2
2 ( −α r ) ⎥
8π GMm e
⎥
⎥
−
⎥
r h2
⎦

α e
2

( −α r )

2αe
−
r

>

Quantizatin in Astrophysics ...

502

Numerical solution of time-dependent gravitational Schrödinger equation

17

> Br := matrix(2,1,[f(r),g(r)]);

f( r ) ⎤
Br := ⎡⎢⎢
⎥⎥
⎣g( r )⎦
> solr := matrixDE(Ar,Br,r);

⎡
solr := ⎢⎢
⎢
⎣
2
2
d
⎛ ⎛d
⎛d
⎡
⎛
⎞
⎞
⎢ DESol⎜ { − ⎜ −⎜ 2 _Y( r ) ⎟ r 3 h 4 α + 2 ⎜ 2 _Y( r ) ⎟ r 2 h 4 + 2 ⎛⎜⎜ _Y( r ) ⎞⎟⎟ r h 4
⎢
⎜
⎜ ⎜ dr
⎟
⎜ dr
⎟
⎝ dr
⎠
⎣
⎝
⎝ ⎝
⎠
⎝
⎠
d
d
− ⎛⎜⎜ _Y( r ) ⎞⎟⎟ r 3 h 4 α 2 + 2 ⎛⎜⎜ _Y( r ) ⎞⎟⎟ r 2 h 4 α
d
r
d
⎝
⎠
⎝ r
⎠
−8e

( −2 α r )

− 128 e

−6e
−2e

_Y( r ) π 2 G M m 2 α e

( −2 α r )

( −2 α r )

_Y( r ) π 4 G 2 M 2 m 4 + e

_Y( r ) α 4 h 4 r 2 + 12 e

( −α r )

(α r)

(α r)

( −2 α r )

r h 2 + 64 e
( −2 α r )

( −2 α r )

_Y( r ) π 4 G 2 M 2 m 4 α r

_Y( r ) α 5 h 4 r 3

_Y( r ) α 3 h 4 r − 8 e

( −α r )

(α r)

( −2 α r )

_Y( r ) α 2 h 4

( −α r )

r h 4 f( r ) e
+e
r 3 h 4 f( r ) α 2 e
−2e
r 2 h 4 f( r ) α e
( −α r ) 3 4 ⎛ d
(α r)
( −α r ) 2 4 ⎛ d
(α r)
+e
r h ⎜⎜ f( r ) ⎟⎟⎞ α e
−2e
r h ⎜⎜ f( r ) ⎞⎟⎟ e
d
r
d
r
⎝
⎠
⎝
⎠

+8e
+e

( −α r )

( −α r )

r 2 h 2 π 2 G M m 2 f( r ) α − 16 e

r 3 h 4 α 3 g( r ) − 4 e

( −α r )

( −α r )

(α r)

r h 2 π 2 G M m 2 f( r )

r 2 h 4 α 2 g( r ) + 4 e

( −α r )

⎞
r h 4 α g( r ) ⎟⎟
⎠

( r 2 h4

⎞⎤
( α r − 2 ) ) }, { _Y( r ) } ⎟ ⎥
⎟⎥
⎠⎦
2
2
⎡⎛
⎛
⎞
⎞
⎛ ⎛d
⎛d
⎢ ⎜ −8 π 2 G M m 2 DESol⎜ { − ⎜ −⎜ 2 _Y( r ) ⎟ r 3 h 4 α + 2 ⎜ 2 _Y( r ) ⎟ r 2 h 4
⎢⎜
⎜
⎜ ⎜ dr
⎟
⎜ dr
⎟
⎣⎝
⎝
⎝ ⎝
⎠
⎝
⎠
d
d
d
4
3
4
2
2
4
+ 2 ⎛⎜⎜ _Y( r ) ⎞⎟⎟ r h − ⎛⎜⎜ _Y( r ) ⎞⎟⎟ r h α + 2 ⎛⎜⎜ _Y( r ) ⎞⎟⎟ r h α
⎠
⎠
⎠
⎝ dr
⎝ dr
⎝ dr
−8e

( −2 α r )

− 128 e
−6e
−2e

_Y( r ) π 2 G M m 2 α e

( −2 α r )

( −2 α r )

_Y( r ) π 4 G 2 M 2 m 4 + e

_Y( r ) α 4 h 4 r 2 + 12 e

( −α r )

(α r)

(α r)

( −2 α r )

r h 2 + 64 e
( −2 α r )

( −2 α r )

_Y( r ) π 4 G 2 M 2 m 4 α r

_Y( r ) α 5 h 4 r 3

_Y( r ) α 3 h 4 r − 8 e

( −α r )

(α r)

( −2 α r )

_Y( r ) α 2 h 4

( −α r )

r h 4 f( r ) e
+e
r 3 h 4 f( r ) α 2 e
−2e
r 2 h 4 f( r ) α e
( −α r ) 3 4 ⎛ d
(α r)
( −α r ) 2 4 ⎛ d
(α r)
+e
r h ⎜⎜ f( r ) ⎟⎟⎞ α e
−2e
r h ⎜⎜ f( r ) ⎞⎟⎟ e
⎝ dr
⎠
⎝ dr
⎠
+8e
+e

( −α r )

( −α r )

r 2 h 2 π 2 G M m 2 f( r ) α − 16 e

r 3 h 4 α 3 g( r ) − 4 e

Quantizatin in Astrophysics ...

( −α r )

( −α r )

(α r)

r h 2 π 2 G M m 2 f( r )

r 2 h 4 α 2 g( r ) + 4 e

503

( −α r )

⎞
r h 4 α g( r ) ⎟
⎟
⎠

( r 2 h4

18

V. Christianto & D. Rapoport
2
(α r) ⎛ d
⎛
⎞
⎞
⎛ ⎛d
⎜ DESol⎜ { − ⎜ −⎜ 2 _Y( r ) ⎟ r 3 h 4 α
( α r − 2 ) ) }, { _Y( r ) } ⎟⎟ + r e
⎜ dr
⎜
⎟
⎜ ⎜ dr
⎝
⎝
⎠
⎠
⎝ ⎝
2
d
d
⎛d
⎞
+ 2 ⎜⎜ 2 _Y( r ) ⎟⎟ r 2 h 4 + 2 ⎛⎜⎜ _Y( r ) ⎞⎟⎟ r h 4 − ⎛⎜⎜ _Y( r ) ⎞⎟⎟ r 3 h 4 α 2
d
r
d
⎝ r
⎠
⎝
⎠
⎝ dr
⎠
(
−2
α
r
)
(α r)
d
+ 2 ⎛⎜⎜ _Y( r ) ⎞⎟⎟ r 2 h 4 α − 8 e
_Y( r ) π 2 G M m 2 α e
r h2
d
r
⎝
⎠

+ 64 e
+e

( −2 α r )

( −2 α r )

−8e

_Y( r ) π 4 G 2 M 2 m 4 α r − 128 e

_Y( r ) α 5 h 4 r 3 − 6 e

( −2 α r )

_Y( r ) α 2 h 4 − 2 e

( −2 α r )

( −α r )

( −2 α r )

_Y( r ) π 4 G 2 M 2 m 4

_Y( r ) α 4 h 4 r 2 + 12 e
(α r)

( −2 α r )

( −α r )

_Y( r ) α 3 h 4 r
(α r)

r h 4 f( r ) e
+e
r 3 h 4 f( r ) α 2 e
( −α r ) 2 4
(α r)
( −α r ) 3 4 ⎛ d
(α r)
−2e
r h f( r ) α e
+e
r h ⎜⎜ f( r ) ⎞⎟⎟ α e
⎠
⎝ dr
( −α r ) 2 4 ⎛ d
(
α
r
)
(
−α
r
)
−2e
r h ⎜⎜ f( r ) ⎟⎟⎞ e
+8e
r 2 h 2 π 2 G M m 2 f( r ) α
⎝ dr
⎠
( −α r )
( −α r ) 3 4 3
( −α r ) 2 4 2
− 16 e
r h 2 π 2 G M m 2 f( r ) + e
r h α g( r ) − 4 e
r h α g( r )
⎞
r h 4 α g( r ) ⎟⎟
⎠
(α r)
⎡
⎤
⎤
f( r ) ⎥ ⎥
⎢ 0, − r e
⎢
α
(
α
r
−
2 ) ⎥⎦ ⎥⎥⎦
⎣
+4e

( −α r )

⎞⎞ ⎞
( r 2 h 4 ( α r − 2 ) ) }, { _Y( r ) } ⎟⎟ ⎟⎟ h 2 ⎟⎟
⎠⎠ ⎠

> #Note that this result is very different from standard
solution of gravitation Schrodinger equation described by
Nottale or Rubcic&Rubcic (1998):

a :=

Quantizatin in Astrophysics ...

4 π2 G M m2
h2

504

⎤
( α ( α r − 2 ) h 2 ) ⎥⎥,
⎦

Quantization in Astrophysics, Brownian Motion, and Supersymmetry
The present book discusses, among other things, various quantization phenomena found in
Astrophysics and some related issues including Brownian Motion. With recent discoveries of
exoplanets in our galaxy and beyond, this Astrophysics quantization issue has attracted
numerous discussions in the past few years.
Most chapters in this book come from published papers in various peer-reviewed journals,
and they cover different methods to describe quantization, including Weyl geometry,
Supersymmetry, generalized Schrödinger, and Cartan torsion method. In some chapters
Navier-Stokes equations are also discussed, because it is likely that this theory will remain
relevant in Astrophysics and Cosmology
While much of the arguments presented in this book are theoretical, nonetheless we
recommend further observation in order to verify or refute the propositions described herein.
It is of our hope that this volume could open a new chapter in our knowledge on the
formation and structure of Astrophysical systems.
The present book is also intended for young physics and math fellows who perhaps will find
the arguments described here are at least worth pondering.

ISBN 81-902190-9-X

90000>

9 788190 219099

OPPORTUNITIES
IN

NUCLEAR ASTROPHYSICS

ORIGIN OF THE ELEMENTS

i

OCTOBER 1, 1999

OPPORTUNITIES IN NUCLEAR ASTROPHYSICS
Conclusions of a Town Meeting held at the University of Notre Dame
7-8 June 1999

Prepared by the Joint Institute for Nuclear Astrophysics
Michigan State University and the University of Notre Dame
ii

PREFACE
A Town Meeting on Opportunities in Nuclear Astrophysics was held at the University of
Notre Dame, on June 7-8, 1999. The meeting was organized by the University of Notre
Dame and Michigan State University with the help of a steering committee and was cosponsored by the American Physical Society’s Division of Nuclear Physics. Its goal was to
define and summarize the state of the field of nuclear astrophysics and the opportunities for
the future of the field. Even though the meeting was organized on relatively short notice, it
was attended by over 170 individuals from 68 institutions worldwide, a sign of the great
interest in nuclear astrophysics.
The meeting consisted of 18 plenary talks, plus two working group sessions involving a total
of 13 working groups. Each of these groups gave a brief presentation of its conclusions and
also wrote a synopsis that was used for the preparation of this white paper. The white paper
was prepared by a sub-set of the writing committee and was commented upon by other
members of that committee, as well as some consultants, when expert advice was required in
a particular area.
The Workshop Program, lists of the attendees and of the steering and writing committees can
be found in the Appendices.
The White Paper begins with two sections, Challenges in Nuclear Astrophysics and
Summary and Recommendations, that are meant to serve as a brief statement of the
challenges presented by the field and of how these challenges can be met by the research
community. The main part of the document begins with an overview, to provide a guide to
what follows, a survey of some of the observational constraints, a detailed discussion of
various astrophysical phenomena, and, finally, a discussion of the facilities and equipment
needed to address the most important opportunities in nuclear astrophysics.
In order to facilitate reading of the individual sections, redundant information has been
provided in many cases.

Cover: First image from the Chandra X-ray Observatory. The image of the Cassiopeia A supernova
remnant reveals a fast outer shock wave and a slower inner shock wave. The bright object near the
center may be a black hole or neutron star remnant of the supernova explosion.

iii

CONTENTS
I.

CHALLENGES IN NUCLEAR ASTROPHYSICS .………………………………………

1

II.

SUMMARY AND RECOMMENDATIONS …………………………………………… ..

6

III.

OVERVIEW………………………………………………………………………………...

9

IV.

ABUNDANCE CONSTRAINTS ON NUCLEOSYNTHESIS ……………………………

13

V.

NON-STELLAR EVENTS ………………………………………………………………...

16

VI.

NUCLEOSYNTHESIS AND STELLAR EVOLUTION…………………………………..

19

VII.

SUPERNOVAE …………………………………………………………………………….

22

VIII.

CATACLYSMIC BINARY STARS ……………………………………………………….

27

IX.

SYNTHESIS OF THE HEAVY ELEMENTS ………… ………………………………….

31

X.

NEUTRON STARS ………………………………………………………………………...

36

XI.

NEUTRINOS IN ASTROPHYSICS ……………………………………………………….

39

XII.

NUCLEAR STRUCTURE AND DATA IN NUCLEAR ASTROPHYSICS ……………… 43

XIII.

REQUIRED ACCELERATORS …………………………………………………………… 47

XIV.

EQUIPMENT NEEDS FOR LABORATORY ASTROPHYSICS ………………………… 50

XV.

OUTLOOK …………………………………………………………………………………. 54

XVI.

APPENDICES ……………………………………………………………………………… 55

iv

I. CHALLENGES IN NUCLEAR ASTROPHYSICS
Introduction
Carl Sagan’s statement, “We are made of star stuff,” reflects the origin in stellar nuclear
processes of the elements that comprise humankind and the whole of the universe, out to the
remotest of galaxies. Moreover, the energy on which we depend for life originates in nuclear
reactions at the center of the nearest star, our sun.
Our understanding of these processes has developed greatly in the last 75 years. It was
known early on that nuclear reactions must provide the energy for the sun. No other
processes, chemical or gravitational, could yield the sun’s luminosity over its 4.6 billion-year
life. More direct evidence for stellar nuclear reactions was the observation of the element
technetium in the atomic spectra of distant stars. All isotopes of this element live less than
4.5 million years, and any of them present at the formation of a star would have decayed
during the much longer stellar lifetime. The observed technetium must then have been made
in the star. In the 1930’s Bethe and von Weizsäcker proposed a scenario for nuclear burning,
a detailed set of reactions, the CNO cycles, that converted hydrogen to helium in the first
stages of stellar evolution. Then, in 1957, the famous paper by Burbidge, Burbidge, Fowler
and Hoyle, B2FH, laid out a scenario for the production of the elements, which was
elaborated in the succeeding years. A major development in the 1970’s was the big-bang
description of the early universe, which explained the formation of the very light elements.
In spite of major efforts in observation, experiment, and theory over the past twenty years, a
multitude of challenging questions remain. We are still not certain that we understand
hydrogen burning in our own sun and whether the neutrinos it emits reach the earth
unaltered. We seem to understand the production of heavy elements by neutron capture in red
giant stars, but we are not sure where all the neutrons come from. We analyze precisely the
dust of stellar processes in meteoritic inclusions, but we cannot match the results with our
present models. We see the characteristic features of the rapid neutron capture process in the
galactic elemental abundance distribution, but we are still not sure where and when the
process occurs. In the tiny amounts of neutron deficient stable isotopes we see the signature
of an enormous gamma flux, but we do not know where or when it occurs. We analyze the
outbursts of novae and supernovae, but what we see often disagrees with what was expected.
We see gigantic explosions in distant galaxies, but we are only beginning to formulate the
detailed descriptions that will allow us to match models with observations.
The challenge of understanding how nuclear processes influence astrophysical phenomena
remains. New astronomical observations, new experimental capabilities, and new
computational power present an opportunity for an important advance in our knowledge of
the cosmos.
Revolution in observation
In the last decade there has been a revolution in astronomical observation over a wide range
of the electromagnetic spectrum. Results from satellite based observatories, from the Hubble
Space Telescope, and from large-aperture earth-based telescopes have, for example, yielded
measurements of the primeval deuterium abundance and of the abundances of elements in

1

very old stars. These observations determine the average baryon density of the universe and
indicate that the heavy elements were already being formed in the earliest stellar generations.
They may provide insight into the history of nucleosynthesis, star formation and supernova
explosions. Abundances observed in the ejecta of nova explosions challenge our models of
nucleosynthesis and energy production in novae.
Optical, infrared, and ultraviolet observatories provide only a part of the new information.
Observations of gamma-ray lines from the Compton Gamma-Ray Observatory have opened a
new field of observation and produced results that will guide our understanding of
nucleosynthesis in nova and supernova explosions. The INTEGRAL high-resolution gammaray observatory and other future instruments will provide a wealth of data in this regime.
Neutrino detectors originally designed to measure the temperature at the center of the sun
have indeed verified that nuclear reactions in the sun produce solar neutrinos and have raised
the possibility that neutrinos oscillate from one species to another and, therefore, have mass.
These neutrino oscillations may have large effects on energy transport and element synthesis
in supernovae. Presolar grains, originating in the stellar wind of stars late in their evolution,
in supernovae, or in novae have been found in meteorites. The isotopes found in these grains
provide detailed information on the processes in individual stellar sites and will help
distinguish the results of individual events from the grand average normally seen at the
surfaces of stars. A final example is the observed decay of magnetic fields in accreting
neutron stars, which is probably affected by the ashes of rapid hydrogen burning on the
neutron star’s surface.
This summary, far from complete, illustrates the wide range of phenomena that need to be
explained and the intimate connections among apparently disparate observations.
Fortunately, experimental, computational, and theoretical advances have taken place in both
nuclear physics and astrophysics. Together these advances will enable us to obtain key data
and to simulate many of these phenomena in the laboratory or computer, greatly increasing
our understanding of stars and of the cosmos.
New accelerator facilities
Nuclear astrophysics has traditionally concentrated on measurements that were relevant for
understanding stellar evolution and that were carried out at low-energy stable-beam
accelerators. Now, needs for data that require new experimental capabilities have opened a
new era of measurement.
These measurements are often limited by the extremely low reaction cross sections, which in
turn are reflected in the long lifetimes of stars. Only in a single case, 3He + 3He → 2p + 4He,
have measurements reached the actual energy range of non-explosive stellar processes.
Dedicated sources of high-intensity low-energy beams are required to explore other reactions
at stellar energies. Locating facilities underground would reduce the large radiation
background from cosmic rays and other sources of natural radioactivity, and, in certain cases,
make possible measurements at lower energies. Further development of intense heavy ion
beams is necessary to explore the low energy range by using beams of heavy projectiles to
bombard light targets, the inverse of the usual procedure.

2

An exceptionally important advance is the development of accelerator systems for the
production of beams of short-lived isotopes. Many of the nuclei that are formed in stellar
explosions are short-lived, but do live long enough to encounter and react with other nuclei in
the hot, dense astrophysical environment. It has been difficult to produce sufficient quantities
of these nuclei to carry out meaningful experiments, but in the past ten years modifications of
existing facilities have made this possible in selected cases. A new generation of facilities,
about to come on-line, will have intensities larger by several orders of magnitude and will
greatly expand the range of studies related to astrophysics. Finally, accelerator systems that
promise still more intense beams of radioactive ions are under active consideration and are
required to study many of the most interesting phenomena.
The development of intense neutron sources will open new fields of investigation. Present
sources can provide some of the information necessary for understanding the production of
heavy elements in helium burning stars. A new generation of neutron sources in the U.S.
(the SNS) and Europe will permit important experiments on radioactive targets. These
facilities will also provide a potent source of neutrinos and make it possible to measure
neutrino-nucleus interactions that are important in supernova evolution and for calibration of
supernova neutrino detectors.
New detectors
Coupled with these accelerator developments has been a revolution in detection apparatus,
including large spectrographs with high resolution; mass separators with high selectivity;
arrays of silicon, germanium, and scintillation detectors covering the majority of the event
space; and enhanced computing power to handle the resulting data streams. As a result, one
can employ detectors at accelerators and in space (for cosmic ray observations) that are much
more complex than those used previously.
Detectors of solar and supernova neutrinos are a special case. They are large and comparable
to accelerators in cost. Yet, the problems they address, the structure of stellar interiors and
the nature of neutrinos, are so important that their results are of interest to much of the
nuclear astrophysics community and will greatly affect our understanding of astronomical
objects.
Still another special case is the LIGO detector for gravitational waves. LIGO may appear to
address phenomena far from the interest of nuclear physicists, but will observe the signal
from mergers of neutron stars and from supernovae. The nature of the neutron-star signal
will depend on the nuclear equation of state that is studied experimentally in nuclear
reactions. And predicting the observational signal from an asymmetric supernova explosion
requires a variety of detailed information from nuclear physics.
Computational capabilities
Advances in computational capability has occurred just in time to allow us to take advantage
of the new observations and nuclear physics results. For example, it may soon be possible to
begin study of a supernova explosion in two and then three dimensions with accurate atomic
and nuclear physics input. This is a major computational effort, which will eventually require
computing power well beyond the present state of the art. Three-dimensional simulations of

3

stellar evolution, supernova explosions, nova explosions, and accretion processes on neutron
stars and black holes, are required to model the complex structure of these events and their
associated nucleosynthesis and energy generation. These problems will require large
investments in calculations of nuclear structure, the nuclear equation of state, neutrino
interactions with matter, and hydrodynamics at nuclear densities.
Manpower
As we assess our understanding of the role of nuclear physics in the cosmos, it is necessary to
consider also the structural and manpower issues in the field. The national manpower base in
nuclear astrophysics, experimental and theoretical, needs to be enhanced to take up these
challenges. Many of the analysis tools required to obtain nuclear structure information from
experiments remain to be developed, especially for the weakly bound exotic nuclei important
in many astrophysical processes. It will require a significant effort to develop the next
generation of expertise needed to connect the experimental measurements to the
astrophysical needs and to summarize this knowledge in a form useful for astrophysical
calculations.
Major challenges
•= Our earth and its rich biology depend on the many heavy elements synthesized during
stellar evolution and in violent events like supernovae. What are the nuclear processes
responsible for nucleosynthesis and when and where do they take place? What are the
characteristics − temperature, density, and composition − of the nucleosynthesis sites?
•= What is the explanation for the shortfall of neutrinos observed from our sun? Is the
current discrepancy entirely the result of new physics beyond our standard theory of
electroweak interactions, or does it represent, at least in part, some misunderstanding of
the nuclear reactions that power our sun? What new technologies can nuclear physicists
and others exploit to measure the entire spectrum of solar neutrinos?
•= What drives the spectacular stellar explosions known as supernovae? What are the
processes leading to the Type Ia supernovae used as standard candles in determining the
acceleration of the universe? How do we correctly model core evolution in core-collapse
supernovae? What determines whether a neutron star or black hole is left as a remnant?
What is the site of the r-process? Do supernovae produce most of the short-lived gammaemitting nucleus 26Al that is found widely distributed in the galaxy? Detection of the
various neutrino species emitted in such explosions may determine whether massive
neutrinos play a central role in cosmology; what detectors might nuclear physicists
construct for this purpose?
•= What are the processes that led to the isotopic abundances observed in the presolar grains
found in meteorites? Can these abundances serve to test models of s-process and rprocess nucleosynthesis of the heavy elements in individual stars?
•= What is the nature of the explosions that occur in accreting double-star systems? Can
developments of present accretion models accurately describe novae, Type Ia
supernovae, X-ray bursts, and X-ray pulsars? Do the proposed models for

4

nucleosynthesis on the surface of accreting white dwarfs describe the abundance
distributions in the ejecta of novae? How do the microscopic time scales of nuclear
reactions affect the time scale of an X-ray burst?
•=

Does the big bang accurately describe the process that created the lightest elements?
Can nuclear cross sections be measured with accuracy sufficient for big-bang calculations
of the universal baryon density to provide a stringent cross check of the baryon density
obtained from future measurements of the universal background radiation? Can results
from the Relativistic Heavy Ion Collider (RHIC) on the nature of the quark/gluon to
hadron phase transition shed light on the nature of inhomogeneities in the universal
density? Might there exist, in the nucleosynthesis expected from an inhomogeneous
universe, evidence of early exotic states of high-temperature hadronic matter?

•= Earth is bathed in a sea of cosmic radiation, much of it affected by nuclear processes
occurring in our galaxy. How can further measurements of nuclear properties such as
lifetimes, gamma-ray lines, and spallation cross sections help determine the origin of this
radiation? How can we exploit unstable nuclei as cosmological clocks of past events in
our galaxy? What is the origin of the highest energy cosmic rays?
•= What exotic forms of nuclear matter exist at the extraordinary densities characteristic of
neutron stars? What connections can be established between the observed properties of
such stars – masses, radii, rotation rates, and electromagnetic emissions – and the
behavior of nuclear matter under exotic conditions? How do the ashes of nuclear
reactions involving proton-rich nuclei on a neutron-star surface affect its observable
properties?
The following section provides recommendations on how these exciting challenges can be
met by the nuclear astrophysics community.

5

II. SUMMARY AND RECOMMENDATIONS
This is a special time for nuclear astrophysics:
•= There is a wealth of new observations that require detailed nuclear data for a credible
explanation.
•= New accelerators and detection techniques provide an unprecedented and growing
capability for providing the needed nuclear data.
•= Exponentially growing computational power makes it possible to include the resulting
microphysics in simulations of stellar evolution and the sequence of events leading to
novae, X-ray bursts, and supernova explosions.
An enhanced program in nuclear astrophysics, theoretical and experimental, will greatly
advance our understanding of the cosmos. It will strengthen observational and computational
programs by providing the essential foundation necessary for the interpretation and
simulation of new results.
The infrastructure in nuclear astrophysics, especially in manpower and theoretical modeling,
needs to be enhanced to meet these challenges. The field would benefit greatly from further
co-ordination of its efforts and the increased educational opportunities for graduate students.
Investment in specialized detection systems is also warranted.
The many connections among astrophysical observables and astrophysical processes demand
that nuclear astrophysicists take a broad view of their field. For example, evidence for
neutrino oscillations follows from experiments with neutrino detectors for solar and
atmospheric neutrinos. These oscillations may strongly affect element synthesis and the
evolution of supernovae.
The following are essential ingredients for these enhancements:
Facility needs and uses – see Section XIII
This summary includes needs that have already been identified. Other opportunities and
techniques will surely arise in the future.
•= A vigorous program of astrophysics studies at the new and upgraded radioactive ion
beam facilities. Both fragmentation and ISOL facilities are necessary to obtain the
required information.
•= Measurements of reaction cross sections with intense beams from low-energy stablebeam accelerators. Background reduction techniques are important. Measurements of
reaction rates by indirect techniques and of spallation cross sections with higher energy
stable-beam accelerators that provide energies from a few MeV to several GeV/nucleon.
•= Operation and construction of solar neutrino detectors and the construction of advanced
supernova neutrino detectors for studies of neutrino processes in supernovae.

6

•= Utilization of neutron spallation sources for s- and r-process related experiments and
possible measurements of neutrino-nucleus cross sections. Neutron beams from electron
linacs and other sources, and gamma-ray beams from free-electron laser and synchrotron
radiation sources are also important.
Major equipment needs – see Section XIV.
•= Highly segmented high-efficiency gamma-ray detector arrays for the study of capture
reactions and for the study of the structure of nuclei involved in the s-, r- and rpprocesses.
•= Large solid angle, good angular resolution, silicon detector arrays for the study of
reactions with radioactive beams. These studies will provide masses, resonance energies,
level parameters, spectroscopic strength, and decay widths for nuclei involved in
astrophysical processes. These detectors require new developments in fast, high-density,
low-noise electronics with a low cost per channel.
•= High efficiency neutron detection for β-delayed neutron decay measurements. High
efficiency and high angular resolution neutron detection for neutrons from charge
exchange reactions to determine weak interaction rates in supernova processes and
Coulomb-breakup experiments to determine neutron capture rates on radioactive nuclei.
•= Magnetic spectrographs with large solid angle acceptances and mass separators with high
beam rejection efficiency to provide the high selectivity and resolution required for many
experiments. Advanced focal-plane detectors for high rate operation over a broad range
of ion charge, velocity, and mass.
•= Traps of ions and atoms for studies that require isolated atoms, including measurements
of masses and studies of weak interaction properties.
•= Dense targets (gas jet and gas cell targets, and liquid and solid targets at cryogenic
temperatures), for measurements of small cross sections and for studies with radioactive
beams. Development of targets of long-lived radioactive nuclei, especially for studies of
s- and p-process nuclei. The expense and availability of rare and separated stable isotopes
needs to be addressed.
Manpower
•= The infrastructure in nuclear astrophysics, especially in manpower and theoretical
modeling, should be strengthened. Additional support for graduate and postdoctoral
education is needed. Enhanced and structured collaborations among institutions with
different capabilities would contribute greatly to the advance of the field.
Nuclear and astrophysics theory – see Section XII
•= Nuclear structure and reaction theory is an essential ingredient of nuclear astrophysics. A
major effort is required to connect theoretical calculations to experimental data, to
determine astrophysical reaction rates, and to summarize this knowledge in a form useful
for astrophysical calculations. A related effort must ensure that the extant experimental
7

data are available to nuclear theorists and astrophysicists for calculation of reaction rates
or validation of theoretical models.
•= Multidimensional simulations will be an important bridge between nuclear data and
astrophysical observations. Group efforts involving both nuclear physicists and
astrophysicists are probably the best approach to the simulation of important and complex
phenomena such as supernovae. Major computing resources (tera- and ultimately petaflop scale) will be required.

8

III. OVERVIEW
For orientation, the following is an overview of the basic phenomena in nuclear astrophysics
from the beginning of the universe in the big bang through the evolution of stars. While the
broad characteristics of these processes seem fairly well established, many detailed
predictions conflict with astronomical observations. Such discrepancies are not surprising,
since much of the nuclear physics presently used in calculations of stellar evolution and of
nucleosynthesis is taken from rather uncertain extrapolations or theoretical predictions. New
experimental and theoretical tools now make it possible to study key nuclear processes in the
cosmos and to put the nuclear physics aspects of stellar evolution on a much more solid
footing than was possible in the past. Major advances in our understanding of the cosmos are
within reach. The nuclear physics issues will be discussed more fully in the chapters
following this overview.
The beginning
Our universe is thought to have formed about 15 billion years ago in a hot dense fireball, the
big bang. As time progressed, the universe expanded and cooled. After a microsecond, a soup
of quarks and gluons condensed into protons and neutrons and the era of nuclear physics
began. After another 10 seconds, the universe had cooled to the point that the lightest nuclei,
isotopes of hydrogen, helium and a tiny amount of lithium, could form. These nuclei are the
ashes of the earliest element-forming processes−all that follows begins with nuclear reactions
among the nuclei of these three elements.
After about 200,000 years, the primordial nuclei and electrons combined. This occurred at a
temperature of several thousand kelvins, roughly that at which the most refractory elements
melt. It marked the beginning of the epoch in which density fluctuations could begin to grow
rapidly, leading to the galaxies and clusters of galaxies that we observe today.
Stellar evolution
The first stars formed from small-scale density fluctuations in regions of relatively high
density. Gravitational attraction caused those regions to condense, converting some of the
gravitational potential energy into thermal energy, until a temperature high enough to initiate
nuclear reactions was reached, about 10 million kelvins, an extremely high temperature by
terrestrial standards. These stars then synthesize heavier elements by fusion of their light
constituents, thereby releasing energy to stabilize the star against further gravitational
contraction.
The central core of a star that is sufficiently massive will evolve through several stages. At
each stage, fusing nuclei produce enough energy to heat the core of the star and generate a
pressure that successfully opposes the inward force of gravity. When the nuclear fuel is
exhausted, the central source of energy is gone and the core contracts under gravity. As the
core compresses, gravitational energy is converted into heat until the core temperature is
sufficient to ignite the nuclear fuel that powers the next stage. The successive stages
characterize stellar evolution, with the nuclear ashes of each stage providing the fuel for the
subsequent stage.

9

For example, the first stage, hydrogen
burning, converts four protons into a
4
He nucleus. The 4He ashes of
hydrogen burning become the fuel for
helium burning, where helium nuclei
combine to form 12C and 16O. For a
star with a mass similar to the sun this
is the last burning stage. After
throwing off much of its envelope as a
planetary nebula, such a star will
ultimately become a white dwarf,
supported against gravitational
collapse by the outward pressure of its
(degenerate) electrons and slowly
cooling. For more massive stars,
subsequent stages of stellar evolution
produce successively heavier nuclei
from 12C and 16O by carbon, neon,
oxygen and silicon burning. These
processes are responsible for the
synthesis of most of the nuclei from
neon to somewhat beyond iron.
During some helium burning phases,
reactions produce a sufficient number
of neutrons to synthesize about half of
the heavy elements by the s-process:
the slow (s = slow as compared to the
beta-decay lifetimes of intermediate
unstable capture products) successive
capture of neutrons by heavy seed
nuclei present in the star. The sprocess leads to abundance peaks near
nuclear masses of A = 88, 138, and
208.

Summary of conditions in various astrophysical burning
phases.
Upper Left: Burning in stellar cores and shells. The
extended shapes take into account a stellar mass range of
about 1 to 30 solar masses.
Lower Left: Same for several explosive phenomena
Right: The Gamow window for H, He and 12C capture on
12
C. This window encompasses the range of center-of-mass
bombarding energies which contributes to energy
generation and nucleosynthesis, and for which
astrophysical reaction rates must be known. It also
determines the range of excitation energies for resonances
and continuum excitations that are studied by indirect
means.

During its final stage of evolution, a
massive star forms a core consisting
of iron-like (i.e. A ≈ 56) elements, the most tightly bound nuclei in nature. No further
nuclear energy source is available, and the outward pressure eventually becomes insufficient
to counteract gravity. The iron core collapses and then rebounds when it reaches supranuclear
density, ultimately producing a massive explosion, a supernova that blows off the outer
envelope of the star. A neutron star or a black hole is left behind. In this process, most of the
energy is released as a flood of neutrinos that can be observed in terrestrial detectors.
Attempts to understand the supernova process are a major effort of present-day nuclear
astrophysics.

10

Supernovae are probably the sites of the r-process, the neutrino-process, and the gammaprocess. The r-process synthesizes the other half of the nuclei heavier than iron by rapid (r =
rapid) capture of many neutrons by seed nuclei in an event lasting several seconds. To
proceed so rapidly, the process must take place in a dense bath of neutrons and involve nuclei
far (20 to 30 neutrons) to the neutron-rich side of stability. It produces abundance peaks at
masses around A = 80, 130, and 195. The neutrino-process possibly occurs in the flux of
neutrinos that emanates from the nascent neutron star as the high temperature core cools.
High-energy neutrinos remove neutrons and protons from the more abundant nuclei and
synthesize certain important but rare nuclides. Finally, the gamma-process probably occurs
in the hot photon bath near the core of a supernova during explosive neon burning. Photons
remove neutrons, protons and alpha particles from heavy seed nuclei and thereby produce
rare proton-rich nuclei.
Phenomena in binary systems
More than half of the stars is found in systems of two stars with different masses. After the
more massive component has evolved to a white dwarf or neutron star, the system may
undergo a variety of dramatic phenomena. This typically occurs when the compact object
gravitationally captures (accretes) material from its less evolved companion. For example,
the accretion of hydrogen on a white dwarf may ignite a nova explosion in which the
brightness of the binary system increases by many orders of magnitude. Related phenomena
result from accretion onto a neutron star. Slow accretion is thought to lead to a high
temperature explosion observed as an X-ray burst that lasts for tens of seconds. More rapid
accretion leads to steady burning and is probably the mechanism for the repetitive emission
of X-ray pulsars, seen when the X-ray emitting hot spot rotates into the line of sight of the
earth. The energy generation in novae comes from the rapid catalytic burning of hydrogen to
helium in the hot-CNO cycles or other cyclic processes; that for X-ray bursts comes from the
αp and rp processes, rapid sequences of alpha or proton capture interspersed with beta-decay.
These processes involve proton-rich unstable nuclei.
The phenomena described above are repetitive. But rapid accretion on a white dwarf, or
merger with another white dwarf may cause its complete disruption, resulting in a Type Ia
supernova explosion. These explosions generate much of the iron in the universe, and have
served as markers of standard brightness for studies of cosmological expansion rates.
Seeing stellar cores
We cannot use optical telescopes to see the stellar core where nuclear reactions occur;
absorption of light limits us to viewing the stellar surface. So it is natural to ask for evidence
that nuclear reactions actually occur. Stellar spectra of some stars show lines corresponding
to technetium, an element that has no stable form, indicating that the technetium was formed
recently in the observed star. Technetium, and other elements, can be brought to the stellar
surface by convective processes that dredge up material from deep in the star. Their
abundances then provide a measure of the conditions there. Less directly, computer codes
that describe stellar evolution and the synthesis of the elements, provide strong evidence that
stars evolve and synthesize nuclei in general accord with the processes outlined above.
However, this description is far from complete and does not yet describe much of the data
that have become available from powerful new observatories. The task of nuclear

11

astrophysics is to make this description fully quantitative by obtaining better nuclear input
data and by creating more complete theoretical descriptions.
Recently, a variety of observations has given us glimpses of the conditions in stellar cores
and presented new challenges to nuclear astrophysics. Detection of neutrinos from the sun
has advanced our understanding of solar processes and of the nature of the neutrinos
themselves. Detection of neutrinos from SN1987A gives us confidence that the corecollapse picture of certain supernovae is qualitatively correct; detection of neutrinos from
another supernova with an advanced detector would provide a detailed snapshot of what
happens in the last few seconds of a massive star’s life. Observations of gamma rays from
some of the elements made near the core of such a supernova also shed light on
nucleosynthesis deep in the supernova. Presolar grains, originating in the stellar wind of stars
late in their evolution, in supernovae, or in novae have been found in meteorites and provide
information on nucleosynthesis in individual events taking place deep inside stars.

12

IV. ABUNDANCE CONSTRAINTS ON NUCLEOSYNTHESIS
Nucleosynthesis is the production of the elements
that constitute the baryonic matter of the universe.
We now understand that nuclear processes,
operating both in the early universe and in stars,
are responsible for the synthesis of the elements.
This universal nuclear history of the matter is
written in the compositions of its diverse
constituents: stars, interstellar (and intergalactic)
gas and dust, meteorites, and cosmic rays.
Over the past quarter century, advances in
experimental and observational techniques and
facilities (e.g. the Hubble Space Telescope, the
Compton Gamma-Ray Observatory, the Rosat Xray Observatory, the Keck telescopes) have made
possible accurate determinations of the elemental
and isotopic abundances in many astronomical
environments. Such studies impose increasingly
stringent constraints on models of stellar
evolution and nucleosynthesis and help identify
critical areas in which nuclear physics input is
essential.

Hubble Space Telescope image of a
supernova remnant known as N132D in
the Large Magellanic Cloud. Massive stars
synthesize elements in their cores through
nuclear fusion. Starting with the light
elements of hydrogen and helium, they
produce progressively heavier elements,
carbon, oxygen, nitrogen, etc., up through
iron. At the end of their lives they explode
in a spectacular supernova, scattering these
elements into space, thereby contributing
material to the formation of other stars and
star systems. The elements making up life
on Earth originated in such stellar ovens.
Analysis of the emitted light allows
astronomers to explore the details of this
nuclear processing. It reveals luminous
clouds of supernova debris energized by
shockwaves -- singly ionized sulfur
appears red, doubly ionized oxygen, green,
and singly ionized oxygen, blue.
Credit: J. Morse (STScI) and NASA.

Nucleosynthesis sites
Detailed numerical models of nucleosynthesis
have served to identify promising sites for the
various processes. In the pregalactic
nucleosynthesis era, the big bang created the light
nuclei 1H, 2H, 3He, 4He, and 7Li. Reactions
between nuclei in the interstellar gas and highenergy cosmic rays also form these elements, as
may interactions of neutrinos with heavier nuclei
in supernovae. Some of the heavier elements (at a
concentration of 10-4 that of the sun) may have been formed prior to galactic formation. The
synthesis of the bulk of the heavy nuclei present in galactic matter, however, is generally
attributed to processes in stellar and supernova environments, following the formation of
galaxies. Helium burning stars with Msun < M < 10 Msun, are the predominant source of
carbon, nitrogen, and roughly half the nuclei heavier than iron. Massive stars (M > 10 Msun)
and the associated supernovae (SNe II) produce most of the nuclear species from oxygen
through zinc and a significant fraction of the heavier nuclei. Nuclei up to calcium are made
primarily in the stable burning presupernova phases, while the iron-peak nuclei are products
of explosive silicon burning. Type Ia supernovae produce a substantial fraction of the ironpeak nuclei and some intermediate mass isotopes in explosive carbon, oxygen, and silicon
burning processes.

13

Observational constraints on nucleosynthesis
•= Big bang synthesis
of the light nuclei
1
H, 2H, 3He, 4He,
and 7Li is
constrained by the
abundances of
these isotopes
observed in our
galaxy and other
galaxies, and in
distant gas clouds.
•= Cosmic ray
spallation
synthesis of the
light isotopes 6Li,
7
Li, 9Be, 10B, and
11
B is constrained
by recent studies
of lithium,
beryllium, and
boron abundances
in old metal poor
stars in the
galactic halo.
•=

Map, made by the COMPTEL telescope on the Compton Gamma Ray
Observatory, of 1.809 MeV gamma rays from radioactive 26Al. Since 26Al has
a lifetime of about a million years, the sky image shows the spatial distribution
of 26Al producing events integrated over the past million years. Because the
galaxy is transparent to gamma rays, the image shows the location of
nucleosynthesis throughout the galaxy. Events that produce 26Al appear to lie
in our galaxy, with concentrations at the inner galaxy, and near the Cygnus
and Vela regions.
Credit: NASA, Compton Gamma Ray Observatory, COMPTEL
Collaboration. Courtesy of R. Diehl, MPE Garching.

Nucleosynthesis via the CNO, NeNa, and MgAl burning cycles is reflected in
abundances in globular cluster giants. For example, significant depletions of oxygen are
anticorrelated with the abundances of nitrogen, sodium, and aluminum. These
observations bear on the nucleosynthetic and convective processes in massive stars.

•= Successive phases of carbon, neon, and oxygen burning, associated with massive star and
Type II supernova environments, are responsible for the enhancements of the α-elements
O, Ne, Mg… compared to the Fe observed in old stars located in the halo of our galaxy.
This overproduction of α-elements in Type II supernovae must ultimately be
compensated for by other processes, presumably with material rich in Fe from Type Ia
supernovae.
•= Nucleosynthesis processes in novae are constrained by the abundances of many elements
with masses A < 35, found in nova ejecta (see Section VIII).
•= Recent studies of extremely metal-deficient halo stars find r-process abundances in the
mass range from barium through the actinides that are closely proportional to solar
system r-process abundances. Data on some of the same stars show that r-process nuclei
with A < 130 are not produced in solar proportions relative to the heavy r-process
14

elements. This could mean that r-process production of light nuclei is sensitive to the
details of a specific site, or it may be evidence that multiple r-process sites are
contributing.
•= Nuclei in the iron-peak region are probably the products of explosive nucleosynthesis in
core-collapse and Type Ia supernovae. Observations of gamma rays from Supernova
1987A revealed that approximately 0.075 Msun of 56Ni was ejected with an isotopic ratio
57
Ni/56Ni that is approximately 1.5 times that seen in the solar system. Gamma rays from
44
Ti have been detected from the remnant of the Cas A supernova. These results can be
used to determine how much material was ejected into the interstellar medium (compared
to that accreted on the supernova remnant) by a supernova explosion, as well as the
conditions deep inside the exploding star.
•= A variety of sources can produce the 26Al in the galaxy. A map of the distribution of
gamma rays from 26Al is shown in the figure in this section. When more information is
available for the distribution of 60Fe gamma rays, a comparison of these distributions
should strongly constrain the nature of the sources of 26Al.
•= Presolar grains, tiny (1.6 nm to 20 µm) inclusions found in meteorites, yield isotopic
abundances corresponding to individual astrophysical events, some apparently from sprocess sites and planetary nebulae, and others from supernovae. These abundances serve
to test models of s-process nucleosynthesis in shell-helium burning (AGB) stars and of
explosive burning in supernova shock fronts.
These great advances in observation draw attention to the fact that we have far to go to reach
a real understanding of the processes involved. Many details will be given in the remainder
of this document. For example: we do not know the sources of the cosmic rays, we do not
understand the origin of Type Ia supernovae, we cannot consistently calculate a SNe II
explosion, we do not know where the r-process takes place, and we do not yet have a
convincing explanation of why we observe fewer neutrinos than expected from our sun.

15

Critical density for

The big bang
Our universe began about 15 billion
years ago in the big bang, a hot, dense
expanding fireball. Early in the
expansion, the effects of quantum
gravity, grand unification of forces, and
inflation were the most relevant; it is in
this era that the dominance of baryons
over anti-baryons was first established.
There is a general understanding of what
occurred at these early times, but many
issues are still unsettled. After about a
microsecond, nucleons and other hadrons
condensed from a soup of quarks and
gluons during the quark/gluon to hadron
phase transition. After about 10 seconds
black body photon energies were small
enough, compared to the binding
energies of the lightest nuclei, that
synthesis of the light elements began.
This era is the domain of nuclear physics
and astrophysics. Research at the
Relativistic Heavy Ion Collider (RHIC)
may help establish the nature of the
phase transition leading to hadrons.
Studies of the reactions that create the
elements have led to an accurate
determination of the average amount of
normal matter (baryons) in the
universe.

H0 = 65 km/s/Mpc

V. NON-STELLAR EVENTS

Summary of big-bang production of the light elements;
predicted abundance is plotted against the mean
density of baryons. The widths of the curves indicate
the theoretical uncertainties, and the vertical yellow
band is the consistency interval in density where the
predicted abundances of all four light elements agree
with their measured primeval abundances. The blue
band in the consistency interval corresponds to a
recent determination of the primeval deuterium
abundance. The consistent value is larger than the
amount of luminous matter seen in stars and galaxies
and smaller than the total value of the mass seen in
galactic clusters. This indicates that there must be
both baryonic and non-baryonic dark matter in the
universe.

This result follows from a comparison
of the number of nuclei synthesized in
the big bang, as a function of the
average baryon density parameter Ωb, to
their observed primordial values.
Adapted from D.N. Schramm and M. Turner. Reviews
Determination of the primordial
of Modern Physics 70, 303 (1998). Figure courtesy of
abundances requires both careful
K. Nollett,
observation and an understanding of
how galactic evolution might have altered the observed abundances. It is a triumph of
modern cosmology that the predictions for 2H, 3,4He and 7Li, which range over nine orders of
magnitude, agree with observation at nearly the same value of Ωb. This serves as evidence
for the big-bang paradigm and provides a measure of the universal density of baryons. The
latest estimates give a value Ωb ≈ 0.05, where Ω =1 is the highest density that allows the

16

universe to expand forever. Important conclusions follow from combining this result with
other evidence: that there are both normal (baryonic) and exotic (non-baryonic) types of dark
matter, and that the number of light neutrino species is less than 3.2. The big-bang limit on
neutrino species applies to neutrinos with a mass less than about 1 MeV and hence provides
somewhat different information than the value based on the shape of the Z0 resonance of
particle physics.
We are now entering an era in which more precise predictions of big-bang nucleosynthesis
will be needed, presenting a real challenge to nuclear astrophysicists. Determinations of Ωb
depend most critically on the measured deuterium abundance. Better deuterium
measurements will be made, and the uncertainties in the nuclear reaction rates leading to
deuterium will then dominate the uncertainty in Ωb. An independent estimate of Ωb will soon
be available from measurements of the cosmic microwave background; comparison with an
accurate value from big-bang nucleosynthesis will be a powerful consistency check on the
big-bang model. It may then be possible to determine the universal density from the
deuterium abundance observations alone and to predict the big-bang production of 3,4He and
7
Li. These abundances can then be used to test models of stellar evolution. For example,
following this approach for 7Li, the current big-bang model predicts an abundance larger than
observed. This result can be compared with the predictions of Li depletion on the surfaces of
stars to provide a unique test of stellar models.
Nucleosynthesis in the big bang involves sequences of nuclear reactions among all the light
nuclei. More accurate cross sections are needed for a number of these reactions to improve
the accuracy of predicted element production in the big bang. For deuterium synthesis the
required cross sections are (in order of importance): d(d,n)3He, d(p,γ)3He, d(d,p)3H and
p(n,γ)d. For 7Li synthesis important reactions (in addition to those for deuterium) are:
3
He(4He,γ)7Be, 7Li(p,n)7Be and 3He(d, p)4He. The temperatures involved are sufficiently
high that cross section measurements are usually performed over the entire relevant energy
range, which varies from tens of keV to hundreds of keV. Such energies are available at a
variety of modern experimental facilities.
It is an open question as to whether a signature of the quark/gluon to hadron phase transition
can be found in the light-element abundances. If the transition is sufficiently sharp (first
order), density inhomogeneities may develop and change the nature of the nucleosynthesis.
Calculations indicate that the constraint of fitting the observed abundances does not allow
average baryon densities greatly different than those of the standard big bang. However,
nuclei that are not made appreciably in the standard big bang − beryllium and boron, for
example − may be produced, providing observational signatures for inhomogeneities in the
early universe. Rates for reactions involving short-lived nuclei, of which the most important
involve 8Li, are required to address this open question. For example, the 8Li(α,n)11B reaction
is thought to lie on the dominant pathway for production of all nuclei heavier than 11B.
Further studies, both experimental and theoretical, are required to determine the reactions
that are the most important for production of heavier nuclides, and the clearest observational
signatures of possible inhomogeneities.

17

Galactic cosmic rays
An enigmatic population of relativistic ions and electrons, the Galactic Cosmic Rays (GCRs),
fills interstellar space in our galaxy. Models of GCR propagation based upon known breakup
cross sections, and the fraction of radioactive products that survive, can be used to determine
the average age of the GCRs, as well as the average density and total amount of material they
pass through during their lifetime in the galaxy. The cosmic rays also contribute to the
synthesis of the light elements, especially Li, Be and B, via breakup (spallation) reactions on
4
He, C, N and O nuclei in the interstellar medium. Other mechanisms also produce these
nuclei: 7Li is made in the big bang, and 7Li and 11B are potentially made by the neutrinoprocess in which neutrinos break up 4He and 12C in the helium and carbon shells of
supernovae. The breakup products are processed by nuclear reactions in the hot environment,
and the resulting 7Li and 11B are eventually ejected into the interstellar medium. It is
important to understand the contributions of these various mechanisms to determine whether
we have a consistent picture of the process. This is not yet possible and uncertainties in the
cross sections for production in the cosmic rays contribute to the problem. Breakup cross
sections for C, N and O on hydrogen and helium are reasonably well known, but there are
few measurements for important two-step processes, such as the breakup of C to form B,
which then breaks up to form Be or Li.
The origin of the cosmic rays has remained obscure despite decades of research. Recent highprecision measurements of GCR composition on the Ulysses, ACE, and Mir spacecraft have
apparently ruled out two possible sources: fresh supernova ejecta and solar-like sources with
preferential acceleration of easily ionized elements. Expected measurements from next
generation detectors (including the Advanced Cosmic-ray Composition Experiment for
Space Station (ACCESS), and the Extremely Heavy Cosmic-ray Composition Observer
(ECCO)), promise to explore the limits of the supernova shock acceleration mechanism, to
measure the GCR abundances of all individual elements in the periodic table, and to provide
an independent GCR age using actinide clocks. The source of the highest energy cosmic rays,
with energies in excess of 1020 eV is also uncertain−it is not clear whether they originate in
the galaxy.
Accurate spallation cross sections are required to determine the elemental and isotopic
abundances of the cosmic ray source from cosmic ray abundance observations, and to
measure energy-dependent flight distances and flight times (using radioactive secondary
nuclei as clocks) of the cosmic rays. Our understanding of these aspects of the cosmic ray
composition is presently limited by the inaccuracies of the cross sections that affect them and
in some cases by the lifetimes of the clock nuclei. The most important measurements are for
hydrogen targets and include spallation cross sections leading to individual isotopes for 56Fe,
60
Ni, and 28Si at bombarding energies between about 200 and 2000 MeV/nucleon; spallation
cross sections to individual elements for Sr, Ba, and Pb between 1 and 10 GeV/nucleon; and
elemental spallation and fission cross sections for uranium at several GeV/nucleon.

18

VI. NUCLEOSYNTHESIS AND STELLAR EVOLUTION
Stellar evolution is characterized by a
series of long-lived stable periods
separated by phases of rapid core
contraction. The stable phases are
brought about by quiescent nuclear
burning deep in the stellar interior,
which produces sufficient energy to
stabilize the star against gravitational
collapse. The duration of the stable
periods and the associated element
synthesis and energy generation depend
upon the rates of the active nuclear
reactions. After the nuclear fuel is
depleted, the core contracts, and is
heated by the release of gravitational
energy until it reaches the temperature
and density necessary to trigger a
burning phase fueled by the ashes of the
preceding phase. During these
contraction phases, nuclear burning
develops in thin shells surrounding the
contracting core. Steep temperature
gradients result, and can cause rapid
convection that dredges up freshly
synthesized material from the burning
zones into the stellar atmosphere where
it can be observed.

A star forming region DEM192 in the Large Magellanic
Cloud. After a star is born, it may develop a strong wind
which pushes away nearby gas; it may be so hot that the
emitted light boils away nearby dust and gas, or it may
be so massive that it evolves quickly to form a
supernova and catapults its elements back to the
interstellar medium.
Credit: C. Smith (U. Michigan), Curtis Schmidt
Telescope, CTIO, Chile.

Although these complex processes
appear to be understood in broad outline, there are severe and often unacknowledged
problems, due in part to the lack of reliable experimental reaction rates. Only one reaction,
3
He(3He,2p)4He, has been studied at the energies important for non-explosive stellar burning.
All other rates are based on extrapolations of experimental laboratory data down to stellar
energies. The uncertainties of these extrapolations may span an order of magnitude or more
in some cases, with major consequences for nucleosynthesis and energy generation. For
example, recent observations of 7Li, 23Na, and 27Al in the stellar atmospheres of red giant
stars have proven to be difficult to explain, and the isotopic abundances found in meteoritic
inclusions, condensed in the wind of shell-helium burning (AGB) stars and planetary
nebulae, don’t match the anticipated ratios. Such open questions require continued
experimental and theoretical effort.
Stellar-core burning
During hydrogen burning in the so-called main sequence stars, four hydrogen nuclei fuse to
form helium in the stellar core. For stars of less than 1.5 solar masses the fusion process is

19

dominated by the pp-chains. The slowest reaction, p + p fusion to form deuterium,
determines the lifetime of the hydrogen burning phase. This reaction is mediated by the weak
interaction, and its rate is based only on theoretical calculations – it is too small to be
determined experimentally. Other processes of interest are the 3He(α,γ)7Be and 7Be(p,γ)8B
capture reactions. A detailed knowledge of these rates is necessary to predict the solar
neutrino flux and to interpret results from solar neutrino detectors (see Section XI).
For more massive stars hydrogen burning proceeds by the various CNO cycles, which
convert hydrogen to helium, and carbon and oxygen to nitrogen in the stellar core. The rate
of energy generation in the CNO cycle is determined by the slow 14N(p, γ)15O reaction,
which has sizeable uncertainties. This uncertainty results in significant uncertainties in the
ages obtained for old globular clusters. Convective mixing remains a difficult problem for
stellar models. In addition to the carbon isotopes, the abundances of the oxygen isotopes
appear to be sensitive functions of mixing. The relevant reactions, particularly those
involving 17O, require re-examination.
A period of core helium burning follows, and converts helium to 12C and 16O. Its lifetime
depends on the triple-alpha reaction, the sequential fusion of three 4He particles to form 12C.
Because He-burning reactions are typically harder to measure than H-burning reactions (due
to the larger Coulomb-barrier and higher background levels), some basic questions
concerning He-burning remain unanswered. The essential features of the triple alpha reaction
are understood: the rate is based on the level parameters of the second excited 0+ state in 12C.
Other effects, often not taken into account are tail-contributions of the resonance,
interference effects with other 0+ levels, and contributions from higher energy resonances.
Such effects may become important for determining X-ray burst ignition or for modeling
Type Ia supernovae (see Section VIII). The 12C(α,γ)16O reaction helps to determine the mass
of the core following He-burning, and the C/O ratio which greatly influences the future
evolution of the star. For these reasons this reaction is of central importance to stellar
evolution. After years of concerted effort, the rate for this reaction is still not known with
anywhere near the required accuracy of about 20%. Similar arguments and difficulties exist
for the subsequent 16O(α,γ)20Ne reaction where the rate is also not well known in the stellar
energy range.
Helium burning is also responsible for producing the neutrons that synthesize heavy elements
in the s-process. The rates of the relevant reactions, 13C(α,n)16O and 22Ne(α,n)25Mg, and the
associated reactions 14N(α,γ)18F and 18O(α,γ)22Ne are based only on measurements well
above the stellar energy range and therefore require further study. The uncertainties in these
rates are at least three orders of magnitude at stellar temperatures, with possibly significant
consequences for our present interpretation of s-process nucleosynthesis within the
framework of stellar models (see Section IX).
The subsequent heavy-ion burning phases depend on the nucleosynthesis of 12C and 16O
during the He-burning phase. Both the ensuing fusion reactions themselves, 12C+12C,
12
C+16O, and 16O+16O, and capture of protons and alpha particles by the fusion products are
important. Neither the fusion processes, nor the subsequent proton and α capture reactions
are sufficiently well known for reliable modeling of the later phases of stellar evolution.
20

In the burning stages that follow oxygen burning, nuclei heavier than calcium are synthesized
in the presupernova phase of the star. Nucleosynthesis increasingly occurs in a state of full
or partial nuclear statistical equilibrium in which nuclear binding energies and partition
functions mainly determine the synthesized abundances. So long as the “freeze-out” is
sufficiently rapid, individual rates are not very important. This is not to say that rates on
particular species in the Fe group can be ignored. When a system cools from statistical
equilibrium, it falls out of equilibrium at a temperature of roughly three billion kelvins. After
this point, the species produced will again be sensitive to individual reaction rates. Rates for
electron capture also begin to become important during the equilibrium phase.
Stellar shell burning − Red Giant and AGB stars
Hydrogen shell burning around an inert helium core powers the initial Red-Giant phase of
stellar evolution just before the onset of core-helium-burning. In general, nucleosynthesis in
hydrogen and helium burning shells takes place at higher temperatures but lower densities
than the corresponding core burning. As a result, higher Z nuclei are involved in the process.
Besides the CNO cycles, reactions involving the Ne-Na and the MgAl cycles can take place
in hydrogen burning shells. Strong convective mixing processes bring the freshly synthesized
material to the stellar surface. There it is directly accessible to observation and offers one of
the few opportunities to test nucleosynthesis models directly.
Rapid convective processes are of particular importance in the intermittent shell burning
cycles of core-carbon-burning stars in their asymptotic giant branch phase (AGB stars). In
stars with M < 4 Msun, helium flash induced pulsations lead to strong envelope mixing. Fresh
fuel is transported to the burning zones while the synthesized products are dredged up from
the burning zones to the outer atmospheres of the star and substantially modify the surface
composition. Radiation driven winds eject the material into interstellar space, triggering the
formation of planetary nebulae. The helium burning shell is the site for the main component
of the s-process, thought to be driven by the 13C(α,n)16O and 22Ne(α, n)25Mg reactions (see
Section IX).
There are considerable discrepancies, owing partly to the treatment of convective processes
in one-dimensional models and partly to the lack of reliable experimental reaction rates for
proton and alpha capture in the Ne to Si range. Until these rates are better known, it will be
difficult to isolate and resolve the astrophysical uncertainties.

21

VII. SUPERNOVAE
Supernovae are the most spectacular
objects in the cosmos. A star brightens
by many orders of magnitude, sometimes
becomes visible to the naked eye during
daylight, and leaves behind striking
remnants such as the Crab Nebula. There
are two principal types of supernovae.
Core-collapse supernovae, for example,
Types Ib, Ic, and II supernovae, result
from the collapse of the iron core of a
massive star at the end of its life. Type Ia
supernovae are probably triggered either
by the merger of two white-dwarf stars or
by fast accretion onto the surface of a
single white dwarf. Supernova
explosions are responsible for the
synthesis of the majority of the elements.
In addition, the assumption that Type Ia
supernovae are standard candles, with a
determinable intrinsic brightness, lies
behind recent evidence that the
expansion rate of the universe is
increasing. The following discussion
concentrates on core collapse
supernovae. However, it is also of great
importance to understand the Type Ia
mechanism better, and to determine the
reliability of the standard-candle
assumption.

Hubble Space Telescope image of Supernova 1987a. In
February 1987, light reached Earth from a star which
exploded in the nearby Large Magellanic Cloud.
Supernova 1987a remains the closest supernova since
the invention of the telescope. The explosion
catapulted a tremendous amount of gas, light, and
neutrinos into interstellar space. When observed by the
Hubble Space Telescope (HST) in 1994, large rings
were discovered whose origin is still mysterious. More
recent HST observations shown in the inset have
uncovered the expanding fireball from the exploding
star. These high resolution images resolve two blobs
flung out from the central explosion.
Credit: C. S. J. Pun (GSFC) & R. Kirshner (CfA),

One of the most important and
WFPC2, HST, NASA.
challenging problems in nuclear
astrophysics is to understand the explosion mechanism of core collapse supernovae and the
associated element synthesis. Core collapse supernovae are extraordinary events, releasing
1053 erg of energy in the form of neutrinos of all types at the staggering rate of 1057 neutrinos
per second (see Section XI), and generating the conditions for the synthesis of many new
nuclei. The explosion ejects these nuclei into the interstellar medium, where they can be
incorporated into new stellar systems like our own, forming the basis for life itself. Left in
the wake of the explosion is a relativistic object, a black hole or a neutron star that may
contain new forms of hadronic matter. As a result, a supernova is a unique cosmic
environment for studies of nucleosynthesis; neutrino properties; and nuclear matter at
extremes of density, temperature, and neutron to proton ratio.

22

The core collapse mechanism
In our current picture of the supernova process, an outgoing shock wave − formed when the
iron-like core of a massive fully evolved star collapses gravitationally and rebounds at high
(supranuclear) densities − stalls as a result of energy losses due to nuclear dissociation and
neutrino emission. A few seconds later, the shock is re-energized by the intense neutrino flux
emerging from the protoneutron-star at the stellar center. The shock induces explosive
nucleosynthesis in the outer layer, leading to the observed explosion. The observation of
neutrinos from SN1987A gives us some confidence that this picture is at least qualitatively
correct. Yet this attractive idea has not borne fruit−supernovae simulations have not
consistently resulted in explosions. It is not clear where the fault lies: in inaccurate
knowledge of the nuclear structure physics involved and of the nuclear equation of state, or
in the manner by which energy is transported by neutrinos, or in the absence of
multidimensional calculations that can fully take into account the relevant microphysics.
To remedy this situation, computer simulations involving accurate multidimensional,
neutrino-energy-dependent radiation transport and radiation hydrodynamics must be
supported by commensurate improvements in the microphysics. This will require the
integration of state of the art supernova simulation and nuclear structure computation to
model both the explosion mechanism and supernova nucleosynthesis. Only a consistent
treatment of the explosion and its nucleosynthesis will allow the use of observational
constraints such as isotopic abundances, gamma, and neutrino fluxes, to obtain information
on the explosion mechanism. Challenges include modeling lepton number losses during the
infall stage, the stellar core equation of state, post-core-bounce neutrino heating, and the
nucleosynthesis that takes place in the ejecta.
Toward a solution
There is great promise for carrying out such calculations because of imminent advances in
our knowledge of nuclear properties and the other microphysics required for accurate
theoretical modeling, coupled with rapidly growing computing power and new observations
that will constrain the theoretical results. For example:
•= Radioactive and stable ion beams from new and upgraded facilities will provide
information on nuclear structure and reactions that affect all stages of the explosion,
from the evolution of the presupernova star, to reactions in the ejecta resulting from
the explosion. They will also provide benchmarks to validate the theoretical
calculations that in the end must provide the many hundreds of nuclear properties
needed to describe the explosion.
•= Observatories such as the Keck telescopes, the Hubble Space Telescope, the Compton
Gamma-ray Observatory, and their successors, will provide a flood of new data at
many wavelengths on the composition and morphology of supernova ejecta.
Measurements of heavy elements in extremely metal-poor stars and of the time
evolution of Li, Be, and B abundances will directly test the r-process, the ν-process,
and other nucleosynthesis mechanisms that supernova simulations attempt to model.
Individual grains of supernova material are being harvested from meteorites. These

23

observations will provide a signature of the explosion mechanism, the associated
nucleosynthesis, and the subsequent cooling of the ejecta.
•= Neutrino detectors such as Super-Kamiokande and the Sudbury Neutrino Observatory
promise thousands of neutrino events from the next galactic supernova and will
provide detailed neutrino ``light curves'' for comparison with supernova models. An
advanced detector may also provide discrimination among different neutrino types.
KARMEN, LSND, and other low-energy neutrino experiments have demonstrated the
feasibility of measuring inclusive and exclusive neutrino-nucleus cross sections that
will help calibrate these advanced detectors.
•= Gravitational wave observatories such as LIGO and VIRGO will bring additional and
complementary information from deep within the explosion, allowing us to learn
about asymmetry in the stellar core collapse and the different modes of supernova
convection thought to play a role in the explosion mechanism.
•= Neutrino oscillations may play a central role in supernova evolution. A fundamental
understanding of the explosion mechanism coupled with detectors that measure the
fluxes of different neutrino species from the next galactic supernova will allow us to
extract information on neutrino physics that is not otherwise accessible.
•= The advent of large-scale computing resources will make it possible to tackle
multidimensional simulations with realistic neutrino transport.
Supernovae of Type Ia
Supernovae of Type Ia have assumed an important role in cosmological studies. Their great
brightness, and the apparent ability to calibrate their luminosity phenomenologically, makes
them useful for measurements of cosmological distance and expansion rates. Unfortunately,
we lack a full understanding of the explosion mechanism, which casts some doubt on their
calibration. Several mechanisms have been proposed, but a comparison of predicted and
observed spectra points towards thermonuclear explosions on accreting white dwarfs with
high accretion rates. The accreted hydrogen is rapidly converted to helium and subsequently,
by He burning, to carbon and oxygen which accumulates on the surface of the C/O white
dwarf. If the growing mass of the white dwarf exceeds the Chandrasekhar mass of about 1.4
Msun, contraction sets in, carbon ignites by fusion reactions with screening enhancements, and
a thermonuclear runaway starts at the center. Expansion and convective instabilities lead to
burning-front propagation via heat convection, which accelerates, presumably to supersonic
speed, and finally turns into a detonation, causing a complete disruption with no remnant left
behind.
Required nuclear physics information
Type Ia and core-collapse supernovae share a strong dependence on the rate, r12α, of the 12C +
α→16O + γ reaction. This rate determines the C/O ratio of a white dwarf, and hence the peak
luminosity obtained from the runaway thermonuclear burning that powers Type Ia
supernovae. Whether Type Ia supernovae can serve as standard candles may, therefore,
depend on r12α. In the case of core-collapse supernovae, the size of the iron-like core (and the
24

energy dissipated by the outgoing shock wave in nuclear disintegration) depends on the
predominance of C or O in presupernova evolution and hence on r12α.==As shown in Section
XIII, a variety of facilities can contribute to the study of this important reaction.
The rates of weak interactions such as electron capture and beta-decay in hot dense
environments are also important in both cases. These processes affect the electron pressure
and the neutron to proton ratio of the precollapse core. In the case of core collapse
supernovae, estimates indicate that successive electron capture and beta-decay, together with
the associated neutrino emission, might lower the temperature of the iron-like core by as
much as 10%. High-energy fragmentation facilities with beam energies greater than about
120 MeV/nucleon, can provide experimental information on weak interaction rates for the
most important radioactive participants (e.g. 56Ni) through studies of hadronic charge
exchange with beams of radioactive nuclei bombarding light targets. These are similar to
earlier (p,n) reaction studies, but done in inverse kinematics so as to study radioactive nuclei.
For core-collapse supernovae, two further inputs are needed. Required information on
inelastic neutrino-nucleus interactions for core collapse and protoneutron-star cooling could
be provided by studying inelastic scattering of 100-200 MeV/nucleon beams of hydrogen or
6
Li from stable nuclei of interest. One must choose conditions that mimic the neutrinoinduced processes: small momentum transfer and quantum number selection of axial
processes. For unstable nuclei, one would bombard hydrogen or 6Li targets (the quantum
numbers of 6Li states can be chosen to select only axial transitions) with radioactive beams.
Information on the nuclear equation of state from sub- to supranuclear densities could be
provided by studies of nucleus-nucleus reactions in the 100-500 MeV/nucleon range, using
radioactive beams to adjust the neutron to proton ratio of the reactants. Future detection of
supernova neutrinos may provide unique information on neutrino interactions in hot dense
matter. For the description Type Ia supernovae, an accurate description of the 12C + 12C
reactions that initiate the process leading to detonation is required.
For supernova nucleosynthesis, experimental and theoretical results are needed for properties
of nuclei on and near the r-process path, for neutrino-nucleus interactions relevant to the
neutrino-and the r-process (see Section IX), and for the rates for explosive nucleosynthesis in
supernova ejecta. Information on the neutrino-nucleus reaction can be obtained from
hadronic charge exchange and inelastic scattering as described in the previous paragraph or,
in the future for stable nuclei, by direct measurement of neutrino induced processes.
Shock-induced explosive processes in supernova ejecta often occur in statistical equilibrium
where the resulting composition depends mainly on the nuclear masses and stellar variables.
But there are also cases where equilibrium is not achieved and nuclear reaction rates at high
temperatures must be known, in most cases for unstable nuclei. The inner stellar zones are
dominated by explosive Si-burning, which synthesizes proton-rich nuclei such as 56,57Ni and
44
Ti. Direct observation of these radioisotopes in supernova remnants, via their gamma rays,
yields information about the thermodynamic conditions of the innermost ejected material and
provides an important test of supernova model predictions. The reactions leading to these
nuclei are of particular importance. Shock induced O burning produces mainly 40Ca, 36Ar,
32
S, and 28Si in statistical equilibrium, while explosive Ne-burning mainly synthesizes 16O,

25

24

Mg, and 28Si. Explosive processes in O and Ne layers are also likely sites for the p-process,
which is discussed in Section IX. Depending on the amount of C, He, and H fuel, the shock
front can also induce explosive C, He, and H burning in the outer layers of the star. However,
due to the low densities only minor modifications in the abundance distribution are expected.
For many of these reaction and structure needs, theoretical calculations will provide the bulk
of the information; the experimental studies will be limited to the more important cases, and
serve to validate and refine the theoretical approaches. Much progress has been made in the
area of large-basis shell-model calculations, which will continue to expand the range of
applicability of this more complete method. Quantum Monte Carlo methods allow the full fpshell to be taken into account for the mass region A = 40-60. Such calculations provide
detailed wave functions from which the masses of nuclei far from stability, Gamow-Teller
strength functions for electron capture, beta-decay and neutrino interactions, spectroscopic
factors, level densities and other quantities important for astrophysical calculations can be
obtained. The fp-shell calculations have been used to calculate the electron-capture rates that
play a central role in stellar collapse and supernova formation.

26

VIII. CATACLYSMIC BINARY STARS

Thermonuclear explosions in accreting binary star systems − novae, X-ray bursts and Type Ia
supernovae − produce the most common explosive astrophysical events. At the conceptual
level, the nature of the explosion mechanism seems reasonably well understood, but there are
considerable discrepancies between the predicted observables and observations. An
understanding of the time evolution of energy generation and nucleosynthesis, and of the
nature of mixing and convective processes, is necessary to explain the observed luminosities
and the abundance distribution in the
ejected material. The proposed
mechanism involves binary systems
with one (or two) degenerate objects,
such as white dwarfs or neutron stars,
and is characterized by the revival of a
dormant object via mass flow from the
binary companion. The observed
differences in the luminosity, time
scale, and periodicity depend on the
accretion rate and on the nature of the
accreting object. These events involve
nuclear processes at extreme
Artist’s drawing of a nova before the nova explosion.
temperatures and densities and
Our Sun is unusual in that it is alone - most stars occur in
synthesize a number of the important
binary systems. In a binary system, the higher mass star
isotopes that make up our world.
will evolve faster and will eventually become a compact
object - either a white dwarf star, a neutron star, or a

Low accretion rates lead to a pileup of
black hole. When the lower mass star later evolves into
an expansion phase, it may be so close to the compact
unburned hydrogen, and the ignition of
star that its outer atmosphere actually falls onto the
hydrogen burning via pp-chains in an
compact star. Gas in this accretion disk heats up, and
environment supported by electron
eventually falls onto the compact star. Steady or
degeneracy pressure. Once a critical
explosive burning of the accreted gas may occur leading
mass layer is attained there are large
to novae explosions, supernovae of Type Ia, X-ray
bursters, or X-ray pulsars. In the case shown, gas is
enhancements in the rate of the
accreting onto a white-dwarf star.
reactions. On white dwarfs this triggers
nova events, and on neutron stars it
Credit: S. Shore
results in X-ray bursts. High accretion
rates cause high temperatures in the accreted envelope and less degenerate conditions, and
usually result in stable H-burning or only weak flashes. High accretion rates on white dwarfs
may cause Type Ia supernovae (see Section VII); high accretion rates on neutron stars may
explain X-ray pulsars.
Nova explosions
Novae are well described as thermonuclear runaways triggered by ignition of the slowly
accreted hydrogen rich envelope via the pp-chains in degenerate electron conditions where
the pressure is almost independent of temperature. This leads to a runaway situation in which
the temperature rises and energy generation rates increase without expansion. During the
thermonuclear runaway sufficient energy is released to raise the temperatures to 200 - 400

27

million kelvins before the degeneracy is lifted. The actual ignition temperature for novae is
well below these peak temperatures. Rates of many nuclear reactions at energies below a few
hundred keV are needed for a complete description of the ignition process in the various
accreted layers of material. The main energy
generation in novae, however, comes from
the hot CNO cycle and is limited by the betadecay rates of 14O and 15O. A theoretical
description of the accompanying
nucleosynthesis requires detailed knowledge
of the rates of proton capture processes on
short-lived radioactive nuclei in the CNO
mass range. Detailed observations of the
abundance distributions in nova ejecta
confirm the general picture, but there are
significant discrepancies that need to be
addressed.
Elemental abundance variations found in
"knots" of material ejected from nova
explosions may be the key for understanding
convective and mixing processes during the
nova explosion. Observations of overabundances in the Ne to S mass range, in the
so-called Ne-novae, probably result from
thermonuclear runaways on massive
accreting O-Ne-Mg white dwarfs with mixing
of oxygen, neon and magnesium into the
accreting envelope. The nature of this mixing
process is not understood. Extensive studies
of proton induced nuclear reactions for both
stable and radioactive nuclei in the Ne to Na
and S to P mass ranges are necessary to
understand this phenomenon.

Hubble Space Telescope image of the recurrent
nova T Pyxidis, 6,000 light-years away in the
dim southern constellation Pyxis. The image
reveals that the ejected material does not lie in
smooth shells, but in more than 2,000 gaseous
blobs or knots packed into an area that is 1
light-year across. The blobs may have been
produced by the nova explosion, the subsequent
expansion of gaseous debris, or collisions
between fast-moving and slow-moving gas from
several eruptions.
Credits: Mike Shara, Bob Williams, and David
Zurek (Space Telescope Science Institute);
Roberto Gilmozzi (European Southern
Observatory); Dina Prialnik (Tel Aviv
University); and NASA.

Observations of the gamma radioactivity in novae, especially from the radioisotopes 22Na
and 26Al, will continue to play an important role in constraining the Ne-nova models−the
predicted intensity is larger than the observed limit on the intensity of the 22Na gamma ray.
Studies of the nuclear reactions involved in the synthesis of these radioisotopes, such as
21
Na(p,γ) and 22Mg(p, γ), are therefore particularly important. It appears that peak
temperatures in nova explosions may be much higher than (the currently accepted) 400
million kelvins. At such high temperatures, reactions can break out from the hot CNO cycle
for some novae – there is evidence for breakout in one recent nova observation.
Measurements of nuclear reactions on proton-rich unstable isotopes − including some above
mass 40, the traditional endpoint of nova nucleosynthesis studies – are required to understand
this higher-temperature nuclear burning.

28

X-ray bursts
X-ray bursts are thought to result from thermonuclear runaways in the hydrogen rich
envelope of an accreting neutron star. Low accretion rates favor a sudden local ignition of the
material with a subsequent rapid spread over the neutron star surface. Ignition of the triplealpha reaction and breakout reactions from the hot CNO cycles trigger a thermonuclear
runaway driven by the αp- and the rp-processes. The αp-process is a sequence of (α,p) and
(p,γ)=reactions that convert the 14O and 18Ne ashes of the hot CNO cycles to isotopes in the
34
Ar to 38Ca range. The rp-process is a sequence of rapid proton captures leading to the
proton drip line, followed by beta-decays of drip line nuclei that convert material from the Ar
to Ca range into 56Ni. The runaway freezes out in thermal equilibrium at peak temperatures
of around 2.0 to 3.0 billion kelvins. Re-ignition takes place during the subsequent cooling
phase of the explosion via the rp-process beyond 56Ni. The nucleosynthesis in the cooling
phase of the burst considerably alters the abundance distribution in the atmosphere, the
ocean, and subsequently the crust of the neutron star. This may have a significant impact on
the thermal structure of the neutron star surface and on the evolution of oscillations (waves)
in the oceans.
Nuclear reaction and structure studies on the neutron deficient side of the valley of stability
are essential for an understanding of these processes. Measurements of the breakout reactions
will set stringent limits on the ignition conditions for the thermonuclear runaway;
measurements of alpha and proton capture on neutron deficient radioactive nuclei below 56Ni
will set limits on the time scale for the runaway itself and on the hydrogen to seed ratio for
the rp process beyond 56Ni. Nuclear structure and nuclear reaction measurements near the
double closed shell nucleus 56Ni determine the conditions for the re-ignition of the burst in its
cooling phase. Information beyond 56Ni, especially in the Ge to Kr mass regions, is needed
to determine the final fate of the neutron star crust. The nuclear structure information needed
to calculate the flow of nuclear reactions in X-ray bursts and the time dependence of the
energy generation, includes masses, beta-decay lifetimes (also needed for isomeric or
thermally populated excited states), level positions, and proton separation energies,
especially in the Ge to Kr mass region. The rates of two-proton capture reactions that may
bridge the drip line at waiting points are of special importance.
X-ray pulsars
X-ray pulsars are usually described as accreting neutron stars with high accretion rates. This
leads to steady burning of the accreted material via αp- and the rp-processes on the surface of
the neutron star. Detailed studies of the nucleosynthesis suggest that the accreted material is
rapidly converted to heavier elements in the mass 80 to 100 range. This drastically changes
the composition of the crust and the ocean of the neutron star; the original iron crust is
replaced by a mixture of significantly more massive elements. As a result, the composition of
the neutron star crust in a binary system is substantially different from that in an isolated
neutron star. This composition change may have important effects on the thermal and
electromagnetic conditions at the neutron star surface, and will affect the observed decay of
the magnetic field of neutron stars. It will also change the sequence of electron captures in
the deeper crust, which may affect the emission of gravitational radiation from the neutron
star surface.

29

The final composition of the crust depends strongly on the nuclear physics associated with
the rp-process and on the endpoint of the rp-process, which itself is directly correlated with
the accretion rate. For experimental confirmation in the lower mass range, studies similar to
those for the X-ray burst simulations are required. However, for large accretion rates the
endpoint of the rp-process is expected to lie in the mass 150 range. This requires a new range
of nuclear structure data near the limits of stability. Of particular interest are beta-decay
lifetimes, and especially beta-delayed proton and beta-delayed alpha decays. If these
processes dominate, as is expected for the decay of very neutron deficient tellurium, iodine,
and xenon isotopes, one reaches a natural halting point for the rp-process.
Black hole and neutron star accretion disks
Many of the accretion processes on neutron stars and black holes are mediated by accretion
disks. Nuclear processes that occur in this disk at low density and high temperature may alter
the abundance distribution of the accreted material, affecting nucleosynthesis and energy
release following ignition of the accreted material. One possible effect is spallation and
fragmentation of the high velocity accreting material during its interaction with the outer
atmosphere of the accreting object. Further theoretical study is needed to establish the
importance of these phenomena.

30

IX. SYNTHESIS OF THE HEAVY ELEMENTS
Almost all nuclei heavier than iron are made
by neutron capture on lighter seed nuclei in
the s- and r- processes. A few rare isotopes
are made by rather different mechanisms,
known collectively as the p-process. The
observed abundances of the elements and
their isotopes in our solar system suggest
that these processes take place at different
characteristic time scales, temperatures, and
neutron densities. Given nuclear physics
input of sufficient breadth and accuracy, the
synthesized abundances in this mass region
provide a fertile field for developing and
testing models of stars, stellar explosions,
and more exotic phenomena such as the
merger of two neutron stars.

Scanning electron microscope image of presolar
silicon carbide grains from the Murchison
meteorite. It is thought that these grains formed in
the atmosphere of a red giant star, survived the
formation of our solar system, and were
transported to earth intact inside of this meteorite;
hence, they preserve within them the signature of
the environment in which they were created. The
signature is the precisely measured relative
abundances of isotopes of zirconium and
molybdenum. Other grains or aggregates of grains
contain other heavy elements. Their abundances
can be compared to the signature calculated from
various models for red giant stars. Precise neutron
capture cross sections are required if such
comparisons are to be used for improving stellar
models.

The s-process
The solar-system abundances reveal that the
s-process takes place under conditions where
the time interval between neutron captures is
longer than the average lifetime for betadecay. As a result, the s-process proceeds
through nuclides near the valley of stability
and the resulting isotopic abundances are
typically inversely proportional to the
neutron capture reaction rates. The neutron
(Photo courtesy of Andrew Davis, University of
Chicago.)
capture rates on closed neutron shell
isotopes are small, resulting in an
enrichment of these isotopes and the characteristic s-process peaks in the solar-system
abundance distribution. Models of the s-process also help to unravel the complex origin of
the heavy nuclides−many of them are made by both the r- and s-processes. Because the sprocess is better understood, the r-process abundances (the most important constraint on rprocess models) are obtained from the measured solar abundances by subtracting the
calculated s-process contributions. An important reaction in this regard is the neutron
capture rate for 181Hf (n, γ); this rate determines whether there is a significant s-process
contribution to the abundance of 182Hf, which is nominally an r-process nucleus. An accurate
value of the r-process abundance of 182Hf may help to determine whether there are two types
of r-process (see Section III).
The observed s-process abundance distribution can be explained by two components, each
with a different origin. The main component dominates nucleosynthesis in the region
between Sr and Pb, and is associated with He-shell flashes in low mass (less than 4 Msun)
AGB stars (see Section VII). The weak s-process component is responsible for
31

nucleosynthesis up to the A = 90 range and is thought to take place in massive stars (10-30
Msun) during their He and C burning phases as discussed in Section VI. In addition to the
synthesis of these heavier elements, the s-process is responsible for modifying the isotopic
patterns of many lighter elements. The neutron sources are 13C(α, n)16O and 22Ne(α, n)
25
Mg for the main component and 22Ne(α, n)25Mg for the weak component. Isotope
production is directly proportional to the rates of these reactions. Unfortunately the rates
have considerable uncertainties in the stellar energy region. Given the complexity of the
nuclear structure involved, detailed measurements will be necessary to reduce these
uncertainties.
Research on the s-process has undergone a renaissance in recent years, thanks to new more
realistic stellar models, new extremely precise observational data, and improved
experimental nuclear physics techniques and data. Until recently, even rather schematic sprocess models reproduced the observed abundances rather well. However, as the precision
of both the observations and the neutron capture cross section data has improved, it has
become clear that more sophisticated models are needed. These new stellar s-process models
are beginning to provide details about the inner workings of stars in which the s-process
occurs and about related issues such as galactic chemical evolution. The lack of high quality
neutron-capture data is now limiting further progress in these areas.
One example of precise new observational data is the discovery in certain meteorites of
presolar grains. These grains of refractory material (e.g. SiC) apparently formed in a star in
which the s-process was occurring, and found their way to the region of the galaxy in which
our solar system was forming. Some of these grains survived the formation of our solar
system, and were transported to earth intact inside meteorites; they preserve within
themselves the signature of the environment in which they were created. This signature,
relative abundances of isotopes of neodymium and other elements, can be compared to the
abundances predicted by models for the s-process in AGB stars (see Section VI). Neutron
capture cross sections accurate at the 1% level are required if these comparisons are to be
useful for improving the stellar models.
One difference between the new stellar and older schematic models of the s- process is that
neutron capture during the main phase of the s-process is now thought to take place at much
lower temperatures (kT ~ 8 keV) than previously assumed (kT ~ 30 keV). This change in
temperature requires extending many of the previous neutron capture cross section
measurements to lower energies. Data of the requisite accuracy −=1% for stable nuclei − in
this energy range can be obtained at electron linear accelerators or at spallation based white
neutron sources.
An important opportunity is presented by the large neutron fluxes at present and planned
spallation sources. These large fluxes make possible measurements of capture cross sections
for radioactive nuclides that are branch points in the s-process. For these nuclides the neutron
capture rate is comparable to the rate of radioactive decay. Data on these branch points can
strongly constrain the neutron density and temperature of the s-process site.

32

The r-process
The r-process takes place in an environment of high temperature, exceeding l09 kelvins, and
high neutron density, greater than 1020/cm3, in an event lasting several seconds. In this
circumstance the interval between neutron captures is much shorter than the lifetime for betadecay. A rapid succession of neutron captures on a seed nucleus finally produces a nucleus
with a neutron binding energy sufficiently small that the rate of capture is balanced by the
rate of photodisintegration induced by the ambient blackbody photons. After some time at
this waiting point, a beta-decay occurs and the capture process can begin again. As a result,
nuclei tend to pile up at the waiting points. In this equilibrium approximation, the abundance
of a nucleus is proportional to its half-life. A special case, where pile-up is large, occurs near
neutron shell closures, because beta-decay lifetimes are long and because waiting points
recur after only a single neutron capture. At the end of the r-process event, the radioactive
products decay back toward the valley of stability, sometimes emitting neutrons in the
process. The resulting abundance peaks, occurring on the low-mass side of the s-process
peaks, are the signature of the r-process.
At present the astrophysical site of the r-process is under debate. The hot neutrino-heated
bubble outside the protoneutron-star in a supernova is in many ways an ideal site. Since rprocess abundances appear to be independent of the preexisting heavy-element enrichment of
the star (Section IV) the r-process site must produce its own seeds − the hot-bubble site
seems to do so. However, some models suggest that the entropy in this bubble is too small to
reproduce the observed abundance distribution. Consideration of general relativistic effects
may resolve this problem. Or, it may be necessary to consider other sites such as merging
neutron stars.
Despite these uncertainties, the general features of the r-process outlined above determine
what nuclear information is required. The path of r-process flow is through neutron-rich
nuclei, far from the valley of stability. It passes through nuclei with neutron binding energies
of about 1-4 MeV, depending upon parameters such as neutron flux and temperature. For
nuclei on the r-process path one needs to know masses with an accuracy of 100 keV or better,
beta-decay lifetimes, and the number of neutrons that are emitted in the beta-decay chain
leading back to the valley of stability. When the reactions freeze out as the temperature and
neutron density decline near the end of the event, it may become necessary to know some
radiative neutron capture cross sections. Even at spallation neutron sources, neutron capture
measurements will be possible only for radioisotopes near the valley of stability. These data
may serve to calibrate nuclear models for calculation of reaction rates for more neutron-rich
nuclides. Measurements of Coulomb-breakup − the inverse of the (n, γ) reaction − and of (d,
p) with radioactive beams can also provide information on these cross sections.
Mass measurements can be made, for example, using Penning traps, by comparing time of
flight and magnetic rigidity, either in storage rings or with spectrographs, or by measuring
beta-decay endpoints in coincidence with the associated gamma rays. Observation of the
gamma rays also provides the information on nuclear excited states that is necessary to
calculate partition functions as well as corrections to beta-decay rates and capture cross
sections. The technique of choice will depend on the lifetimes and achievable beam
intensities. Both ISOL and fragmentation facilities will be needed for these measurements.
33

Lifetime measurements are possible with very small intensities, perhaps a few per hour, and
can be carried out by a variety of production techniques that can provide isotopically clean
beams: for example by standard ∆E-E time-of-flight techniques, very-high-resolution mass
separators, or chemically selective laser ion sources. Near the production limits,
fragmentation facilities that can work with beams of a few per hour will probably be most
effective.
Even the next generation radioactive beam facilities will not allow one to study all of the
nuclei on the r-process path; some theoretical extrapolations will still be necessary. Making
them more reliable will require systematic studies of neutron-rich nuclei far from the betastability line. Properties of interest include energies of single-particle states and associated
shell structures, spectroscopic factors, nuclear deformations, level densities, and betastrength distributions. These studies have begun for nuclei near doubly magic 132Sn and for
neutron-rich lighter nuclei. Both ISOL and fragmentation facilities will be needed to obtain
the required information.
A particularly striking feature of some theoretical calculations is the prediction that the
strength of shell closures decreases −=i.e. the gaps between single particle energy levels
diminish −=as neutron-rich nuclei become less bound near the neutron drip line. Weaker shell
gaps affect mass and half-life predictions, which in r-process models results in a filling of the
valleys below the main r-process peaks and a better description of the abundances. This
apparently supports the theoretical conclusion and indicates that the r-process is sensitive to
decreased shell gaps. However, it is difficult to know whether other processes might produce
similar effects, especially since the site for the r-process is still uncertain. Nuclear properties
should be based on nuclear physics measurements, if at all possible, rather than inferred from
complex astrophysical phenomena – else the arguments for an accurate description of the rprocess become somewhat circular.
There is evidence that the intense neutrino flux present in a supernova site can remove
nucleons from r-process peak nuclei, thereby producing some rare and previously underproduced nuclei. If this process could be calculated precisely − which requires accurate
values of the breakup cross sections −=it could help to determine the number of neutrinos
passing through the site, and the endpoint of the r-process.
The p-process nuclei
The r- or s-processes cannot make certain rare nuclides that lie on the proton rich side of the
valley of stability. Understanding the synthesis of these p-process nuclei has been a longstanding challenge, partly because at least three different processes can produce them: the
gamma-process, the rp-process, and the neutrino-process. The gamma-process is the result of
a high temperature stellar environment, such as that found in supernovae, and involves
photo-erosion of preexisting abundant heavy nuclides by (γ, n), (γ, p), and (γ, α) reactions
induced by the ambient blackbody photons. The nuclear statistical model should be
applicable for the calculation of the rates of these reactions. Comparison with (n, γ) and (p, γ)
data show that these rates (and their inverses) can often be predicted to within a factor of two.
In contrast, (α, γ) measurements indicate that the calculated rates can be wrong by a factor of
ten or more, apparently because of poorly known alpha-particle optical potentials at such low
34

energies. These uncertainties might be reduced substantially by low-energy (n, α)
measurements that can constrain the alpha potentials. Studies of Coulomb-breakup of
radioactive nuclei at a fragmentation facility or using a free electron laser facility should also
provide information on these reactions.
The rp-process that occurs in the hydrogen-rich layer accreted on a neutron star may be
responsible for production of some of the lighter p-process nuclides: isotopes of Mo and Ru,
whose anomalously high abundances have been difficult to produce in the gamma-process
models. The rp-process is discussed in detail in Section VIII. An additional issue is whether
the radiation pressure is sufficient to overcome the enormous gravitational attraction of the
neutron star and blow a small fraction of the produced material into the interstellar medium.
This depends on the rate of energy production. To determine the feasibility of the rp-process
will require measurements of masses and beta-decay lifetimes for the progenitors of Mo and
Ru (light Pd, Ag, and Cd isotopes), and around lower-mass bottlenecks such as 72Kr.
The neutrino-process occurs in the high neutrino flux produced by the protoneutron-star in a
core-collapse supernovae (see Section VII). More abundant nuclei are excited to unbound
states (which later undergo particle decay) by neutrino induced inelastic scattering or charge
exchange reactions. This mechanism can affect the abundances of r-process nuclei,
especially those on the lower-mass side of the r-process peaks, and produce a significant
abundance of rare isotopes like 7Li, 11B, 19F, 138La, and 180Ta. It may also produce light pprocess nuclei such as 92Mo. The cross sections for the relevant neutrino-induced reactions
can be inferred from studies of inelastic hadron scattering and charge exchange at energies of
100-200 MeV/nucleon. Such experiments can be done for stable or radioactive isotopes at
fragmentation facilities. Measurements of neutrino-induced reactions on 12C and 56Fe have
been performed at LSND and KARMEN, showing that such studies are feasible. In the
future, better measurements may be possible at new spallation neutron sources.

35

X. NEUTRON STARS
At the end of the life of a massive star, its
iron-like core collapses and the resulting
supernova explosion disperses the outer part
of the star into the interstellar medium. In
most cases the explosion leaves behind a part
of the core, an extraordinarily dense object, a
neutron star with a typical mass 1.5 times that
of the sun and a radius of about 10 km.
Intense gravitational forces compress the star,
which at its center has a density 1015 times
that of water.
As one descends from a neutrons star’s
surface toward its center one passes through
an iron-like solid crust; a region where
energetic electrons are captured by nuclei
whose ratio of neutrons to protons thereby
increases; into regions where nuclei may have
linear or sheet-like shapes; then after about
one kilometer into a region containing a
nuclear fluid mostly composed of neutrons
and trace amounts of protons and electrons;
and finally at higher densities into a more or
less unknown region near the stellar center,
which may contain muons, heavy relatives of
electrons, pions, kaons, lambda or sigma
baryons, or even quark matter or quark-matter
droplets. It appears that large parts of a
neutron star could exist in a mixed phase, for
example, a mixture of ordinary hadronic
matter and quark matter.

An X-ray image of the Puppis A supernova
remnant, created using data from the ROSAT
High Resolution Imager. Puppis A is the
supernova remnant of one of the brightest sources
in the X-ray sky, with shocked gas clouds still
expanding and radiating X-rays. In the inset
close-up view, a faint pinpoint source of X-rays is
visible which is most likely the young neutron
star, kicked out by the asymmetric explosion and
moving away from the site of the original
supernova at about 600 miles per second. Directly
viewing a neutron star is difficult because it is
small (roughly 10 miles in diameter) and
therefore dim, but this newly formed neutron star
is intensely hot, glowing in X-rays.
Credit: S. Snowden, R. Petre (LHEA/GSFC), C.
Becker (MIT) et al., ROSAT Project, NASA.

The experimental and theoretical exploration of the structure of neutron-star matter and the
determination of the equation of state (EOS) associated with such high-density matter is of
key importance for understanding the physics of neutron stars and supernova explosions.
Direct observational evidence about the structure of neutron stars has been limited to their
masses, rotation rates, and occasional glitches in the regular rotations of pulsars. Indirect
techniques involving quasi-periodic oscillations in binary neutron star systems provide weak
limits on neutron star masses and radii. Now it appears that one may be able to measure
directly the radii and surface temperatures in selected cases, and when LIGO is operational,
to observe the gravitational wave signature of the merger of two neutron stars. The
observations will be sensitive to the equation of state of neutron-star matter, while the surface
temperature will also provide a measure of the cooling rate of the star by emission of

36

neutrinos. This cooling depends as well on the possible presence of particles other than
neutrons, protons, muons, and electrons in the stellar core. In total, these developments make
understanding of dense neutron-rich matter an urgent issue.
Since the properties of a neutron star depend on the nuclear equation of state, and hence on
the compressibility and symmetry energy of nuclear matter, it is important to determine these
quantities. The compressibility can be obtained from the frequencies and strengths of nuclear
vibrations that involve the compression of nuclear material, the isoscalar monopole and
isoscalar dipole resonances. Observation of these resonances with considerably improved
sensitivity is important, and should be supplemented by a search for weak components of
their strength that might change the average vibration frequency. On the theoretical front,
calculations should be carried out to develop interactions that reproduce the resonance data.
The apparent 30-40% discrepancy between the compressibilities deduced from the isoscalarmonopole and isoscalar-dipole resonances needs to be addressed both experimentally and
theoretically.
The bulk symmetry energy of nuclear matter describes the dependence of the energy on the
relative number of neutron and protons; it reflects the difference in the nuclear interaction
between two neutrons or two protons, and the stronger interaction between a neutron and a
proton. The symmetry energy is of great importance for studies of neutron stars,
nucleosynthesis, and supernovae. It affects the collapse of massive stars, neutrino emission
rates, the cooling rates of protoneutron-stars, and the predicted correlation between the radius
of a neutron star and the pressure of neutron-star matter near normal nuclear densities. Both
the magnitude and the density dependence of the symmetry energy are poorly known. Since
the surface symmetry energy of nuclei (dependent on their radius) is related to the density
dependence of the bulk symmetry energy, measurements that can distinguish between the
volume and surface symmetry energies in nuclei are particularly valuable.
Experimental information on the symmetry energy could be obtained from: (1)
Determination of the masses of heavy neutron rich nuclei far from the valley of stability; (2)
Positions and strength distributions of isovector resonances; and (3) Observation of the
collective flow of matter, momentum, and energy in collisions of neutron rich systems at
energies of roughly 100-500 MeV/nucleon, with the N/Z ratio adjusted using radioactive
beams.
The equation of state of high-density matter is being studied theoretically with three distinct
approaches: Schrödinger-based treatments using on two- and three-nucleon interactions,
effective field-theoretical approaches, and relativistic Dirac-Brueckner approaches. While all
of these approaches yield satisfactory results for normal nuclear density, considerable
differences persist in their medium and high-density behaviors, largely because of the poorly
constrained many-body interactions. These uncertainties manifest themselves in significant
uncertainties in the predicted radius (7-15 km), maximum mass (1.5-2.5 Msun), and maximum
rotation rate of a neutron star. Several theoretical approaches (automated algebra techniques,
Monte Carlo techniques in a potential model approach, and field-theoretical methods in
which the uncertain four-body interaction could be calibrated by measurements of neutron
star radii) may make it possible to better constrain the high-density behavior. There is

37

renewed interest in performing better many-body calculations to explore the possibility of
charged pion or kaon condensates in neutron star matter.
Relative to nucleons-only matter, neutron star properties may be significantly affected by the
presence of other particles (hyperons, deltas), or novel states of matter such as pion or kaon
condensates, or deconfined quark matter. The latter, being sought at heavy ion colliders,
could lead to metastable neutron stars that subsequently end their lives as black holes, or
could manifest itself in a hysteresis in the spin-down rate of rotating neutron stars (pulsars).
Such stars could anomalously spin up for about 106 years before resuming a normal spindown.
Another possibility is that hadronic and quark matter exist in a mixed phase. Quark matter
could first appear as small droplets, which grow and gradually become the dominant matter
at higher densities deep within the star. The resulting inhomogeneities in the matter could
have dramatic effects on the cooling properties of neutron stars. There is a strong connection
between these phenomena and experiments at heavy ion colliders. If these experiments
determine the conditions for deconfinement, and bulk properties such as the surface tension
between confined and deconfined phases, better models of neutron stars should follow.
The resistance to the neutrino flow, or neutrino opacity, for neutrinos in dense nuclear matter
has major consequences for the behavior of neutron stars. At high densities the typical
momentum transferred in scattering processes is larger than the internucleon (or interquark)
separation. To predict the opacity in these conditions, one must determine the collective
response of the matter: screening, inhomogeneities, and fluctuations all play a role in
determining density and spin-related correlations. Neutrinos might then serve as probes of
the properties of very dense, neutron-rich hadronic matter. An elevated opacity, perhaps
caused by quark matter droplets, might lead to an extended tail in the cooling curve of the
protoneutron-star formed in a supernova. It is a theoretical challenge to make this connection
quantitative, and an experimental challenge to measure precisely the neutrino flux from the
next galactic supernova.

38

XI. NEUTRINOS IN ASTROPHYSICS
More than 60 years after the neutrino was
proposed by Pauli as a way to preserve
conservation of energy and angular momentum
in beta-decay, its nature remains enigmatic.
Nevertheless, neutrinos play a fundamental role
in the evolution of the universe, in the creation
of the elements in the big bang and in
supernovae, and as a potential signature of new
physics beyond the Standard Model of nuclear
and particle physics.
Nuclear physics and astrophysics are central to
our efforts to understand the neutrino. The first
hints of neutrino mass emerged from the
neutrino fluxes obtained from solar neutrino
experiments. Double beta-decay is our best test
of the charge conjugation properties of
neutrinos, distinguishing Majorana from Dirac
masses. Tritium beta-decay provides our most
stringent bound on kinematical masses. Finally,
nucleosynthesis in the big bang and within
supernovae provides powerful constraints on
the number of neutrino generations and mixing.
Neutrino physics is a bridge connecting nuclear
physics, particle physics, and astrophysics.
Astronomical sources of neutrinos may present
the best opportunities to understand the
fundamental properties of neutrinos and
neutrinos may be the most fundamental source
of information about the interior properties of
stars.

The SNO detector, shown in the artist's
conception above, was built 2100 meters
below ground, in INCO's Creighton mine near
Sudbury, Ontario. SNO uses 1000 tons of
heavy water, contained in a 12m diameter
acrylic vessel shown in blue at the center of
the figure. Neutrinos react with the heavy
water (D2O) to produce flashes of light called
Cerenkov radiation. This light is then detected
with a geodesic array of 9600 light detectors
(photomultiplier tubes) surrounding the heavy
water vessel. Location in the deepest part of
the mine provides a large overburden of rock
to shield from cosmic rays. The detector
laboratory is kept exceptionally clean to
reduce background radiation signals which
would otherwise hide the very weak signal
from neutrinos. The detector is operating and
has seen events produced by neutrinos.

Solar neutrinos and neutrino oscillations
The last decade has seen major advances in
neutrino physics and in our understanding of
the importance of neutrinos in astrophysics. It
appears that neutrinos can oscillate or change from one type (flavor) to another, implying that
different neutrinos have different masses. These properties are not included in the simplest
Standard Model of particle and nuclear physics. Strong indications of oscillations came first
from solar-neutrino experiments with the chlorine detector. The deficit of detected electron
neutrinos could be explained if some of them oscillated into muon neutrinos on the way to
the chlorine detector. Since it was not sensitive to muon neutrinos, these oscillations could
resolve the solar neutrino problem. Related evidence came later from the Liquid Scintillator
Neutrino Detector (LSND) at Los Alamos. In 1998, the SuperKamiokande collaboration

39

reported evidence for oscillations of neutrinos resulting from the decay of pions and muons
in the atmosphere. Recent theoretical advances show that passage of neutrinos through the
matter of the sun or the Earth on their way to a terrestrial detector could enhance the effects
of neutrino oscillation in an energy dependent way (MSW mechanism) and that neutrinos
could affect the explosive nucleosynthesis that occurs within supernovae.
A straightforward picture of oscillations among the three known neutrino flavors, the
electron, muon and tau neutrinos cannot explain the available data: there are only two mass
differences between three neutrinos and the experiments, if all are correct, require three
differences. Scenarios that include sterile neutrinos, neutrinos that have no interactions with
other particles, can accommodate all experiments. For the first time, we have a road map of
where to look in neutrino physics to explain the data. Much of the needed exploration
involves the study of astrophysical sources of neutrinos. The coming decade will be an
exciting time, as the long-standing mysteries surrounding the neutrino begin to fall one by
one. The resolution of these uncertainties will have a strong impact on our understanding of
astrophysical phenomena.
A set of complementary solar neutrino experiments (Chlorine, Kamiokande,
SuperKamiokande, SAGE, and GALLEX) has yielded a powerful result. Chlorine,
Kamiokande and SuperKamiokande are sensitive mainly to high-energy neutrinos from the
decay of 8B produced in the sun, and SAGE and GALLEX mainly to neutrinos from the
decay of 7Be and the low energy neutrinos from the fusion of two protons. Yet no
combination of undistorted solar neutrino fluxes can reproduce all the experimental rates.
Neutrino oscillations can account for the data, although a single, consistent solution has not
yet emerged. The strong possibility of matter enhancements has enriched the experimental
possibilities: observable distortions of the neutrino energy spectrum, day-night effects, and
yearly variations can all occur. An experiment just beginning at the Sudbury Neutrino
Observatory (SNO) can detect muon or tau neutrinos with high efficiency and distinguish
them from electron neutrinos. Whether SNO confirms neutrino oscillations by direct
measurement of an enhanced neutral current rate (tau or muon neutrinos), or shows that the
oscillation phenomenon likely involves sterile neutrinos, more precise and powerful detectors
will be needed to define the physics. The challenge of building detectors to measure the
spectrum and flavor of pp and 7Be neutrinos is still largely unmet. The Borexino liquidscintillator detector will provide the first spectral information on 7Be. New developments that
could provide the means for studying low energy neutrinos are of great interest for a next
generation detector.
The supernova mechanism and supernova neutrinos
Supernova explosions (see Section VII) present a unique opportunity to use neutrinos to
probe fundamental and interrelated issues: the mechanism driving the supernova, the masses
and mixings of neutrinos, and the synthesis of new nuclei within our galaxy. The goal is
similar to the initial goal of solar neutrino detection, to probe nuclear processes otherwise
hidden in stellar cores.

The supernova mechanism is one of the important “grand challenge” problems in

40

computational physics. Resolving the puzzle will have far-reaching implications for
astrophysics and will help define the neutrino signature of supernova neutrino mixing. As so
much of the underlying physics − neutrino diffusion, the core bounce, weak interaction rates,
and explosive nucleosynthesis − is nuclear in origin, nuclear astrophysicists are leading the
efforts to understand the supernova mechanism. The great current excitement in this field is
driven by the near-term prospect of massively parallel calculations in which 2- and 3-D
modeling with full Boltzmann transport becomes possible for the first time, by the promise of
reliable nuclear data as input for these calculations, and by the possibility of detecting the
neutrinos emitted by the next supernova.
The resulting improved understanding of the supernova mechanism will make the explosion
a more powerful "laboratory" for astrophysics and particle physics. Neutrinos directly
influence supernova nucleosynthesis, controlling the neutron-proton balance responsible for
the r-process and synthesizing some light and heavy nuclei by neutrino induced breakup.
Future radioactive beam experiments will determine the nuclear structure properties which
affect the r-process and supernova neutrino flux measurements will similarly constrain the
astrophysical setting in which this synthesis takes place. These complementary constraints
will allow great progress.
An understanding of the time structure of the neutrino burst − its rise time, the possibility of a
pulse of very short duration, and its long time evolution − is crucial to future kinematic tests
of neutrino masses by time-of-flight delays. A delayed collapse into a black hole could well
lead to a sudden turnoff of the neutrino burst. Another important issue is the nature of
oscillations from electron to tau neutrinos, a potentially fascinating probe of cosmologically
interesting tau-neutrino masses: even modest mixing strength could produce large effects on
the supernova neutrino flux. Model-independent arguments show that the temperature
characterizing muon and tau neutrino sources is about twice that of electron neutrinos. As a
result, an electron neutrino to tau-neutrino oscillation leads to a distinctive temperature and
neutrino energy inversion that can have a distinctive signature in terrestrial detectors.
Supernova neutrino detection will also make an important contribution to conventional
astronomy. The neutrinos leave the core hours before the electromagnetic radiation leaves the
envelope and could give an early warning of a galactic supernova. It may also be possible to
determine the supernova direction to within about five degrees by using the forward-peaked
neutrino-electron scattering signal. This would allow astronomical observations from the
earliest possible time.
Facilities for neutrino physics
The importance of the issues addressed by neutrinos from supernovae warrants a strong
effort to assure that neutrinos from the next galactic supernova will be detected with good
statistics. If SNO and SuperKamiokande are operating at the time of the next supernovae,
they will yield thousands of events for a supernova similar to SN1987A. However, it is
necessary to obtain the time structure and energy distributions for separate flavors − for these
purposes new types of detector are needed. These detectors must be operated as
observatories, since galactic supernovae occur only about once every 30 years. Proposals are
being developed, for example, for detectors of neutrino induced breakup reactions involving

41

lead or iron, and for a one-kiloton iodine detector. Other ideas should emerge as the field
grapples with the task of continually monitoring the galactic neutrino flux, in concert with
other types of astrophysical observatories.
Beam-stop neutrinos and reactor antineutrinos are mainstays of research into neutrino
properties because of their intense fluxes and moderate energies. The LSND result is of
fundamental importance in cosmology. It appears to define a lower limit to the mass
contribution of hot dark matter at a level that would significantly affect the large-scale
structure of galaxy clusters and superclusters. For this reason, new experiments are essential.
The new Spallation Neutron Source could greatly improve the search for oscillations under
conditions in which LSND finds a signal. In addition, the intense radioactivity of the beam
stop might serve as a neutrino source.
Low energy and intermediate energy accelerators are important in this phase of development
of neutrino physics. Measurements of the (p, n) and (3He,t) reaction at energies of 100-200
MeV/nucleon are often the most practical technique available for estimating neutrino cross
sections important to supernova or solar neutrino detection. The connections between rprocess nucleosynthesis and supernova neutrino oscillations promise to illuminate both
processes. Measurements of masses and beta-decay rates at future radioactive beam facilities
are our best hope of making these connections quantitative. Finally, given the subtle
distortions now being sought in SNO and SuperKamiokande, the energy spectrum and
intensity of solar neutrinos must be better defined if one is to obtain reliable information
from solar neutrino detectors. This will involve experiments on pp-chain and CNO-cycle
reactions, e.g. 3He(α, γ)7Be, 7Be(p, γ)8B, and 14N(p, γ)15O, and on the shape of the beta-decay
spectrum of 8B. Theory will also play an important role: "exact" few-body techniques will
likely provide the best constraints on the intensity of the highest energy neutrinos from the
sun, those from the poorly understood 3He+p → 4He + e++ν reaction.

42

XII. NUCLEAR STRUCTURE AND DATA IN NUCLEAR ASTROPHYSICS

100

r-process abundances

Nuclear structure and nuclear
astrophysics are intricately
connected. Energy generation
in stars, nucleosynthesis,
stellar explosions, neutron
stars, and neutrino interactions
are all affected by nuclear
properties. The distinction
between research in nuclear
physics and nuclear
astrophysics is often only in
the specific application of the
results.

10-1
10-2
Observation
Pronounced shell Structure
Quenched shell structure

10-3
10-4

120
140
160
180
200
Data requirements
mass number A
Nuclear quantities of interest
can be grouped roughly into
three categories: global nuclear Observed abundances of r-process nuclei compared to two
predictions of these abundances from r-process network
properties (masses, lifetimes,
calculations. The calculations are based on two different
nuclear matter), properties of
spherical models of nuclear structure. In one of these (labeled
excited states (isomers, level
“pronounced shell structure”) the nuclear shell structure is similar
densities, electroweak
to that of stable nuclei. In the other (labeled “quenched shell
structure”) the shell gaps are reduced as predicted by some
strengths, decay rates), and
structure calculations. At the nuclear masses noted by the orange
reaction aspects (cross
arrows, the quenched calculations better describe the observed
sections, resonance structure,
abundances: the r-process abundances are sensitive to details of
interference properties). Each
nuclear structure.
of these properties has a direct
Adapted from B. Pfeiffer, et al., Z. Physik A357, 253 (1997)
impact on astrophysical
processes and on the
observable signatures they leave behind. Measured and compiled values are usually limited
to nuclei that are close to the valley of stability. Many nuclear properties that are important
for the understanding of astrophysical processes have not been measured, and some may
never be accessible in the laboratory − nuclear theory will play a central role in providing this
information.

Although many astrophysically important neutron-rich nuclei may not be experimentally
accessible, they are important for nucleosynthesis in cataclysmic events such as supernovae.
Consequently, new insights into nuclear structure in this terra incognita can sometimes be
offered by astrophysical data. For instance, the analysis of the r-process abundances provides
an indirect hint that shell splittings may be smaller for large neutron excess. Such hints
provide an impetus to confirm or refute this assumption more directly by studies in nuclear
physics.

43

Theoretical approaches, progress and directions
Nuclear theorists attempt to correlate known data by practical models and to predict the
behavior of nuclear systems in new regimes. Theory plays an important role in the analysis of
experiments, and in suggesting what new data would be valuable in future work.
Microscopic nuclear models continue to improve due to advances in theoretical techniques
and computational resources and to improvements in the data that are used to constrain the
input parameters. Exact theoretical solutions are still not possible for most nuclei, but one can
obtain essentially exact solutions for systems containing up to eight nucleons, and very good
results for infinite nuclear matter. The precise data on nucleon-nucleon scattering are used as
input, and three-body interactions are also important. Most of the reaction rates for big-bang
nucleosynthesis and solar neutrino production can be addressed by these ab initio
calculations. Faddeev and hyperspherical harmonic methods give exact 3- and 4-body
scattering solutions today, and Green's function Monte Carlo methods are promising for
larger nuclei, up to at least A = 12. Since most low-energy reactions occur at large radii,
further work is needed to incorporate appropriate asymptotic forms for the bound and
continuum wave functions. These exact methods are also important for understanding the
validity and limitations of the more approximate many-body techniques that must be used for
heavier nuclei.
For the entire range of larger nuclei one relies upon mean field and shell-model configuration
mixing methods. Both of these methods have recently been refined and improved. Of the
mean field models, the energy-density functional methods have been most successful. For
example, in the Skyrme-Hartree-Fock method the nuclear properties are related to a few (612) parameters of a density-dependent interaction that are determined by fitting nuclear data.
This method, together with quasiparticle RPA calculations of the Gamow-Teller strength
functions, provides an initial assessment of the global mass, beta-decay, and electron capture
properties that are needed to describe supernova-core collapse, the r-process, and rp-process.
The beta-decay half-lives are determined by the small part of the Gamow-Teller resonance
that lies at low excitation energy. This aspect of the calculations needs to be improved.
Hartree-Fock might be most appropriate for spherical and deformed regions, but for
intermediate regions, the generator coordinate method or its equivalent should be used.
Large-basis shell-model calculations provide a more complete picture, in principle, but have
been limited by the large dimensions of the valence spaces encountered in typical
astrophysics applications. However, the newly developed quantum Monte Carlo and
quantum Monte Carlo diagonalization methods, and a greatly improved direct
diagonalization approach, have greatly expanded the range of shell-model applications. It is
now possible to take the full fp-shell basis into account for the mass region A = 40-60. These
calculations require, as input, interactions (G matrices) for extremely large model spaces.
They are obtained from modern nucleon-nucleon potentials, with appropriate (e.g. monopole
term) corrections. Ultimately, they will be obtained in a model-independent way by fitting to
observed energy levels. The direct diagonalization approach provides detailed wave
functions from which one can obtain information important for astrophysics: the masses of
nuclei far from stability, Gamow-Teller strength functions, spectroscopic factors, and level
densities. For example, the fp-shell calculations have provided the weak interaction rates that

44

are needed to understand stellar-core collapse and supernova formation. Monte Carlo
methods have been used in the sdg-shell for nuclei in the 100Sn region. One can expect much
more progress in this direction in the future.
For light nuclei one can consider "no-core" model spaces that take into account up to about
10 harmonic-oscillator shells. The interactions for these calculations are obtained from
nucleon-nucleon scattering G-matrix elements, and improved methods, using the BlochHorowitz method to describe effective many-body interactions, are now being explored. The
shell-model codes OXBASH and ANTOINE are available for use by researchers, and other
more powerful codes are being developed.
Challenges for nuclear theory from the astrophysics viewpoint
•= To determine the cross sections for important reactions in few-body systems. Could the
3
He + p → 4He+ νe + e+ cross section possibly be large enough (over 10 times the present
estimate) to explain the high-energy flux in Super-Kamiokande?
•= To develop a continuum shell-model method to deal with the contributions of unbound
orbitals as one nears the neutron drip line. Are the shell splittings reduced (quenched) for
n-rich nuclei? Can one find signatures of this quenching in the observable regions,
thereby giving guidance to experimental studies?
•= To determine the weak interaction response of nuclei for both neutron and proton rich
nuclei. Perform detailed large basis shell model calculations to provide the electron
capture and beta-decay strengths necessary to simulate the evolution of the supernova
core and the r-process. Obtain the global neutrino interaction rates for both charged and
neutral-currents necessary to assess the role of neutrino induced reactions and energy
transfer in supernovae – transitions to unstable states should be included.
•= To obtain appropriate asymptotic forms of many-body wave functions for reaction
calculations and their comparison with experimental results.
•= To develop a reliable temperature, density, N/Z dependent equation of state appropriate
for use in various astrophysical sites, including neutron stars and supernovae.
Application to nuclear astrophysics
The interaction of nuclear theory with astrophysical theory is complicated. In the context of
nuclear physics, when a nuclear structure calculation has been completed it is compared with
experiment, and if necessary revisited taking the experimental results into account. In the
context of nuclear astrophysics several additional steps are necessary. Since information on a
broad range of nuclei is often required, development of an extrapolation or interpolation
procedure may be necessary if the theoretical calculations are difficult or time consuming.
The theory must then be validated by comparison to an appropriate range of experimental
data. If its predictions are not as accurate as is required, the theory must be “normalized” to
experiment in an appropriate fashion. Then, relevant cross sections or lifetimes must be
calculated. This may involve straightforward computations of matrix elements or strengths.
Or it may, for example, involve computations of inelastic neutrino scattering cross sections,

45

or of radiative capture cross sections for isolated resonances, or the use of statistical
approaches. Finally, these results must be expressed in terms of temperature dependent
astrophysical rates.
The complexity of the above task means that considerable thought must be given to deciding
when a new theory is sufficiently developed and sufficiently different from its predecessors
to warrant revising the reaction rates. It also means that a coordinated effort must be made to
address these problems − the theoretical strength at any single institution will be insufficient.
Perhaps such a coordinated effort could help to ensure that the structure of the rate
calculations and of the astrophysics programs that use them is such that improved theoretical
and experimental information can be incorporated with relative ease.
An important role of theory in nuclear astrophysics lies in providing guidance on the required
measurements. Many astrophysical processes involve networks of reactions and decays;
often it is far from clear whether a change in a particular nuclear property will have an
impact in the astrophysical scenario. At other times, reactions occur under conditions of
thermal equilibrium, and only the masses of nuclei and beta-decay strength are important.
The importance of a given reaction may depend on the specific situation, for example on the
mass of the star involved. Finally, it is clear that this process must be iterated, since changes
in some rates may affect the importance of others. It will not be simple to provide guidance
on the precision with which a given rate must be known, but it is essential to provide this
guidance so that our experimental and theoretical efforts can proceed efficiently.
A related issue is the need to ensure that the available data are quickly and conveniently
accessible to those involved in using them directly for the generation of rates for
astrophysical models, or as part of the evaluation process for theoretical calculations. Else
much of the effort involved in making these difficult measurements will be wasted.

46

XIII. REQUIRED ACCELERATORS
Experimental nuclear astrophysics is often viewed as involving long experiments using small
accelerators to measure cross sections at lower and lower energies, and then finding a way to
extrapolate still further to the energies relevant to stellar environments. While this certainly
describes the “classical” approach for determination of reaction rates, this view was never
fully correct; in some important cases, such as the triple alpha process, rates were based on
indirect structure measurements. Low energy studies will remain important, and will
typically take place at the limits of detectability for the reaction products. Dedicated sources
of low energy, high intensity (1016/sec range) beams with extraordinary stability and energy
accuracy are required. Passive or active techniques for reduction of natural and beaminduced radiation background will often be important. For some problems, underground
facilities or inverse kinematic techniques with intense heavy ion beams may be used to
optimize signal to background ratios.
Information equivalent to that from the direct measurement of cross sections can often be
obtained from measurements of the properties of nuclear states. These indirect approaches
will be important in the future. Partly this is because some astrophysical cross sections are so
small that direct measurements are impossibly prolonged. Partly it is because indirect
measurements provide a more efficient way of obtaining information that could in principle
be determined directly. And partly, in particularly difficult or important cases, the different
systematic errors associated with direct and indirect approaches may provide a better
estimate of the total uncertainties and thereby, a more useful result.
A broad range of complementary approaches and facilities is required to address the many
challenges described in this report. In addition to the low energy stable-beam facilities
mentioned above, these approaches include (1) Radioactive ion beams that cover the range of
nuclei produced in the cosmos with energies from about 100 keV/nucleon to hundreds of
MeV/nucleon. Intensities will vary widely, but it will be desirable to perform certain
experiments on nuclei approaching the limits of stability, with beams of 1-100 ions/day. (2)
Stable beams with high energy, from a few MeV/nucleon to several GeV/nucleon and
moderate intensity. (3) Spallation neutron sources and electron linacs for neutron and
neutrino-induced reactions. (4) Free-electron lasers, for studies with high-energy gamma
rays.
To illustrate these approaches, we will use as examples two reactions that have been studied
by a variety of approaches: the 12C + 4He → 16O + γ and the 7Be + p → 8B + γ reactions. The
first of these reactions is important in determining whether the ashes of helium burning stars
are dominantly carbon or oxygen, and the second determines the flux of high energy
neutrinos emitted by the sun. The earliest studies of these reactions used the classical
method: low energy alpha particles or protons bombarded targets of 12C and (radioactive)
7
Be, and the reaction products were detected. However, as theoretical descriptions became
more reliable, it became clear that the accuracy of these results and their extrapolations to
astrophysical energies were inadequate. Systematic differences among various data sets were
also a problem. This led to new approaches to these determinations.

47

For the 12C + 4He case, higher energy indirect experiments such as (7Li, t) that transferred
(added) alpha particles to 12C provided information about the properties of the important
nuclear states in 16O. Additional information was obtained from the beta-decay of the
radioactive nucleus 16N. Elastic alpha scattering measurements gave information about the
interference effects between the different resonances. High-energy electron beams incident
on 16O are being used to study the inverse of this reaction. And in the near future, it may be
possible to study the reaction in reverse using intense beams of photons bombarding 16O. For
the 7Be + p case, a high-energy beam of radioactive 8B bombarded a heavy target and was
broken up by the electric field of the target. Such Coulomb-breakup reactions are equivalent,
by detailed balance, to the inverse process and can be used to study capture reactions
involving short-lived nuclei such as 8B. The 7Be + p → 8B + γ=reaction is nonresonant, and
studies of reactions that transfer protons and measure asymptotic normalization coefficients
(ANC) yielded the cross section at astrophysical energies; this technique has a wide range of
applications. The reaction is also being studied directly, both with protons and with beams of
radioactive 7Be ions using inverse kinematics.
Both direct and indirect methods and a variety of accelerators have been used for the above
studies. These include low energy and tandem accelerators of protons, alpha particles, 12C,
and 7Be; 16N ions produced by an ISOL facility and tandem accelerators; high energy beams
of radioactive 8B produced using nuclear reaction and fragmentation techniques; high energy
beams of 7Be and 10B from a stable beam cyclotron; and medium energy electron beams.
Beams of gamma rays from the free-electron-laser facility HIGS can provide high
luminosities for measurements of (inverse) radiative capture cross sections. One need not
understand all these approaches in detail to realize that a variety of accelerators will
contribute in an important way to such measurements. In addition, high power lasers can
produce plasmas with temperatures and densities similar to those in stars, and present an
opportunity to study nuclei in stellar-plasma conditions.
Some of the facilities that are needed are shown in the following tables, categorized by
reaction or site. In examining these tables several points should be kept in mind. First, a type
of facility is grayed-in if it is useful for studying a given class of reaction. Several types of
facilities will usually be required to study all reaction of a given class. Second, several
approaches to a measurement are often necessary for understanding the systematic
uncertainties that plague these difficult measurements. And third, the tables are almost
certainly incomplete − new approaches will be developed and shown to be useful for these
measurements or there may have been oversights. The overall message of the tables is that a
variety of approaches are required and appropriate for studies in nuclear astrophysics.

48

Stellar Burning
FACILITY/REACTIONS

H

He

HI

s

Explosive Burning
r

rp

αp

γ

ν=

Low-E Stable Beam
High-E Stable Beam

RIB-ISOL
RIB-Fragmentation
Spallation n (ν)
ν)=
ν)=source
Free Electron Laser

This table shows the experimental methods that are useful for attacking a particular type of problem. The
symbols H, He, and HI refer to hydrogen burning, helium burning and heavy-ion burning in stars in quasistatic
burning phases. The s-process is denoted by “s”. The symbols r, rp, αp, γ, υ refer to the r-process, rp-process,
αp-process, gamma process, and the neutrino process. All of these occur in explosive environments, such as in
supernovae or binary systems.

SITE

Big Bang

Cosmic
Rays

Supernovae

Neutron
Stars

Low-E Stable Beam
High-E Stable Beam
RIB-ISOL
RIB-Fragmentation
Neutrino sources

This table gives similar information for other sites. Supernovae here refers to the various weak process that
occur in and near supernova cores, including the electron capture that makes the core neutron-rich, and neutrino
breakup processes. It also includes explosive versions of H, He and HI reactions noted in the table above.

49

XIV. EQUIPMENT NEEDS FOR LABORATORY ASTROPHYSICS
The strong scientific case for studying the role of nuclei in the cosmos warrants a concerted
experimental effort. Although some experiments can be done with existing devices, many
will require a new generation of experimental equipment. Conceptual designs exist for some
of this equipment, but development activities will be required for gamma ray and neutron
detection, highly integrated electronics, and high-density gas or liquid targets. Stable beam
experiments must often push the limits of low cross sections and low background rates.
Radioactive beam experiments must push the limits of efficient operation with extremely low
beam intensities. This section outlines the equipment that is needed to address these limits.
We have concentrated on detectors and equipment that will be used by a variety of
experimenters, for a variety of experiments at a given facility. We do not discuss the large
detectors necessary for neutrino detection or neutrino-induced experiments. These devices
are important for an understanding of astrophysical phenomena but have a cost comparable
to many accelerator facilities, are special purpose in nature, and must generally be justified
on an individual basis. We have also cast much of the discussion in terms of the requirements
of experiments that use radioactive beams, simply because they are less familiar. The general
requirements are similar for experiments with low-energy stable beams, although the details
of the detection devices will differ.
General requirements
The broad range of experimental conditions with respect to energy, mass, and intensity,
requires a correspondingly broad array of experimental apparatus. Most of the radioactive
beam experiments and many with stable beams will be done in inverse kinematics, where a
lighter target is bombarded with a heavier beam. Inverse kinematics experiments require high
granularity charged particle and gamma-ray detectors to compensate for the large kinematic
or Doppler shifts. In addition, the decay of scattered radioactive beam particles will produce
a high background rate in particle and gamma-ray detectors, which again requires segmented
detector systems. In experiments with low intensity beams, detector systems must cover a
large fraction of the total solid angle. Selection of the reactions of interest in experiments
with impure beams (a common situation) requires coincident detection of the outgoing
particles with good mass and charge identification. For all these reasons, requirements for
experimental equipment differ from those of the current generation.
Gamma-ray detectors
Gamma-ray detection will be important in many experiments. Arrays of clustered
germanium detectors can be configured in a compact geometry and can be used for gamma
detection in decay studies, or at the focal plane of a mass separator for isomer studies, or for
beta-gamma-gamma coincidence experiments. But they do not have sufficient segmentation
for many purposes. Highly segmented arrays such as GAMMASPHERE lack the necessary
overall efficiency and position resolution. Approaches that track events in germanium
detectors as in the next-generation approaches typified by GRETA (Gamma-Ray Energy
Tracking Array) or GARBO (Gamma-Ray Box) are required. Devices with similar
characteristics could increase gamma-ray detection efficiency to 50% from around 10% and

50

will be able to work in high- background environments. The position resolution would be 2
mm compared to the current 20 mm, allowing adequate Doppler correction.
Particle detectors
The common use of inverse kinematic techniques affects the nature of required particle
detectors. In the many cases where direct measurements of reaction rates are not possible,
these rates must be inferred from the resonance or continuum parameters determined
indirectly from nuclear reactions. The targets will typically be light nuclei, often protons or
deuterons. In inverse kinematics the light recoil ions are found near 0 degrees for (p, d)-like
(pick-up) reactions, near 90 degrees for inelastic and elastic scattering, and near 180 degrees
in (d, p)-like (stripping) reactions. The detector must have excellent position resolution to
allow correction for the large kinematic shift with angle. The low intensity of secondary
beams requires that the array cover the angular range of interest, typically about 50 degrees
in the laboratory. A large area array of silicon strip detectors could meet these needs; its
design should be modular and flexible to allow for a variety of uses. Silicon strip detector
technology will also be employed in the focal plane detector systems of recoil separators and
magnetic spectrographs. Due to the importance of this technology, immediate development
of highly integrated electronics is a high priority; no commercially available electronic
systems provide the required resolution in energy (20 keV) and time (150 ps), along with
other necessary features.
Many experiments will require the detection of neutrons. These include beta-delayed neutron
decay of nuclei on the r-process path, (d, n) stripping reactions to determine proton
spectroscopic factors for nuclei involved in novae and X-ray bursts, (p, n) reactions for the
determination of weak interaction rates, and Coulomb-breakup experiments to infer (n, γ)
reaction rates from detailed balance. The detection of high-energy neutrons typically requires
a large area detector with moderate resolution, such as the existing neutron walls at
fragmentation facilities. However, inverse kinematics experiments involve low energy
neutrons and require high granularity. For this case, evaluation of various prototype detectors
is required before a decision on detector type can be made. Both detector arrays and walls
will probably be important.
Magnetic detectors
Several types of magnetic devices will be required. Studies of radiative capture reactions
(with both stable and radioactive beams) require mass separators with a modest solid angle
and high beam rejection capability, up to at least 1015:1. These devices can be designed for
relatively low rigidity, and should be flexible and allow various optical configurations.
Target arrangements should allow for gamma-ray detection and the use of gas cell and gas jet
targets. Nuclear structure studies at future radioactive ion beam facilities, necessary to test
nuclear models used in the prediction of r- and rp-process nuclei, will require a large
acceptance, high mass-resolving power (>350) separator. The target area should allow for
the use of nearly 4π gamma-ray and particle detector arrays. Beam rejection should be
maximized to allow clean experiments in the focal plane.
Another required device is a magnetic spectrograph with large solid angle (20 to 100 msr)
and good energy resolution (1 part in 104). This device will be critical for transfer reaction

51

studies at lower energy to extract information on parameters for resonant reaction rates, and
for breakup studies at higher energies to measure ground-state structure and Coulombbreakup cross sections.
Other instrumentation
Many of the reactions of astrophysical interest require targets of pure hydrogen and helium
isotopes for study of hydrogen or alpha-induced reactions. Examples are radiative proton
capture by light nuclei and (d, p) reactions on neutron-rich nuclei that can be studied most
easily in inverse kinematics. The targets should be windowless and should allow operation
with many different gases. It would be desirable to have target densities of 1019/cm2 in order
to compensate for weak beam intensities, but this will be difficult to achieve, especially for
light gases. For higher energy experiments, thin, uniform liquid hydrogen targets will be
useful; significant efforts will be necessary develop such targets.
Certain specialized equipment will be required for particular experiments. For example, ion
traps will be used for precision mass measurements; and germanium detector arrays and tape
transport (or equivalent means of removing radioactive ion buildup) will be required for
decay-spectroscopy studies. Implantation stations and chemical separation stations will be
required for producing the radioactive targets that may be preferable to radioactive beams for
half-lives greater than about one day.
In many cases, especially for studies related to the s- and p-process, targets of rare or
separated isotopes are required. The considerable cost or unavailability of such material
limits the experimental opportunities.
A summary of some of these equipment needs is given in the table on the following page.

52

LE-SB

HE-SB

RIB
ISOL

RIB
FRAG

SNS

FEL

Gamma array-segmented
Silicon-Strip Arrays
Neutron Array
Spectrograph
Mass Separator
Gas/Liquid Targets
Radioactive Targets
Traps
This table shows the general types of apparatus that will be required by experiments with various types of
facilities. Of course, the precise nature of a particular device will differ depending on the facility. Here LE-SB
and HE-SB stand for low and high energy stable beam facilities; RIB ISOL and RIB FRAG stand for the two
types of radioactive beam facilities (ISOL and Fragmentation); SNS stands for Spallation Neutron Source but
for some purposes includes linacs and other neutron sources; FEL stands for Free Electron Laser.

53

XV. OUTLOOK
It is an exciting time for nuclear astrophysics. There are opportunities to reach a
new level of understanding of nucleosynthesis in the big bang, of the evolution of
stars, and of explosive events such as supernovae, novae, x-ray-bursts, x-ray
pulsars and neutron star mergers. An exquisitely detailed record of these events
will flow from new astronomical observatories. Nuclear physics is so inextricably
involved in astronomical phenomena, however, that only with a much better
knowledge of nuclei and nuclear reactions can we obtain a deep understanding of
these phenomena.
Fortunately, promising new facilities exist or are on the horizon. New radioactive beam
facilities will produce elements previously made only in stars and elucidate those of their
properties important for cosmic phenomena. Stable beam accelerators with exceptional
intensity and cleanliness will study nuclear reactions at energies close to those found in
stellar environments. Powerful neutron sources will delineate processes that create the heavy
elements. More powerful detectors of neutrinos from the sun and supernovae will provide
information on neutrino properties, and on the role of neutrinos in explosive processes.
With wise investment of our resources, great strides in our knowledge of the cosmos should
be possible in the near future. We can anticipate finding the sources of energy density in the
universe, based on nucleosynthesis in the big bang, measurements of the cosmic background
radiation, and measurements of the acceleration of the cosmic expansion, using Type Ia
supernovae as standard candles. We will probably have a solution of the solar neutrino
problem and a picture of the nature of neutrinos: the number of neutrino types and their
masses. We can expect to have assembled the requisite combination of computational power
and nuclear physics knowledge to model supernova explosions. We should know where the
heavy elements are formed and understand the processes that make them. We should have
understood the complex mixing and mass loss dynamics, which accompanies late stellar
evolution. We should have an improved understanding of the structure of neutron stars and of
how their properties are modified by nuclear burning on their surfaces in binary systems.
With luck, we will have constructed a detector for supernova neutrinos and have observed a
(rare) supernova explosion.
We can anticipate further challenges. Given the explosive growth of terrestrial and satellitebased observatories, it seems certain that new phenomena will be observed and that new
knowledge of nuclear physics will be crucial to understanding them.

54

APPENDIX A: PROGRAM

Town Meeting on Opportunities in Nuclear Astrophysics
PROGRAM

Sunday June, 6
7-9 p.m.

Reception at the Morris Inn

Monday June, 7
Room 127 - Nieuwland Science Hall:
8:30

Welcome - Jeffrey C. Kantor, Vice President and Associate Provost,
University of Notre Dame

8:40

Introduction - J. Kolata, Univ. of Notre Dame

Chair – C. Barnes
8:50-9:20

New observational evidence for nucleosynthesis - J. Truran, Univ.
of Chicago

9:30-9:45

NuPECC recommendations in nuclear astrophysics - F. Thielemann,
Univ. of Basel

9:50-10:20

Creation of the lighter elements - M. Turner, Univ. of Chicago

10:30

Coffee

Chair – B. Balantekin
11:00-11:30

The solar neutrino problem and supernova neutrino detection
H. Robertson, Univ. of Washington

11:40 –12:10

Quasistatic evolution and nucleosynthesis in massive stars - R.
Hoffman, LLNL

12:20-1:30

Lunch (South Dining Hall)

Chair – S. Shore
1:30-2:00

Neutron sources and the s-process in red giant stars - R. Gallino,
Univ. of Torino; A. Davis, Univ. of Chicago

55

2:10-2:40

Explosions in binary systems - S. Starrfield, Arizona State

2:50-3:20

The rp-process in X-ray bursters and X-ray pulsars - H. Schatz, GSI

3:30

Coffee

Chair-J. Lattimer
4:00-4:30

Supernova evolution - T. Mezzacappa, ORNL

4:40-5:10

Supernova nucleosynthesis - F. Thielemann, Univ. of Basel

5:20-5:50

Nuclear physics in the r-process - K.-L. Kratz, Univ. of Mainz

6:30

Dinner in the working group rooms

7:00-9:00

Working Group Session I

* Group #1:
* Group #2:
* Group #3:
* Group #3a:
* Group #4:
* Group #5:
* Group #6:

(NSH - Nieuwland Science Hall)

123 NSH -

Data needs for stellar burning process (A. Champagne,
P. Koehler)
180 NSH - Data needs for explosive H/He burning (M. Smith, H.
Schatz)
284 NSH - Data needs for supernova explosions (K. Rykaczewski,
Y.Z. Qian)
415 NSH - Neutron stars and the nuclear equation of state (U.
Garg, M. Prakash)
404A NSH - Data needs for big-bang nucleosynthesis, cosmic rays
and the origin of rare isotopes (R. Boyd, A. Westphal)
184 NSH - Data needs to understand nuclear structure effects (A.
Aprahamian, W. Nazarewicz)
182 NSH - Nuclear theory needs ( B.A. Brown, R. Wiringa)

Tuesday June, 8
Room 127 - Nieuwland Science Hall:
Chair – M. Thoennessen
8:30-9:00

Opportunities and Challenges for Nuclear Theory - W. Haxton Univ.
of Washington

9:10-9:35

Opportunities with low energy accelerators - M. Wiescher, Univ. of
Notre Dame

56

9:40-10:05

Opportunities with neutron sources - F. Käppeler, Karlsruhe

10:10

Coffee

Chair – C. Davids
10:40-11:05

Opportunities with ISOL facilities - M. Smith, ORNL

11:10-11:35

Opportunities with fragmentation facilities - B. Sherrill, MSU/NSCL

11:40-12:05

The need for indirect measurements - A. Champagne, Univ. of
North Carolina

12:10-12:35

Experimental equipment needs - I.Y. Lee, LBL

12:45-1:30

Lunch (South Dining Hall)

1:30-3:30

Working Group II (NSH - Nieuwland Science Hall; O'Shag O'Shaughnessy Hall)

* Group #7:

284 NSH -

* Group #8:

207 O'Shag -

* Group #9:

123 NSH -

* Group #10:

180 NSH -

* Group #11:

208 O'Shag -

* Group #12:

415 NSH -

Indirect experimental approaches (P. Parker, B.
Tribble)
Opportunities with real and virtual photons (M. Gai,
H. Weller)
Opportunities and instruments for radioactive beam
facilities (J. Kolata, E. Rehm)
Opportunities and instrumentation for low energy
stable beam facilities (C. Brune, M. Hofstee)
Opportunities and instrumentation for neutron
facilities (B. Haight, P. Koehler)
Weak interactions and neutrino detectors (H.
Robertson, A. Garcia)

Chair – S. Austin
3:30-3:40

Nuclear Data Database - P. Parker, Yale

3:45

Reports of Working Groups (10 min. each) coffee will be available

6:00

Wrap-up

57

APPENDIX B: ATTENDEES
There were 174 attendees from 68 institutions, 52 in the U.S.
Aguilera
Ajzenberg-Selove
Alahari
Andrzejewski
Anthony
Aprahamian
Austin
Azhari
Baktash
Balantekin
Barnes
Baumann
Beaulieu
Becchetti
Bernstein
Bertulani
Blackmon
Boyd
Brenner
Britt
Brown
Brune
Bulgac
Caggiano
Calaprice
Champagne
Chavez
Cherubini
Clement
Collon
Connell
Couture
Culp
Daly
Davids
Davids
Davis
Dean
DeBraeckeleer
DeSouza
Engel
Fowler

Eli
Fay
Navin
Jozef
Don W.
Ani
Sam M.
Afshin
Cyrus
Baha
Charles A.
Thomas
Luc
Fred D.
Lee A.
Carlos A.
Jeff C.
Richard N.
Daeg S.
Harold C.
Alex
Carl R.
Aurel
Jac
Frank
Arthur E.
Efrain R.
Silvio
Ralph R.
Philippe A.
James J.
Aaron J.
Fred
Jason T.
Barry S.
Cary N.
Andrew M.
David J.
Ludwig
Romualdo
Jonathan
Malcolm M.

Inst. Nac. Investigaciones Nucl. (Mexico)
U. Pennsylvania
NSCL/Michigan State
ORNL/Univ. of Lodz (Poland)
NSCL/Michigan State
Notre Dame
NSCL/Michigan State
Cyclotron Inst./Texas A&M
ORNL
U. Wisconsin-Madison
Caltech
NSCL/Michigan State
IUCF
U. Michigan
LLNL
Univ. Fed. do Rio de Janeiro (Brazil)
ORNL
Ohio State
Clark University
U.S. Dept. of Energy
NSCL/Michigan State
U. North Carolina-Chapel Hill
U. Washington
ANL
Princeton Univ.
Univ. North Carolina
Instituto de Fisica/UNAM (Mexico)
IPN/Univ. Catholique de Louvain (Belgium)
NSCL/Michigan State
Inst. für Rad. und Kernphysik (Austria)
U. Chicago
Notre Dame
Tennessee Tech.
Notre Dame
NSCL/Michigan State
ANL
U. Chicago
ORNL
Duke Univ.
Indiana Univ.
Univ. North Carolina
LANL
58

Fox
Freeman
Frekers
Frisch
Gai
Gallino
Galonsky
Garcia
Garg
Gelbke
Giesen
Gledenov
Goerres
Goodman
Gould
Grimes
Gross
Guber
Guimaraes
Haight
Halderson
Hale
Harss
Haxton
Heckman
Hencheck
Henning
Hix
Hoffman
Hofstee
Horowitz
Howes
Islam
Janssens
Jiang
Jose
Kaeppeler
Karwowski
Keister
Kemper
Koehler
Kolata
Kozub
Kratz
Kumar

John D.
Charlie G.
Dieter
Priscilla
Moshe
Roberto
Aaron
Alejandro
Umesh
Claus-Konrad
Ulrich
Yuriy M.
Joachim
Charles D.
Chris
Steven M.
Carl J.
Klaus H.
Valdir
Robert C.
Dean W.
Gerald M.
Boris
Wick
Paul R.
Michael
Walter F.
William R.
Robert D.
Mariet A.
Charles J.
Ruth H.
Mohammed
Robert V.
Cheng Lie
Jordi
Franz
Hugon J.
Bradley D.
Kirby W.
Paul E.
James J.
Raymond L.
Karl-Ludwig
Krishna

Florida State
SUNY Geneseo
Univ. Münster (Germany)
U. Chicago
U. Connecticut
Dipt. di Fisica Generale/Univ. Torino (Italy)
NSCL/Michigan State
Notre Dame
Notre Dame
NSCL/Michigan State
Notre Dame
ORNL/JIHIR-UT (Russia)
Notre Dame
IUCF
N. Carolina State U/TUNL
Ohio Univ.
ORISE/ORNL
ORNL
Notre Dame
LANL
Western Michigan
LANL
ANL
U. Washington
NSCL/Michigan State
U. Wisconsin - Green Bay
ANL
U. Tennessee/ORNL
LLNL
Colorado School of Mines
Indiana Univ.
Ball State
Ball State
ANL
ANL
Univ. Politecnica de Catalunya (Spain)
Forschungszentrum Karlsruhe (Germany)
Univ. North Carolina
NSF
Florida State
ORNL
Notre Dame
Tennessee Tech.
Inst. Kernchemie/U. Mainz (Germany)
Tennessee Tech.
59

Lacey
Lande
Lattimer
Lee
Lister
Liu
Liu
Lizcano
Lofy
Luke
Lynch
Mantica
Martinez-Quiroz
Massey
Mathews
McEllistrem
Mezzacappa
Morrissey
Mukhamedzhanov
Murphy
Nakamura
Nazarewicz
Olsen
Ortiz
Pancella
Parker
Peaslee
Peterson
Piechaczek
Pieper
Prakash
Prior
Prisciandaro
Qian
Rapaport
Ravenhall
Rehm
Ressler
Robertson
Rundberg
Runkle
Rykaczewski
Santi
Sawyer
Schatz

Roy
Kenneth
James M.
I-Yang
Kim
Tianxiao
XiaoDong
David
Patrick A.
John
Bill
Paul
Enrique
Thomas N.
Grant J.
Marcus T.
Anthony
David J.
Akram
Alexander S.
Takashi
Witold
Michele
Maria-Esther S.
Paul
Peter
Graham
Donald
Andreas
Steven C.
Madappa
Richard M.
Joann I.
Yong-Zhong
Jack
D. G.
Ernst K.
Jennifer-Jo
Hamish
Robert S.
Bob C.
Krzysztof P.
Peter A.
Raymond F.
Hendrik

SUNY Stony Brook
U. Pennsylvania
SUNY Stony Brook
LBNL
ANL
NSCL/Michigan State
NSCL/Michigan State
Inst. Nac. Investigaciones Nucl. (Mexico)
NSCL/Michigan State
LLNL
NSCL/Michigan State
NSCL/Michigan State
Inst. Nac. Investigaciones Nucl. (Mexico)
Ohio Univ.
Notre Dame
U. Kentucky
ORNL
NSCL/Michigan State
Texas A&M
Ohio State
NSCL/Michigan State
U. Tennessee
SUNY Geneseo
Inst. de Fisica, National U. Mexico
Western Michigan
Yale
Hope College
Notre Dame
Louisiana State
ANL
SUNY Stony Brook
N. Georgia College & State U/TUNL
NSCL/Michigan State
LANL
Ohio Univ.
U. Illinois/Urbana-Champaign
ANL
U. Maryland/ANl
U. Washington
LANL
Univ. North Carolina/TUNL
ORNL/Physics Division
Notre Dame
U. California-Santa Barbara
GSI (Germany)
60

Schiffer
Schwartz
Seabury
Segel
Serot
Sherrill
Shore
Shotter
Siemssen
Smith
Smith
Sonzogni
Soramel
Starrfield
Stephan
Strayer
Tan
Ternovan
Thielemann
Thoennessen
Tischhauser
Tribble
Truran
Tryggestad
Tsang
Tsentalovich
Turner
Ullmann
Van Wormer
Verde
Voytas
Walters
Weber
Weil
Weller
Westphal
Wiescher
Winger
Wiringa
Wu
Xu
Zganjar

John P.
Brook
Edward H.
Ralph
Brian D.
Bradley M.
Steven
Alan
Rolf H.
Donald L.
Michael S.
Alejandro A.
Francesca
Sumner
Andreas E.
Michael R.
Wanpeng
Christopher
Friedrich K.
Michael R.
Paul D.
Robert E.
Jim
Erik J.
Betty
Genya
Michael S.
John L.
Laura A.
Guiseppe M.
Paul A.
William B.
Fridolin
Jesse L.
Henry R.
Andrew J.
Michael
Jeff A.
Robert
Jianshi
Hushan
Edward F.

ANL
SUNY Geneseo
LANL
Northwestern
Indiana U.
NSCL/Michigan State
Indiana U.-South Bend
Edinburgh Univ. (Scotland)
ANL
ANL
ORNL
ANL
Universita' di Udine (Italy)
Arizona State
Triangle U. Nuclear Laboratory
ORNL
NSCL/Michigan State
Wittenberg Univ.
Univ. of Basel (Switzerland)
NSCL/Michigan State
Notre Dame
Texas A&M
U. Chicago
NSCL/Michigan State
NSCL/Michigan State
MIT/Bates
U. Chicago
LANL
Hiram College
NSCL/Michigan State
Wittenberg Univ.
U. Maryland
LBNL
U. Kentucky
TUNL/Duke Univ.
U.C. Berkeley
Notre Dame
Mississippi State
ANL
Fayetteville State U.
NSCL/Michigan State
Louisiana State

61

APPENDIX C: STEERING AND WRITING COMMITTEES
Steering Committee
Sam Austin
Richard Boyd
Art Champagne
Wick Haxton
Kevin Lesko
Peter Parker
Ernst Rehm
Michael Smith
Robert Tribble
James Truran
Michael Wiescher

Writing Committee
Sam Austin
Richard Boyd
Art Champagne
Wick Haxton
Paul Koehler
Kevin Lesko
Peter Parker
Ernst Rehm
Brad Sherrill
Michael Smith
Robert Tribble
James Truran
Michael Wiescher

Consultants
Ani Aprahamian
John Beacom
Andrew Davis
C. Konrad Gelbke
Franz Kaeppeler
Grant Mathews
Filomina Nunes
Anthony Mezzacappa
Hendrik Schatz
Steve Shore
Friedrich-Karl Thielemann
Fridolin Weber

62

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

–

1(44)

Neutrino AstroPhysics
Supernova Neutrino Detection

November 6, 2003
title.tex

–

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

–

2(44)

Content of the Lecture

Intro: Neutrino Properties
Supernova Properties (neutrino related)
(Supernova) Neutrino Detection
Underground Physics
Underground Laboratories
Supernova SN1987A
What will be learned ?

November 6, 2003
sisallys.tex

–

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

–

3(44)

–

Introduction
Neutrino Properties

• three flavors (types): electron, muon and tau neutrinos, and their anti-neutrinos
• Recent data from the Sudbury Neutrino Observatory (SNO) and from the SuperKamiokande (Super-K, SK) demonstrated that neutrinos oscillate
☞ neutrinos have mass
• SK :
atmospheric νµ −→ νx (not νe)
SNO :
solar νe −→ νµτ
• Neutrino properties are still poorly known
• Interact extremely weakly with matter
• can be used to investigate the supernova core collapse or the interior of the Sun
(in the Sun: ∼2 sec for ν to get from centre to surface, gammas are captured)
• very difficult to observe experimentally (needs huge-size detectors)
• Experimental observation of neutrinos gives information on neutrinos and/or on their source
• Neutrino sources
• Sun (νe), Supernova (all)
• Atmosphere (νµτ )
• Nuclear reactors, Earth’s crust (β -decay of fission fragments, ν̄e)
• Accelerator (muon factory, νµτ )

November 6, 2003
neutrino intro.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

–

4(44)

–

Supernova Properties
(1) Energy release

3 GM2core
3 GM2NS
59
46
−
≈ 3 × 10 MeV ≈ 3 × 10 J
∆ EB ≈
5 RNS
5 Rcore

• Kinetic energy of explosion
≈ 10−2·∆EB
• EM radiation (incl. optically visible part) ≈ 10−4·∆EB
• Rest, ∼99%, of the energy is taken away by neutrinos
☞ ∼1% of νe from an initial breakout burst
☞ ∼99% are ν ν̄ pairs of all flavors from the cooling phase

November 6, 2003
SN prop i.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

–

5(44)

–

Supernova Properties
(2) Neutrino emission

• The Energy of emitted neutrinos can be described by Boltzmann distribution
hEνe i ≈ 11 MeV
hEν̄e i ≈ 16 MeV
hEνµτ i ≈ 25 MeV
• Neutrinopulse is quite short
☞ duration ∼ 10 – 20 s
Lνe (t) ≈ Lν̄e (t) ≈ Lνµτ (t)

• Number of emitted neutrinos: ∼1058 of all types
• Number of expected supernova explosions in our Galaxy: ∼ 3± 1 per century

November 6, 2003
SN prop ii.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

Supernova Properties
(3) Core collapse

November 6, 2003
SN prop iii.tex

–

6(44)

–

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

–

7(44)

–

Supernova Neutrino Detection
and
Detectors

November 6, 2003
SN detec.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

–

8(44)

–

Supernova Neutrino Detection
(1) General

Several kinds of detectors are capable of detecting supernova neutrino burst.
Detectors dedicated to the supernova neutrino detection don’t still exist, even
thought some of them are proposed.
Thus, most of the detectors described here have primary purpose other than supernova
neutrino detection, for example, proton decay search, solar neutrino physics, or neutrino
oscillation physics.

•
•
•
•
•
•

Scintillation Detectors

(MACRO, LVD, Borexino, KamLAND, Baksan, Mini-BooNE)

Water Čerenkov Detectors

(SK, SNO, UNO, Hyper-K)

Heavy Water Čerenkov Detectors

(SNO)

Long String Water Čerenkov Detectors
High-Z Detectors
Liquid Argon

(AMANDA)

(ONMIS, LAND)
(ICANOE (or ICARUS), LANNDD)

November 6, 2003
SN detec i.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

–

9(44)

Supernova Neutrino Detection
(3) Scintillation Detectors : General

• Usually liquid scintillators
☞ material CxHy : C9H12 pseudocumene (PC)
C9 H10 phenyl-o-xylylethane (PXE)

☞ surrounded by PMTs

• Reactions
• ν –e scattering:
νx + e− → νx + e−
• inverse β -decay:
ν̄e + p → e+ + n
• CC-capture of ν̄e:
ν̄e + 12C → 12B + e+
• CC-capture of νe:
νe + 12C → 12N + e−
• NC-excitation of 12C: νx + 12C → 12C∗ + νx0
• Detectors
• KamLAND in Kamioka
• BOREXINO in Gran Sasso
(ready, but not yet running due to a leak)

• Very little pointing and weak flavor capability

November 6, 2003
SN detec iiia.tex

–

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 10(44)

–

Supernova Neutrino Detection
(3.1) Scintillation Detectors : BOREXINO
Consists of 300 tons of pseudocumene (PC, C9H12)

November 6, 2003
SN detec iiib.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 11(44)

–

Supernova Neutrino Detection
(4) (Heavy) Water Čerenkov Detectors : General

• Volume of clear water (H2O) or heavy water (D2O),
vieved by PMTs
• Reactions in H2O
• inverse β -decay:
ν̄e + p → e+ + n
• CC-capture of ν̄e:
ν̄e + 16O → 16N + e+
• CC-capture of νe:
νe + 16,18O → 16,18F + e−
• NC-excitation of 16O: νx + 16O → 16O∗ + νx0
• Reactions in D2O
• CC-breakup :
νe + d → p + p + e−
• NC-breakup :
νx + d → p + n + νx
• Elastic scattering (ES): νx + e− → νx + e−
• Detectors
• SNO in Sudbury (H2O, D2O)
• Kamiokande II, Super-Kamiokande in Kamioka
• IMB in Fairport (Ohio) (not in use)
• H2O : Some pointing and flavor capability
D2O : Very good flavor sensitivity, some pointing
November 6, 2003
SN detec iva.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 12(44)

Supernova Neutrino Detection
(4.1) Heavy Water Čerenkov Detectors : SNO

November 6, 2003
SN detec ivb.tex

–

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 13(44)

Supernova Neutrino Detection
(5) Long String Water Čerenkov Detectors

• ∼1 km long strings of PMTs in very clean water
or ice
• Reactions similar to water Čerenkov (mainly ν̄e)
• Detectors
• AMANDA in the Antarctic ice
• Antares in the Mediterranean
• Baikal in the Lake Baikal
• NESTOR in the Mediterranean

November 6, 2003
SN detec v.tex

–

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 14(44)

–

Supernova Neutrino Detection
(6) High-Z Detectors : General

• Large quantity of Pb, Pb(ClO4)2, or Fe
(few to tens of kT)
• Pb, Fe
☞ scintillator (neutron counter)
Pb(ClO4)2 ☞ Čerenkov
• Advantages
• Pb (and Fe) has relatively high cross section,
and it is low-cost material
• Pb has small neutron capture cross section

• Reactions
• NC : νx + 208Pb → 208Pb∗ + νx0 → 208−xPb + xn
• CC : νe + 208Pb → 208Bi∗ + e− → 208−xBi + xn
• Detectors
• OMNIS, LAND (only proposed) (these would be dedicated SN neutrino detectors)
• Good flavor capability, no pointing

November 6, 2003
SN detec via.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 15(44)

–

Supernova Neutrino Detection
(6.1) High-Z Detectors : OMNIS

• Observatory for Multiflavor Neutrino Interactions
from Supernova
• Dedicated supernova neutrino detector
• Pb as metal or as perchlorate (with or without Fe)
• Modular structure

Comparison of single- and double-neutron events, per kT of material, no oscillation
Material, event type
CC-νe
CC-ν̄e
NC-νe
NC-ν̄e
NC-νx
Total
Pb, single-n
Pb, double-n
Fe, single-n

59
26
4

0
0
5

8
0
2

37
1
6

677
20
146

781
47
163

Number of detected neutron events versus supernova distance (16 0.5 kT Pb modules)
Distance
0.20 kpc
0.50 kpc
1.0 kpc
2.0 kpc
4.0 kpc
8.0 kpc
16 kpc
6
6
Counts
3×10
0.5×10
112000
27500
6860
1740
440

November 6, 2003
SN detec vib.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 16(44)

–

Supernova Neutrino Detection
(7) LANNDD, 70 kton TPC at WIPP
Liquid Argon Neutrino and Nucleon Deacy Detector in magnetic field

November 6, 2003
SN detec vii.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 17(44)

–

Supernova Neutrino Detection
(8) Summary of SN Neutrino Detectors
Detector

Type

Mass [kT]

Location

Events
at 8 kpc

Status

Flavor

Super-K

WaterČerenkov
Light water
Heavy water
Scintillator
Scintillator
Scintillator
Scintillator
Scintillator
Long String (water)
Liquid Argon

32

Japan

7000

ν̄e

1.0
1.4
1
1
0.3
0.33
0.7
0.4/PMT
2.4

Canada
Italy
Japan
Italy
Russia
USA
South Pole
Italy

450
350
200
300
100
50
200
N/A
200

Running again
for SN by Nov 02
running

2–3
70
600

USA (?)
USA (?)
USA(?)

1000

Japan

SNO
LVD
KamLAND
BOREXINO
Baksan
Mini-BooNe
AMANDA
ICARUS
OMNISS
LANNDD
UNO
Hyper-K

Pb
Liquid Argon
WaterČerenkov
WaterČerenkov

November 6, 2003
SN detec viii.tex

running
running
ready 2003
running
running
running
running (?)

ν̄e
all
ν̄e
ν̄e
ν̄e
ν̄e
ν̄e
ν̄e
νe

>1000
6000
105

proposed
proposed
proposed

all
νe
ν̄e

105

proposed
2009

ν̄e

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 18(44)

–

Underground Physics
and
Underground Laboratories

November 6, 2003
UG phys.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 19(44)

–

What Drives Physics Underground (i) ?

• High-energy cosmic rays (CR)
☞ mostly protons (80%), up to iron (∼1%)
☞ energy interval 106 eV – 1020 eV
☞ origin and composition not fully known
☞ collide with atmosphere atoms at 10 – 20
km
• In the atmosphere CR produce an
Extensive Air Shower (EAS)
☞ may contain millions of particles
☞ hadrons (pions, kaons), electrons,
positrons, fotons, muons
• CR flux at the sea level

∼5 × 109 m−2y−1

November 6, 2003
air shower i.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 20(44)

–

What Drives Physics Underground (ii) ?

• Electrons, positrons, fotons and hadrons
stop into the surface
• Muons may penetrate extremely deep
underground
☞ direct background from primary muons
☞ secondary background from spallation
reaction products (neutrons)
• Needs hundreds of metres of rock for
shielding

☞ Underground Laboratories

November 6, 2003
air shower ii.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 21(44)

World’s Underground Laboratories
mines and tunnels

The three largest underground laboratories at the moment are

• Kamioka, Japan ☞ Measurement of atmospheric neutrinos
• SNO, Sudbury, Canada ☞ Measurement of solar neutrinos
• Gran Sasso, Italy

November 6, 2003
underground labs i.tex

–

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 22(44)

The Kamioka Observatory
• Mozumi mine of the Kamioka Mining Company
☞ mine not active
☞ laboratory started 1983

• Experiments
- KamiokaNDE
- Super-Kamiokande
- KamLAND
• The Depth :
1000 m (2700 mwe)

November 6, 2003
./kirjasto/underground labs/SK lab i.tex

–

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 23(44)

–

KamiokaNDE and Super-Kamiokande Experiment
Kamioka Nucleon Decay Experiment

•
•
•
•
•
•

to search for proton decay
observed also Solar, atmospheric, and supernova neutrinos
water Čerenkov detector ∼1.3 ktons of H2O
Solar and atmospheric neutrinos
oscillation of accelerator-produced neutrinos : K2K
Japan - USA collaboration

•
•
•
•
•

water Čerenkov detector ∼22 ktons of H2O
uses elastic scattering of electrons (ES)
PMTs : 11146 20-inch, 1885 8-inch
the muon rate at SK: 1.88 Hz (reduced by ∼10−5)
located 1000 m underground (2700 mwe)

November 6, 2003
./kirjasto/underground labs/SK lab ii.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 24(44)

–

Super-Kamiokande measurement of Solar 8B and hep neutrinos
[S. Fukuda et al., (SK-Collaboration), PRL86(2001)5651]

• SK-detector sensitive only electron neutrinos
• measurement time : 1258 days (May 1996 – Sept. 2000)
• analysis threshold 6.5 MeV (280 days) and 5.0 MeV (978 days)
☞ only high-energy solar neutrinos
• Solar neutrino events : 18464 ± 204(stat) +646
−556 (syst)
☞ Φe = 2.32 ± 0.03(stat)
= Flux at 1 AU
☞ 45.1 ± 0.5(stat)

+0.08
−0.07 (syst)

+1.6
−1.4 (syst)

%

× 106

cm−2 · s−1

of the BP2000 SMM prediction

• measured also the hep-neutrino flux, the day-versus-night flux asymmetry, and the
seasonal dependence of the flux

November 6, 2003
SK i.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 25(44)

–

The Sudbury Neutrino Observatory (SNO)
• Laboratory works in the active mine
☞ large centre of mining operation
☞ owned by INCO Ltd
• The Depth : 2073 m (6010 mwe)
• Access by elevator and narrow tunnel

November 6, 2003
./kirjasto/underground labs/SNO lab i.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 26(44)

–

The SNO Detector

•
•
•
•
•
•

November 6, 2003
./kirjasto/underground labs/SNO lab ii.tex

imaging water Čerenkov detector
diameter 12 m
1000 tons of ultrapure D20
ultrapure H20 shield
9450 20-cm PMTs

∼55% of the light produced 7 m of the centre
of the detector will strike a PMT

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 27(44)

–

SNO measurement of Solar 8B neutrinos
[Q.R. Ahmad et al., (SNO-Collaboration), PRL87(2001)071301-1,
Q.R. Ahmad et al., (SNO-Collaboration), PRL89(2002)011301-2]

• SNO detected Solar 8B neutrinos via three reaction
- the CC reaction −→ νe
- the NC reaction −→ νx (x = e,µ,τ )
- the ES of electrons −→ νx (x = e,µ,τ )

☞ i.e. SNO can measure all neutrino flavors
• measurement time : 306 days (Nov. 1999 – May 2001)
• Solar neutrino events : 1967.7 +61.9
for CC events
−60.9
+49.5
576.5 −48.9
for NC events
+26.4
263.6 −25.6
for ES events
• 8B Fluxes at 1 AU :
☞ Φe = 1.76 ± 0.05(stat) ± 0.09(syst) × 106

cm−2 · s−1

6
☞ Φµτ = 3.41 ± 0.45(stat) +0.48
cm−2 · s−1
−0.45 (syst) × 10
• agrees well with the SK data
• The SNO measurement is the first direct evidence for neutrino flavor oscillations

November 6, 2003
SNO i.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

Gran Sasso

• the largest underground laboratory
in the world
• locates beside the Gran Sasso tunnel
(10.4 km long) on the highway
connecting Terano and Rome
• three large experimental halls
• maximum depth is 1400 metres
(3800 mwe)
• started 1980’s
• expansion not possible

November 6, 2003
./kirjasto/underground labs/GS lab i.tex

– 28(44)

–

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

Gran Sasso Experiment
• Finished and running experiments
- Solar neutrinos ☞ Gallex, GNO
- supernova neutrinos ☞ LVD
- atmospheric neutrinos ☞ MACRO
- dark matter ☞ DAMA, CRESST, HDMS
- ββ decay ☞ Heidelberg-Moscow
• Future experiments (under cosntruction or proposed)
- GENIUS-TF : ββ decay, dark matter
- CUORICINO : ββ decay
- OPERA : neutrino oscillation
- BOREXINO, LENS : Solar neutrinos
- MONOLITH : atmospheric neutrinos

November 6, 2003
./kirjasto/underground labs/GS lab ii.tex

– 29(44)

–

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

Pyhäsalmi mine

November 6, 2003

– 30(44)

–

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 31(44)

–

Advantages of the Pyhäsalmi mine

• DEEP
The mine is the deepest operational base-metal
mine in Euroopa
• DISTANCE
A long way to CERN (2288 km)
• VOLUME
New spaces and caverns can be excavated
• INFRASTRUCTURE
Good traffic conditions around a year, modern
infrastructure
• Very stable bedrock
• Public local support

November 6, 2003

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 32(44)

–

Location & Distances
• in the middle of Finland along the highway
4 (E75)
• between Jyväskylä and Oulu
• the distances are: 165 km to Oulu, 180
km to Jyväskylä and 475 km to Helsinki.

November 6, 2003

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 33(44)

–

Deep Underground Laboratory

• New laboratory can be constructed at ∼ 1440
metres level
(corresponding to ∼ 4000 mwe)
• Lot of space for new large-scale experiments
• Prefeasibility design and study completed
• Estimated cost-estimate ∼ 15 – 20 Meuros

November 6, 2003

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

Supernova 1987A

November 6, 2003
SN1987A o.tex

– 34(44)

–

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 35(44)

–

Supernova 1987A
(1) General Properties

• Astronomical Event of the Decade
• 167000 light years away, in the Large Magellanic Cloud
(5 times further than the average distance expected for a Galactic supernova)

• Only stellar collapse, so far, from which neutrinos have been detected
• 20 neutrinos were detected by Kamiokande and IMB (Irvine Michigan Brookhaven
experiment) detectors (water Čerenkov)
• In addition, some events were also registered at Baksan and Mont Blanc detectors
(liquid scintillator)

November 6, 2003
SN1987A i.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 36(44)

–

Supernova 1987A
(2) Kamiokande II and IMB Detectors

• Both detectors were designed to search for proton decay
• Water Čerenkov detectors, surrounded by PMTs
• Kamiokande at 2700 mwe, IMB at 1570 mwe
• IMB had about 10 times larger mass
☞ it was less sensitivity to low-energy events because of higher background rates
☞ it also had lower efficiency in the collection of Čerenkov light
• Energy threshold in the IMB was about 20 MeV, and in Kamiokande II about 6 MeV

November 6, 2003
SN1987A ii.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 37(44)

–

Supernova 1987A
(3) Measured Neutrino Signal

Event
no.

Time(UT)
on Feb. 23

Energy
[MeV]

Angle
[deg.]

Event
no.

Time(UT)
on Feb. 23

Energy
[MeV]

Angle
[deg.]

K-1
K-2
K-3
K-4
K-5
K-6
K-7
K-8
K-9
K-10
K-11
K-12

7:35:35.00
35.11
35.20
35.32
35.51
35.69
36.54
36.73
36.92
44.22
45.43
47.44

20.0±2.9
13.5±3.2
7.5±2.0
9.2±2.7
12.8±2.9
6.3±1.7
35.4±8.0
21.0±4.2
19.8±3.2
8.6±2.7
13.0±2.6
8.9±1.9

18±18
40±27
108±32
70±30
135±23
68±77
32±16
30±18
38±22
122±30
49±26
91±39

IMB-1
IMB-2
IMB-3
IMB-4
IMB-5
IMB-6
IMB-7
IMB-8

7:35:41.37
41.79
42.02
42.52
42.92
44.06
46.38
46.96

38±7
37±7
28±6
39±7
36±9
36±6
19±5
22±5

80±10
44±15
56±20
65±20
33±15
51±10
42±20
104±20

November 6, 2003
SN1987A iv.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 38(44)

Supernova 1987A
(4) Measured Energy Spectrum

November 6, 2003
SN1987A v.tex

–

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 39(44)

–

Supernova 1987A
(5) What were learned ?

• Pre-existing calculations were in quite good agreement with the observed neutrino
burst
• Quite low statistics, and only ν̄e were detected
☞ important aspects of the theory could not be tested,
for example, most of the energy is believed to be emitted in higher-temperature
muon and tau neutrinos (and anti-neutrinos)
• New limits were obtained, for example, on the mass, charge, magnetic moment,
decay rate, limiting velocity
☞ m(ν̄e) ≤ 15 ∼ 30 eV

November 6, 2003
SN1987A vi.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

Physics

November 6, 2003
phys.tex

– 40(44)

–

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 41(44)

–

What can be Learned from a Galactic Supernova
Neutrino Signal ?

Measurement of flavor, energy, and time spectra of supernova neutrino burst gives
information on

• Neutrino Physics
• Supernova Mechanism (core collapse)
In addition,

• Early alert for astronomers

November 6, 2003
physik i.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 42(44)

–

What will be Learned ?
(1) Neutrino Physics

• Neutrino absolute mass
☞ timo-of-flight delay in [s] compared to a zero-mass particle
∆t(E) = 0.0515(

mν 2
) ·D
E

mν is the mass of a neutrino flavor in [eV], E is the energy in [MeV], and D is
the distance in [kpc]
- look for: flavor-dependent delay
energy-dependent time spread

• Neutrino oscillation
☞ flavor transformation inside the core (or in the Earth)
- look for: spectral distortions (non-equal Luminosities)
high-energy νe , ν̄e

• Always at least some core-collapse model dependence
November 6, 2003
physik ii.tex

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

– 43(44)

What will be Learned ?
(2) Supernova Core Collapse Physics

• Information about
☞ explosion mechanism, supernova evolution in time
☞ convection, hydrodynamic instabilities
☞ proto neutron-star EoS
☞ black hole formation mechanism
☞ neutrino roles in r-process (in production of heavy nuclei)
• Signatures
☞ risetime
☞ breakout, luminosity cutoff (☞ black hole formation)
☞ pulsation
☞ cooling
by measuring flavor, energy, and time structure of the neutrino burst
• Requires knowledge on neutrino properties

November 6, 2003
physik iii.tex

–

Nuclear Astrophysics 2003 / Neutrino AstroPhysics

What will be learned ?
(3) The Early Alert

November 6, 2003
physik iv.tex

– 44(44)

–

MODERN COSMOLOGY

Studies in High Energy Physics, Cosmology and Gravitation
Other books in the series
Electron–Positron Physics at the Z
M G Green, S L Lloyd, P N Ratoff and D R Ward
Non-accelerator Particle Physics
Paperback edition
H V Klapdor-Kleingrothaus and A Staudt
Ideas and Methods of Supersymmetry and Supergravity
or A Walk Through Superspace
Revised edition
I L Buchbinder and S M Kuzenko
Pulsars as Astrophysical Laboratories for Nuclear and Particle Physics
F Weber
Classical and Quantum Black Holes
Edited by P Fré, V Gorini, G Magli and U Moschella
Particle Astrophysics
Revised paperback edition
H V Klapdor-Kleingrothaus and K Zuber
The World in Eleven Dimensions
Supergravity, Supermembranes and M-Theory
Edited by M J Duff
Gravitational Waves
Edited by I Ciufolini, V Gorini, U Moschella and P Fré

MODERN COSMOLOGY
Edited by
Silvio Bonometto
Department of Physics,
University of Milan—Bicocca, Milan

Vittorio Gorini and Ugo Moschella
Department of Chemical, Mathematical and Physical Sciences,
University of Insubria at Como

I NSTITUTE OF P HYSICS P UBLISHING
B RISTOL AND P HILADELPHIA

c IOP Publishing Ltd 2002
All rights reserved. No part of this publication may be reproduced, stored
in a retrieval system or transmitted in any form or by any means, electronic,
mechanical, photocopying, recording or otherwise, without the prior permission
of the publisher. Multiple copying is permitted in accordance with the terms
of licences issued by the Copyright Licensing Agency under the terms of its
agreement with the Committee of Vice-Chancellors and Principals.
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library.
ISBN 0 7503 0810 9
Library of Congress Cataloging-in-Publication Data are available

Commissioning Editor: James Revill
Production Editor: Simon Laurenson
Production Control: Sarah Plenty
Cover Design: Victoria Le Billon
Marketing Executive: Laura Serratrice
Published by Institute of Physics Publishing, wholly owned by The Institute of
Physics, London
Institute of Physics Publishing, Dirac House, Temple Back, Bristol BS1 6BE, UK
US Office: Institute of Physics Publishing, The Public Ledger Building, Suite
1035, 150 South Independence Mall West, Philadelphia, PA 19106, USA
Typeset in LATEX 2" by Text 2 Text, Torquay, Devon
Printed in the UK by MPG Books Ltd, Bodmin, Cornwall

Contents

Preface
1

2

The physics of the early universe (an overview)
Silvio Bonometto
1.1 The physics of the early universe: an overview
1.1.1 The middle-age cosmology
1.1.2 Inflationary theories
1.1.3 Links between cosmology and particle physics
1.1.4 Basic questions and tentative answers
An introduction to the physics of cosmology
John A Peacock
2.1 Aspects of general relativity
2.1.1 The equivalence principle
2.1.2 Applications of gravitational time dilation
2.2 The energy–momentum tensor
2.2.1 Relativistic fluid mechanics
2.3 The field equations
2.3.1 Newtonian limit
2.3.2 Pressure as a source of gravity
2.3.3 Energy density of the vacuum
2.4 The Friedmann models
2.4.1 Cosmological coordinates
2.4.2 The redshift
2.4.3 Dynamics of the expansion
2.4.4 Solutions to the Friedmann equation
2.4.5 Horizons
2.4.6 Observations in cosmology
2.4.7 The meaning of an expanding universe
2.5 Inflationary cosmology
2.5.1 Inflation field dynamics
2.5.2 Ending inflation
2.5.3 Relic fluctuations from inflation

xiii
1
1
2
4
6
7
9
9
11
12
13
14
16
16
17
17
19
19
21
22
24
27
27
29
32
34
36
38

Contents

vi

2.6

2.7

2.8

3

2.5.4 Gravity waves and tilt
2.5.5 Evidence for vacuum energy at late times
2.5.6 Cosmic coincidence
Dynamics of structure formation
2.6.1 Linear perturbations
2.6.2 Dynamical effects of radiation
2.6.3 The peculiar velocity field
2.6.4 Transfer functions
2.6.5 The spherical model
Quantifying large-scale structure
2.7.1 Fourier analysis of density fluctuations
2.7.2 The CDM model
2.7.3 Karhunen–Loève and all that
2.7.4 Projection on the sky
2.7.5 Nonlinear clustering: a problem for CDM?
2.7.6 Real-space and redshift-space clustering
2.7.7 The state of the art in LSS
2.7.8 Galaxy formation and biased clustering
Cosmic background fluctuations
2.8.1 The hot big bang and the microwave background
2.8.2 Mechanisms for primary fluctuations
2.8.3 The temperature power spectrum
2.8.4 Large-scale fluctuations and CMB power spectrum
2.8.5 Predictions of CMB anisotropies
2.8.6 Geometrical degeneracy
2.8.7 Small-scale data and outlook
References

Cosmological models
George F R Ellis
3.1 Introduction
3.1.1 Spacetime
3.1.2 Field equations
3.1.3 Matter description
3.1.4 Cosmology
3.2 1 + 3 covariant description: variables
3.2.1 Average 4-velocity of matter
3.2.2 Kinematic quantities
3.2.3 Matter tensor
3.2.4 Electromagnetic field
3.2.5 Weyl tensor
3.3 1 + 3 Covariant description: equations
3.3.1 Energy–momentum conservation equations
3.3.2 Ricci identities

40
42
43
47
47
50
53
54
57
58
59
61
63
68
72
74
76
81
86
86
88
90
93
95
97
100
104
108
108
109
109
110
111
112
112
113
114
114
115
115
115
116

Contents

4

vii

3.3.3 Bianchi identities
3.3.4 Implications
3.3.5 Shear-free dust
3.4 Tetrad description
3.4.1 General tetrad formalism
3.4.2 Tetrad formalism in cosmology
3.4.3 Complete set
3.5 Models and symmetries
3.5.1 Symmetries of cosmologies
3.5.2 Classification of cosmological symmetries
3.6 Friedmann–Lemaı̂tre models
3.6.1 Phase planes and evolutionary paths
3.6.2 Spatial topology
3.6.3 Growth of inhomogeneity
3.7 Bianchi universes (s = 3)
3.7.1 Constructing Bianchi universes
3.7.2 Dynamical systems approach
3.7.3 Isotropization properties
3.8 Observations and horizons
3.8.1 Observational variables and relations: FL models
3.8.2 Particle horizons and visual horizons
3.8.3 Small universes
3.8.4 Observations in anisotropic and inhomogeneous models
3.8.5 Proof of almost-FL geometry
3.8.6 Importance of consistency checks
3.9 Explaining homogeneity and structure
3.9.1 Showing initial conditions are irrelevant
3.9.2 The explanation of initial conditions
3.9.3 The irremovable problem
3.10 Conclusion
References

119
120
120
121
121
123
124
124
124
127
130
131
131
132
132
133
134
138
139
139
141
141
142
143
146
146
147
150
153
154
154

Inflationary cosmology and creation of matter in the universe
Andrei D Linde
4.1 Introduction
4.2 Brief history of inflation
4.2.1 Chaotic inflation
4.3 Quantum fluctuations in the inflationary universe
4.4 Quantum fluctuations and density perturbations
4.5 From the big bang theory to the theory of eternal inflation
4.6 (P)reheating after inflation
4.7 Conclusions
References

159
159
160
161
164
168
169
172
183
183

viii
5

6

7

Contents
Dark matter and particle physics
Antonio Masiero and Silvia Pascoli
5.1 Introduction
5.2 The SM of particle physics
5.2.1 The Higgs mechanism and vector boson masses
5.2.2 Fermion masses
5.2.3 Successes and difficulties of the SM
5.3 The dark matter problem: experimental evidence
5.4 Lepton number violation and neutrinos as HDM candidates
5.4.1 Experimental limits on neutrino masses
5.4.2 Neutrino masses in the SM and beyond
5.4.3 Thermal history of neutrinos
5.4.4 HDM and structure formation
5.5 Low-energy SUSY and DM
5.5.1 Neutralinos as the LSP in SUSY models
5.5.2 Neutralinos in the minimal supersymmetric SM
5.5.3 Thermal history of neutralinos and CDM
5.5.4 CDM models and structure formation
5.6 Warm dark matter
5.6.1 Thermal history of light gravitinos and WDM models
5.7 Dark energy, CDM and xCDM or QCDM
5.7.1 CDM models
5.7.2 Scalar field cosmology and quintessence
References

186
186
188
189
191
192
192
194
194
195
196
197
198
198
199
200
202
203
203
204
205
206
207

Supergravity and cosmology
Renata Kallosh
6.1 M/string theory and supergravity
6.2 Superconformal symmetry, supergravity and cosmology
6.3 Gravitino production after inflation
6.4 Super-Higgs effect in cosmology
6.5 MP → ∞ limit
References

211
211
212
215
216
217
218

The cosmic microwave background
Arthur Kosowsky
7.1 A brief historical perspective
7.2 Physics of temperature fluctuations
7.2.1 Causes of temperature fluctuations
7.2.2 A formal description
7.2.3 Tight coupling
7.2.4 Free-streaming
7.2.5 Diffusion damping
7.2.6 The resulting power spectrum
7.3 Physics of polarization fluctuations

219
220
222
223
224
226
227
227
228
229

Contents

7.4

7.5

7.6

7.7
8

7.3.1 Stokes parameters
7.3.2 Thomson scattering and the quadrupolar source
7.3.3 Harmonic expansions and power spectra
Acoustic oscillations
7.4.1 An oscillator equation
7.4.2 Initial conditions
7.4.3 Coherent oscillations
7.4.4 The effect of baryons
Cosmological models and constraints
7.5.1 A space of models
7.5.2 Physical quantities
7.5.3 Power spectrum degeneracies
7.5.4 Idealized experiments
7.5.5 Current constraints and upcoming experiments
Model-independent cosmological constraints
7.6.1 Flatness
7.6.2 Coherent acoustic oscillations
7.6.3 Adiabatic primordial perturbations
7.6.4 Gaussian primordial perturbations
7.6.5 Tensor or vector perturbations
7.6.6 Reionization redshift
7.6.7 Magnetic fields
7.6.8 The topology of the universe
Finale: testing inflationary cosmology
References

Dark matter search with innovative techniques
Andrea Giuliani
8.1 CDM direct detection
8.1.1 Status of the DM problem
8.1.2 Neutralinos
8.1.3 The galactic halo
8.1.4 Strategies for WIMP direct detection
8.2 Phonon-mediated particle detection
8.2.1 Basic principles
8.2.2 The energy absorber
8.2.3 Phonon sensors
8.3 Innovative techniques based on phonon-mediated devices
8.3.1 Basic principles of double readout detectors
8.3.2 CDMS, EDELWEISS and CRESST experiments
8.3.3 Discussion of the CDMS results
8.4 Other innovative techniques
References

ix
230
231
232
234
235
236
237
238
239
239
241
242
244
247
251
252
254
254
255
255
257
257
257
258
261
264
264
264
265
266
267
271
272
272
273
273
273
274
276
279
280

x
9

Contents
Signature for signals from the dark universe
The DAMA Collaboration
9.1 Introduction
9.2 The highly radiopure ∼100 kg NaI(Tl) set-up
9.3 Investigation of the WIMP annual modulation signature
9.3.1 Results of the model-independent approach
9.3.2 Main points on the investigation of possible systematics
in the new DAMA/NaI-3 and 4 running periods
9.3.3 Results of a model-dependent analysis
9.4 DAMA annual modulation result versus CDMS exclusion plot
9.5 Conclusion
References

282
282
285
286
286
287
290
292
294
295

10 Neutrino oscillations: a phenomenological overview
GianLuigi Fogli
10.1 Introduction
10.2 Three-neutrino mixing and oscillations
10.3 Analysis of the atmospheric data
10.4 Analysis of the solar data
10.4.1 Total rates and expectations
10.4.2 Two-flavour oscillations in vacuum
10.4.3 Two-flavour oscillations in matter
10.4.4 Three-flavour oscillations in matter
10.5 Conclusions
References

296
296
297
298
302
302
305
305
308
309
311

11 Highlights in modern observational cosmology
Piero Rosati
11.1 Synopsis
11.2 The cosmological framework
11.2.1 Friedmann cosmological background
11.2.2 Observables in cosmology
11.2.3 Applications
11.3 Galaxy surveys
11.3.1 Overview
11.3.2 Survey strategies and selection methods
11.3.3 Galaxy counts and evolution
11.3.4 Colour selection techniques
11.3.5 Star formation history in the universe
11.4 Cluster surveys
11.4.1 Clusters as cosmological probes
11.4.2 Cluster search methods
11.4.3 Determining m and 
References

312
312
312
313
314
317
321
321
322
325
328
331
334
334
337
339
342

Contents
12 Clustering in the universe: from highly nonlinear structures to
homogeneity
Luigi Guzzo
12.1 Introduction
12.2 The clustering of galaxies
12.3 Our distorted view of the galaxy distribution
12.4 Is the universe fractal?
12.4.1 Scaling laws
12.4.2 Observational evidences
12.4.3 Scaling in Fourier space
12.5 Do we really see homogeneity?
Variance on ∼1000h −1 Mpc scales
12.5.1 The REFLEX cluster survey
12.5.2 ‘Peaks and valleys’ in the power spectrum
12.6 Conclusions
References

xi

344
344
344
347
353
353
355
357
359
359
361
363
364

13 The debate on galaxy space distribution: an overview
Marco Montuori and Luciano Pietronero
13.1 Introduction
13.2 The standard approach of clustering correlation
13.3 Criticisms of the standard approach
13.4 Mass–length relation and conditional density
13.5 Homogeneous and fractal structure
13.6 ξ(r ) for a fractal structure
13.7 Galaxy surveys
13.7.1 Angular samples
13.7.2 Redshift samples
13.8 (r ) analysis
13.9 Interpretation of standard results
References

367
367
367
368
369
369
369
370
371
371
372
374
376

14 Gravitational lensing
Philippe Jetzer
14.1 Introduction
14.1.1 Historical remarks
14.2 Lens equation
14.2.1 Point-like lenses
14.2.2 Thin lens approximation
14.2.3 Lens equation
14.2.4 Remarks on the lens equation
14.3 Simple lens models
14.3.1 Axially symmetric lenses
14.3.2 Schwarzschild lens
14.3.3 Singular isothermal sphere

378
378
379
381
381
383
384
386
390
390
393
395

Contents

xii

14.3.4 Generalization of the singular isothermal sphere
14.3.5 Extended source
14.3.6 Two point-mass lens
14.4 Galactic microlensing
14.4.1 Introduction
14.5 The lens equation in cosmology
14.5.1 Hubble constant from time delays
14.6 Galaxy clusters as lenses
14.6.1 Weak lensing
14.6.2 Comparison with results from x-ray observations
References
15 Numerical simulations in cosmology
Anatoly Klypin
15.1 Synopsis
15.2 Methods
15.2.1 Introduction
15.2.2 Equations of evolution of fluctuations in an expanding
universe
15.2.3 Initial conditions
15.2.4 Codes
15.2.5 Effects of resolution
15.2.6 Halo identification
15.3 Spatial and velocity biases
15.3.1 Introduction
15.3.2 Oh, bias, bias
15.3.3 Spatial bias
15.3.4 Velocity bias
15.3.5 Conclusions
15.4 Dark matter halos
15.4.1 Introduction
15.4.2 Dark matter halos: the NFW and the Moore et al profiles
15.4.3 Properties of dark matter halos
15.4.4 Halo profiles: convergence study
References
Index

396
397
398
398
398
406
409
409
413
416
417
420
420
421
421
423
425
429
433
437
439
439
440
442
447
451
451
451
454
457
462
471
474

Preface

Cosmology is a new science, but cosmological questions are as old as mankind.
Turning philosophical and metaphysical problems into problems that physics can
treat and hopefully solve has been an achievement of the 20th century. The main
contributions have come from the discovery of galaxies and the invention of a
relativistic theory of gravitation. At the edge of the new millennium, in the spring
of 2000, SIGRAV—Società Italiana di Relatività e Gravitazione (Italian Society
of Relativity and Gravitation) and the University of Insubria sponsored a doctoral
school on ‘Relativistic Cosmology: Theory and Observation’, which took place
at the Centre for Scientific Culture ‘Alessandro Volta’, located in the beautiful
environment of Villa Olmo in Como, Italy. This book brings together the reports
of the courses held by a number of outstanding scientists currently working in
various research fields in cosmology. Topics covered range over several different
aspects of modern cosmology from observational matters to advanced theoretical
speculations.
The main financial support for the school came from the University of
Insubria at Como–Varese. Other contributors were the Department of Chemical,
Physical and Mathematical Sciences of the same University, the National Institute
of Nuclear Physics and the Physics Departments of the Universities of Milan,
Turin, Rome La Sapienza and Rome Tor Vergata.
We are grateful to all the members of the scientific organizing committee and
to the scientific coordinator of the Centro Volta, Professor Giulio Casati, for their
invaluable help in the organization. We also acknowledge the essential support
of the secretarial conference staff of the Centro Volta, in particular of Chiara
Stefanetti.
S Bonometto, V Gorini and U Moschella
23 January 2001

xiii

Chapter 1
The physics of the early universe (an
overview)
Silvio Bonometto
Department of Physics, University of Milan–Bicocca, Milan, Italy

1.1 The physics of the early universe: an overview
Modern cosmology has a precise birthdate, Hubble’s discovery of Cepheids and
ordinary stars in Nebulae. The nature of nebulae had been disputed for centuries.
As early as 1755, in his General History of Nature and Theory of the Sky,
Immanuel Kant suggested that nebulae could be galaxies. The main objection
to this hypothesis has been supernovae. Today we know that, close to its peak, a
supernova can exceed the luminosity of its host galaxy. But, while this remained
unknown, single stars as luminous as whole nebulae were a severe objection to
the claim that nebulae were made of as many as hundreds of billions stars. For
instance, in 1893, the British astronomer Mary Clark reported the observation of
two stellar bursts in a single nebula, one 25 years after the other. She wrote that:
The light of the nebula has been practically cancelled by the bursts, which. . .
should have been of an order of magnitude so large, that even our imagination
refuses in conceiving it. Clark was not alone in having problems conceiving the
energetics of supernovae.
After the recognition that most nebulae were galaxies, Hubble also claimed
that they receded from one another, as fragments of a huge explosion. Such an
expansive trend, currently named the Hubble flow, has been confirmed by the
whole present data-set. Although there are no doubts that Hubble’s intuition was
great, the point is that his data-set did not show that much. At the distances where
he pretended to see an expansive trend, the ‘Hubble flow’ is still dominated by
peculiar motions of individual galaxies. Discovering the true nature of nebulae
was, however, essential. It is the galactic scale which sets the boundary above
which dynamical evolution is mostly due to pure gravity. Dissipative forces, of
course, still play an essential role above such a scale. But even the huge x-ray
1

2

The physics of the early universe (an overview)

emission from galaxy clusters, now the principal tool for their detection, bears
limited dynamical effects.
Galaxies, therefore, are the inhabitants of a super-world whose rules are
set by relativistic gravitation. Their average distances are gradually increasing,
within the Hubble flow. The Friedmann equations tell us the ensuing rate of
matter density decrease and how such a rate varies with density itself. No doubts,
then, that the early universe must have been very dense. The cosmic clock, telling
us how long ago density was above a given level, is set by the Hubble constant
H = 100h km s−1 Mpc−1 . Here h conveys our residual ignorance, but it is likely
that 0.6 < h < 0.8, while almost no one suggests that h lies outside the interval
0.5–0.9. (One can appreciate how far from reality Hubble was, considering that
he had estimated that h  5.)
A realistic measure of h came shortly before the discovery of the cosmic
background radiation (CBR). The Friedmann equations could then also determine
how temperature varies with time and it was soon clear that, besides being dense,
the early universe was hot. This defined the early environment and, until the
1980s, modern cosmologists essentially used known physics within the frame
of such exceptional environments. In a sense, this extended Newton’s claim
that the same gravity laws hold on Earth and in the skies. On the basis of
spectroscopical analysis it had already become clear that such a claim could be
extended beyond gravity to the laws governing all physical phenomena, thereby
leading cosmologists to extend these laws back in time, besides far in space.
1.1.1 The middle-age cosmology
This program, essentially based on the use of general relativity, led to great results.
It was shown that, during its early stages, the universe had been homogeneous
and isotropic, apart from tiny fluctuations, seeds of the present inhomogeneities.
Cosmic times (t) can be associated with redshifts (z), which relate the scale factor
a(t) to the present scale factor a0 , through the relation
1 + z = a0 /a(t).
The redshift z also tells us the temperature of the background radiation, which is
T0 (1 + z) (T0  2.73 K is today’s temperature).
On average, linearity held for z > 30–100. For z > 1000, the high-energy
tail of the black body (BB) distribution contained enough photons, with an energy
exceeding BH = 13.6 eV, to keep all baryonic matter ionized. Roughly above the
same redshift, the radiation density exceeds the baryon density. This occurs above
the so-called equivalence redshift z eq = 2.5 × 104b h 2 . Here b is the ratio
between the present density of baryon matter and the present critical density ρcr ,
setting the boundary between parabolic and hyperbolic models. It can be shown
that ρcr = 3H02/8π G.
The relativistic theory of fluctuation growth, developed by Lifshitz, also
showed that, in their linear stages, inhomogeneities would grow proportionally

The physics of the early universe: an overview

3

to (1 + z)−1 , if the content of the universe were assumed to be a single fluid.
This moderate growth rate tells us that the actual inhomogeneities could not arise
from purely statistical fluctuations. When the Lifshitz result was generalized to
any kind of matter contents, it also became clear that fluctuations compatible with
observed anisotropies in the CBR were too small to turn into galaxies, unless
another material component existed, already fully decoupled from radiation at
z  1000, besides baryons.
Various hypotheses were then put forward, on the nature of such dark matter,
whose density, today, is c ρcr . (The world is then characterized by an overall
matter density parameter m = c + b .) But, as far as cosmology is concerned,
only the redshift z d when the quanta of dark matter become non-relativistic
matters. Let Md be the mass scale entering the horizon at z d and let us also recall
that the mass scale entering the horizon at z eq = 2.5 × 104m h 2 is ∼1016 M .
Early fluctuations, over scales <Md , are fully erased by free-streaming, at the
horizon entry. If one wants to preserve a fluctuation spectrum extending to quite
small scales, it is therefore important for z d to be large.
As far as cosmology is concerned, the nature of dark matter can therefore
be classified according to the minimal size of fluctuations able to survive. If
fluctuations are preserved down to scales well below the galactic scale (Mg ∼
108 –1012 M ), we say that dark matter is cold. If dark matter particles are too
fast, and become non-relativistic only at late times, so that Md > Mg , we say that
dark matter is hot. In principle, in the latter case galaxies could also form, because
of the fragmentation of greater structures in their nonlinear collapse, which, in
general, is not spherically symmetric. But such top–down scenarios were soon
shown not to fit observational data. This is why cold dark matter (CDM) became
a basic ingredient of all cosmological models.
This argument is quite independent from the assumption that m has to
approach unity, in order for the geometry of spatial world sections to be flat.
However, once we accept that CDM exists, the temptation to imagine that m = 1
is great. There is another class of arguments which prevents b from approaching
unity by itself alone. These are related to the early formation of light elements,
like 2 H, 4 He, 7 Li. The study of big-bang nucleosynthesis (BBNS) has shown
that, in order to obtain the observed abundances of light nuclides, we ought
to have b h 2  0.02. BBNS occurred when the temperature of the universe
was between 900 and 60 keV (ν decoupling and the opening of the deuterium
bottleneck, respectively). At even larger temperatures, strongly interacting matter
had to be in the quark–hadron plasma form. Going backwards in time we reach
Tew , when the weak and electromagnetic interactions separated. To go still further
backwards, we need to speculate on physical theories, as experimental data are
lacking. The physics of cosmology, therefore, starts from hydrodynamics and
reaches advanced particle physics. In this book, a review of the physics of
cosmology is provided in the contribution by John Peacock.
All these ages, starting from the quark–hadron transition, through the
era when lepton pairs were abundant, then through BBNS, to arrive at the

4

The physics of the early universe (an overview)

moment when matter became denser than radiation and finally to matter–radiation
decoupling and fluctuation growth, are the so-called middle ages of the world.
Their study, until the 1980s, was the main duty of cosmologists. Not all problems,
of course, were solved then. Moreover, as fresh data flowed in, theoretical
questions evolved. In his contribution Piero Rosati reviews the present status
of observational cosmology, in relation to the most recent data.
The world we observe today is the result of fluctuation growth through linear
and nonlinear stages. The initial simplicity of the model has been heavily polluted
by nonlinear and dissipative physics. Tracing back the initial conditions from data
requires both a theoretical and a numerical effort. In his contribution Anatoly
Klypin presents such numerical techniques, the role of which is becoming more
and more important. Using recent parallel computing programs, it is now possible
to try to reproduce the events leading to the shaping of the universe.
The point, however, is that, once this self-consistent scenario became clear,
cosmology was ready for another leap. Since the 1980s, it has become a new
paradigm within which very high-energy physics could be tested.
1.1.2 Inflationary theories
The world we observe is extremely complex and inhomogeneous. The level of
inhomogeneity gradually decreases when we go to greater scales (on this subject,
see the contribution by Luigi Guzzo; another less shared point of view is exposed
by Marco Montuori and Luciano Pietronero). But only the observations of CBR
show a ‘substance’ close to homogeneity. In spite of this, the driving scheme
of the cosmological quest had been that the present complexity came from an
initial simplicity and much effort has been spent in developing a framework
able to show that this is what truly occurred. When this desire for unity was
fulfilled, cosmologists realized that it had taken them to a deadlock: the conditions
from which the observed world had evidently arisen, which so nicely fulfilled
their intimate expectations, were so exceptional as to require an exceptional
explanation.
This is the starting point of the next chapter of cosmological research, which
started in the 1980s and was made possible by the great achievements of previous
cosmological research. The new quest took two alternative directions. The most
satisfactory possibility occurred if, starting from generic metric conditions, their
eventual evolution necessarily created the exceptional ‘initial conditions’ needed
to give a start to the observed world. An alternative, weaker requirement, was
that, starting from a generic metric, its eventual evolution necessarily created
somewhere the exceptional ‘initial conditions’ needed to give a start to the
observed world.
The basic paradigm for implementing one of such requirement is set by
inflationary theories. The paradoxes such theories are called to justify can be
listed as follows:
(i) Homogeneity and isotropy: apart from tiny fluctuations, whose distribution

The physics of the early universe: an overview

5

is itself isotropic, the conditions holding in the universe, at z > 1000, are
substantially identical anywhere we can observe them. The domain our
observations reach has a size ∼ct0 (c, the speed of light; t0 , the present
cosmic time). This is the size of the regions causally connected today.
At z ∼ 103, the domain causally connected was smaller, just because the
cosmic time was ∼104.5 times smaller than t0 . Let us take a sphere whose
radius is ∼ct0 . Its surface includes ∼1000 regions which were then causally
disconnected one from another. In spite of that, temperature, fluctuation
spectrum, baryon content, etc, were equal anywhere. What made them so?
(ii) Flatness: According to observations, the present matter density parameter
m cannot deviate from unity by more than a factor 10. (Recent observations
on the CBR have reduced such a possible discrepancy further.) But, in order
for m ∼ 0.1 today, we need to fine-tune the initial conditions, at the Planck
time, by 1:1060. To avoid such tuning we can only assume that the spatial
section of the metric is Euclidean. Then it remains as such forever.
(iii) Fluctuation spectrum: Let us assume that it reads:
P(k) = Ak n .
Here k = 2π/L and L are comoving length scales. This spectral shape,
apparently depending on A and n only (spectral amplitude and spectral
index, respectively), tries to minimize the scale dependence. But a fully
scale-independent spectrum is obtained only if n = 1. It can then be shown
that fluctuations on any scale have an identical amplitude when they enter the
horizon. This fully scale-independent spectrum, first introduced by Harrison
and Zel’dovich, approaches all features of the observed large-scale structure
(LSS). How could such fluctuations arise and why did they have such a
spectrum?
Apart from these basic requirements, there are a few other requests such as
the absence of topological monsters that we shall not discuss here.
The scheme of inflationary theories amounts then to seeking a theory
of fundamental interactions which eliminates these paradoxes. The essential
ingredient in achieving such an aim is to prescribe a long period of cosmic
expansion dominated by a false vacuum, rather than by any kind of substance.
Early periods of vacuum dominance are indeed expected, within most elementary
particle theories, and this sets the bridge between fundamental interaction theories
and cosmological requirements.
In this book, inflationary theories and their framework are discussed in detail
by Andrei Linde and George Ellis, and therefore we refrain from treating them
further in this introduction. Let us rather outline what is the overall resulting
scheme. One assumes that, around the Planck time, the universe emerges from
quantum gravity in a chaotic status. Hence, anisotropies, inhomogeneities,
discontinuities, etc, were dominant then.
However, such a variety of initial conditions has nothing to do with the
present observed variety. The universe is indeed anisotropic, inhomogeneous,

6

The physics of the early universe (an overview)

discontinuous, etc, today; and more and more so, as we go to smaller and smaller
scales. But such secondary chaos has nothing to do with the primeval chaos. It is
a kind of moderate chaos that we have reached after passing through intermediate
highly symmetric conditions. The sequence complex → simple → complex had
to run, so that today’s world could arise.
1.1.3 Links between cosmology and particle physics
There are, therefore, at least two fields where the connections between particle
physics and cosmology have grown strong. As we have just outlined, explaining
why and how an inflationary era arose and runs is certainly a duty that
cosmologists and particle physicists have to fulfill together.
In a sense, however, this is a more speculative domain, compared with the
one opened by the need for a dark component. The first idea on the nature of
dark matter was that neutrinos had mass. A neutrino background, similar to the
CBR, must exist, if the universe ever had a temperature above ∼1 MeV. Such a
background would be made by ∼100 neutrinos/cm3, for each neutrino flavour.
It is then sufficient to assume that neutrinos have a mass ∼10–100 eV, to reach
m ∼ 1.
Such an appealing picture, which needs no hypothetical new quanta, but
refers to surely existing particles only, was, however, shown not to hold.
Neutrinos could be hot dark matter, as they become non-relativistic around z eq .
As we have already stated, the top–down scenario, where structures on galactic
scales form thanks to greater structure fragmentation, is widely contradicted by
observations.
This does not mean that massive neutrinos may not have a role in shaping the
present condition of the universe. Models with a mix of cold and hot dark matter
were considered quite appealing until a couple of years ago. Their importance,
today, has somehow faded, owing to recent data on dark energy. Recent data on
the neutrino mass spectrum are reviewed by Gianluigi Fogli in his contribution.
Alternative ideas on the nature of dark matter then came from
supersymmetries. The lightest neutral supersymmetric partner of existing bosons
is likely to be stable. In current literature this particle is often called the
neutralino. There are quite a few parameters, concerning supersymmetries, which
are not deducible from known data and, after all, supersymmetries themselves
have not yet been shown to be viable. However, well within observationally
acceptable values, it is possible for neutralinos to have mass and abundance such
as to yield m ∼ 1.
In their contribution Antonio Masiero and Silvia Pascoli focus on the
interface between particle physics and cosmology, discussing in detail the nature
of CDM. Andrea Giuliani’s paper deals with current work aiming at detecting
dark matter quanta in laboratories and the contribution by Rita Bernabei et al
relates possible evidence for the detection of neutralinos. Various hypotheses
were considered, about dark matter setting. Its distribution may differ from visible

The physics of the early universe: an overview

7

matter, on various scales. By definition, its main interaction, in the present epoch,
occurs via gravity and gravitational lensing is the basic way to trace its presence.
In his contribution Philippe Jetzer reviews the basic pattern to detect dark matter,
over different scales, using the relativistic bending of light rays.
1.1.4 Basic questions and tentative answers
There can be little doubt that the last century has witnessed a change of the context
within which the very word ‘cosmology’ is used. Man has always asked basic
questions, concerning the origin of the world and the nature of things. The only
answers to such questions, for ages, came from metaphysics or religious beliefs.
During the last century, instead, a large number of such questions could be put
into a scientific form and quite a significant number could be answered.
As an example, it is now clear that the universe is evolutionary. At the
beginning of modern cosmology, models claiming a steady state (SS) of the
universe had been put forward. They have been completely falsified, although
it is now clear that the stationary expansion regime, introduced by SS models,
is not so different from the inflationary expansion regime, needed to make bigbang models self-consistent. Furthermore, if recent measures of the deceleration
parameter are confirmed, we seem to be living today in a phase of accelerated
expansion, quite similar to inflation. It ought to be emphasized that the strength of
the data, supporting this kind of expansion, is currently balanced by the theoretical
prejudices of wise researchers. In fact, an accelerated expansion requires a
desperate fine-tuning of the vacuum energy, which seems to spoil all the beauty
of the inflationary paradigm.
Since Hubble’s hazardous conclusion that the universe was expanding, the
century which has just closed has seen a number of results, initially supported
more by their elegance than by data. The Galilean scheme of experimental
science is not being forgotten, but one must always remember that such a
scheme is far from requiring pure experimental activity. The basic pattern to
physical knowledge is set by the intricate network of observations, experiments
and predictions that the researcher has to base on data, but goes well beyond
them. With the growing complication of current research, the theoretical phase
of scientific thought is acquiring greater and greater weight. During such a stage,
the lead is taken by the same criteria which drove mathematical research to its
extraordinary achievements.
Besides Hubble’s findings, within the cosmological context, we may quote
Peebles’ discovery of the correlation length r0 , based on angular data, which have
recently been shown to allow quite different interpretations. Outside cosmology,
the main example is given by gauge theories, which are now the basic ingredient
of the standard model of fundamental interactions, and were deepened, from 1954
to the early 1970s, only because they were too beautiful not to be true. At least
two other fields of research in fundamental physics are now driven by similar
criteria—supersymmetries and string theories (see the paper by Renata Kallosh).

8

The physics of the early universe (an overview)

While supersymmetries can soon be confirmed, either by the discovery of
neutralinos by passive detectors or at CERN’s new accelerator, string theories
might only find confirmation if signals arriving from the Planck era can be
observed. This might be possible if future analyses of CBR anisotropies and
polarization show the presence of tensor modes. In this book a review of current
procedures for CBR analysis is provided by Arthur Kosowsky.
Also within the cosmological domain, leading criteria linked to aesthetical
categories are now being pursued. However, in this field, the concept of beauty
is often directly connected with ideological prejudices. Questions such as ‘can
the universe tunnel from nothing’ have been asked and replied within precise
physical contexts. It is, however, clear that the ideological charge of such research
is dominant. Moreover, when theoretical results, in this field, are quoted by the
media, the distinction between valid speculations and scientific acquisitions often
fully fades.
But the main question, for physicists, is different. For at least two centuries,
basic mathematics has developed without making reference to experimental
reality. The criterion driving mathematicians to new acquisitions was the
mathematical beauty. Only a tiny part of such mathematical developments then
found a role in physics. Tensor calculus was developed well before Einstein
found a role for it in special and general relativity. Hilbert spaces found a role
in quantum mechanics. Lie groups found a role in gauge theories. But there
are plenty of other chapters of beautiful advanced mathematics which are, as yet,
unexplored by physicists and may remain so forever.
There is, however, no question about that. Mathematics is an intellectual
construction and its advancement is based on intellectual criteria. The problem
arises when physicists begin to use similar criteria to put order in the physical
world. Let us emphasize that this is not new in the history of research. The
Pythagorean school, in ancient Greece, centered its teaching on mathematical
beauty. They also found important physical results, e.g. in acoustics, starting
from their criterion that the world should be a reflection of mathematical purity.
In the ancient world, the views of Pythagoreans were then taken up by the whole
Platonic school, in opposition to the Aristoteleans who thought that the world was
ugly and complicated, so that attempting a quantitative description was in vain.
Even though we now believe that the final word has to be provided by the
experimental data, there is no doubt that theoretical developments, often long and
articulate, are grounded on mathematical beauty. This is true for any field of
physics, of course, but the impact of such criteria in the quest for the origin is
intellectually disturbing. What seems implicit in all this is that the human mind,
for some obscure reason, although in a confused form, owns in itself the basic
categories enabling it to distinguish the truth and to assert what is adherent to
physical reality.
It is not our intention to take a stand on such points. However, we believe that
they should be very present in the mind of all readers, when considering recent
developments in basic physics and modern cosmology.

Chapter 2
An introduction to the physics of cosmology
John A Peacock
Institute for Astronomy, University of Edinburgh, United
Kingdom

In asking me to write on ‘The Physics of Cosmology’, the editors of this book
have placed no restrictions on the material, since the wonderful thing about
modern cosmology is that it draws on just about every branch of physics. In
practice, this chapter attempts to set the scene for some of the later more
specialized topics by discussing the following subjects:
(1)
(2)
(3)
(4)

some cosmological aspects of general relativity,
basics of the Friedmann models,
quantum fields and physics of the vacuum and
dynamics of cosmological perturbations.

2.1 Aspects of general relativity
The aim of general relativity is to write down laws of physics that are valid
descriptions of nature as seen from any viewpoint. Special relativity shares the
same philosophy, but is restricted to inertial frames. The mathematical tool for
the job is the 4-vector; this allows us to write equations that are valid for all
observers because the quantities on either side of the equation will transform in
the same way. We ensure that this is so by constructing physical 4-vectors out of
the fundamental interval
dx µ = (c dt, dx, dy, dz)

µ = 0, 1, 2, 3,

using relativistic invariants such as the the rest mass m and proper time dτ .
For example, defining the 4-momentum P µ = m dx µ /dτ allows an
immediate relativistic generalization of conservation of mass and momentum,
9

10

An introduction to the physics of cosmology

since the equation P µ = 0 reduces to these laws for an observer who sees a
set of slowly-moving particles.
None of this seems to depend on whether or not observers move at constant
velocity. We have in fact already dealt with the main principle of general relativity,
which states that the only valid physical laws are those that equate two quantities
that transform in the same way under any arbitrary change of coordinates. We
may distinguish equations that are covariant—i.e. relate two tensors of the same
rank—and invariants, where contraction of a tensor yields a number that is the
same for all observers:
covariant
P µ = 0
µ
2 2
invariant.
P Pµ = m c
The constancy of the speed of light is an example of this: with dx µ =
(c dt, −dx, −dy, −dz), we have dx µ dx µ = 0.
Before getting too pleased with ourselves, we should ask how we are going
to construct general analogues of 4-vectors. We want general 4-vectors V µ to
transform like dx µ under the adoption of a new set of coordinates x µ :
V

µ

=

∂x µ ν
V .
∂xν

This relation applies for 4-velocity U µ = dx µ /τ , but fails when we try to
differentiate this equation to form the 4-acceleration Aµ = dU µ /dτ :
Aµ=

∂x µ ν
∂2x µ ν
A
+
U .
∂xν
∂τ ∂ x ν

The second term on the right-hand side is zero only when the transformation
coefficients are constants. This is so for the Lorentz transformation, but not in
general.
The need is therefore to be able to remove the effects of such local coordinate
transformations from the laws of physics. Technically, we say that physics should
be invariant under Lorentz group symmetry.
One difficulty with this programme is that general relativity makes no
distinction between coordinate transformations associated with the motion of
the observer and a simple change of variable. For example, we might decide
that henceforth we will write down coordinates in the order (x, y, z, ct) rather
than (ct, x, y, z). General relativity can cope with these changes automatically.
Indeed, this flexibility of the theory is something of a problem: it can sometimes
be hard to see when some feature of a problem is ‘real’, or just an artifact of the
coordinates adopted. People attempt to distinguish this second type of coordinate
change by distinguishing between ‘active’ and ‘passive’ Lorentz transformations;
a more common term for the latter class is gauge transformations.

Aspects of general relativity

11

2.1.1 The equivalence principle
The problem of how to generalize the laboratory laws of special relativity is solved
by using the equivalence principle, in which the physics in the vicinity of freely
falling observers is assumed to be equivalent to special relativity. We can in fact
obtain the full equations of general relativity in this way, in an approach pioneered
by Weinberg (1972). In what follows, Greek indices run from 0 to 3 (spacetime),
Roman from 1 to 3 (spatial). The summation convention on repeated indices of
either type is assumed.
Consider freely falling observers, who erect a special-relativity coordinate
frame ξ µ in their neighbourhood. The equation of motion for nearby particles is
simple:
d2 ξ µ
= 0;
ξ µ = (ct, x, y, z),
dτ 2
i.e. they have zero acceleration, and we have Minkowski spacetime
c2 dτ 2 = ηαβ dξ α dξ β ,
where ηαβ is just a diagonal matrix ηαβ = diag(1, −1, −1, −1). Now suppose
the observers make a transformation to some other set of coordinates x µ . What
results is the perfectly general relation
dξ µ =

∂ξ µ ν
dx ,
∂xν

which, on substitution, leads to the two principal equations of dynamics in general
relativity:
α
β
d2 x µ
µ dx dx
=0
+

αβ
dτ dτ
dτ 2
c2 dτ 2 = gαβ dx α dx β .

At this stage, the new quantities appearing in these equations are defined only in
terms of our transformation coefficients:
µ

∂ x µ ∂ 2ξ ν
∂ξ ν ∂ x α ∂ x β
∂ξ α ∂ξ β
=
ηαβ .
∂xµ ∂xν

αβ =
gµν

This tremendously neat argument effectively uses the equivalence principle
to prove what is often merely assumed as a starting point in discussions of
relativity: that spacetime is governed by Riemannian geometry. There is a metric
tensor, and the gravitational force is to be interpreted as arising from non-zero
derivatives of this tensor.

12

An introduction to the physics of cosmology

The most well-known example of the power of the equivalence principle
is the thought experiment that leads to gravitational time dilation. Consider an
accelerating frame, which is conventionally a rocket of height h, with a clock
mounted on the roof that regularly disgorges photons towards the floor. If the
rocket accelerates upwards at g, the floor acquires a speed v = gh/c in the time
taken for a photon to travel from roof to floor. There will thus be a blueshift in the
frequency of received photons, given by ν/ν = gh/c2 , and it is easy to see that
the rate of reception of photons will increase by the same factor.
Now, since the rocket can be kept accelerating for as long as we like, and
since photons cannot be stockpiled anywhere, the conclusion of an observer on
the floor of the rocket is that in a real sense the clock on the roof is running
fast. When the rocket stops accelerating, the clock on the roof will have gained
a time t by comparison with an identical clock kept on the floor. Finally, the
equivalence principle can be brought in to conclude that gravity must cause the
same effect. Noting that φ = gh is the difference in potential between roof and
floor, it is simple to generalize this to
φ
t
= 2 .
t
c
The same thought experiment can also be used to show that light must be deflected
in a gravitational field: consider a ray that crosses the rocket cabin horizontally
when stationary. This track will appear curved when the rocket accelerates.
2.1.2 Applications of gravitational time dilation
For many purposes, the effects of weak gravitational fields can be dealt with by
bolting gravitational time dilation onto Newtonian physics. One good example is
in resolving the twin paradox (see p 8 of Peacock 1999).
Another nice paradox is the following: Why do distant stars suffer no time
dilation due to their apparently high transverse velocities as viewed from the
frame of the rotating Earth? At cylindrical radius r , a star appears to move at
v = r ω, implying time dilation by a factor   1+r 2 ω2 /2c2 ; this is not observed.
However, in order to maintain the stars in circular orbits, a centripetal acceleration
a = v 2 /r is needed. This is supplied by an apparent gravitational acceleration in
the rotating frame (a ‘non-inertial’ force). The necessary potential is  = r 2 ω2 /2,
so gravitational blueshift of the radiation cancels the kinematic redshift (at least
to order r 2 ). This example captures very well the main philosophy of general
relativity: correct laws of physics should allow us to explain what we see,
whatever our viewpoint.
For a more important practical application of gravitational time dilation,
consider the Sachs–Wolfe effect. This is the dominant source of large-scale
anisotropies in the cosmic microwave background (CMB), which arise from
potential perturbations at last scattering. These have two effects:

The energy–momentum tensor

13

(i) they redshift the photons we see, so that an overdensity cools the background
as the photons climb out, δT /T = δ/c2 ;
(ii) they cause time dilation at the last-scattering surface, so that we seem to
be looking at a younger (and hence hotter) universe where there is an
overdensity.
The time dilation is δt/t = δ/c2 ; since the time dependence of the scale factor
is a ∝ t 2/3 and T ∝ 1/a, this produces the counterterm δT /T = −(2/3)δ/c2.
The net effect is thus one-third of the gravitational redshift:
δ
δT
= 2.
T
3c
This effect was originally derived by Sachs and Wolfe (1967) and bears their
name. It is common to see the first argument alone, with the factor 1/3
attributed to some additional complicated effect of general relativity. However,
in weak fields, general relativistic effects should already be incorporated within
the concept of gravitational time dilation; the previous argument shows that this
is indeed all that is required to explain the full result.

2.2 The energy–momentum tensor
The only ingredient now missing from a classical theory of relativistic gravitation
is a field equation: the presence of mass must determine the gravitational field.
To obtain some insight into how this can be achieved, it is helpful to consider first
the weak-field limit and the analogy with electromagnetism. Suppose we guess
that the weak-field form of gravitation will look like electromagnetism, i.e. that
we will end up working with both a scalar potential φ and a vector potential A
that together give a velocity-dependent acceleration a = −∇φ − Ȧ+v ∧(∇ ∧ A).
Making the usual e/4π0 → Gm substitution would suggest the field equation
∂ ν ∂ν Aµ ≡  Aµ =

4π G µ
J ,
c2

where  is the d’Alembertian wave operator, Aµ = (φ/c, A) is the 4-potential
and J µ = (ρc, j ) is a quantity that resembles a 4-current, whose components are
a mass density and mass flux density. The solution to this equation is well known:

G
[ J µ (x)] 3
µ
d x,
A (r) = 2
c
|r − x|
where the square brackets denote retarded values.
Now, in fact this analogy can be discarded immediately as a theory of
gravitation in the weak-field limit. The problem lies in the vector J µ : what would
the meaning of such a quantity be? In electromagnetism, it describes conservation
of charge via
∂µ J µ = ρ̇ + ∇ · j = 0

14

An introduction to the physics of cosmology

(notice how neatly such a conservation law can be expressed in 4-vector form).
When dealing with mechanics, on the other hand, we have not one conserved
quantity, but four: energy and vector momentum.
The electromagnetic analogy is nevertheless useful, as it suggests that the
source of gravitation might still be mass and momentum: what we need first
is to find the object that will correctly express conservation of 4-momentum.
Informally, what is needed is a way of writing four conservation laws for each
component of P µ . We can clearly write four equations of the previous type in
matrix form:
∂ν T µν = 0.
Now, if this equation is to be covariant, T µν must be a tensor and is known
as the energy–momentum tensor (or sometimes as the stress–energy tensor).
The meanings of its components in words are: T 00 = c2 × (mass density) =
energy density; T 12 = x-component of current of y-momentum etc. From these
definitions, the tensor is readily seen to be symmetric. Both momentum density
and energy flux density are the product of a mass density and a net velocity,
so T 0µ = T µ0 . The spatial stress tensor T i j is also symmetric because any
small volume element would otherwise suffer infinite angular acceleration: any
asymmetric stress acting on a cube of side L gives a couple ∝L 3 , whereas the
moment of inertia is ∝L 5 .
An important special case is the energy–momentum tensor for a perfect fluid.
In matrix form, the rest-frame T µν is given by just diag(c2 ρ, p, p, p) (using the
fact that the meaning of the pressure p is just the flux density of x-momentum in
the x direction etc.). We can bypass the step of carrying out an explicit Lorentz
transformation (which would be rather cumbersome in this case) by the powerful
technique of manifest covariance. The following expression is clearly a tensor
and reduces to the previous rest-frame answer in special relativity:
T µν = (ρ + p/c2)U µ U ν − pg µν .
Thus it must be the general expression for the energy–momentum tensor of a
perfect fluid.
2.2.1 Relativistic fluid mechanics
A nice application of the energy–momentum tensor is to show how it generates
the equations of relativistic fluid mechanics. Given T µν for a perfect fluid, all
that needs to be done is to insert the specific components U µ = γ (c, v) into
the fundamental conservation laws: ∂ T µν /∂ x ν = 0. The manipulation of the
resulting equations is a straightforward exercise. Note that it is immediately clear
that the results will involve the total or convective derivative:
∂
d
≡
+ v · ∇ = γ −1 U µ ∂µ .
dt
∂t

The energy–momentum tensor

15

The idea here is that the changes experienced by an observer moving with the
fluid are inevitably a mixture of temporal and spatial changes. This two-part
derivative arises automatically in the relativistic formulation through the 4-vector
dot product U µ ∂µ , which arises from the 4-divergence of an energy–momentum
tensor containing a term ∝U µ U ν .
The equations that result from unpacking T µν ,ν = 0 in this way have a
µν
familiar physical interpretation. The µ = 1, 2, 3 components of T,ν = 0 give
the relativistic generalization of Euler’s equation for momentum conservation in
fluid mechanics (not to be confused with Euler’s equation in variational calculus):
1
d
v=− 2
(∇ p + ṗv/c2 ),
dt
γ (ρ + p/c2)
and the µ = 0 component gives a generalization of conservation of energy:
d 2
[γ (ρ + p/c2 )] = ṗ/c2 − γ 2 (ρ + p/c2 )∇ · v,
dt
where ṗ ≡ ∂ p/∂t. The meaning of this equation may be made clearer by
introducing one further conservation law: particle number. This is governed by a
4-current having zero 4-divergence:
d µ
J = 0,
dx µ

J µ ≡ nU µ = γ n(c, v).

If we now introduce the relativistic enthalpy w = ρ + p/c2 , then energy
conservation becomes
ṗ
d γw
=
.
dt n
γ nc2
Thus, in steady flow, γ × (enthalpy per particle) is constant.
A very useful general procedure can be illustrated by linearizing the fluid
equations. Consider a small perturbation about each quantity (ρ → ρ + δρ etc)
and subtract the unperturbed equations to yield equations for the perturbations
valid to first order. This means that any higher-order term such as δv · ∇δρ is set
equal to zero. If we take the initial state to have constant density and pressure and
zero velocity, then the resulting equations are simple:
1
∂
δv = −
∇δp
∂t
ρ + p/c2
∂
δρ = − (ρ + p/c2)∇ · δv.
∂t
Now eliminate the perturbed velocity (via the divergence of the first of these
equations minus the time derivative of the second) to yield the wave equation:
  2
∂ρ ∂ δρ
2
∇ δρ −
= 0.
∂ p ∂t 2

16

An introduction to the physics of cosmology

This defines the speed of sound to be cS2 = ∂ p/∂ρ. Notice that, by a fortunate
coincidence, this is exactly the same as is derived from the non-relativistic
equations, although we could not have relied upon this
√ in advance. Thus, the
speed of sound in a radiation-dominated fluid is just c/ 3.

2.3 The field equations
The energy–momentum tensor plausibly plays the role that the charge 4-current
J µ plays in the electromagnetic field equations,  Aµ = µ0 J µ . The tensor on
the left-hand side of the gravitational field equations is rather more complicated.
Weinberg (1972) showed that it is only possible to make one tensor that is linear
in second derivatives of the metric, which is the Riemann tensor:
R

µ

µ

αβγ

µ
∂αβ
∂αγ
µ
σ
=
−
+ σβ γσ α − σµγ βα
.
β
∂x
∂xγ

This tensor gives a covariant description of spacetime curvature. For the field
equations, we need a second-rank tensor to match T µν , and the Riemann tensor
may be contracted to the Ricci tensor R µν , or further to the curvature scalar R:
Rαβ = R µ αβµ

R = Rµ µ = g µν Rµν .
Unfortunately, these definitions are not universally agreed, All authors, however,
agree on the definition of the Einstein tensor G µν :
G µν = R µν − 12 g µν R.
This tensor is what is needed, because it has zero covariant divergence. Since T µν
also has zero covariant divergence by virtue of the conservation laws it expresses,
it therefore seems reasonable to guess that the two are proportional:
G µν = −

8π G µν
T .
c4

These are Einstein’s gravitational field equations, where the correct constant of
proportionality has been inserted. This is obtained by considering the weak-field
limit.
2.3.1 Newtonian limit
The relation between Einstein’s and Newton’s descriptions of gravity involves
taking the limit of weak gravitational fields (φ/c2  1). We also need to consider
a classical source of gravity, with p  ρc2 , so that the only non-zero component
of T µν is T 00 = c2 ρ. Thus, the spatial parts of R µν must be given by
R i j = 12 g i j R.

The field equations

17

Converting this to an equation for R ij , it follows that R = R 00 + 32 R and hence
that
G 00 = G 00 = 2R00 .
Discarding nonlinear terms in the definition of the Riemann tensor leaves
µ

Rαβ

µ
∂αβ
∂αµ
i
=
−
⇒ R00 = −00,i
β
∂x
∂xµ

i
for the case of a stationary field. We have already seen that c2 00
plays the role
of the Newtonian acceleration, so the required limiting expression for G 00 is

G 00 = −

2 2
∇ φ,
c2

and comparison with Poisson’s equation gives us the constant of proportionality
in the field equations.
2.3.2 Pressure as a source of gravity
Newtonian gravitation is modified in the case of a relativistic fluid (i.e. where we
cannot assume p  ρc2 ). It helps to begin by recasting the field equations (this
would also have simplified the previous discussion). Contract the equation using
µ
gµ = 4 to obtain R = (8π G/c4)T . This allows us to write an equation for R µν
directly:
8π G
R µν = − 4 (T µν − 12 g µν T ).
c
Since T = c2 ρ − 3 p, we get a modified Poisson equation:
∇ 2 φ = 4π G(ρ + 3 p/c2).
What does this mean? For a gas of particles all moving at the same speed u, the
effective gravitational mass density is ρ(1 + u 2 /c2 ); thus a radiation-dominated
fluid generates a gravitational attraction twice as strong as one would expect from
Newtonian arguments. In fact, this factor applies also to individual particles and
leads to an interesting consequence. One can turn the argument round by going
to the rest frame of the gravitating mass. We will then conclude that a passing
test particle will exhibit an acceleration transverse to its path greater by a factor
(1 + u 2 /c2 ) than that of a slowly moving particle. This gives an extra factor of
two deflection in the trajectories of photons, which is of critical importance in
gravitational lensing.
2.3.3 Energy density of the vacuum
One consequence of the gravitational effects of pressure that may seem of
mathematical interest only is that a negative-pressure equation of state that

18

An introduction to the physics of cosmology

achieved ρc2 + 3 p < 0 would produce antigravity. Although such a possibility
may seem physically nonsensical, it is in fact one of the most important concepts
in contemporary cosmology. The origin of the idea goes back to the time
when Einstein was first thinking about the cosmological consequences of general
relativity. At that time, the universe was believed to be static—although this was
simply a prejudice, rather than being founded on any observational facts. The
problem of how a uniform distribution of matter could remain static was one
that had faced Newton, and Einstein gave a very simple Newtonian solution. He
reasoned that a static homogeneous universe required both the density, ρ, and
the gravitational potential, , to be constants. This does not solve Poisson’s
equation, ∇ 2  = 4π Gρ, so he suggested that the equation should be changed
to (∇ 2 + λ) = 4π Gρ, where λ is a new constant of nature: the cosmological
constant. Almost as an afterthought, Einstein pointed out that this equation has
the natural relativistic generalization of
G µν + g µν = −

8π G µν
T .
c4

What is the physical meaning of ? In the current form, it represents the
curvature of empty space. The modern approach is to move the  term to the
right-hand side of the field equations. It now looks like the energy–momentum
tensor of the vacuum:
c4 µν
µν
g .
Tvac
=
8π G
How can a vacuum have a non-zero energy density and pressure? Surely these
are zero by definition in a vacuum? What we can be sure of is that the absence
µν
of a preferred frame means that Tvac must be the same for all observers in special
relativity . Now, apart from zero, there is only one isotropic tensor of rank 2:
µν
ηµν . Thus, in order for Tvac to be unaltered by Lorentz transformations, the
only requirement we can have is that it must be proportional to the metric tensor.
Therefore, it is inevitable that the vacuum (at least in special relativity) will have
a negative-pressure equation of state:
pvac = −ρvac c2 .
In this case, ρc2 + 3 p is indeed negative: a positive  will act to cause a largescale repulsion. The vacuum energy density can thus play a crucial part in the
dynamics of the early universe.
It may seem odd to have an energy density that does not change as the
universe expands. What saves us is the peculiar equation of state of the vacuum:
the work done by the pressure is just sufficient to maintain the energy density
constant (see figure 2.1). In effect, the vacuum acts as a reservoir of unlimited
energy, which can supply as much as is required to inflate a given region to any
required size at constant energy density. This supply of energy is what is used
in ‘inflationary’ theories of cosmology to create the whole universe out of almost
nothing.

The Friedmann models

19

Figure 2.1. A thought experiment to illustrate the application of conservation of energy
to the vacuum. If the vacuum density is ρvac then the energy created by withdrawing the
piston by a volume dV is ρvac c2 dV . This must be supplied by work done by the vacuum
pressure pvac dV , and so pvac = −ρvac c2 , as required.

2.4 The Friedmann models
Many of the chapters in this book discuss observational cosmology, assuming a
body of material on standard homogeneous cosmological models. This section
attempts to set the scene by summarizing the key basic features of relativistic
cosmology.
2.4.1 Cosmological coordinates
The simplest possible mass distribution is one whose properties are homogeneous
(constant density) and isotropic (the same in all directions). The first point to
note is that something suspiciously like a universal time exists in an isotropic
universe. Consider a set of observers in different locations, all of whom are at rest
with respect to the matter in their vicinity (these characters are usually termed
fundamental observers). We can envisage them as each sitting on a different
galaxy, and so receding from each other with the general expansion. We can
define a global time coordinate t, which is the time measured by the clocks of
these observers—i.e. t is the proper time measured by an observer at rest with
respect to the local matter distribution. The coordinate is useful globally rather
than locally because the clocks can be synchronized by the exchange of light
signals between observers, who agree to set their clocks to a standard time when,
e.g., the universal homogeneous density reaches some given value. Using this
time coordinate plus isotropy, we already have enough information to conclude
that the metric must take the following form:
c2 dτ 2 = c2 dt 2 − R 2 (t)[ f 2 (r ) dr 2 + g 2 (r ) dψ 2 ].
Here, we have used the equivalence principle to say that the proper time
interval between two distant events would look locally like special relativity to
a fundamental observer on the spot: for them, c2 dτ 2 = c2 dt 2 − dx 2 − dy 2 − dz 2.
Since we use the same time coordinate as they do, our only difficulty is in the
spatial part of the metric: relating their dx etc to spatial coordinates centred on
us.

20

An introduction to the physics of cosmology

Because of spherical symmetry, the spatial part of the metric can be
decomposed into a radial and a transverse part (in spherical polars, dψ 2 =
dθ 2 + sin2 θ dφ 2 ). Distances have been decomposed into a product of a timedependent scale factor R(t) and a time-independent comoving coordinate r . The
functions f and g are arbitrary; however, we can choose our radial coordinate
such that either f = 1 or g = r 2 , to make things look as much like Euclidean
space as possible. Furthermore, we can determine the form of the remaining
function from symmetry arguments.
To get some feeling for the general answer, it should help to think first about
a simpler case: the metric on the surface of a sphere. A balloon being inflated
is a common popular analogy for the expanding universe, and it will serve as a
two-dimensional example of a space of constant curvature. If we call the polar
angle in spherical polars r instead of the more usual θ , then the element of length
on the surface of a sphere of radius R is
dσ 2 = R 2 (dr 2 + sin2 r dφ 2 ).
It is possible to convert this to the metric for a 2-space of constant by the device
of considering an imaginary radius of curvature, R → iR. If we simultaneously
let r → ir , we obtain
dσ 2 = R 2 (dr 2 + sinh2 r dφ 2 ).
These two forms can be combined by defining a new radial coordinate that makes
the transverse part of the metric look Euclidean:


dr 2
2
2
2
2
dσ = R
+ r dφ ,
1 − kr 2
where k = +1 for positive curvature and k = −1 for negative curvature.
An isotropic universe has the same form for the comoving spatial part of its
metric as the surface of a sphere. This is no accident, since it it possible to define
the equivalent of a sphere in higher numbers of dimensions, and the form of the
metric is always the same. For example, a 3-sphere embedded in four-dimensional
Euclidean space would be defined as the coordinate relation x 2 + y 2 + z 2 + w2 =
R 2 . Now define the equivalent of spherical polars and write w = R cos α,
z = R sin α cos β, y = R sin α sin β cos γ , x = R sin α sin β sin γ , where α,
β and γ are three arbitrary angles. Differentiating with respect to the angles gives
a four-dimensional vector (dx, dy, dz, dw), and it is a straightforward exercise to
show that the squared length of this vector is
|(dx, dy, dz, dw)|2 = R 2 [dα 2 + sin2 α(dβ 2 + sin2 β dγ 2 )],
which is the Robertson–Walker metric for the case of positive spatial curvature.
This k = +1 metric describes a closed universe, in which a traveller who sets off

The Friedmann models

21

along a trajectory of fixed β and γ will eventually return to their starting point
(when α = 2π). In this respect, the positively curved 3D universe is identical to
the case of the surface of a sphere: it is finite, but unbounded. By contrast, the
k = −1 metric describes an open universe of infinite extent.
The Robertson–Walker metric (which we shall often write in the shorthand
RW metric) may be written in a number of different ways. The most compact
forms are those where the comoving coordinates are dimensionless. Define the
very useful function

sin r
(k = 1)
Sk (r ) = sinh r (k = −1)
r
(k = 0)
and its cosine-like analogue, Ck (r ) ≡ 1 − k Sk2 (r ). The metric can now be
written in the preferred form that we shall use throughout:
c2 dτ 2 = c2 dt 2 − R 2 (t)[dr 2 + Sk2 (r ) dψ 2 ].
The most common alternative is to use a different definition of comoving distance,
Sk (r ) → r , so that the metric becomes


dr 2
2
2
2 2
2
2
2
c dτ = c dt − R (t)
+ r dψ .
1 − kr 2
There should of course be two different symbols for the different comoving radii,
but each is often called r in the literature, so we have to learn to live with this
ambiguity; the presence of terms like Sk (r ) or 1 − kr 2 will usually indicate
which convention is being used. Alternatively, one can make the scale factor
dimensionless, defining
R(t)
a(t) ≡
,
R0
so that a = 1 at the present.
2.4.2 The redshift
At small separations, where things are Euclidean, the proper separation of two
fundamental observers is just R(t) dr , so that we obtain Hubble’s law, v = H d,
with
Ṙ
H= .
R
At large separations where spatial curvature becomes important, the concept
of radial velocity becomes a little more slippery—but in any case how could one
measure it directly in practice? At small separations, the recessional velocity
gives the Doppler shift
νemit
v
≡1+z 1+ .
νobs
c

22

An introduction to the physics of cosmology

This defines the redshift z in terms of the shift of spectral lines. What is the
equivalent of this relation at larger distances? Since photons travel on null
geodesics of zero proper time, we see directly from the metric that

c dt
.
r=
R(t)
The comoving distance is constant, whereas the domain of integration in time
extends from temit to tobs; these are the times of emission and reception of a
photon. Photons that are emitted at later times will be received at later times,
but these changes in temit and tobs cannot alter the integral, since r is a comoving
quantity. This requires the condition dtemit /dtobs = R(temit )/R(tobs ), which
means that events on distant galaxies time dilate according to how much the
universe has expanded since the photons we see now were emitted. Clearly (think
of events separated by one period), this dilation also applies to frequency, and we
therefore get
R(tobs )
νemit
.
≡1+z =
νobs
R(temit )
In terms of the normalized scale factor a(t) we have simply a(t) = (1 +
z)−1 . Photon wavelengths therefore stretch with the universe, as is intuitively
reasonable.
2.4.3 Dynamics of the expansion
The equation of motion for the scale factor can be obtained in a quasi-Newtonian
fashion. Consider a sphere about some arbitrary point, and let the radius be
R(t)r , where r is arbitrary. The motion of a point at the edge of the sphere
will, in Newtonian gravity, be influenced only by the interior mass. We can
therefore write down immediately a differential equation (Friedmann’s equation)
that expresses conservation of energy: ( Ṙr )2 /2 − G M/(Rr ) = constant. The
Newtonian result that the gravitational field inside a uniform shell is zero does still
hold in general relativity, and is known as Birkhoff’s theorem. General relativity
becomes even more vital in giving us the constant of integration in Friedmann’s
equation:
8π G
ρ R 2 = −kc2 .
Ṙ 2 −
3
Note that this equation covers all contributions to ρ, i.e. those from matter,
radiation and vacuum; it is independent of the equation of state.
For a given rate of expansion, there is thus a critical density that will yield
k = 0, making the comoving part of the metric look Euclidean:
ρc =

3H 2
.
8π G

A universe with a density above this critical value will be spatially closed,
whereas a lower-density universe will be spatially open.

The Friedmann models

23

The ‘flat’ universe with k = 0 arises for a particular critical density. We
are therefore led to define a density parameter as the ratio of density to critical
density:
ρ
8π Gρ
≡
=
.
ρc
3H 2
Since ρ and H change with time, this defines an epoch-dependent density
parameter. The current value of the parameter should strictly be denoted by 0 .
Because this is such a common symbol, we shall keep the formulae uncluttered
by normally dropping the subscript; the density parameter at other epochs will be
denoted by (z). The critical density therefore just depends on the rate at which
the universe is expanding. If we now also define a dimensionless (current) Hubble
parameter as
H0
,
h≡
100 km s−1 Mpc−1
then the current density of the universe may be expressed as
ρ0 = 1.88 × 10−26h 2 kg m−3
= 2.78 × 1011h 2 M Mpc−3 .
A powerful approximate model for the energy content of the universe is to
divide it into pressureless matter (ρ ∝ R −3 ), radiation (ρ ∝ R −4 ) and vacuum
energy (ρ constant). The first two relations just say that the number density
of particles is diluted by the expansion, with photons also having their energy
reduced by the redshift; the third relation applies for Einstein’s cosmological
constant. In terms of observables, this means that the density is written as
8π Gρ
= H02(v + m a −3 + r a −4 )
3
(introducing the normalized scale factor a = R/R0 ). For some purposes, this
separation is unnecessary, since the Friedmann equation treats all contributions to
the density parameter equally:
kc2
= m (a) + r (a) + v (a) − 1.
H 2 R2
Thus, a flat k = 0 universe requires
i = 1 at all times, whatever the
form of the contributions to the density, even if the equation of state cannot be
decomposed in this simple way.
Lastly, it is often necessary to know the present value of the scale factor,
which may be read directly from the Friedmann equation:
R0 =

c
[( − 1)/k]−1/2 .
H0

The Hubble constant thus sets the curvature length, which becomes infinitely
large as  approaches unity from either direction.

24

An introduction to the physics of cosmology

2.4.4 Solutions to the Friedmann equation
The Friedmann equation may be solved most simply in ‘parametric’ form, by
recasting it in terms of the conformal time dη = c dt/R (denoting derivatives
with respect to η by primes):
R2 =

8π G
ρ R4 − k R2 .
3c2

Because H02 R02 = kc2 /( − 1), the Friedmann equation becomes
a2=

k
[r + m a − ( − 1)a 2 + v a 4 ],
( − 1)

which is straightforward to integrate provided v = 0.
To the observer, the evolution of the scale factor is most directly
characterized by the change with redshift of the Hubble parameter and the density
parameter; the evolution of H (z) and (z) is given immediately by the Friedmann
equation in the form H 2 = 8π Gρ/3 − kc2 /R 2 . Inserting this dependence of ρ
on a gives
H 2(a) = H02[v + m a −3 + r a −4 − ( − 1)a −2].
This is a crucial equation, which can be used to obtain the relation between
redshift and comoving distance. The radial equation of motion for a photon is
R dr = c dt = c dR/ Ṙ = c dR/(R H ). With R = R0 /(1 + z), this gives
R0 dr =

c
c
dz =
dz[(1−)(1+z)2 +v +m (1+z)3 +r (1+z)4]−1/2 .
H (z)
H0

This relation is arguably the single most important equation in cosmology, since it
shows how to relate comoving distance to the observables of redshift, the Hubble
constant and density parameters.
Lastly, using the expression for H (z) with (a) − 1 = kc2 /(H 2 R 2 ) gives
the redshift dependence of the total density parameter:
(z) − 1 =

−1
.
1 −  + v a 2 + m a −1 + r a −2

This last equation is very important. It tells us that, at high redshift, all model
universes apart from those with only vacuum energy will tend to look like the
 = 1 model. If  = 1, then in the distant past (z) must have differed from
unity by a tiny amount: the density and rate of expansion needed to have been
finely balanced for the universe to expand to the present. This tuning of the initial
conditions is called the flatness problem.
The solution of the Friedmann equation becomes more complicated if
we allow a significant contribution from vacuum energy—i.e. a non-zero

The Friedmann models

25

cosmological constant. Detailed discussions of the problem are given by Felten
and Isaacman (1986) and Carroll et al (1992); the most important features are
outlined later.
The Friedmann equation itself is independent of the equation of state, and
just says H 2 R 2 = kc2 /( − 1), whatever the form of the contributions to . In
terms of the cosmological constant itself, we have
v =

8π Gρv
c2
=
.
2
3H
3H 2

With the addition of , the Friedmann equation can only in general be solved
numerically. However, we can find the conditions for the different behaviours
described earlier analytically, at least if we simplify things by ignoring radiation.
The equation in the form of the time-dependent Hubble parameter looks like
H2
H02

= v (1 − a −2 ) + m (a −3 − a −2 ) + a −2 .

This equation allows the left-hand side to vanish, defining a turning point in the
expansion. Vacuum energy can thus remove the possibility of a big bang in which
the scale factor goes to zero. Setting the right-hand side to zero yields a cubic
equation, and it is possible to give the conditions under which this has a solution
(see Felten and Isaacman 1986). The main results of this analysis are summed
up in figure 2.2. Since the radiation density is very small today, the main task of
relativistic cosmology is to work out where on the matter –vacuum plane the real
universe lies. The existence of high-redshift objects rules out the bounce models,
so that the idea of a hot big bang cannot be evaded.
The most important model in cosmological research is that with k = 0 ⇒
total = 1; when dominated by matter, this is often termed the Einstein–de Sitter
model. Paradoxically, this importance arises because it is an unstable state: as
we have seen earlier, the universe will evolve away from  = 1, given a slight
perturbation. For the universe to have expanded by so many e-foldings (factors
of e expansion) and yet still have  ∼ 1 implies that it was very close to being
spatially flat at early times.
It now makes more sense to work throughout in terms of the normalized
scale factor a(t), so that the Friedmann equation for a matter–radiation mix is
ȧ 2 = H02(m a −1 + r a −2 ),
which may be integrated to give the time as a function of scale factor:
H0 t =

2
32m

3/2

r + m a(m a − 2r ) + 2r

;

this goes to 23 a 3/2 for a matter-only model, and to 12 a 2 for radiation only.

26

An introduction to the physics of cosmology

Figure 2.2. This plot shows the different possibilities for the cosmological expansion as
a function of matter density and vacuum energy. Models with total  > 1 are always
spatially closed (open for  < 1), although closed models can still expand to infinity if
v = 0. If the cosmological constant is negative, recollapse always occurs; recollapse is
also possible with a positive v if m  v . If v > 1 and m is small, there is the
possibility of a ‘loitering’ solution with some maximum redshift and infinite age (top left);
for even larger values of vacuum energy, there is no big bang singularity.

One further way of presenting the model’s dependence on time is via the
density. Following this, it is easy to show that

1
(matter domination)
t=
6π Gρ

3
(radiation domination).
t=
32π Gρ
An alternative k = 0 model of greater observational interest has a significant
cosmological constant, so that m + v = 1 (radiation being neglected for
simplicity). The advantage of this model is that it is the only way of retaining the
theoretical attractiveness of k = 0 while changing the age of the universe from
the relation H0t0 = 2/3, which characterizes the Einstein–de Sitter model. Since
much observational evidence indicates that H0t0  1, this model has received
a good deal of interest in recent years. To keep things simple we shall neglect
radiation, so that the Friedmann equation is
ȧ 2 = H02[m a −1 + (1 − m )a 2],

The Friedmann models
and the t (a) relation is



a

H0t (a) =
0

x dx
m x + (1 − m )x 4

27

.

The x 4 on the bottom looks like trouble, but it can be rendered tractable by the
substitution y = x 3 |m − 1|/m , which turns the integral into


−1
3 | − 1|/
a
S
m
m
2 k
H0t (a) =
√
.
3
|m − 1|
Here, k in Sk is used to mean sin if m > 1, otherwise sinh; these are still k = 0
models. Since there is nothing special about the current era, we can clearly also
rewrite this expression as
√

2
2 Sk−1 |m (a) − 1|/m (a)
 m (a)−0.3 ,
H (a)t (a) =
√
3
3
|m (a) − 1|
where we include a simple approximation that is accurate to a few per cent over
the region of interest (m & 0.1). In the general case of significant  but k = 0,
this expression still gives a very good approximation to the exact result, provided
m is replaced by 0.7m − 0.3v + 0.3 (Carroll et al 1992).
2.4.5 Horizons
For photons, the radial equation of motion is just c dt = R dr . How far can a
photon get in a given time? The answer is clearly
 t1
c dt
= η,
r =
R(t)
t0
i.e. just the interval of conformal time. What happens as t0 → 0 in this
expression? We can replace dt by dR/ Ṙ, which the Friedmann equation says
is proportional to dR/ ρ R 2 at early times. Thus, this integral converges if
ρ R 2 → ∞ as t0 → 0, otherwise it diverges. Provided the equation of state
is such that ρ changes faster than R −2 , light signals can only propagate a finite
distance between the big bang and the present; there is then said to be a particle
horizon. Such a horizon therefore exists in conventional big-bang models, which
are dominated by radiation at early times.
2.4.6 Observations in cosmology
We can now assemble some essential formulae for interpreting cosmological
observations. Our observables are the redshift, z, and the angular difference
between two points on the sky, dψ. We write the metric in the form
c2 dτ 2 = c2 dt 2 − R 2 (t)[dr 2 + Sk2 (r ) dψ 2 ],

28

An introduction to the physics of cosmology

so that the comoving volume element is
dV = 4π[R0 Sk (r )]2 R0 dr.
The proper transverse size of an object seen by us is its comoving size dψ Sk (r )
times the scale factor at the time of emission:
d" = dψ R0 Sk (r )/(1 + z).
Probably the most important relation for observational cosmology is that between
monochromatic flux density and luminosity. Start by assuming isotropic
emission, so that the photons emitted by the source pass with a uniform flux
density through any sphere surrounding the source. We can now make a shift
of origin, and consider the RW metric as being centred on the source; however,
because of homogeneity, the comoving distance between the source and the
observer is the same as we would calculate when we place the origin at our
location. The photons from the source are therefore passing through a sphere, on
which we sit, of proper surface area 4π[R0 Sk (r )]2 . But redshift still affects the
flux density in four further ways: photon energies and arrival rates are redshifted,
reducing the flux density by a factor (1 + z)2 ; opposing this, the bandwidth dν is
reduced by a factor 1 + z, so the energy flux per unit bandwidth goes down by
one power of 1 + z; finally, the observed photons at frequency ν0 were emitted at
frequency ν0 (1+z), so the flux density is the luminosity at this frequency, divided
by the total area, divided by 1 + z:
Sν (ν0 ) =

L ν ([1 + z]ν0 )
4π R02 Sk2 (r )(1 + z)

.

The flux density received by a given observer can be expressed by definition
as the product of the specific intensity Iν (the flux density received from unit
solid angle of the sky) and the solid angle subtended by the source: Sν = Iν d.
Combining the angular size and flux–density relations thus gives the relativistic
version of surface-brightness conservation. This is independent of cosmology:
Iν (ν0 ) =

Bν ([1 + z]ν0 )
,
(1 + z)3

where Bν is surface brightness (luminosity emitted into unit solid angle per unit
area of source). We can integrate over ν0 to obtain the corresponding total or
bolometric formulae, which are needed, for example, for spectral-line emission:
Stot =

L tot
2
2
4π R0 Sk (r )(1 +
Itot =

Btot
.
(1 + z)4

z)2

;

The Friedmann models

29

Figure 2.3. A plot of dimensionless angular-diameter distance versus redshift for various
cosmologies. Full curves show models with zero vacuum energy; broken curves show flat
models with m + v = 1. In both cases, results for m = 1, 0.3, 0 are shown; higher
density results in lower distance at high z, due to gravitational focusing of light rays.

The form of these relations lead to the following definitions for particular kinds
of distances:
angular-diameter distance:
luminosity distance:

DA = (1 + z)−1 R0 Sk (r )
DL = (1 + z)R0 Sk (r ).

The angular-diameter distance is plotted against redshift for various models in
figure 2.3.
The last element needed for the analysis of observations is a relation between
redshift and age for the object being studied. This brings in our earlier relation
between time and comoving radius (consider a null geodesic traversed by a photon
that arrives at the present):
c dt = R0 dr/(1 + z).
The general relation between comoving distance and redshift was given earlier as
R0 dr =

c
c
dz =
dz[(1−)(1+z)2 +v +m (1+z)3 +r (1+z)4]−1/2 .
H (z)
H0

2.4.7 The meaning of an expanding universe
Finally, having dealt with some of the formal apparatus of cosmology, it may be
interesting to step back and ask what all this means. The idea of an expanding

30

An introduction to the physics of cosmology

universe can easily lead to confusion, and this section tries to counter some of the
more tenacious misconceptions.
The worst of these is the ‘expanding space’ fallacy. The RW metric written
in comoving coordinates emphasizes that one can think of any given fundamental
observer as fixed at the centre of their local coordinate system. A common
interpretation of this algebra is to say that the galaxies separate ‘because the space
between them expands’ or some such phrase. This suggests some completely new
physical effect that is not covered by Newtonian concepts. However, on scales
much smaller than the current horizon, we should be able to ignore curvature and
treat galaxy dynamics as occurring in Minkowski spacetime; this approach works
in deriving the Friedmann equation. How do we relate this to ‘expanding space’?
It should be clear that Minkowski spacetime does not expand – indeed, the very
idea that the motion of distant galaxies could affect local dynamics is profoundly
anti-relativistic: the equivalence principle says that we can always find a tangent
frame in which physics is locally special relativity.
To clarify the issues here, it should help to consider an explicit example,
which makes quite a neat paradox. Suppose we take a nearby low-redshift galaxy
and give it a velocity boost such that its redshift becomes zero. At a later time,
will the expansion of the universe have cause the galaxy to recede from us, so that
it once again acquires a positive redshift? To idealize the problem, imagine that
the galaxy is a massless test particle in a homogeneous universe.
The ‘expanding space’ idea would suggest that the test particle should indeed
start to recede from us, and it appears that one can prove this formally, as follows.
Consider the peculiar velocity with respect to the Hubble flow, δv. A completely
general result is that this declines in magnitude as the universe expands:
δv ∝

1
.
a(t)

This is the same law that applies to photon energies, and the common link is
that it is particle momentum in general that declines as 1/a, just through the
accumulated Lorentz transforms required to overtake successively more distant
particles that are moving with the Hubble flow. So, at t → ∞, the peculiar
velocity tends to zero, leaving the particle moving with the Hubble flow, however
it started out: ‘expanding space’ has apparently done its job.
Now look at the same situation in a completely different way. If the particle is
nearby compared with the cosmological horizon, a Newtonian analysis should be
valid: in an isotropic universe, Birkhoff’s theorem assures us that we can neglect
the effect of all matter at distances greater than that of the test particle, and all that
counts is the mass between the particle and us. Call the proper separation of the
particle from the origin r . Our initial conditions are that ṙ = 0 at t = t0 , when
r = r0 . The equation of motion is just
r̈ =

−G M(r |t)
,
r2

The Friedmann models

31

and the mass internal to r is just
M(r |t) =

4π 3 4π
ρr =
ρ0 a −3r 3 ,
3
3

where we assume a0 = 1 and a matter-dominated universe. The equation of
motion can now be re-expressed as
r̈ = −

0 H02
r.
2a 3

Adding vacuum energy is easy enough:
r̈ = −

H02
r (m a −3 − 2v ).
2

The −2 in front of the vacuum contribution comes from the effective mass density
ρ + 3 p/c2.
We now show that this Newtonian equation is identical to what is obtained
from δv ∝ 1/a. In our present notation, this becomes
ṙ − H (t)r = −H0r0 /a;
the initial peculiar velocity is just −H r , cancelling the Hubble flow. We can
differentiate this equation to obtain r̈ , which involves Ḣ . This can be obtained
from the standard relation
H 2(t) = H02[v + m a −3 + (1 − m − v )a −2 ].
It is then a straightforward exercise to show that the equation for r̈ is the same as
obtained previously (remembering H = ȧ/a).
Now for the paradox. It will suffice at first to solve the equation for the
case of the Einstein–de Sitter model, choosing time units such that t0 = 1, with
H0t0 = 2/3:
r̈ = −2r/9t 2 .
The acceleration is negative, so the particle moves inwards, in complete apparent
contradiction to our ‘expanding space’ conclusion that the particle would tend
with time to pick up the Hubble expansion. The resolution of this contradiction
comes from the full solution of the equation. The differential equation clearly
has power-law solutions r ∝ t 1/3 or t 2/3 , and the combination with the correct
boundary conditions is
r (t) = r0 (2t 1/3 − t 2/3 ).
At large t, this becomes r = −r0 t 2/3 . This is indeed the equation of motion
of a particle moving with the Hubble flow, but it arises because the particle
has fallen right through the origin and emerged on the other side. In no sense,
therefore, can ‘expanding space’ be said to have operated: in an Einstein–de Sitter

32

An introduction to the physics of cosmology

model, a particle initially at rest with respect to the origin falls towards the origin,
passes through it, and asymptotically regains its initial comoving radius on the
opposite side of the sky. This behaviour can be understood quantitatively using
only Newtonian dynamics.
Two further cases are worth considering. In an empty universe, the equation
of motion is r̈ = 0, so the particle remains at r = r0 , while the universe expands
linearly with a ∝ t. In this case, H = 1/t, so that δv = −H r0, which declines
as 1/a, as required. Finally, models with vacuum energy are of more interest.
Provided v > m /2, r̈ is initially positive, and the particle does move away
from the origin. This is the criterion for q0 < 0 and an accelerating expansion. In
this case, there is a tendency for the particle to expand away from the origin, and
this is caused by the repulsive effects of vacuum energy. In the limiting case of
pure de Sitter space (m = 0, v = 1), the particle’s trajectory is
r = r0 cosh H0(t − t0 ),
which asymptotically approaches half the r = r0 exp H0(t − t0 ) that would have
applied if we had never perturbed the particle in the first place. In the case of
vacuum-dominated models, then, the repulsive effects of vacuum energy cause all
pairs of particles to separate at large times, whatever their initial kinematics; this
behaviour could perhaps legitimately be called ‘expanding space’. Nevertheless,
the effect stems from the clear physical cause of vacuum repulsion, and there
is no new physical influence that arises purely from the fact that the universe
expands. The earlier examples have proved that ‘expanding space’ is in general a
fundamentally flawed way of thinking about an expanding universe.

2.5 Inflationary cosmology
We now turn from classical cosmology to aspects of cosmology in which quantum
processes are important. This is necessary in order to solve the major problems
of the simple big bang:
(1) The expansion problem. Why is the universe expanding at t = 0? This
appears as an initial condition, but surely a mechanism is required to lauch
the expansion?
(2) The flatness problem. Furthermore, the expansion needs to be launched at
just the correct rate, so that is is very close to the critical density, and can
thus expand from perhaps near the Planck era to the present (a factor of over
1030).
(3) The horizon problem. Models in which the universe is radiation dominated
(with a ∝ t 1/2 at early times) have a finite horizon. There is apparently no
causal means for different parts of the universe to agree on the mean density
or rate of expansion.
The list of problems with conventional cosmology provides a strong hint that
the equation of state of the universe may have been very different at very early

Inflationary cosmology

33

times. To solve the horizon problem and allow causal contact over the whole
of the region observed at last scattering requires a universe that expands ‘faster
than light’ near t = 0: R ∝ t α , with α > 1. If such a phase had existed, the
integral for the comoving horizon would have diverged, and there would be no
difficulty in understanding the overall homogeneity of the universe—this could
then be established by causal processes. Indeed, it is tempting to assert that the
observed homogeneity proves that such causal contact must once have occurred.
What condition does this place on the equation of state? In the integral for rH ,
we can replace dt by dR/ Ṙ, which the Friedmann equation says is proportional
to dR/ ρ R 2 at early times. Thus, the horizon diverges provided the equation of
state is such that ρ R 2 vanishes or is finite as R → 0. For a perfect fluid with
p ≡ ( − 1) as the relation between pressure and energy density, we have the
adiabatic dependence p ∝ R −3 , and the same dependence for ρ if the rest-mass
density is negligible. A period of inflation therefore needs
 < 2/3 ⇒ ρc2 + 3 p < 0.
Such a criterion can also solve the flatness problem. Consider the Friedmann
equation,
8π Gρ R 2
Ṙ 2 =
− kc2 .
3
As we have seen, the density term on the right-hand side must exceed the
curvature term by a factor of at least 1060 at the Planck time, and yet a more
natural initial condition might be to have the matter and curvature terms being of
comparable order of magnitude. However, an inflationary phase in which ρ R 2
increases as the universe expands can clearly make the curvature term relatively
as small as required, provided inflation persists for sufficiently long.
We have seen that inflation will require an equation of state with negative
pressure, and the only familiar example of this is the p = −ρc2 relation that
applies for vacuum energy; in other words, we are led to consider inflation as
happening in a universe dominated by a cosmological constant. As usual, any
initial expansion will redshift away matter and radiation contributions to the
density, leading to increasing dominance by the vacuum term. If the radiation
and vacuum densities are initially of comparable magnitude, we quickly reach a
state where the vacuum term dominates. The Friedmann equation in the vacuumdominated case has three solutions:
 sinh H t (k = −1)
R∝

cosh H t
exp H t

(k = +1)
(k = 0),

√
c2 /3 =
8π Gρvac /3; all solutions evolve towards the
where H =
exponential k = 0 solution, known as de Sitter spacetime. Note that H is
not the Hubble parameter at an arbitrary time (unless k = 0), but it becomes

34

An introduction to the physics of cosmology

so exponentially fast as the hyperbolic trigonometric functions tend to the
exponential.
Because de Sitter space clearly has H 2 and ρ in the right ratio for  = 1
(obvious, since k = 0), the density parameter in all models tends to unity as the
Hubble parameter tends to H . If we assume that the initial conditions are not fine
tuned (i.e.  = O(1) initially), then maintaining the expansion for a factor f
produces
 = 1 + O( f −2 ).
This can solve the flatness problem, provided f is large enough. To obtain  of
order unity today requires | − 1| . 10−52 at the Grand Unified Theory (GUT)
epoch, and so ln f & 60 e-foldings of expansion are needed; it will be proved
later that this is also exactly the number needed to solve the horizon problem. It
then seems almost inevitable that the process should go to completion and yield
 = 1 to measurable accuracy today.
2.5.1 Inflation field dynamics
The general concept of inflation rests on being able to achieve a negative-pressure
equation of state. This can be realized in a natural way by quantum fields in the
early universe.
The critical fact we shall need from quantum field theory is that quantum
fields can produce an energy density that mimics a cosmological constant. The
discussion will be restricted to the case of a scalar field φ (complex in general, but
often illustrated using the case of a single real field). The restriction to scalar fields
is not simply for reasons of simplicity, but because the scalar sector of particle
physics is relatively unexplored. While vector fields such as electromagnetism
are well understood, it is expected in many theories of unification that additional
scalar fields such as the Higgs field will exist. We now need to look at what these
can do for cosmology.
The Lagrangian density for a scalar field is as usual of the form of a kinetic
minus a potential term:
L = 12 ∂µφ∂ µ φ − V (φ).
In familiar examples of quantum fields, the potential would be
V (φ) = 12 m 2 φ 2 ,
where m is the mass of the field in natural units. However, it will be better to keep
the potential function general at this stage. As usual, Noether’s theorem gives the
energy–momentum tensor for the field as
T µν = ∂ µ φ∂ ν φ − g µν L.
From this, we can read off the energy density and pressure:
ρ = 12 φ̇ 2 + V (φ) + 12 (∇φ)2
p = 12 φ̇ 2 − V (φ) − 16 (∇φ)2 .

Inflationary cosmology

35

If the field is constant both spatially and temporally, the equation of state is then
p = −ρ, as required if the scalar field is to act as a cosmological constant; note
that derivatives of the field spoil this identification.
Treating the field classically (i.e. considering the expectation value φ, we
µν
get from energy–momentum conservation (T;ν = 0) the equation of motion
φ̈ + 3H φ̇ − ∇ 2 φ + dV /dφ = 0.
This can
be derived more easily by the direct route of writing down the action
 also
√
S = L −g d4 x and applying the Euler–Lagrange equation that arises from a
√
stationary action ( −g = R 3 (t) for an FRW model, which is the origin of the
Hubble drag term 3H φ̇).
The solution of the equation of motion becomes tractable if we both ignore
spatial inhomogeneities in φ and make the slow-rolling approximation that |φ̈|
is negligible in comparison with |3H φ̇| and |dV /dφ|. Both these steps are
required in order that inflation can happen; we have shown earlier that the vacuum
equation of state only holds if in some sense φ changes slowly both spatially and
temporally. Suppose there are characteristic temporal and spatial scales T and
X for the scalar field; the conditions for inflation are that the negative-pressure
equation of state from V (φ) must dominate the normal-pressure effects of time
and space derivatives:
V  φ 2 /T 2 ,

V  φ2/ X 2,

hence |dV /dφ| ∼ V /φ must be φ/T 2 ∼ φ̈. The φ̈ term can therefore be
neglected in the equation of motion, which then takes the slow-rolling form for
homogeneous fields:
3H φ̇ = −dV /dφ.
The conditions for inflation can be cast into useful dimensionless forms. The
basic condition V  φ̇ 2 can now be rewritten using the slow-roll relation as
≡

m 2P
(V /V )2  1.
16π

Also, we can differentiate this expression to obtain the criterion V  V /m P .
Using slow-roll
√ once more gives 3H φ̇/m P for the right-hand side, which is in
turn  3H V /m P because φ̇ 2  V , giving finally
m 2P
(V /V )  1
8π
√
√
(recall that for de Sitter space H = 8π GV (φ)/3 ∼ V /m P in natural units).
These two criteria make perfect intuitive sense: the potential must be flat in the
sense of having small derivatives if the field is to roll slowly enough for inflation
to be possible.
η≡

36

An introduction to the physics of cosmology

Similar arguments can be made for the spatial parts. However, they are less
critical: what matters is the value of ∇φ = ∇comovingφ/R. Since R increases
exponentially, these perturbations are damped away: assuming V is large enough
for inflation to start in the first place, inhomogeneities rapidly become negligible.
This ‘stretching’ of field gradients as we increase the cosmological horizon
beyond the value predicted in classical cosmology also solves a related problem
that was historically important in motivating the invention of inflation—the
monopole problem. Monopoles are point-like topological defects that would be
expected to arise in any phase transition at around the GUT scale (t ∼ 10−35 s).
If they form at approximately one per horizon volume at this time, then it follows
that the present universe would contain   1 in monopoles. This unpleasant
conclusion is avoided if the horizon can be made much larger than the classical
one at the end of inflation; the GUT fields have then been aligned over a vast
scale, so that topological-defect formation becomes extremely rare.
2.5.2 Ending inflation
Although spatial derivatives of the scalar field can thus be neglected, the same is
not always true for time derivatives. Although they may be negligible initially,
the relative importance of time derivatives increases as φ rolls down the potential
and V approaches zero (leaving aside the subtle question of how we know that the
minimum is indeed at zero energy). Even if the potential does not steepen, sooner
or later we will have   1 or |η|  1 and the inflationary phase will cease.
Instead of rolling slowly ‘downhill’, the field will oscillate about the bottom of
the potential, with the oscillations becoming damped by the 3H φ̇ friction term.
Eventually, we will be left with a stationary field that either continues to inflate
without end, if V (φ = 0) > 0, or which simply has zero density. This would be
a most boring universe to inhabit, but fortunately there is a more realistic way in
which inflation can end. We have neglected so far the couplings of the scalar field
to matter fields. Such couplings will cause the rapid oscillatory phase to produce
particles, leading to reheating. Thus, even if the minimum of V (φ) is at V = 0,
the universe is left containing roughly the same energy density as it started with,
but now in the form of normal matter and radiation—-which starts the usual FRW
phase, albeit with the desired special ‘initial’ conditions.
As well as being of interest for completing the picture of inflation, it is
essential to realize that these closing stages of inflation are the only ones of
observational relevance. Inflation might well continue for a huge number of efoldings, all but the last few satisfying , η  1. However, the scales that left the
de Sitter horizon at these early times are now vastly greater than our observable
horizon, c/H0, which exceeds the de Sitter horizon by only a finite factor. If
inflation was terminated by reheating to the GUT temperature, then the expansion
factor required to reach the present epoch is
−1
aGUT
 E GUT/E γ .

Inflationary cosmology

37

The comoving horizon size at the end of inflation was therefore
−1
−1
dH (tGUT )  aGUT
[c/HGUT]  [E P /E γ ]E GUT
,

√
2
/E P . For a
where the last expression in natural units uses H  V /E P  E GUT
15
GUT energy of 10 GeV, this is about 10 m. This is a sobering illustration of the
magnitude of the horizon problem; if we relied on causal processes at the GUT
era to produce homogeneity, then the universe would only be smooth in patches
a few comoving metres across. To solve the problem, we need enough e-foldings
of inflation to have stretched this GUT-scale horizon to the present horizon size


3000h −1 Mpc
 60.
Nobs = ln
−1
(E P /E γ )E GUT
By construction, this is enough to solve the horizon problem, and it is also the
number of e-foldings needed to solve the flatness problem. This is no coincidence,
since we saw earlier that the criterion in this case was


aeq
1
.
N & ln
2
2
aGUT
Now, aeq = ργ /ρ, and ρ = 3H 2/(8π G). In natural units, this translates to
−1 ∼ E 2 (c/H )−2 /E 4 . The expression for N is then
ρ ∼ E P2 (c/H0)−2 , or aeq
0
γ
P
identical to that in the case of the horizon problem: the same number of e-folds
will always solve both.
Successful inflation in any of these models requires > 60 e-foldings of
the expansion. The implications of this are easily calculated using the slow-roll
equation, which gives the number of e-foldings between φ1 and φ2 as

N=

H dt = −

8π
m 2P



φ2
φ1

V
dφ.
V

For any potential that is relatively smooth, V ∼ V /φ, and so we get N ∼
(φstart /m P )2 , assuming that inflation terminates at a value of φ rather smaller than
at the start. The criterion for successful inflation is thus that the initial value of
the field exceeds the Planck scale:
φstart  m P .
By the same argument, it is easily seen that this is also the criterion needed
to make the slow-roll parameters  and η  1. To summarize, any model in
which the potential is sufficiently flat that slow-roll inflation can commence will
probably achieve the critical 60 e-foldings. Counterexamples can of course be
constructed, but they have to be somewhat special cases.

38

An introduction to the physics of cosmology

It is interesting to review this conclusion for some of the specific inflation
models listed earlier. Consider a mass-like potential V = m 2 φ 2 . If inflation
starts near the Planck scale, the fluctuations in V are ∼ m 4P and these will drive
φstart to φstart  m P provided m  m P ; similarly, for V = λφ 4 , the condition
is weak coupling: λ  1. Any field with a rather flat potential will thus tend
to inflate, just because typical fluctuations leave it a long way from home in the
form of the potential minimum. In a sense, inflation is realized by means of
‘inertial confinement’: there is nothing to prevent the scalar field from reaching
the minimum of the potential—-but it takes a long time to do so, and the universe
has meanwhile inflated by a large factor.
2.5.3 Relic fluctuations from inflation
The idea of launching a flat and causally connected expanding universe, using
only vacuum-energy antigravity, is attractive. What makes the package of
inflationary ideas especially compelling is that there it is an inevitable outcome
of this process that the post-inflation universe will be inhomogeneous to some
extent. There is not time to go into much detail on this here, but we summarize
some of the key aspects, in order to make a bridge to the following material on
structure formation.
The key idea is to appreciate that the inflaton field cannot be a classical
object, but must display quantum fluctuations. Well inside the horizon of de Sitter
space, these must be calculable by normal flat-space quantum field theory. If we
can calculate how these fluctuations evolve as the universe expands, we have a
mechanism for seeding inhomogeneities in the expanding universe—which can
then grow under gravity to make structure.
To anticipate the detailed treatment, the inflationary prediction is of a
horizon-scale fractional perturbation to the density
δH =

H2
2π φ̇

which can be understood as follows. Imagine that the main effect of fluctuations
is to make different parts of the universe have fields that are perturbed by an
amount δφ. In other words, we are dealing with various copies of the same rolling
behaviour φ(t), but viewed at different times
δt =

δφ
.
φ̇

These universes will then finish inflation at different times, leading to a spread in
energy densities (figure 2.4). The horizon-scale density amplitude is given by the
different amounts that the universes have expanded following the end of inflation:
δH  H δt =

H2
,
2π φ̇

Inflationary cosmology

39

Figure 2.4. This plot shows how fluctuations in the scalar field transform themselves into
density fluctuations at the end of inflation. Different points of the universe inflate from
points on the potential perturbed by a fluctuation δφ, like two balls rolling from different
starting points. Inflation finishes at times separated by δt in time for these two points,
inducing a density fluctuation δ = H δt.

where the last step uses the crucial input of quantum field theory, which says that
the rms δφ is given by H /2π. This is the classical amplitude that results from the
stretching of sub-horizon flat-space quantum fluctuations. We will not attempt to
prove this key result here (see chapter 12 of Peacock 1999, or Liddle and Lyth
1993, 2000).
Because the de Sitter expansion is invariant under time translation, the
inflationary process produces a universe that is fractal-like in the sense that scaleinvariant fluctuations correspond to a metric that has the same ‘wrinkliness’ per
log length-scale. It then suffices to calculate that amplitude on one scale—i.e.
the perturbations that are just leaving the horizon at the end of inflation, so that
super-horizon evolution is not an issue. It is possible to alter this prediction of
scale invariance only if the expansion is non-exponential; we have seen that such
deviations plausibly do exist towards the end of inflation, so it is clear that exact
scale invariance is not to be expected. This is discussed further later.
In summary, we have the following three key equations for basic inflationary
model building. The fluctuation amplitude can be thought of as supplying the
variance per ln k in potential perturbations, which we show later does not evolve
with time:
H4
(2π φ̇)2
8π V
H2 =
3 m 2P

2
≡ 2 (k) =
δH

3H φ̇ = −V .
We have also written once again the exact relation between H and V and the

40

An introduction to the physics of cosmology

slow-roll condition, since manipulation of these three equations is often required
in derivations.
2.5.4 Gravity waves and tilt
The density perturbations left behind as a residue of the quantum fluctuations in
the inflaton field during inflation are an important relic of that epoch, but are not
the only one. In principle, a further important test of the inflationary model is
that it also predicts a background of gravitational waves, whose properties couple
with those of the density fluctuations.
It is easy to see in principle how such waves arise. In linear theory, any
quantum field is expanded in a similar way into a sum of oscillators with the
usual creation and annihilation operators; this analysis of quantum fluctuations in
a scalar field is thus readily adapted to show that analogous fluctuations will be
generated in other fields during inflation. In fact, the linearized contribution
√ of a
gravity wave, h µν , to the Lagrangian looks like a scalar field φ = (m P /4 π)h µν ,
the expected rms gravity-wave amplitude is
h rms ∼ H /m P.
The fluctuations in φ are transmuted into density fluctuations, but gravity waves
will survive to the present day, albeit redshifted.
This redshifting produces a break in the spectrum of waves. Prior to horizon
entry, the gravity waves produce a scale-invariant spectrum of metric distortions,
with amplitude h rms per ln k. These distortions are observable via the large-scale
CMB anisotropies, where the tensor modes produce a spectrum with the same
scale dependence as the Sachs–Wolfe gravitational redshift from scalar metric
perturbations. In the scalar case, we have δT /T ∼ φ/3c2 , i.e. of order the
Newtonian metric perturbation; similarly, the tensor effect is
 
δT
∼ h rms . δH ∼ 10−5 ,
T GW
where the second step follows because the tensor modes can constitute no more
than 100% of the observed CMB anisotropy.
A detailed estimate of the ratio between the tensor effect of gravity waves
and the normal scalar Sachs–Wolfe effect was first analysed in a prescient paper
by Starobinsky (1985). Denote the fractional temperature variance per natural
logarithm of angular wavenumber by 2 (constant for a scale-invariant spectrum).
The tensor and scalar contributions are, respectively,
2T ∼ h 2rms ∼ (H 2/m 2P ) ∼ V /m 4P
2
2S ∼ δH
∼

H2
V3
H6
∼
.
∼
(V )2
φ̇
m 6P V 2

Inflationary cosmology

41

The ratio of the tensor and scalar contributions to the variance of microwave
background anisotropies is therefore proportional to the inflationary parameter
:
2T
 12.4,
2S
inserting the exact coefficient from Starobinsky (1985). If it could be measured,
the gravity-wave contribution to CMB anisotropies would therefore give a
measure of , one of the dimensionless inflation parameters. The less ‘de
Sitter-like’ the inflationary behaviour is, the larger the relative gravitational-wave
contribution is.
Since deviations from exact exponential expansion also manifest themselves
as density fluctuations with spectra that deviate from scale invariance, this
suggests a potential test of inflation. Define the tilt of the fluctuation spectrum
as follows:
2
d ln δH
.
tilt ≡ 1 − n ≡ −
d ln k
We then want to express the tilt in terms of parameters of the inflationary potential,
 and η. These are of order unity when inflation terminates;  and η must
therefore be evaluated when the observed universe left the horizon, recalling that
we only observe the last 60-odd e-foldings of inflation. The way to introduce scale
dependence is to write the condition for a mode of given comoving wavenumber
to cross the de Sitter horizon,
a/k = H −1.
Since H is nearly constant during the inflationary evolution, we can replace
d/d ln k by d ln a, and use the slow-roll condition to obtain
m2 V d
d
φ̇ d
d
=a
=
=− P
.
d ln k
da
H dφ
8π V dφ
We can now work out the tilt, since the horizon-scale amplitude is


4
3
V
H
128π
2
δH
,
=
=
3
(2π φ̇)2
m 6P V 2
and derivatives of V can be expressed in terms of the dimensionless parameters 
and η. The tilt of the density perturbation spectrum is thus predicted to be
1 − n = 6 − 2η.
In section 2.8.5 on CMB anisotropies, we discuss whether this relation is
observationally testable.

42

An introduction to the physics of cosmology

2.5.5 Evidence for vacuum energy at late times
The idea of inflation is audacious, but undeniably speculative. However, once we
accept the idea that quantum fields can generate an equation of state resembling
a cosmological constant, we need not confine this mechanism to GUT-scale
energies. There is no known mechanism that requires the minimum of V (φ) to
lie exactly at zero energy, so it is quite plausible that there remains in the universe
today some non-zero vacuum energy.
The most direct way of detecting vacuum energy has been the immense
recent progress in the use of supernovae as standard candles. Type Ia SNe
have been used as standard objects for around two decades, with an rms scatter
in luminosity of 40%, and so a distance error of 20%. The big breakthrough
came when it was realized that the intrinsic timescale of the SNe correlates with
luminosity (a brighter SNe lasts longer). Taking out this effect produces corrected
standard candles that are capable of measuring distances to about 5% accuracy.
Large search campaigns have made it possible to find of the order of 100 SNe
over the range 0.1 . z . 1, and two teams have used this strategy to make an
empirical estimate of the cosmological distance–redshift relation.
The results of the Supernova Cosmology Project (e.g. Perlmutter et al 1998)
and the High-z Supernova Search (e.g. Riess et al 1998) are highly consistent.
Figure 2.5 shows the Hubble diagram from the latter team. The SNe magnitudes
are K -corrected, so that their variation with redshift should be a direct measure of
luminosity distance as a function of redshift.
We have seen earlier that this is written as the following integral, which must
usually be evaluated numerically:
c
DL (z) = (1 + z)R0 Sk (r ) = (1 + z) |1 − |−1/2
H0


z
|1 − |1/2 dz
,
× Sk
0
(1 − )(1 + z )2 + v + m (1 + z )3
where  = m + v , and Sk is sinh if  < 1, otherwise sin. It is clear from
figure 2.5 that the empirical distance–redshift relation is very different from the
simplest inflationary prediction, which is the  = 1 Einstein–de Sitter model;
by redshift 0.6, the SNe are fainter than expected in this model by about 0.5
magnitudes. If this model fails, we can try adjusting m and v in an attempt to
do better. Comparing each such model to the data yields the likelihood contours
shown in figure 2.6, which can be used in the standard way to set confidence
limits on the cosmological parameters. The results very clearly require a lowdensity universe. For  = 0, a very low density is just barely acceptable, with
m . 0.1. However, the discussion of the CMB later shows that such a heavily
open model is hard to sustain. The preferred model has v  1; if we restrict
ourselves to the inflationary k = 0, then the required parameters are very close to
(m , v ) = (0.3, 0.7).

Inflationary cosmology

43

Figure 2.5. The Hubble diagram produced by the High-z Supernova search team (Riess et
al 1998). The lower panel shows the data divided by a default model (m = 0.2, v = 0).
The results lie clearly above this model, favouring a non-zero . The lowest line is the
Einstein–de Sitter model, which is in gross disagreement with observation.

2.5.6 Cosmic coincidence
This is an astonishing result—an observational detection of the physical reality of
vacuum energy. The error bars continue to shrink, and no convincing systematic
error has been suggested that could yield this result spuriously; this is one of the
most important achievements of 20th century physics.
And yet, accepting the reality of vacuum energy raises a difficult question. If
the universe contains a constant vacuum density and normal matter with ρ ∝ a −3 ,
there is a unique epoch at which these two contributions cross over, and we seem
to be living near to that time. This coincidence calls for some explanation. One
might think of appealing to anthropic ideas, and these can limit  to some extent:
if the universe became vacuum-dominated at z > 1000, gravitational instability as
discussed in the next section would have been impossible—so that galaxies, stars
and observers would not have been possible. However, Weinberg (1989) argues

44

An introduction to the physics of cosmology

Figure 2.6. Confidence contours on the v –m plane, according to Riess et al (1998).
Open models of all but the lowest densities are apparently ruled out, and non-zero  is
strongly preferred. If we restrict ourselves to k = 0, then m  0.3 is required. The
constraints perpendicular to the k = 0 line are not very tight, but CMB data can help here
in limiting the allowed degree of curvature.

that  could have been much larger than its actual value without making observers
impossible. Efstathiou (1995) attempted to construct a probability distribution for
 by taking this to be proportional to the number density of galaxies that result in
a given model. However, there is no general agreement on how to set a probability
measure for this problem.
It would be more satisfactory if we had some physical mechanism that
guaranteed the coincidence, and one possibility has been suggested. We already
have one coincidence, in that we live relatively close in time to the era of matter–
radiation equality (z ∼ 103, as opposed to z ∼ 1080 for the GUT era). What
is required is a cosmological ‘constant’ that switches on around the equality era.
Zlatev et al (1999) have suggested how this might happen. The idea is to use
the vacuum properties of a homogeneous scalar field as the physical origin of
the negative-pressure term detected via SNe. This idea of a ‘rolling’  was first
explored by Ratra and Peebles (1988), and there has recently been a tendency

Inflationary cosmology

45

towards use of the fanciful term ‘quintessence’. In any case, it is important to
appreciate that the idea uses exactly the same physical elements that we discussed
in the context of inflation: there is some V (φ), causing the expectation value of
φ to obey the damped oscillator equation of motion, so the energy density and
pressure are
ρφ = φ̇ 2 /2 + V
pφ = φ̇ 2 /2 − V.
This gives us two extreme equations of state:
(i) vacuum-dominated, with V  φ̇ 2 /2, so that p = −ρ;
(ii) kinetic-dominated, with V  φ̇ 2 /2, so that p = ρ.
In the first case, we know that ρ does not alter as the universe expands, so the
vacuum rapidly tends to dominate over normal matter. In the second case, the
equation of state is the unusual  = 2, so we get the rapid behaviour ρ ∝ a −6 .
If a quintessence-dominated universe starts off with a large kinetic term relative
to the potential, it may seem that things should always evolve in the direction of
being potential-dominated. However, this ignores the detailed dynamics of the
situation: for a suitable choice of potential, it is possible to have a tracker field, in
which the kinetic and potential terms remain in a constant proportion, so that we
can have ρ ∝ a −α , where α can be anything we choose.
Putting this condition in the equation of motion shows that the potential is
required to be exponential in form. More importantly, we can generalize to the
case where the universe contains scalar field and ordinary matter. Suppose the
latter dominates, and obeys ρm ∝ a −α . It is then possible to have the scalar-field
density obeying the same ρ ∝ a −α law, provided
V (φ) =

2
(6/α − 1) exp[−λφ].
λ2

The scalar-field density is ρφ = (α/λ2 )ρtotal (see, e.g., Liddle and Scherrer 1999).
The impressive thing about this solution is that the quintessence density stays a
fixed fraction of the total, whatever the overall equation of state: it automatically
scales as a −4 at early times, switching to a −3 after the matter–radiation equality.
This is not quite what we need, but it shows how the effect of the overall
equation of state can affect the rolling field. Because of the 3H φ̇ term in the
equation of motion, φ ‘knows’ whether or not the universe is matter dominated.
This suggests that a more complicated potential than the exponential may allow
the arrival of matter domination to trigger the desired -like behaviour. Zlatev et
al suggest two potentials which might achieve this:
V (φ) = M 4+β φ −β

or

V (φ) = M 4 [exp(m P /φ) − 1].

The evolution in these potentials may be described by w(t), where w = p/ρ. We
need w  1/3 in the radiation era, changing to w  −1 today. The evolution

46

An introduction to the physics of cosmology

Figure 2.7. This figure, taken from Zlatev et al (1999), shows the evolution of the density
in the ‘quintessence’ field (top panel), together with the effective equation of state of the
quintessence vacuum (bottom panel), for the case of the inverse exponential potential. This
allows vacuum energy to lurk at a few per cent of the total throughout the radiation era, but
switching on a cosmological constant after the universe becomes matter dominated.

in the inverse exponential potential is shown in figure 2.7, demonstrating that the
required behaviour can be found. However, a slight fine-tuning is still required, in
that the trick only works for M ∼ 1 meV, so there has to be an energy coincidence
with the energy scale of matter–radiation equality.
So, the idea of tracker fields does not remove completely the puzzle
concerning the level of the present-day vacuum energy. In a sense, relegating the

Dynamics of structure formation

47

solution to a potential of unexplained form may seem a retrograde step. However,
it is at least a testable step: the prediction of figure 2.7 is that w  −0.8 today, so
that the quintessence density scales as ρ ∝ a −0.6 . This is a significant difference
from the classical w = −1 vacuum energy, and it should be detectable as the
SNe data improve. The existing data already require approximately w < −0.5,
so there is the entrancing prospect that the equation of state for the vacuum will
soon become the subject of experimental study.

2.6 Dynamics of structure formation
The overall properties of the universe are very close to being homogeneous; and
yet telescopes reveal a wealth of detail on scales varying from single galaxies
to large-scale structures of size exceeding 100 Mpc. This section summarizes
some of the key results concerning how such structure can arise via gravitational
instability.
2.6.1 Linear perturbations
The study of cosmological perturbations can be presented as a complicated
exercise in linearized general relativity; fortunately, much of the essential physics
can be extracted from a Newtonian approach. We start by writing down the
fundamental equations governing fluid motion (non-relativistic for now):
Euler:
energy:
Poisson:

∇p
Dv
=−
− ∇
Dt
ρ
Dρ
= −ρ∇ · v
Dt
∇ 2  = 4π Gρ,

where D/Dt = ∂/∂t +v·∇ is the usual convective derivative. We now produce the
linearized equations of motion by collecting terms of first order in perturbations
about a homogeneous background: ρ = ρ0 + δρ etc. As an example, consider the
energy equation:
[∂/∂t + (v0 + δv) · ∇](ρ0 + δρ) = −(ρ0 + δρ)∇ · (v0 + δv).
For no perturbation, the zero-order equation is
(∂/∂t + v0 · ∇)ρ0 = −ρ0 ∇ · v0 ;
since ρ0 is homogeneous and v0 = H x is the Hubble expansion, this just says
ρ̇0 = −3Hρ0. Expanding the full equation and subtracting the zeroth-order
equation gives the equation for the perturbation:
(∂/∂t + v0 · ∇)δρ + δv · ∇(ρ0 + δρ ) = −(ρ0 + δρ)∇ · δv − δρ∇ · v0 .

48

An introduction to the physics of cosmology

Now, for sufficiently small perturbations, terms containing a product of
perturbations such as δv · ∇δρ must be negligible in comparison with the firstorder terms. Remembering that ρ0 is homogeneous leaves the linearized equation
[∂/∂t + v0 · ∇]δρ = −ρ0 ∇ · δv − δρ∇ · v0 .
It is straightforward to perform the same steps with the other equations; the
results look simpler if we define the fractional density perturbation
δ≡

δρ
.
ρ0

As before, when dealing with time derivatives of perturbed quantities, the full
convective time derivative D/Dt can always be replaced by d/dt ≡ ∂/∂t + v0 · ∇,
which is the time derivative for an observer comoving with the unperturbed
expansion of the universe. We then can write
∇δp
d
δv = −
− ∇δ − (δv · ∇)v0
dt
ρ0
d
δ = −∇ · δv
dt
∇ 2 δ = 4π Gρ0 δ.
There is now only one complicated term to be dealt with: (δv · ∇)v0 on the righthand side of the perturbed Euler equation. This is best attacked by writing it in
components:
[(δv · ∇)v0 ] j = [δv]i ∇i [v0 ] j = H [δv] j ,
where the last step follows because v0 = H x0 ⇒ ∇i [v0 ] j = H δi j . This
leaves a set of equations of motion that have no explicit dependence on the global
expansion speed v0 ; this is only present implicitly through the use of convective
time derivatives d/dt.
These equations of motion are written in Eulerian coordinates: proper length
units are used, and the Hubble expansion is explicitly present through the velocity
v0 . The alternative approach is to use the comoving coordinates formed by
dividing the Eulerian coordinates by the scale factor a(t):
x(t) = a(t)r(t)
δv(t) = a(t)u(t).
The next step is to translate spatial derivatives into comoving coordinates:
∇x =

1
∇r .
a

To keep the notation simple, subscripts on ∇ will normally be omitted hereafter,
and spatial derivatives will be with respect to comoving coordinates. The

Dynamics of structure formation

49

linearized equations for conservation of momentum and matter as experienced
by fundamental observers moving with the Hubble flow then take the following
simple forms in comoving units:
g
∇δp
ȧ
u̇ + 2 u = −
a
a
ρ0
δ̇ = −∇ · u,
where dots stand for d/dt. The peculiar gravitational acceleration ∇δ/a is
denoted by g.
Before going on, it is useful to give an alternative derivation of these
equations, this time working in comoving length units right from the start. First
note that the comoving peculiar velocity u is just the time derivative of the
comoving coordinate r:
ẋ = ȧ r + a ṙ = H x + a ṙ,
where the right-hand side must be equal to the Hubble flow H x, plus the
peculiar velocity δv = au. In this equation, dots stand for exact convective
time derivatives—i.e. time derivatives measured by an observer who follows a
particle’s trajectory—rather than partial time derivatives ∂/∂t. This allows us to
apply the continuity equation immediately in comoving coordinates, since this
equation is simply a statement that particles are conserved, independent of the
coordinates used. The exact equation is
D
ρ0 (1 + δ) = −ρ0 (1 + δ)∇ · u,
Dt
and this is easy to linearize because the background density ρ0 is independent of
time when comoving length units are used. This gives the first-order equation
δ̇ = −∇ · u immediately. The equation of motion follows from writing the
Eulerian equation of motion as ẍ = g0 + g, where g = ∇δ/a is the
peculiar acceleration defined earlier, and g0 is the acceleration that acts on a
particle in a homogeneous universe (neglecting pressure forces, for simplicity).
Differentiating x = a r twice gives
ẍ = a u̇ + 2ȧu +

ä
x = g0 + g.
a

The unperturbed equation corresponds to zero peculiar velocity and zero peculiar
acceleration: (ä/a)x = g0 ; subtracting this gives the perturbed equation of
motion u + 2(ȧ/a)u = g, as before. This derivation is rather more direct than the
previous route of working in Eulerian space. Also, it emphasizes that the equation
of motion is exact, even though it happens to be linear in the perturbed quantities.
After doing all this, we still have three equations in the four variables δ, u,
δ and δp. The system needs an equation of state in order to be closed; this may

50

An introduction to the physics of cosmology

be specified in terms of the sound speed
cs2 ≡

∂p
.
∂ρ

Now think of a plane-wave disturbance δ ∝ e−ik·r , where k is a comoving
wavevector; in other words, suppose that the wavelength of a single Fourier mode
stretches with the universe. All time dependence is carried by the amplitude of
the wave, and so the spatial dependence can be factored out of time derivatives in
these equations (which would not be true with a constant comoving wavenumber
k/a). An equation for the amplitude of δ can then be obtained by eliminating u:
ȧ
δ̈ + 2 δ̇ = δ(4π Gρ0 − cs2 k 2 /a 2 ).
a
This equation is the one that governs the gravitational amplification of density
perturbations.
There is a critical proper wavelength, known as the Jeans length, at which
we switch from the possibility of exponential growth for long-wavelength modes
to standing sound waves at short wavelengths. This critical length is

π
,
λJ = cs
Gρ
and clearly delineates the scale at which sound waves can cross an
object in about the time needed for gravitational free-fall collapse. When
considering perturbations in an expanding background, things are more complex.
Qualitatively, we expect to have no growth when the ‘driving term’ on the righthand side is negative. However, owing to the expansion, λJ will change with time,
and so a given perturbation may switch between periods of growth and stasis.
2.6.2 Dynamical effects of radiation

√
At early enough times, the universe was radiation dominated (cs = c/ 3) and
the analysis so far does not apply. It is common to resort to general relativity
perturbation theory at this point. However, the fields are still weak, and so it is
possible to generate the results we need by using special relativity fluid mechanics
and Newtonian gravity with a relativistic source term. For simplicity, assume
that accelerations due to pressure gradients are negligible in comparison with
gravitational accelerations (i.e. restrict the analysis to λ  λJ from the start).
The basic equations are then a simplified Euler equation and the full energy and
gravitational equations:
Euler:
energy:
Poisson:

Dv
= −∇
Dt
D
∂
(ρ + p/c2 ) = ( p/c2) − (ρ + p/c2 )∇ · v
Dt
∂t
∇ 2  = 4π G(ρ + 3 p/c2 ).

Dynamics of structure formation

51

For total radiation domination, p = ρc2 /3, and it is easy to linearize these
equations as before. The main differences come from factors of 2 and 4/3 due to
the non-negligible contribution of the pressure. The result is a continuity equation
∇ · u = −(3/4)δ̇, and the evolution equation for δ:
32π
ȧ
Gρ0 δ,
δ̈ + 2 δ̇ =
a
3
so the net result of all the relativistic corrections is a driving term on the right-hand
side that is a factor 8/3 higher than in the matter-dominated case.
In both matter- and radiation-dominated universes with  = 1, we have
ρ0 ∝ 1/t 2 :
matter domination (a ∝ t 2/3 ):
radiation domination (a ∝ t 1/2 ):

4π Gρ0 =

2
3t 2

32π Gρ0 /3 =

1
.
t2

Every term in the equation for δ is thus the product of derivatives of δ and powers
of t, and a power-law solution is obviously possible. If we try δ ∝ t n , then
the result is n = 2/3 or −1 for matter domination; for radiation domination,
this becomes n = ±1. For the growing mode,
these can be combined rather

conveniently using the conformal time η ≡ dt/a:
δ ∝ η2 .
Recall that η is proportional to the comoving size of the horizon.
It is also interesting to think about the growth of matter perturbations in
universes with non-zero vacuum energy, or even possibly some other exotic
background with a peculiar equation of state. The differential equation for δ is
as before, but a(t) is altered. The way to deal with this is to treat a spherical
perturbation as a small universe. Consider the Friedmann equation in the form
2 2
(ȧ)2 = tot
0 H0 a + K ,

where K = −kc2/R02 ; this emphasizes that K is a constant of integration. A
second constant of integration arises in the expression for time:
 a
ȧ −1 da + C.
t=
0

This lets us argue as before in the case of decaying modes: if a solution to the
Friedmann equation is a(t, K , C), then valid density perturbations are




∂ ln a
∂ ln a
δ∝
or
.
∂K t
∂C t

52

An introduction to the physics of cosmology

Since ∂(ȧ 2 )/∂ K = 1, this gives the growing and decaying modes as

a
(ȧ/a) 0 (ȧ)−3 da (growing mode)
δ∝
(ȧ/a)
(decaying mode).
(Heath 1977, see also section 10 of Peebles 1980).
The equation for the growing mode requires numerical integration in general,
with ȧ(a) given by the Friedmann equation. A very good approximation to the
answer is given by Carroll et al (1992):



−1
5
1
1
δ(z = 0, )
4/7
 m m − v + 1 + m
.
1 + v
δ(z = 0,  = 1)
2
2
70
This fitting formula for the growth suppression in low-density universes is an
invaluable practical tool. For flat models with m +v = 1, it says that the growth
suppression is less marked than for an open universe—approximately 0.23 as
against 0.65 if  = 0. This reflects the more rapid variation of v with redshift;
if the cosmological constant is important dynamically, this only became so very
recently, and the universe spent more of its history in a nearly Einstein–de Sitter
state by comparison with an open universe of the same m .
What about the case of collisionless matter in a radiation background? The
fluid treatment is not appropriate here, since the two species of particles can
interpenetrate. A particularly interesting limit is for perturbations well inside the
horizon: the radiation can then be treated as a smooth, unclustered background
that affects only the overall expansion rate. This is analogous to the effect of ,
but an analytical solution does exist in this case. The perturbation equation is as
before
ȧ
δ̈ + 2 δ̇ = 4π Gρm δ,
a
but now H 2 = 8π G(ρm + ρr )/3. If we change variable to y ≡ ρm /ρr = a/aeq,
and use the Friedmann equation, then the growth equation becomes
δ +

2 + 3y
3
δ −
δ=0
2y(1 + y)
2y(1 + y)

(for k = 0, as appropriate for early times). It may be seen by inspection that a
growing solution exists with δ = 0:
δ ∝ y + 2/3.
It is also possible to derive the decaying mode. This is simple in the radiationdominated case (y  1): δ ∝ − ln y is easily seen to be an approximate solution
in this limit.
What this says is that, at early times, the dominant energy of radiation drives
the universe to expand so fast that the matter has no time to respond, and δ is
frozen at a constant value. At late times, the radiation becomes negligible, and the

Dynamics of structure formation

53

growth increases smoothly to the Einstein–de Sitter δ ∝ a behaviour (Mészáros
1974). The overall behaviour is therefore similar to the effects of pressure on
a coupled fluid: for scales greater than the horizon, perturbations in matter and
radiation can grow together, but this growth ceases once the perturbations enter
the horizon. However, the explanations of these two phenomena are completely
different.
2.6.3 The peculiar velocity field
The foregoing analysis shows that gravitational collapse inevitably generates
deviations from the Hubble expansion, which are interesting to study in detail.
Consider first a galaxy that moves with some peculiar velocity in an
otherwise uniform universe. Even though there is no peculiar gravitational
acceleration acting, its velocity will decrease with time as the galaxy attempts
to catch up with successively more distant (and therefore more rapidly receding)
neighbours. If the proper peculiar velocity is v, then after time dt the galaxy
will have moved a proper distance x = v dt from its original location. Its near
neighbours will now be galaxies with recessional velocities H x = H v dt, relative
to which the peculiar velocity will have fallen to v − H x. The equation of motion
is therefore just
ȧ
v̇ = −H v = − v,
a
with the solution v ∝ a −1 : peculiar velocities of non-relativistic objects suffer
redshifting by exactly the same factor as photon momenta. It is often convenient
to express the peculiar velocity in terms of its comoving equivalent, v ≡ au,
for which the equation of motion becomes u̇ = −2H u. Thus, in the absence of
peculiar accelerations and pressure forces, comoving peculiar velocities redshift
away through the Hubble drag term 2H u.
If we now include the effects of peculiar acceleration, this simply adds the
acceleration g on the right-hand side. This gives the equation of motion
u̇ +

g
2ȧ
u=− ,
a
a

where g = ∇δ/a is the peculiar gravitational acceleration. Pressure terms have
been neglected, so λ  λJ . Remember that throughout we are using comoving
length units, so that ∇proper = ∇/a. This equation is the exact equation of motion
for a single galaxy, so that the time derivative is d/dt = ∂/∂t + u · ∇. In linear
theory, the second part of the time derivative can be neglected, and the equation
then turns into one that describes the evolution of the linear peculiar velocity field
at a fixed point in comoving coordinates.
The solutions for the peculiar velocity field can be decomposed into modes
either parallel to g or independent of g (these are the homogeneous and
inhomogeneous solutions to the equation of motion). The interpretation of
these solutions is aided by knowing that the velocity field satisfies the continuity

54

An introduction to the physics of cosmology

equation: ρ̇ = −∇ · (ρv) in proper units, which obviously takes the same form
ρ̇ = −∇ · (ρu) if lengths and densities are in comoving units. If we express the
density as ρ = ρ0 (1+δ) (where in comoving units ρ0 is just a number independent
of time), the continuity equation takes the form
δ̇ = −∇ · [(1 + δ)u],
which becomes just
∇ · u = −δ̇
in linear theory when both δ and u are small. This states that it is possible to have
vorticity modes with ∇ · u = 0, for which δ̇ vanishes. We have already seen that
δ either grows or decays as a power of time, so these modes require zero density
perturbation, in which case the associated peculiar gravity also vanishes. These
vorticity modes are thus the required homogeneous solutions, and they decay as
v = au ∝ a −1 , as with the kinematic analysis for a single particle. For any
gravitational-instability theory, in which structure forms via the collapse of small
perturbations laid down at very early times, it should therefore be a very good
approximation to say that the linear velocity field must be curl-free.
For the growing modes, we want to try looking for a solution u = F(t)g.
Then using continuity plus Gauss’s theorem, ∇ · g = 4π Gaρδ, gives us
δv =

2 f ()
g,
3H 

where the function f () ≡ (a/δ) dδ/da. A very good approximation to this
(Peebles 1980) is g  0.6 (a result that is almost independent of ; Lahav et
al 1991). Alternatively, we can work in Fourier terms. This is easy, as g and k
are parallel, so that ∇ · u = −ik · u = −iku. Thus, directly from the continuity
equation,
iH f ()a
δk k̂.
δvk = −
k
The 1/k factor shows clearly that peculiar velocities are much more sensitive
probes of large-scale inhomogeneities than are density fluctuations. The existence
of large-scale homogeneity in density requires n > −3, whereas peculiar
velocities will diverge unless n > −1 on large scales.
2.6.4 Transfer functions
We have seen that power spectra at late times result from modifications of any
primordial power by a variety of processes: growth under self-gravitation; the
effects of pressure; dissipative processes. We now summarize the two main ways
in which the power spectrum that exists at early times may differ from that which
emerges at the present, both of which correspond to a reduction of small-scale
fluctuations (at least, for adiabatic fluctuations; we shall not consider isocurvature
modes here):

Dynamics of structure formation

55

(1) Radiation effects. Prior to matter–radiation equality, we have already
seen that perturbations inside the horizon are prevented from growing by
radiation pressure. Once z eq is reached, if collisionless dark matter dominates,
perturbations on all scales can grow. We therefore expect a feature in the transfer
function around k ∼ 1/rH (z eq ). In the matter-dominated approximation, we get
dH =

2c
(z)−1/2 ⇒ deq = 39(h 2)−1 Mpc.
H0

The exact distance–redshift relation is
R0 dr =

dz
c
,
H0 (1 + z) 1 + m z + (1 + z)2 r

from which it follows
√ that the correct answer for the horizon size including
radiation is a factor 2 − 1 smaller: deq = 16.0(h 2)−1 Mpc.
(2) Damping. In addition to having their growth retarded, very small-scale
perturbations will be erased entirely, which can happen in one of two ways.
For collisionless dark matter, perturbations are erased simply by free-streaming:
random particle velocities cause blobs to disperse. At early times (kT > mc2 ), the
particles will travel at c, and so any perturbation that has entered the horizon will
be damped. This process switches off when the particles become non-relativistic;
for massive particles, this happens long before z eq (resulting in cold dark matter
(CDM)). For massive neutrinos, however, it happens at z eq : only perturbations
on very large scales survive in the case of hot dark matter (HDM). In a purely
baryonic universe, the corresponding process is called Silk damping: the mean
free path of photons due to scattering by the plasma is non-zero, and so radiation
can diffuse out of a perturbation, convecting the plasma with it.
The overall effect is encapsulated in the transfer function, which gives the
ratio of the late-time amplitude of a mode to its initial value:
Tk ≡

δk (z = 0)
,
δk (z)D(z)

where D(z) is the linear growth factor between redshift z and the present. The
normalization redshift is arbitrary, so long as it refers to a time before any scale
of interest has entered the horizon.
It is invaluable in practice to have some accurate analytic formulae that fit the
numerical results for transfer functions. We give below results for some common
models of particular interest (illustrated in figure 2.8, along with other cases where
a fitting formula is impractical). For the models with collisionless dark matter,
B   is assumed, so that all lengths scale with the horizon size at matter–
radiation equality, leading to the definition
q≡

k
h 2

Mpc−1

.

56

An introduction to the physics of cosmology

Figure 2.8. A plot of transfer functions for various models. For adiabatic models, Tk → 1
at small k, whereas the opposite is true for isocurvature models. A number of possible
matter contents are illustrated: pure baryons; pure CDM; pure HDM; MDM (30% HDM,
70% CDM). For dark-matter models, the characteristic wavenumber scales proportional
to h 2 . The scaling for baryonic models does not obey this exactly; the plotted cases
correspond to  = 1, h = 0.5.

We consider the following cases:
(1) adiabatic CDM;
(2) adiabatic massive neutrinos (one massive, two massless); and
(3) isocurvature CDM; these expressions come from Bardeen et al (1986;
BBKS).
Since the characteristic length-scale in the transfer function depends on the
horizon size at matter–radiation equality, the temperature of the CMB enters.
In these formulae, it is assumed to be exactly 2.7 K; for other values, the
characteristic wavenumbers scale ∝ T −2 . For these purposes massless neutrinos
count as radiation, and three species of these contribute a total density that is 0.68
that of the photons.
ln(1 + 2.34q)
[1 + 3.89q + (16.1q)2 + (5.46q)3 + (6.71q)4]−1/4
2.34q
(2) Tk = exp(−3.9q − 2.1q 2)
(3) Tk = (5.6q)2(1 + [15.0q + (0.9q)3/2 + (5.6q)2]1.24 )−1/1.24 .
(1) Tk =

The case of mixed dark matter (MDM: a mixture of massive neutrinos and CDM)
is more complex. See Pogosyan and Starobinksy (1995) for a fit in this case.

Dynamics of structure formation

57

These expressions assume pure dark matter, which is unrealistic. At least
for CDM models, a non-zero baryonic density lowers the apparent dark-matter
density parameter. We can define an apparent shape parameter  for the transfer
function:
q ≡ (k/ h Mpc−1 )/ ,
and  = h in a model with zero baryon content. This parameter was originally
defined by Efstathiou et al (1992), in terms of a CDM model with B = 0.03.
Peacock and Dodds (1994) showed that the effect of increasing B was to
preserve the CDM-style spectrum shape, but to shift to lower values of . This
shift was generalized to models with  = 1 by Sugiyama (1995):
√
 = h exp[−B (1 + 2h/)].
Note the oscillations in T (k) for high baryon content; these can be significant even
in CDM-dominated models when working with high-precision data. Eisenstein
and Hu (1998) are to be congratulated for their impressive persistence in finding
an accurate fitting formula that describes these wiggles. This is invaluable for
carrying out a search of a large parameter space. An interesting question is
whether these ‘wiggles’ survive evolution into the nonlinear regime: Meiksin et al
(1999) showed that most do not, but that observable signatures of baryons remain
on large scales.
2.6.5 The spherical model
An overdense sphere is a very useful nonlinear model, as it behaves in exactly
the same way as a closed sub-universe. The density perturbation needs not be a
uniform sphere: any spherically symmetric perturbation will clearly evolve at a
given radius in the same way as a uniform sphere containing the same amount of
mass. In what follows, therefore, density refers to the mean density inside a given
sphere. The equations of motion are the same as for the scale factor, and we can
therefore write down the cycloid solution immediately. For a matter-dominated
universe, the relation between the proper radius of the sphere and time is
r = A(1 − cos θ )
t = B(θ − sin θ ),
and A3 = G M B 2 , just from r̈ = −G M/r 2 . Expanding these relations up to
order θ 5 gives r (t) for small t:
  
  
1 6t 2/3
A 6t 2/3
1−
,
r
2 B
20 B
and we can identify the density perturbation within the sphere:
 
3 6t 2/3
.
δ
20 B

58

An introduction to the physics of cosmology

This all agrees with what we knew already: at early times the sphere expands with
the a ∝ t 2/3 Hubble flow and density perturbations grow proportional to a.
We can now see how linear theory breaks down as the perturbation evolves.
There are three interesting epochs in the final stages of its development, which
we can read directly from the above solutions. Here, to keep things simple, we
compare only with linear theory for an  = 1 background.
(1) Turnround. The sphere breaks away from the general expansion and reaches
a maximum radius at θ = π, t = π B. At this point, the true density
enhancement with respect to the background is just [ A(6t/B)2/3/2]3 /r 3 =
9π 2 /16  5.55.
(2) Collapse. If only gravity operates, then the sphere will collapse to a
singularity at θ = 2π. This occurs when δlin = (3/20)(12π)2/3  1.69.
(3) Virialization. Consider the time at which the sphere has collapsed by a
factor 2 from maximum expansion. At this point, it has kinetic energy
K related to potential energy V by V = −2K . This is the condition for
equilibrium, according to the virial theorem. For this reason, many workers
take this epoch as indicating the sort of density contrast to be expected as
the endpoint of gravitational collapse. This occurs at θ = 3π/2, and the
corresponding density enhancement is (9π + 6)2 /8  147, with δlin  1.58.
Some authors prefer to assume that this virialized size is eventually achieved
only at collapse, in which case the contrast becomes (6π)2 /2  178.
These calculations are the basis for a common ‘rule of thumb’, whereby one
assumes that linear theory applies until δlin is equal to some δc a little greater than
unity, at which point virialization is deemed to have occurred. Although this only
applies for  = 1, analogous results can be worked out from the full δlin(z, )
and t (z, ) relations; δlin  1 is a good criterion for collapse for any value of 
likely to be of practical relevance. The full density contrast at virialization may
be approximated by
1 + δvir  178−0.7
(although flat -dominated models show less dependence on ; Eke et al 1996).

2.7 Quantifying large-scale structure
The next step is to see how these theoretical ideas can be confronted with
statistical measures of the observed matter distribution, and to summarize what
is known about the dimensionless density perturbation field
δ(x) ≡

ρ(x) − ρ
.
ρ

A critical feature of the δ field is that it inhabits a universe that is isotropic
and homogeneous in its large-scale properties. This suggests that the statistical

Quantifying large-scale structure

59

properties of δ should also be homogeneous, even though it is a field that describes
inhomogeneities.
We will often need to use the · · · symbol, that denotes averaging over an
ensemble of realizations of the statistical δ process. In practice, this will usually
be equated to the spatial average over a sufficiently large volume. Fields that
satisfy this property, whereby
volume average ↔ ensemble average
are termed ergodic.
2.7.1 Fourier analysis of density fluctuations
It is often convenient to consider building up a general field by the superposition
of many modes. For a flat comoving geometry, the natural tool for achieving this
is via Fourier analysis. How do we make a Fourier expansion of the density field
in an infinite universe? If the field were periodic within some box of side L, then
we would just have a sum over wave modes:

F(x) =
Fk e−ik·x .
The requirement of periodicity restricts the allowed wavenumbers to
harmonic!boundary conditions
kx = n

2π
,
L

n = 1, 2 . . . ,

with similar expressions for k y and k z . Now, if we let the box become arbitrarily
large, then the sum will go over to an integral that incorporates the density of
states in k-space, exactly as in statistical mechanics. The Fourier relations in n
dimensions are thus
 n 
L
Fk (k) exp(−ik · x) dn k
F(x) =
2π
 n 
1
F(x) exp(ik · x) dn x.
Fk (k) =
L
As an immediate example of the Fourier machinery in action, consider the
important quantity
ξ(r) ≡ δ(x)δ(x + r),
which is the autocorrelation function of the density field—usually referred to
simply as the correlation function. The angle brackets indicate an averaging over
the normalization volume V . Now express δ as a sum and note that δ is real, so
that we can replace one of the two δ’s by its complex conjugate, obtaining


∗ i(k −k)·x −ik·r
.
δk δk e
e
ξ=
k

k

60

An introduction to the physics of cosmology

Alternatively, this sum can be obtained without replacing δδ by δδ ∗ , from the
relation between modes with opposite wavevectors that holds for any real field:
δk (−k) = δk∗ (k). Now, by the periodic boundary conditions, all the cross terms
with k = k average to zero. Expressing the remaining sum as an integral, we
have

V
ξ(r) =
|δk |2 e−ik·r d3 k.
(2π)3
In short, the correlation function is the Fourier transform of the power spectrum.
This relation has been obtained by volume averaging, so it applies to the specific
mode amplitudes and correlation function measured in any given realization of
the density field. Taking ensemble averages of each side, the relation clearly
also holds for the ensemble average power and correlations—which are really the
quantities that cosmological studies aim to measure. We shall hereafter often use
the alternative notation
P(k) ≡ |δk |2 
for the ensemble-average power.
In an isotropic universe, the density perturbation spectrum cannot contain
a preferred direction, and so we must have an isotropic power spectrum:
|δk |2 (k) = |δk |2 (k). The angular part of the k-space integral can therefore be
performed immediately: introduce spherical polars with the polar axis along k,
and use the reality of ξ so that e−ik·x → cos(kr cos θ ). In three dimensions, this
yields

sin kr
V
4πk 2 dk.
P(k)
ξ(r ) =
(2π)3
kr
We shall usually express the power spectrum in dimensionless form, as the
variance per ln k (2 (k) = dδ 2 /d ln k ∝ k 3 P[k]):

V
2 3 ∞
sin kr 2
3
k
r dr.
2 (k) ≡
4πk
P(k)
=
ξ(r )
3
(2π)
π
kr
0
This gives a more easily visualizable meaning to the power spectrum than does
the quantity V P(k), which has dimensions of volume: 2 (k) = 1 means that
there are order-unity density fluctuations from modes in the logarithmic bin
around wavenumber k. 2 (k) is therefore the natural choice for a Fourier-space
counterpart to the dimensionless quantity ξ(r ).
This shows that the power spectrum is a central quantity in cosmology,
but how can we predict its functional form? For decades, this was thought to
be impossible, and so a minimal set of assumptions was investigated. In the
absence of a physical theory, we should not assume that the spectrum contains
any preferred length scale, otherwise we should then be compelled to explain this
feature. Consequently, the spectrum must be a featureless power law:
|δk |2  ∝ k n .
The index n governs the balance between large-and small-scale power.

Quantifying large-scale structure

61

A power-law spectrum implies a power-law correlation function. If ξ(r ) =
(r/r0 )−γ , with γ = n + 3, the corresponding 3D power spectrum is
2 (k) =

2
(2 − γ )π
(kr0 )γ (2 − γ ) sin
≡ β(kr0 )γ
π
2

(= 0.903(kr0)1.8 if γ = 1.8). This expression is only valid for n < 0 (γ < 3); for
larger values
 ∞of n, ξ must become negative at large r (because P(0) must vanish,
implying 0 ξ(r )r 2 dr = 0). A cut-off in the spectrum at large k is needed to
obtain physically sensible results.
Most important of all is the scale-invariant spectrum, which corresponds to
the value n = 1, i.e. 2 ∝ k 4 . To see how the name arises, consider a perturbation
δ in the gravitational potential:
∇ 2 δ = 4π Gρ0 δ ⇒ δk = −4π Gρ0 δk /k 2 .
The two powers of k pulled down by ∇ 2 mean that, if 2 ∝ k 4 for the
power spectrum of density fluctuations, then 2 is a constant. Since potential
perturbations govern the flatness of spacetime, this says that the scale-invariant
spectrum corresponds to a metric that is a fractal: spacetime has the same degree
of ‘wrinkliness’ on each resolution scale.
2.7.2 The CDM model
The CDM model is the simplest model for structure formation, and it is worth
examining in some detail. The CDM linear-theory spectrum modifications are
illustrated in figure 2.9. The primordial power-law spectrum is reduced at large
k, by an amount that depends on both the quantity of dark matter and its nature.
Generally the bend in the spectrum occurs near 1/k of the order of the horizon
size at matter–radiation equality, proportional to (h 2 )−1 . For a pure CDM
universe, with scale-invariant initial fluctuations (n = 1), the observed spectrum
depends only on two parameters. One is the shape  = h, and the other is
a normalization. On the shape front, a government health warning is needed,
as follows. It has been quite common to take -based fits to observations as
indicating a measurement of h, but there are three reasons why this may give
incorrect answers:
(1) The dark matter may not be CDM. An admixture of HDM will damp the
spectrum more, mimicking a lower CDM density.
(2) Even in a CDM-dominated universe, baryons can have a significant effect,
making  lower than h.
(3) The strongest (and most-ignored) effect is tilt: if n = 1, then even in a
pure CDM universe a -model fit to the spectrum will give a badly incorrect
estimate of the density (the change in h is roughly 0.3(n − 1); Peacock and
Dodds 1994).

62

An introduction to the physics of cosmology

Figure 2.9. This figure illustrates how the primordial power spectrum is modified as a
function of density in a CDM model. For a given tilt, it is always possible to choose a
density that satisfies both the COBE and cluster normalizations.

The other parameter is the normalization. This can be set at a number of
points. The COBE normalization comes from large-angle CMB anisotropies,
and is sensitive to the power spectrum at k  10−3 h Mpc−1 . The alternative
is to set the normalization near the quasilinear scale, using the abundance of rich
clusters. Many authors have tried this calculation, and there is good agreement on
the answer:
σ8  (0.5 − 0.6)−0.6
m ,
where σ8 is the fractional rms variation in the linear density field, when convolved
with a sphere of radium 8h −1 Mpc (White et al 1993, Eke et al 1996, Viana and
Liddle 1996). In many ways, this is the most sensible normalization to use for
LSS studies, since it does not rely on an extrapolation from larger scales.

Quantifying large-scale structure

63

2.7.3 Karhunen–Loève and all that
A key question for these statistical measures is how accurate they are—i.e. how
much does the result for a given finite sample depart from the ideal statistic
averaged over an infinite universe? Terminology here can be confusing, in that a
distinction is sometimes made between sampling variance and cosmic variance.
The former is to be understood as arising from probing a given
√ volume only with a
finite number of galaxies (e.g. just the bright ones), so that N statistics limit our
knowledge of the mass distribution within that region. The second term concerns
whether we have reached a fair sample of the universe, and depends on whether
there is significant power in density perturbation modes with wavelengths larger
than the sample depth. Clearly, these two aspects are closely related.
The quantitative analysis of these errors is most simply performed in Fourier
space, and was given by Feldman et al (1994). The results can be understood
most simply by comparison with an idealized complete and uniform survey of a
volume L 3 , with periodicity scale L. For an infinite survey, the arbitrariness of
the spatial origin means that different modes are uncorrelated:
δk (ki )δk∗ (k j ) = P(k)δi j .
Each mode has an exponential distribution in power (because the complex
coefficients δk are 2D Gaussian-distributed variables on the Argand plane), for
which the mean and rms are identical. The fractional uncertainty in the mean
power measured over some k-space volume is then just determined by the number
of uncorrelated modes averaged over
δ P̄
1
= 1/2 ;
P̄
Nmodes


Nmodes =

L
2π

3 
d3 k.

The only subtlety is that, because the density field is real, modes at k and −k are
perfectly correlated. Thus, if the k-space volume is a shell, the effective number
of uncorrelated modes is only half this expression.
Analogous results apply for an arbitrary survey selection function. In the
continuum limit, the Kroneker delta in the expression for mode correlation would
be replaced a term proportional to a delta-function, δ[ki − k j ]. Now, multiplying
the infinite ideal survey by a survey window, ρ(r), is equivalent to convolution
in the Fourier domain, with the result that the power per mode is correlated over
k-space separations of order 1/D, where D is the survey depth.
Given this expression for the fractional power, it is clear that the precision of
the estimate can be manipulated by appropriate weighting of the data: giving
increased weight to the most distant galaxies increases the effective survey
volume, boosting the number of modes. This sounds too good to be true, and
of course it is: the previous expression for the fractional power error applies to
the sum of true clustering power and shot noise. The latter arises because we
transform a point process. Given a set of N galaxies, we would estimate Fourier

64

An introduction to the physics of cosmology

coefficients via δk = (1/N)
is

i

exp(−ik · x i ). From this, the expectation power

|δk |2  = P(k) + 1/N.
The existence of an additive discreteness correction is no problem, but the
fluctuations on the shot noise hide the signal of interest. Introducing weights
boosts the shot noise, so there is an optimum choice of weight that minimizes
the uncertainty in the power after shot-noise subtraction. Feldman et al (1994)
showed that this weight is
w = (1 + n̄ P)−1 ,
where n̄ is the expected galaxy number density as a function of position in the
survey.
Since the correlation of modes arises from the survey selection function,
it is clear that weighting the data changes the degree of correlation in k space.
Increasing the weight in low-density regions increases the effective survey
volume, and so shrinks the k-space coherence scale. However, the coherence
scale continues to shrink as distant regions of the survey are given greater weight,
whereas the noise goes through a minimum. There is thus a trade-off between the
competing desirable criteria of high k-space resolution and low noise. Tegmark
(1996) shows how weights may be chosen to implement any given prejudice
concerning the relative importance of these two criteria. See also Hamilton
(1997a, b) for similar arguments.
Finally, we note that this discussion strictly applies only to the case of
Gaussian density fluctuations—which cannot be an accurate model on nonlinear
scales. In fact, the errors in the power spectrum are increased on nonlinear scales,
and modes at all k have their amplitudes coupled to some degree by nonlinear
evolution. These effects are not easy to predict analytically, and are best dealt with
by running numerical simulations (see Meiksin and White 1999, Scoccimarro et
al 1999).
Given these difficulties with correlated results, it is attractive to seek a
method where the data can be decomposed into a set of statistics that are
completely uncorrelated with each other. Such a method is provided by the
Karhunen–Loève formalism. Vogeley and Szalay (1996) argued as follows.
Define a column vector of data d; this can be quite abstract in nature, and could
be e.g. the numbers of galaxies in a set of cells, or a set of Fourier components
of the transformed galaxy number counts. Similarly, for CMB studies, d could
be δT /T in a set of pixels, or spherical-harmonic coefficients a"m . We assume
that the mean can be identified and subtracted off, so that d = 0 in ensemble
average. The statistical properties of the data are then described by the covariance
matrix
Ci j ≡ di d ∗j 
(normally the data will be real, but it is convenient to keep things general and
include the complex conjugate).

Quantifying large-scale structure

65

Suppose we seek to expand the datavector in terms of a set of new
orthonormal vectors:

d=
ai ψ i ; ψ ∗i · ψ j = δi j .
i

The expansion coefficients are extracted in the usual way: a j = d · ψ ∗j . Now
require that these coefficients be statistically uncorrelated, ai a ∗j  = λi δi j (no
sum on i ). This gives
ψ ∗i · d d ∗  · ψ j = λi δi j ,
where the dyadic d d ∗  is C, the correlation matrix of the data vector: (d d ∗ )i j ≡
di d ∗j . Now, the effect of operating this matrix on one of the ψ i must be expandable
in terms of the complete set, which shows that the ψ j must be the eigenvectors of
the correlation matrix:
d d ∗  · ψ j = λ j ψ j .
Vogeley and Szalay (1996) further show that these uncorrelated modes are
optimal for representing the data: if the modes are arranged in order of decreasing
λ, and the series expansion truncated after n terms, the rms truncation error is
minimized for this choice of eigenmodes. To prove this, consider the truncation
error
n
∞


=d−
ai ψ i =
ai ψ i .
i=1

The square of this is
 2  =

i=n+1
∞


|ai |2 ,

i=n+1

|2 

ψ ∗i

· C · ψ i , as before. We want to minimize  2  by varying the
where |ai =
ψ i , but we need to do this in a way that preserves normalization. This is achieved
by introducing a Lagrange multiplier, and minimizing

ψ ∗i · C · ψ i + λ(1 − ψ ∗i · ψ i ).
This is easily solved if we consider the more general problem where ψ ∗i and ψ i
are independent vectors:
C · ψ i = λψi .
In short, the eigenvectors of C are optimal in a least-squares sense for expanding
the data. The process of truncating the expansion is a form of lossy data
compression, since the size of the data vector can be greatly reduced without
significantly affecting the fidelity of the resulting representation of the universe.
The process of diagonalizing the covariance matrix of a set of data also goes
by the more familiar name of principal components analysis (PCA), so what is the
difference between the KL approach and PCA? In the previous discussion, they

66

An introduction to the physics of cosmology

are identical, but the idea of choosing an optimal eigenbasis is more general than
PCA. Consider the case where the covariance matrix can be decomposed into a
‘signal’ and a ‘noise’ term:
C = S + N,
where S depends on cosmological parameters that we might wish to estimate,
whereas N is some fixed property of the experiment under consideration. In the
simplest imaginable case, N might be a diagonal matrix, so PCA diagonalizes
both S and N . In this case, ranking the PCA modes by eigenvalue would
correspond to ordering the modes according to signal-to-noise ratio. Data
compression by truncating the mode expansion then does the sensible thing: it
rejects all modes of low signal-to-noise ratio.
However, in general these matrices will not commute, and there will not
be a single set of eigenfunctions that are common to the S and N matrices.
Normally, this would be taken to mean that it is impossible to find a set
of coordinates in which both are diagonal. This conclusion can however be
evaded, as follows. When considering the effect of coordinate transformations
on vectors and matrices, we are normally forced to consider only rotation-like
transformations that preserve the norm of a vector (e.g. in quantum mechanics,
so that states stay normalized). Thus, we write d = R · d, where R is unitary,
so that R · R † = I . If R is chosen so that its columns are the eigenvalues of
N , then the transformed noise matrix, R · N · R † , is diagonal. Nevertheless, if
the transformed S is not diagonal, the two will not commute. This apparently
insuperable problem can be solved by using the fact that the data vectors are
entirely abstract at this stage. There is therefore no reason not to consider the
further transformation of scaling the data, so that N becomes proportional to the
identity matrix. This means that the transformation is no longer unitary – but
there is no physical reason to object to a change in the normalization of the data
vectors.
Suppose we therefore make a further transformation
d = W ·d .
The matrix W is related to the rotated noise matrix:
√
√
N = diag(n 1 , n 2 , . . .) ⇒ W = diag(1/ n 1 , 1/ n 2 , . . .).
This transformation is termed prewhitening by Vogeley and Szalay (1996), since
it converts the noise matrix to white noise, in which each pixel has a unit noise
that is uncorrelated with other pixels. The effect of this transformation on the full
covariance matrix is
Ci j ≡ di d j ∗  ⇒ C = (W · R) · C · (W · R)† .
After this transformation, the noise and signal matrices certainly do commute,
and the optimal modes for expanding the new data are once again the PCA

Quantifying large-scale structure

67

eigenmodes in the new coordinates:
C · ψ i = λψ i .
These eigenmodes must be expressible in terms of some modes in the original
coordinates, ei :
ψ i = (W · R) · ei .
In these terms, the eigenproblem is
(W · R) · C · (W · R)† · (W · R) · ei = λ(W · R) · ei .
This can be simplified using W † · W = N

−1

and N

−1

= R · N −1 R † , to give

C · N −1 · ei = λei ,
so the required modes are eigenmodes of C · N −1 . However, care is required when
considering the orthonormality of the ei : ψ †i · ψ j = e†i · N −1 · e j , so the ei are
not orthonormal. If we write d = i ai ei , then
ai = (N −1 · ei )† · d ≡ ψ †i · d.
Thus, the modes used to extract the compressed data by dot product satisfy
C · ψ = λN · ψ, or finally
S · ψ = λ N · ψ,
given a redefinition of λ. The optimal modes are thus eigenmodes of N −1 · S,
hence the name signal-to-noise eigenmodes (Bond 1995, Bunn 1995).
It is interesting to appreciate that the set of KL modes just discussed is also
the ‘best’ set of modes to choose from a completely different point of view:
they are the modes that are optimal for estimation of a parameter via maximum
likelihood. Suppose we write the compressed data vector, x, in terms of a nonsquare matrix A (whose rows are the basis vectors ψ ∗i ):
x = A · d.
The transformed covariance matrix is
D ≡ x x †  = A · C · A† .
For the case where the original data obeyed Gaussian statistics, this is true for the
compressed data also, so the likelihood is
−2 ln L = ln det D + x ∗ · D −1 · x + constant.
The normal variance on some parameter p (on which the covariance matrix
depends) is
d2 [−2 ln L]
1
=
.
2
σp
dq 2

68

An introduction to the physics of cosmology

Without data, we do not know this, so it is common to use the expectation value
of the right-hand side as an estimate (recently, there has been a tendency to dub
this the ‘Fisher matrix’).
We desire to optimize σp by an appropriate choice of data-compression
vectors, ψ i . By writing σp in terms of A, C and d, it may eventually be shown
that the desired optimal modes satisfy


d
C · ψ = λ C · ψ.
dp
For the case where the parameter of interest is the cosmological power, the
matrix on the left-hand side is just proportional to S, so we have to solve the
eigenproblem
S · ψ = λC · ψ.
With a redefinition of λ, this becomes
S · ψ = λN · ψ.
The optimal modes for parameter estimation in the linear case are thus identical
to the PCA modes of the prewhitened data discussed earlier. The more general
expression was given by Tegmark et al (1997), and it is only in this case, where
the covariance matrix is not necessarily linear in the parameter of interest, that the
KL method actually differs from PCA.
The reason for going to all this trouble is that the likelihood can now be
evaluated much more rapidly, using the compressed data. This allows extensive
model searches over large parameter spaces that would be infeasible with the
original data (since inversion of an N × N covariance matrix takes a time
proportional to N 3 ). Note, however, that the price paid for this efficiency is that
a different set of modes need to be chosen depending on the model of interest,
and that these modes will not in general be optimal for expanding the dataset
itself. Nevertheless, it may be expected that application of these methods will
inevitably grow as datasets increase in size. Present applications mainly prove that
the techniques work: see Matsubara et al (2000) for application to the LCRS (Las
Campanas Redshift Survey), or Padmanabhan et al (1999) for the UZC (Updated
Zwicky Catalog) survey. The next generation of experiments will probably be
forced to resort to data compression of this sort, rather than using it as an elegant
alternative method of analysis.
2.7.4 Projection on the sky
A more common situation is where we lack any distance data; we then deal with
a projection on the sky of a magnitude-limited set of galaxies at different depths.
The statistic that is observable is the angular correlation function, w(θ ), or its
angular power spectrum 2θ . If the sky were flat, the relation between these would

Quantifying large-scale structure

69

be the usual Hankel transform pair:
 ∞
2θ J0 (K θ ) dK /K ,
w(θ ) =
0
 ∞
2
2
θ = K
w(θ )J0 (K θ )θ dθ.
0

For power-law clustering, w(θ ) = (θ/θ0 )− , this gives
2θ (K ) = (K θ0 ) 21−

(1 − /2)
,
(/2)

which is equal to 0.77(K θ0) for  = 0.8. At large angles, these relations
are not quite correct. We should really expand the sky distribution in spherical
harmonics:

δ(q̂) =
a"m Y"m (q̂),
where q̂ is a unit vector that specifies direction on the sky. The functions Y"m
are the eigenfunctions of the angular part of the ∇ 2 operator: Y"m (θ, φ) ∝
exp(imφ)P"m (cos θ ), where P"m are the associated Legendre polynomials (see
e.g. section 6.8 of Press et al 1992). Since the spherical harmonics satisfy the
orthonormality relation Y"m Y"∗m d2 q = δ"" δmm , the inverse relation is

∗ 2
a"m = δ(q̂)Y"m
d q.
The analogues of the Fourier relations for the correlation function and power
spectrum are
m=+"
1   m2
|a" | P" (cos θ )
4π
" m=−"
 1
|a"m |2 = 2π
w(θ )P" (cos θ ) d cos θ.

w(θ ) =

−1

For small θ and large ", these go over to a form that looks like a flat sky, as
follows. Consider the asymptotic forms for the Legendre polynomials and the J0
Bessel function:




1
1
2
θ− π
cos " +
P" (cos θ ) 
π" sin θ
2
4



1
2
J0 (z) 
cos z − π ,
πz
4
for respectively " → ∞, z → ∞; see chapters 8 and 9 of Abramowitz and
Stegun (1965). This shows that, for "  1, we can approximate the small-angle

70

An introduction to the physics of cosmology

correlation function in the usual way in terms of an angular power spectrum 2θ
and angular wavenumber K :


∞

w(θ ) =
0

2θ (K )J0 (K θ )

2θ (K = " + 12 ) =

dK
K

2" + 1  m 2
|a" | .
8π
m

An important relation is that between the angular and spatial power spectra.
In outline, this is derived as follows. The perturbation seen on the sky is


∞

δ(q̂) =

δ( y)y 2 φ(y) dy,

0


where φ(y) is the selection function, normalized such that y 2 φ(y) dy = 1,
and y is comoving distance. The function φ is the comoving density of objects
in the survey, which is given by the integrated luminosity function down to
the luminosity limit corresponding to the limiting flux of the survey seen at
different redshifts; a flat universe ( = 1) is assumed for now. Now write
down the Fourier expansion of δ. The plane waves may be related to spherical
harmonics via the expansion of a plane wave in spherical Bessel functions
j"(x) = (π/2x)1/2 Jn+1/2 (x) (see chapter 10 of Abramowitz and Stegun (1965)
or section 6.7 of Press et al (1992)):
∞

(2" + 1)i " P" (cos θ ) j"(kr ),

eikr cos θ =

0

plus the spherical harmonic addition theorem
P" (cos θ ) =

m=+"
4π  ∗
Y"m (q̂)Y"m (q̂ );
2" + 1

q̂ · q̂ = cos θ.

m=−"

These relations allow us to take the angular correlation function w(θ ) =
δ(q̂)δ(q̂ ) and transform it to give the angular power spectrum coefficients. The
actual manipulations involved are not as intimidating as they may appear, but they
are left as an exercise and we simply quote the final result (Peebles 1973):

|a"m |2 

= 4π

dk
 (k)
k
2

2


2

y φ(y) j"(ky) dy

.

What is the analogue of this formula for small angles? Rather than
manipulating large-" Bessel functions, it is easier to start again from the
correlation function. By writing as before the overdensity observed at a particular

Quantifying large-scale structure

71

direction on the sky as a radial integral over the spatial overdensity, with a
weighting of y 2 φ(y), we see that the angular correlation function is

δ(q̂1 )δ(q̂2 ) =
δ( y1 )δ( y2 )y12 y22 φ(y1 )φ(y2 ) dy1 dy2 .
We now change variables to the mean and difference of the radii, y ≡ (y1 + y2 )/2;
x ≡ (y1 − y2 ). If the depth of the survey is larger than any correlation length, we
only get a signal when y1  y2  y. If the selection function is a slowly varying
function, so that the thickness of the shell being observed is also of order of the
depth, the integration range on x may be taken as being infinite. For small angles,
we then obtain Limber’s equation:

 ∞
 ∞ 
4 2
2
2
2
w(θ ) =
y φ dy
ξ
x +y θ
dx
−∞

0

(see sections 51 and 56 of Peebles 1980). Theory usually supplies a prediction
about the linear density field in the form of the power spectrum, and so it is
convenient to recast Limber’s equation:
 ∞
 ∞
w(θ ) =
y 4 φ 2 dy
π2 (k)J0 (kyθ ) dk/k 2.
0

0

This power-spectrum version of Limber’s equation is already in the form required
for the relation to the angular power spectrum, and so we obtain the direct smallangle relation between spatial and angular power spectra:

π
2
θ =
2 (K /y)y 5 φ 2 (y) dy.
K
This is just a convolution in log space, and is considerably simpler to evaluate and
interpret than the w–ξ version of Limber’s equation.
Finally, note that it is not difficult to make allowance for spatial curvature in
this discussion. Write the RW metric in the form


2
dr
+ r 2θ 2 ;
c2 dτ 2 = c2 dt 2 − R 2
1 − kr 2
for k = 0, the notation y = R0r was used for comoving distance, where
R0 = (c/H0)|1 − |−1/2. The radial increment of comoving distance was
dx = R0 dr , and the comoving distance between two objects was (dx 2 + y 2 θ 2 )1/2 .
To maintain this version of Pythagoras’s theorem, we clearly need to keep the
definition of y and redefine radial distance: dx = R0 dr C(y), where C(y) =
[1 − k(y/R0 )2 ]−1/2. The factor C(y) appears in the non-Euclidean comoving
volume element, dV ∝ y 2 C(y) dy, so that we now require the normalization
equation for φ to be

∞

0

y 2 φ(y)C(y) dy = 1.

72

An introduction to the physics of cosmology

The full version of Limber’s equation therefore gains two powers of C(y), but
one of these is lost in converting between R0 dr and dx:

 ∞
 ∞ 
dx
2 4 2
2
2
2
w(θ ) =
.
[C(y)] y φ dy
ξ
x +y θ
C(y)
0
−∞
The net effect is therefore to replace φ 2 (y) by C(y)φ 2 (y), so that the full powerspectrum equation is

π
2θ =
2 (K /y)C(y)y 5φ 2 (y) dy.
K
It is also straightforward to allow for evolution. The power version of Limber’s
equation is really just telling us that the angular power from a number of different
radial shells adds incoherently, so we just need to use the actual evolved power at
that redshift. These integral equations can be inverted numerically to obtain the
real-space 3D clustering results from observations of 2D clustering; see Baugh
and Efstathiou (1993, 1994).
2.7.5 Nonlinear clustering: a problem for CDM?
Observations of galaxy clustering extend into the highly nonlinear regime, ξ .
104 , so it is essential to understand how this nonlinear clustering relates to the
linear-theory initial conditions. A useful trick for dealing with this problem is to
think of the density field under full nonlinear evolution as consisting of a set of
collapsed, virialized clusters. What is the density profile of one of these objects?
At least at separations smaller than the clump separation, the density profile of the
clusters is directly related to the correlation function, since this just measures the
number density of neighbours of a given galaxy. For a very steep cluster profile,
ρ ∝ r − , most galaxies will lie near the centres of clusters, and the correlation
function will be a power law, ξ(r ) ∝ r −γ , with γ = . In general, because
the correlation function is the convolution of the density field with itself, the two
slopes differ. In the limit that clusters do not overlap, the relation is γ = 2 − 3
(for 3/2 <  < 3; see Peebles 1974 or McClelland and Silk 1977). In any case,
the critical point is that the correlation function may be be thought of as arising
directly from the density profiles of clumps in the density field.
In this picture, it is easy to see how ξ will evolve with redshift, since clusters
are virialized objects that do not expand. The hypothesis of stable clustering states
that, although the separation of clusters will alter as the universe expands, their
internal density structure will stay constant with time. This hypothesis clearly
breaks down in the outer regions of clusters, where the density contrast is small
and linear theory applies, but it should be applicable to small-scale clustering.
Regarding ξ as a density profile, its small-scale shape should therefore be fixed
in proper coordinates, and its amplitude should scale as (1 + z)−3 owing to the
changing mean density of unclustered galaxies, which dilute the clustering at high

Quantifying large-scale structure

73

redshift. Thus, with ξ ∝ r −γ , we obtain the comoving evolution
ξ(r, z) ∝ (1 + z)γ −3

(nonlinear).

Since the observed γ  1.8, this implies slower evolution than is expected in the
linear regime:
ξ(r, z) ∝ (1 + z)−2 g()
(linear).
This argument does not so far give a relation between the nonlinear slope γ and
the index n of the linear spectrum. However, the linear and nonlinear regimes
match at the scale of quasilinearity, i.e. ξ(r0 ) = 1; each regime must make
the same prediction for how this break point evolves. The linear and nonlinear
predictions for the evolution of r0 are, respectively, r0 ∝ (1 + z)−2/(n+3) and
r0 ∝ (1 + z)−(3−γ )/γ , so that γ = (3n + 9)/(n + 5). In terms of an effective index
γ = 3 + n NL , this becomes
n NL = −

6
.
5+n

The power spectrum resulting from power-law initial conditions will evolve selfsimilarly with this index. Note the narrow range predicted: −2 < n NL < −1 for
−2 < n < +1, with an n = −2 spectrum having the same shape in both linear
and nonlinear regimes.
For many years it was thought that only these limiting cases of extreme
linearity or nonlinearity could be dealt with analytically, but in a marvelous
piece of alchemy, Hamilton et al (1991; HKLM) suggested a general way of
understanding the linear ↔ nonlinear mapping. This initial idea was extended
into a workable practical scheme by Peacock and Dodds (1996), allowing the
effects of nonlinear evolution to be calculated to a few per cent accuracy for a
wide class of spectra.
Indications from the angular clustering of faint galaxies (Efstathiou et al
1991) and directly from redshift surveys (Le Fèvre et al 1996) are that the
observed clustering of galaxies evolves at about the linear-theory rate for z . 0.5,
rather more rapidly than the scaling solution would indicate. However, any
interpretation of such data needs to assume that galaxies are unbiased tracers of
the mass, whereas the observed high amplitude of clustering of quasars at z  1
(r0  7h −1 Mpc; see Shanks et al 1987, Shanks and Boyle 1994) were an early
warning that some high-redshift objects had clustering that is apparently not due
to gravity alone. When it eventually became possible to measure correlations of
normal galaxies at z & 1 directly, a similar effect was found, with the comoving
strength of clustering being comparable to its value at z = 0 (e.g. Adelberger et
al 1998, Carlberg et al 2000). This presumably states that the increasing degree
of bias due to high-redshift galaxies being rare objects swamps the gravitational
evolution of density fluctuations.
A number of authors have pointed out that the detailed spectral shape
inferred from galaxy data appears to be inconsistent with that of nonlinear

74

An introduction to the physics of cosmology

evolution from CDM initial conditions. (e.g. Efstathiou et al 1990, Klypin et
al 1996, Peacock 1997). Perhaps the most detailed work was carried out by the
Virgo consortium, who carried out N = 2563 simulations of a number of CDM
models (Jenkins et al 1998). Their results are shown in figure 2.10, which gives
the nonlinear power spectrum at various times (cluster normalization is chosen
for z = 0) and contrasts this with the APM data. The lower small panels are the
scale-dependent bias that would be required if the model did, in fact, describe the
real universe, defined as
1/2
 2
gals (k)
.
b(k) ≡
2mass
In all cases, the required bias is non-monotonic; it rises at k & 5h −1 Mpc, but also
displays a bump around k  0.1h −1 Mpc. If real, this feature seems impossible
to understand as a genuine feature of the mass power spectrum; certainly, it is not
at a scale where the effects of even a large baryon fraction would be expected to
act (Eisenstein et al 1998, Meiksin et al 1999).
2.7.6 Real-space and redshift-space clustering
Peculiar velocity fields are responsible for the distortion of the clustering pattern
in redshift space, as first clearly articulated by Kaiser (1987). For a survey
that subtends a small angle (i.e. in the distant-observer approximation), a good
approximation to the anisotropic redshift-space Fourier spectrum is given by the
Kaiser function together with a damping term from nonlinear effects:
δks = δkr (1 + βµ2 )D(kσ µ),
0.6 /b, b being the linear bias parameter of the galaxies under study,
where β = m
and µ = k̂ · r̂. For an exponential distribution of relative small-scale peculiar
velocities (as seen empirically), the damping function is D(y)  (1 + y 2 /2)−1/2,
and σ  400 km s−1 is a reasonable estimate for the pairwise velocity dispersion
of galaxies (e.g. Ballinger et al 1996).
In principle, this distortion should be a robust way to determine  (or at
least β). In practice, the effect has not been easy to see with past datasets. This is
mainly a question of depth: a large survey is needed in order to beat down the shot
noise, but this tends to favour bright spectroscopic limits. This limits the result
both because relatively few modes in the linear regime are sampled, and also
because local survey volumes will tend to violate the small-angle approximation.
Strauss and Willick (1995) and Hamilton (1998) review the practical application
of redshift-space distortions. In the next section, preliminary results are presented
from the 2dF Galaxy Redshift Survey, which shows the distortion effect clearly
for the first time.
Peculiar velocities may be dealt with by using the correlation function
evaluated explicitly as a 2D function of transverse (r⊥ ) and radial (r# ) separation.

Quantifying large-scale structure

75

Figure 2.10. The nonlinear evolution of various CDM power spectra, as determined by
the Virgo consortium (Jenkins et al 1998). The broken curves show the evolving spectra
for the mass, which at no time match the shape of the APM data. This is expressed in the
lower small panels as a scale-dependent bias at z = 0: b2 (k) = PAPM /Pmass .

Integrating along the redshift axis then gives the projected correlation function,
which is independent of the velocities
 ∞
 ∞
r dr
ξ(r⊥ , r# ) dr# = 2
ξ(r )
.
wp (r⊥ ) ≡
2 − r 2 )1/2
(r
−∞
r⊥
⊥

76

An introduction to the physics of cosmology

In principle, this statistic can be used to recover the real-space correlation function
by using the inverse relation for the Abel integral equation:

1 ∞
dy
ξ(r ) = −
wp (y) 2
.
π r
(y − r 2 )1/2
An alternative notation for the projected correlation function is #(r⊥ ) (Saunders
et al 1992). Note that the projected correlation function is not dimensionless, but
has dimensions of length. The quantity #(r⊥ )/r⊥ is more convenient to use in
practice as the projected analogue of ξ(r ).
2.7.7 The state of the art in LSS
We now consider the confrontation of some of these tools with observations.
In the past few years, much attention has been attracted by the estimate of
the galaxy power spectrum from the automatic plate measuring (APM) survey
(Baugh and Efstathiou 1993, 1994, Maddox et al 1996). The APM result was
generated from a catalogue of ∼106 galaxies derived from UK Schmidt Telescope
photographic plates scanned with the Cambridge APM machine; because it is
based on a deprojection of angular clustering, it is immune to the complicating
effects of redshift-space distortions. The difficulty, of course, is in ensuring that
any low-level systematics from e.g. spatial variations in magnitude zero point are
sufficiently well controlled that they do not mask the cosmological signal, which
is of order w(θ ) . 0.01 at separations of a few degrees.
The best evidence that the APM survey has the desired uniformity is the
scaling test, where the correlations in fainter magnitude slices are expected to
move to smaller scales and be reduced in amplitude. If we increase the depth of
the survey by some factor D, the new angular correlation function will be
w (θ ) =

1
w(Dθ ).
D

The APM survey passes this test well; once the overall redshift distribution
is known, it is possible to obtain the spatial power spectrum by inverting a
convolution integral:
 ∞
 ∞
w(θ ) =
y 4 φ 2 dy
π2 (k)J0 (kyθ ) dk/k 2
0

0

(where zero spatial curvature is assumed). Here, φ(y) is the comoving density at
comoving distance y, normalized so that y 2 φ(y) dy = 1.
This integral was inverted numerically by Baugh and Efstathiou (1993), and
gives an impressively accurate determination of the power spectrum. The error
estimates are derived empirically from the scatter between independent regions
of the sky, and so should be realistic. If there are no undetected systematics, these
error bars state that the power is very accurately determined. The APM result

Quantifying large-scale structure

77

has been investigated in detail by a number of authors (e.g. Gaztañaga and Baugh
1998, Eisenstein and Zaldarriaga 1999) and found to be robust; this has significant
implications if true.
Because of the sheer number of galaxies, plus the large volume surveyed,
the APM survey outperforms redshift surveys of the past, at least for the purpose
of determining the power spectrum. The largest surveys of recent years (CfA:
Huchra et al 1990, LCRS: Shectman et al 1996, PSCz: Saunders et al 2000)
contain of the order of 104 galaxy redshifts, and their statistical errors are
considerably larger than those of the APM. On the other hand, it is of great
importance to compare the results of deprojection with clustering measured
directly in 3D.
This comparison was carried out by Peacock and Dodds (1994; PD94). The
exercise is not straightforward, because the 3D results are affected by redshiftspace distortions; also, different galaxy tracers can be biased to different extents.
The approach taken was to use each dataset to reconstruct an estimate of the
linear spectrum, allowing the relative bias factors to float in order to make these
estimates agree as well as possible (figure 2.11). To within a scatter of perhaps
a factor 1.5 in power, the results were consistent with a   0.25 CDM model.
Even though the subsequent sections will discuss some possible disagreements
with the CDM models at a higher level of precision, the general existence of
CDM-like curvature in the spectrum is likely to be an important clue to the nature
of the dark matter.
An important general lesson can be drawn from the lack of large-amplitude
features in the power spectrum. This is a strong indication that collisionless
matter is deeply implicated in forming large-scale structure. Purely baryonic
models contain large bumps in the power spectrum around the Jeans’ length
prior to recombination (k ∼ 0.03h 2 Mpc−1 ), whether the initial conditions are
isocurvature or adiabatic. It is hard to see how such features can be reconciled
with the data, beyond a ‘visibility’ in the region of 20%.
The proper resolution of many of the observational questions regarding the
large-scale distribution of galaxies requires new generations of redshift survey
that push beyond the N = 105 barrier. Two groups are pursuing this goal. The
Sloan survey (e.g. Margon 1999) is using a dedicated 2.5-m telescope to measure
redshifts for approximately 700 000 galaxies to r = 18.2 in the North Galactic
Cap. The 2dF Galaxy Redshift Survey (e.g. Colless 1999) is using a fraction
of the time on the 3.9-m Anglo-Australian Telescope plus Two-Degree Field
spectrograph to measure 250 000 galaxies from the APM survey to BJ = 19.45
in the South Galactic Cap. At the time of writing, the Sloan spectroscopic survey
has yet to commence. However, the 2dFGRS project has measured in excess of
100 000 redshifts, and some preliminary clustering results are given here. For
more details of the survey, particularly the team members whose hard work has
made all this possible, see http://www.mso.anu.edu.au/2dFGRS/.
One of the advantages of 2dFGRS is that it is a fully sampled survey,
so that the space density out to the depth imposed by the magnitude limit

78

An introduction to the physics of cosmology

Figure 2.11. The PD94 compilation of power-spectrum measurements. The upper panel
shows raw power measurements; the lower shows these data corrected for relative bias,
nonlinear effects and redshift-space effects.

(median z = 0.12) is as high as nature allows: apart from a tail of low surface
brightness galaxies (inevitably omitted from any spectroscopic survey), the
2dFGRS measure all the galaxies that exist over a cosmologically representative
volume. It is the first to achieve this goal. The fidelity of the resulting map of the
galaxy distribution can be seen in figure 2.12, which shows a small subset of the
data: a slice of thickness 4 degrees, centred at declination −27◦.
An issue with using the 2dFGRS data in their current form is that the
sky has to be divided into circular ‘tiles’ each two degrees in diameter (‘2dF’
= ‘two-degree field’, within which the AAT is able to measure 400 spectra
simultaneously; see http://www.aao.gov.au/2df/ for details of the instrument). The
tiles are positioned adaptively, so that larger overlaps occur in regions of high
galaxy density. In this way, it is possible to place a fibre on >95% of all galaxies.

Quantifying large-scale structure

79

Figure 2.12. A four-degree thick slice of the southern strip of the 2dF Galaxy Redshift
Survey. This restricted region alone contains 16 419 galaxies.

However, while the survey is in progress, there exist parts of the sky where
the overlapping tiles have not yet been observed, and so the effective sampling
fraction is only 50%. These effects can be allowed for in two different ways. In
clustering analyses, we compare the counts of pairs (or n-tuplets) of galaxies in
the data to the corresponding counts involving an unclustered random catalogue.
The effects of variable sampling can therefore be dealt with either by making the
density of random points fluctuate according to the sampling, or by weighting
observed galaxies by the reciprocal of the sampling factor for the zone in which
they lie. The former approach is better from the point of view of shot noise, but
the latter may be safer if there is any suspicion that the sampling fluctuations are
correlated with real structure on the sky. In practice, both strategies give identical
answers for the results below.
At the two-point level, the most direct quantity to compute is the redshift–
space correlation function. This is an anisotropic function of the orientation of a
galaxy pair, owing to peculiar velocities. We therefore evaluate ξ as a function
of 2D separation in terms of coordinates both parallel and perpendicular to the
line of sight. If the comoving radii of two galaxies are y1 and y2 and their total
separation is r , then we define coordinates
π ≡ |y1 − y2 |;

σ =

r 2 − π 2.

The correlation function measured in these coordinates is shown in figure 2.13. In

80

An introduction to the physics of cosmology

Figure 2.13. The redshift–space correlation function for the 2dFGRS, ξ(σ, π), plotted
as a function of transverse (σ ) and radial (π) pair separation. The function was
estimated by counting pairs in boxes of side 0.2h −1 Mpc, and then smoothing with
a Gaussian of rms width 0.5h −1 Mpc. This plot clearly displays redshift distortions,
with ‘fingers of God’ elongations at small scales and the coherent Kaiser flattening at
large radii. The overplotted contours show model predictions with flattening parameter
β ≡ 0.6 /b = 0.4 and a pairwise dispersion of σp = 4h −1 Mpc. Contours are plotted at
ξ = 10, 5, 2, 1, 0.5, 0.2, 0.1.

evaluating ξ(σ, π), the optimal radial weight discussed earlier has been applied,
so that the noise at large r should be representative of true cosmic scatter.
The 2dFGRS results for the redshift-space correlation function results are
shown in figure 2.13, and display very clearly the two signatures of redshiftspace distortions discussed earlier. The fingers of God from small-scale random
velocities are very clear, as indeed has been the case from the first redshift surveys
(e.g. Davis and Peebles 1983). However, this is arguably the first time that the
large-scale flattening from coherent infall has been really obvious in the data.
A good way to quantify the flattening is to analyse the clustering as a function
of angle into Legendre polynomials:
2" + 1
ξ" (r ) =
2



1
−1

ξ(σ = r sin θ, π = r cos θ )P" (cos θ ) d cos θ.

Quantifying large-scale structure

81

Figure 2.14. The flattening of the redshift–space correlation function is quantified by
the quadrupole-to-monopole ratio, ξ2 /ξ0 . This quantity is positive where fingers-of-God
distortion dominates, and is negative where coherent infall dominates. The full curves show
model predictions for β = 0.3, 0.4 and 0.5, with σp = 4h −1 Mpc (full), plus β = 0.4 with
σp = 3, 4 and 5h −1 Mpc (chain). At large radii, the effects of fingers-of-God are becoming
relatively small, and values of β  0.4 are clearly appropriate.

The quadrupole-to-monopole ratio should be a clear indicator of coherent infall.
In linear theory, it is given by
4β/3 + 4β 2 /7
ξ2
= f (n)
,
ξ0
1 + 2β/3 + β 2 /5
where f (n) = (3 + n)/n (Hamilton 1992). On small and intermediate scales, the
effective spectral index is negative, so the quadrupole-to-monopole ratio should
be negative, as observed.
However, it is clear that the results on the largest scales are still significantly
affected by finger-of-God smearing. The best way to interpret the observed effects
is to calculate the same quantities for a model. To achieve this, we use the
observed APM 3D power spectrum, plus the distortion model discussed earlier.
This gives the plots shown in figure 2.14. The free parameter is β, and this has
a best-fit value close to 0.4, approximately consistent with other arguments for a
universe with  = 0.3 and a small degree of large-scale galaxy bias.
2.7.8 Galaxy formation and biased clustering
We now come to the difficult question of the relation between the galaxy
distribution and the large-scale density field. The formation of galaxies must be

82

An introduction to the physics of cosmology

a non-local process to some extent, and the modern paradigm was introduced by
White and Rees (1978): galaxies form through the cooling of baryonic material
in virialized halos of dark matter. The virial radii of these systems are in excess of
0.1 Mpc, so there is the potential for large differences in the correlation properties
of galaxies and dark matter on these scales.
A number of studies have indicated that the observed galaxy correlations
may indeed be reproduced by CDM models. The most direct approach is a
numerical simulation that includes gas, and relevant dissipative processes. This
is challenging, but just starting to be feasible with current computing power
Pearce et al 1999). The alternative is ‘semi-analytic’ modelling, in which the
merging history of dark-matter halos is treated via the extended Press–Schechter
theory (Bond et al 1991), and the location of galaxies within halos is estimated
using dynamical-friction arguments (e.g. Kauffmann et al 1993, 1999, Cole et
al 1994, Somerville and Primack 1999, van Kampen et al 1999, Benson et al
2000a, b). Both these approaches have yielded similar conclusions, and shown
how CDM models can match the galaxy data: specifically, the low-density flat
CDM model that is favoured on other grounds can yield a correlation function
that is close to a single power law over 1000 & ξ & 1, even though the mass
correlations show a marked curvature over this range (Pearce et al 1999, Benson
et al 2000a; see figure 2.15). These results are impressive, yet it is frustrating
to have a result of such fundamental importance emerge from a complicated
calculational apparatus. There is thus some motivation for constructing a simpler
heuristic model that captures the main processes at work in the full semi-analytic
models. The following section describes an approach of this sort (Peacock and
Smith 2000; see also Seljak 2000).
An early model for galaxy clustering was suggested by Neyman et al
(1953), in which the nonlinear density field was taken to be a superposition of
randomly placed clumps. With our present knowledge about the evolution of
CDM universes, we can make this idealized model considerably more realistic:
hierarchical models are expected to contain a distribution of masses of clumps,
which have density profiles that are more complicated than isothermal spheres.
These issues are well studied in N-body simulations, and highly accurate fitting
formulae exist, both for the mass function and for the density profiles. Briefly, we
use the mass function of Sheth and Tormen (1999; ST) and the halo profiles of
Moore et al (1999; M99).
√
√
f (ν) = 0.216 17[1 + ( 2/ν 2 )0.3 ] exp[−ν 2 /(2 2)]
⇒ F(> ν) = 0.322 18[1 − erf(ν/23/4)]
√
+ 0.147 65[0.2, ν 2/(2 2)],
where  is the incomplete gamma function.
Recently, it has been claimed by Moore et al (1999; M99) that the commonly
adopted density profile of Navarro et al (1996; NFW) is in error at small r . M99

Quantifying large-scale structure

83

Figure 2.15. The correlation function of galaxies in the semi-analytical simulation of an
LCDM universe by Benson et al (2000a).

proposed the alternative form
ρ/ρb =

c
3/2
y (1 +

y 3/2 )

(r < rvir );

y ≡ r/rc .

Using this model, it is then possible to calculate the correlations of the nonlinear
density field, neglecting only the large-scale correlations in halo positions. The
power spectrum determined in this way is shown in figure 2.16, and turns out to
agree very well with the exact nonlinear result on small and intermediate scales.
The lesson here is that a good deal of the nonlinear correlations of the dark matter
field can be understood as a distribution of random clumps, provided these are
given the correct distribution of masses and mass-dependent density profiles.
How can we extend this model to understand how the clustering of galaxies
can differ from that of the mass? There are two distinct ways in which a degree
of bias is inevitable:
(1) Halo occupation numbers. For low-mass halos, the probability of obtaining
an L ∗ galaxy must fall to zero. For halos with mass above this lower limit,

84

An introduction to the physics of cosmology

Figure 2.16. The power spectrum for the CDM model. The full lines contrast the linear
spectrum with the nonlinear spectrum, calculated according to the approximation of PD96.
The spectrum according to randomly placed halos is denoted by open circles; if the linear
power spectrum is added, the main features of the nonlinear spectrum are well reproduced.

the number of galaxies will in general not scale with halo mass.
(2) Non-locality. Galaxies can orbit within their host halos, so the probability of
forming a galaxy depends on the overall halo properties, not just the density
at a point. Also, the galaxies will end up at special places within the halos:
for a halo containing only one galaxy, the galaxy will clearly mark the halo
centre. In general, we expect one central galaxy and a number of satellites.
The numbers of galaxies that form in a halo of a given mass is the prime quantity
that numerical models of galaxy formation aim to calculate. However, for a given
assumed background cosmology, the answer may be determined empirically.
Galaxy redshift surveys have been analysed via grouping algorithms similar to the
‘friends-of-friends’ method widely employed to find virialized clumps in N-body
simulations. With an appropriate correction for the survey limiting magnitude,
the observed number of galaxies in a group can be converted to an estimate of
the total stellar luminosity in a group. This allows a determination of the All
Galaxy System (AGS) luminosity function: the distribution of virialized clumps
of galaxies as a function of their total luminosity, from small systems like the
Local Group to rich Abell clusters.
The AGS function for the CfA survey was investigated by Moore et al
(1993), who found that the result in blue light was well described by
dφ = φ ∗ [(L/L ∗ )β + (L/L ∗ )γ ]−1 dL/L ∗ ,

Quantifying large-scale structure

85

Figure 2.17. The empirical luminosity–mass relation required to reconcile the observed
AGS luminosity function with two variants of CDM. L ∗ is the characteristic luminosity in
the AGS luminosity function (L ∗ = 7.6 × 1010 h −2 L ). Note the rather flat slope around
M = 1013 –1014 h −1 M , especially for CDM.

where φ ∗ = 0.001 26h 3 Mpc−3 , β = 1.34, γ = 2.89; the characteristic
luminosity is M ∗ = −21.42 + 5 log10 h in Zwicky magnitudes, corresponding
to MB∗ = −21.71 + 5 log10 h, or L ∗ = 7.6 × 1010 h −2 L , assuming MB = 5.48.
One notable feature of this function is that it is rather flat at low luminosities, in
contrast to the mass function of dark-matter halos (see Sheth and Tormen 1999).
It is therefore clear that any fictitious galaxy catalogue generated by randomly
sampling the mass is unlikely to be a good match to observation. The simplest
cure for this deficiency is to assume that the stellar luminosity per virialized halo is
a monotonic, but nonlinear, function of halo mass. The required luminosity–mass
relation is then easily deduced by finding the luminosity at which the integrated
AGS density (> L) matches the integrated number density of halos with mass
>M. The result is shown in figure 2.17.
We can now return to the halo-based galaxy power spectrum and use the
correct occupation number, N, as a function of mass. This needs a little care at
small numbers, however, since the number of halos with occupation number unity
affects the correlation properties strongly. These halos contribute no correlated
pairs, so they simply dilute the signal from the halos with N ≥ 2. The existence
of antibias on intermediate scales can probably be traced to the fact that a large
fraction of galaxy groups contain only one > L ∗ galaxy. Finally, we need
to put the galaxies in the correct location, as discussed before. If one galaxy
always occupies the halo centre, with others acting as satellites, the small-scale

86

An introduction to the physics of cosmology

Figure 2.18. The power spectrum for a galaxy catalogue constructed from the CDM
model. A reasonable agreement with the APM data (full line) is achieved by simple
empirical adjustment of the occupation number of galaxies as a function of halo mass,
plus a scheme for placing the halos non-randomly within the halos. In contrast, the galaxy
power spectrum differs significantly in shape from that of the dark matter (linear and
nonlinear theory shown as in figure 2.16).

correlations automatically follow the slope of the halo density profile, which
keeps them steep. The results of this exercise are shown in figure 2.18.
The results of this simple model are encouragingly similar to the scaledependent bias found in the detailed calculations of Benson et al (2000a), shown
in figure 2.15. There are thus grounds for optimism that we may be starting to
attain a physical understanding of the origin of galaxy bias.

2.8 Cosmic background fluctuations
2.8.1 The hot big bang and the microwave background
What was the state of matter in the early phases of the big bang? Since the presentday expansion will cause the density to decline in the future, conditions in the past
must have corresponded to high density—and thus to high temperature. We can
deal with this quantitatively by looking at the thermodynamics of the fluids that
make up a uniform cosmological model.
The expansion is clearly adiathermal, since the symmetry means that there
can be no net heat flow through any surface. If the expansion is also reversible,
then we can go one step further, because entropy change is defined in terms of

Cosmic background fluctuations

87

the heat that flows during a reversible change. If no heat flows during a reversible
change, then entropy must be conserved, and the expansion will be adiabatic.
This can only be an approximation, since there will exist irreversible microscopic
processes. In practice, however, it will be shown later that the effects of these
processes are overwhelmed by the entropy of thermal background radiation in the
universe. It will therefore be an excellent approximation to treat the universe as
if the matter content were a simple dissipationless fluid undergoing a reversible
expansion. This means that, for a ratio of specific heats , we get the usual
adiabatic behaviour
T ∝ R −3(−1).
For radiation,  = 4/3 and we get just T ∝ 1/R. A simple model for the
energy content of the universe is to distinguish pressureless ‘dust-like’ matter (in
the sense that p  ρc2 ) from relativistic ‘radiation-like’ matter (photons plus
neutrinos). If these are assumed not to interact, then the energy densities scale as
ρm ∝ R −3

ρr ∝ R −4

The universe must therefore have been radiation-dominated at some time in
the past, where the densities of matter and radiation cross over. To anticipate,
we know that the current radiation density corresponds to thermal radiation with
T  2.73 K. In addition to this CMB, we also expect a background in neutrinos.
This arises in the same way as the CMB: both photons and neutrinos are in thermal
equilibrium at high redshift, but eventually fall out of equilibrium as the universe
expands and reaction timescales lengthen. Subsequently, the number density of
frozen-out background particles scales as n ∝ a −3 , exactly as expected for a
thermal background with T ∝ 1/a. The background appears to stay in thermal
equilibrium even though it has frozen out. If the neutrinos are massless and
therefore relativistic, they contribute an energy density comparable to that of the
photons (to be exact, a factor 0.68 times the photon density—see p 280 of Peacock
(1999)). If there are no other contributions to the energy density from relativistic
particles, then the total effective radiation density is r h 2  4.2 × 10−5 and the
redshift of matter–radiation equality is
1 + z eq = 23 900h 2(T /2.73 K)−4 .
The time of this change in the global equation of state is one of the key
epochs in determining the appearance of the present-day universe. By a
coincidence, this epoch is close to another important event in cosmological
history: recombination. Once the temperature falls below 104 K, ionized
material can form neutral hydrogen. Observational astronomy is only possible
from this point on, since Thomson scattering from electrons in ionized material
prevents photon propagation. In practice, this limits the maximum redshift of
observational interest to about 1000; unless  is very low or vacuum energy is
important, a matter-dominated model is therefore a good approximation to reality.

88

An introduction to the physics of cosmology

In a famous piece of serendipity, the redshifted radiation from the lastscattering photosphere was detected as a 2.73 K microwave background by
Penzias and Wilson (1965). Since the initial detection of the microwave
background at λ = 7.3 cm, measurements of the spectrum have been made over
an enormous range of wavelengths, from the depths of the Rayleigh–Jeans regime
at 74 cm to well into the Wien tail at 0.5 mm. The most accurate measurements
come from COBE—the NASA cosmic background explorer satellite. Early data
showed the spectrum to be very close to a pure Planck function (Mather et al
1990), and the final result verifies the lack of any distortion with breathtaking
precision. The COBE temperature measurement and 95% confidence range of
T = 2.728 ± 0.004 K
improves significantly on the ground-based experiments. The lack of distortion
in the shape of the spectrum is astonishing, and limits the chemical potential
to |µ| < 9 × 10−5 (Fixsen et al 1996). These results also allow the limit
y . 1.5 × 10−5 to be set on the Compton-scattering distortion parameter.
These limits are so stringent that many competing cosmological models can be
eliminated.
2.8.2 Mechanisms for primary fluctuations
At the last-scattering redshift (z  1000), gravitational instability theory says
that fractional density perturbations δ & 10−3 must have existed in order for
galaxies and clusters to have formed by the present. A long-standing challenge
in cosmology has been to detect the corresponding fluctuations in brightness
temperature of the CMB radiation, and it took over 25 years of ever more stringent
upper limits before the first detections were obtained, in 1992. The study of CMB
fluctuations has subsequently blossomed into a critical tool for pinning down
cosmological models.
This can be a difficult subject; the treatment given here is intended to be the
simplest possible. For technical details see, e.g., Bond (1997), Efstathiou (1990),
Hu and Sugiyama (1995), Seljak and Zaldarriaga (1996); for a more general
overview, see White et al (1994) or Partridge (1995). The exact calculation of
CMB anisotropies is complicated because of the increasing photon mean free
path at recombination: a fluid treatment is no longer fully adequate. For full
accuracy, the Boltzmann equation must be solved to follow the evolution of the
photon distribution function. A convenient means for achieving this is provided
by the public domain CMBFAST code (Seljak and Zaldarriaga 1996). Fortunately,
these exact results can usually be understood via a more intuitive treatment, which
is quantitatively correct on large and intermediate scales. This is effectively what
would be called local thermodynamic equilibrium in stellar structure: imagine
that the photons we see each originated in a region of space in which the radiation
field was a Planck function of a given characteristic temperature. The observed

Cosmic background fluctuations

89

Figure 2.19. Illustrating the physical mechanisms that cause CMB anisotropies. The
shaded arc on the right represents the last-scattering shell; an inhomogeneity on this
shell affects the CMB through its potential, adiabatic and Doppler perturbations. Further
perturbations are added along the line of sight by time-varying potentials (the Rees–Sciama
effect) and by electron scattering from hot gas (the Sunyaev–Zeldovich effect). The density
field at last scattering can be Fourier analysed into modes of wavevector k. These spatial
perturbation modes have a contribution that is, in general, damped by averaging over the
shell of last scattering. Short-wavelength modes are more heavily affected (1) because
more of them fit inside the scattering shell and (2) because their wavevectors point more
nearly radially for a given projected wavelength.

brightness temperature field can then be thought of as arising from a superposition
of these fluctuations in thermodynamic temperature.
We distinguish primary anisotropies (those that arise due to effects at the
time of recombination) from secondary anisotropies, which are generated by
scattering along the line of sight. There are three basic primary effects, illustrated
in figure 2.19, which are important on respectively large, intermediate and small
angular scales:
(1) Gravitational (Sachs–Wolfe) perturbations. Photons from high-density
regions at last scattering have to climb out of potential wells, and are thus
redshifted.
(2) Intrinsic (adiabatic) perturbations. In high-density regions, the coupling
of matter and radiation can compress the radiation also, giving a higher
temperature.
(3) Velocity (Doppler) perturbations. The plasma has a non-zero velocity
at recombination, which leads to Doppler shifts in frequency and hence
brightness temperature.
To make quantitative progress, the next step is to see how to predict the size of
these effects in terms of the spectrum of mass fluctuations.

90

An introduction to the physics of cosmology

2.8.3 The temperature power spectrum
The statistical treatment of CMB fluctuations is very similar to that of spatial
density fluctuations. We have a 2D field of random fluctuations in brightness
temperature, and this can be analysed by the same tools that are used in the case
of 2D galaxy clustering.
Suppose that the fractional temperature perturbations on a patch of sky of
side L are Fourier expanded:

L2
δT
TK exp(−iK · X) d2 K
(X) =
T
(2π)2

1
δT
(X) exp(iK · X) d2 X,
TK (K ) = 2
L
T
where X is a 2D position vector on the sky, and K is a 2D wavevector. This is
only a valid procedure if the patch of sky under consideration is small enough to
be considered flat; we give the full machinery later. We will normally take the
units of length to be angle on the sky, although they could also in principle be
h −1 Mpc at a given redshift. The relation between angle and comoving distance
on the last-scattering sphere requires the comoving angular-diameter distance to
the last-scattering sphere; because of its high redshift, this is effectively identical
to the horizon size at the present epoch, RH :
2c
 m H0
2c
RH  0.4
 m H0
RH =

(open)
(flat);

the latter approximation for models with m + v = 1 is due to Vittorio and Silk
(1991).
As with the density field, it is convenient to define a dimensionless power
spectrum of fractional temperature fluctuations,
2

L
T 2 ≡ (2π)
2π K 2 |TK |2 ,
2
so that T 2 is the fractional variance in temperature from modes in a unit range
of ln K . The corresponding dimensionless spatial statistic is the two-point
correlation function


δT
δT
C(θ ) =
(ψ) (ψ + θ ) ,
T
T
which is the Fourier transform of the power spectrum, as usual:

dK
.
C(θ ) = T 2 (K )J0 (K θ )
K

Cosmic background fluctuations

91

Here, the Bessel function comes from the angular part of the Fourier transform:

exp(ix cos φ) dφ = 2π J0 (x).
Now, in order to predict the observed anisotropy of the microwave
background, the problem we must solve is to integrate the temperature
perturbation field through the last-scattering shell. In order to do this, we assume
that the sky is flat; we also neglect curvature of the 3-space, although this is only
strictly valid for flat models with k = 0. Both these restrictions mean that the
results are not valid for very large angles. Now, introducing the Fourier expansion
of the 3D temperature perturbation field (with coefficients Tk3D ) we can construct
the observed 2D temperature perturbation field by integrating over k space and
optical depth:

V
δT
=
Tk3D e−ik·r d3 k e−τ dτ.
T
(2π)3
A further simplification is possible if we approximate e−τ dτ by a Gaussian in
comoving radius:
exp(−τ ) dτ ∝ exp[−(r − rLS )2 /2σr2 ] dr.
This says that we observe radiation from a last-scattering shell centred at
comoving distance rLS (which is very nearly identical to rH , since the redshift
is so high). The thickness of this shell is of the order of the mean free path to
Compton scattering at recombination, which is approximately
σr = 7(h 2 )−1/2 Mpc
(see p 287 of Peacock 1999).
The 2D power spectrum is thus a smeared version of the 3D one: any
feature that appears at a particular wavenumber in 3D will cause a corresponding
feature at the same wavenumber in 2D. A particularly simple converse to this
rule arises when there are no features: the 3D power spectrum is scale-invariant
2 = constant). In this case, for scales large enough that we can neglect the
(T3D
radial smearing from the last-scattering shell,

T2D2 = T3D2
so that the pattern on the CMB sky is also scale invariant. To apply this machinery
for a general spectrum, we now need quantitative expressions for the spatial
temperature anisotropies.
Sachs–Wolfe effect. To relate to density perturbations, use Poisson’s equation
∇ 2 δk = 4π Gρδk . The effect of ∇ 2 is to pull down a factor of −k 2 /a 2 (a 2
because k is a comoving wavenumber). Eliminating ρ in terms of  and z LS
gives
 
(1 + z LS ) H0 2 δk (z LS )
Tk = −
.
2
c
k2

92

An introduction to the physics of cosmology

Doppler source term. The effect here is just the Doppler effect from the
scattering of photons by moving plasma:
δT
δv · r̂
=
.
T
c
Using the standard expression for the linear peculiar velocity, the
corresponding k-space result is
 
H0 δk (z LS )
k̂ · r̂.
Tk = −i (1 + z LS )
c
k
Adiabatic source term. This is the simplest of the three effects mentioned
earlier:
δk (z LS )
Tk =
,
3
because δn γ /n γ = δρ/ρ and n γ ∝ T 3 . However, this simplicity conceals a
paradox. Last scattering occurs only when the universe recombines, which
occurs at roughly a fixed temperature: kT ∼ χ, the ionization potential
of hydrogen. Surely, then, we should just be looking back to a surface
of constant temperature? Hot and cold spots should normalize themselves
away, so that the last-scattering sphere appears uniform. The solution is that
a denser spot recombines later: it is therefore less redshifted and appears
hotter. In algebraic terms, the observed temperature perturbation is
 
δρ
δz
δT
=
,
=−
T obs
1+z
ρ
where the last expression assumes linear growth, δ ∝ (1 + z)−1 . Thus,
even though a more correct picture for the temperature anisotropies seen
on the sky is of a crinkled surface at constant temperature, thinking of hot
and cold spots gives the right answer. Any observable cross-talk between
density perturbations and delayed recombination is confined to effects of
order higher than linear.
We now draw these results together to form the spatial power spectrum of
CMB fluctuations in terms of the power spectrum of mass fluctuations at last
scattering:
T3D2 = [( fA + fSW )2 (k) + fV2(k)µ2 ]2k (zLS ),
where µ ≡ k̂ · r̂. The dimensionless factors can be written most simply as
2
(k DLS )2
2
fV =
k DLS
fA = 1/3,

f SW = −

Cosmic background fluctuations
where
DLS =

2c
1/2
 m H0

93

(1 + z LS )−1/2 = 184(h 2)−1/2 Mpc

is the comoving horizon size at last scattering (a result that is independent of
whether there is a cosmological constant).
We can see immediately from these expressions the relative importance
of the various effects on different scales. The Sachs–Wolfe effect dominates
for wavelengths &1h −1 Gpc; Doppler effects then take over but are almost
immediately dominated by adiabatic effects on the smallest scales.
These expressions apply to perturbations for which only gravity has been
important up until last scattering, i.e. those larger than the horizon at z eq . For
smaller wavelengths, a variety of additional physical processes act on the radiation
perturbations, generally reducing the predicted anisotropies. An accurate
treatment of these effects is not really possible without a more complicated
analysis, as is easily seen by considering the thickness of the last-scattering
shell, σr = 7(h 2 )−1/2 Mpc. This clearly has to be of the same order of
magnitude as the photon mean free path at this time; on any smaller scales, a
fluid approximation for the radiation is inadequate and a proper solution of the
Boltzmann equation is needed. Nevertheless, some qualitative insight into the
small-scale processes is possible. The radiation fluctuations will be damped
relative to the baryon fluid by photon diffusion, characterized by the Silkdamping scale, λS = 2.7(Bh 6 )−1/4 Mpc. Below the horizon scale at z eq ,
16(h 2 )−1 Mpc, there is also the possibility that dark-matter perturbations can
grow while the baryon fluid is still held back by radiation pressure, which results
in adiabatic radiation fluctuations that are less than would be predicted from the
dark-matter spectrum alone. In principle, this suggests a suppression factor of
(1 + z eq )/(1 + z LS ) or roughly a factor 10. In detail, the effect is an oscillating
function of scale, since we have seen that baryonic perturbations oscillate as sound
waves when they come inside the horizon:



1/4
δb ∝ (3cS ) exp ± i kcS dτ ;
here, τ stands for conformal time. There is thus an oscillating signal in the CMB,
depending on the exact phase of these waves at the time of last scattering. These
oscillations in the fluid of baryons plus radiation cause a set of acoustic peaks in
the small-scale power spectrum of the CMB fluctuations (see later).
2.8.4 Large-scale fluctuations and CMB power spectrum
The flat-space formalism becomes inadequate for very large angles; the proper
basis functions to use are the spherical harmonics:

δT
(q̂) =
a"m Y"m (q̂),
T

94

An introduction to the physics of cosmology

where q̂ is a unit vector that specifies direction on the sky. Since the spherical
harmonics satisfy the orthonormality relation Y"m Y"∗ m d2 q = δ"" δmm , the
inverse relation is

δT ∗ 2
m
a" =
Y d q.
T "m
The analogues of the Fourier relations for the correlation function and power
spectrum are
m=+"
1   m2
|a" | P" (cos θ )
4π
" m=−"
 1
|a"m |2 = 2π
C(θ )P" (cos θ ) d cos θ.

C(θ ) =

−1

These are exact relations, governing the actual correlation structure of the
observed sky. However, the sky we see is only one of infinitely many possible
realizations of the statistical process that yields the temperature perturbations; as
with the density field, we are more interested in the ensemble average power. A
common notation is to define C" as the expectation value of |a"m |2 :
C(θ ) =

1 
(2" + 1)C" P" (cos θ ),
4π
"

C" ≡ |a"m |2 ,

where now C(θ ) is the ensemble-averaged correlation. For small θ and large ",
the exact form reduces to a Fourier expansion:
 ∞
,
C(θ ) =
T 2(K )J0 (K θ ) dK
K
0

T 2 (K = " + 12 ) =

(" + 12 )(2" + 1)
C" .
4π

The effect of filtering the microwave sky with the beam of a telescope may
be expressed as a multiplication of the C" , as with convolution in Fourier space:
CS (θ ) =

1 
(2" + 1)W"2 C" P" (cos θ ).
4π
"

When the telescope beam is narrow in angular terms, the Fourier limit can be
used to deduce the appropriate "-dependent filter function. For example, for a
Gaussian beam of FWHM (full-width to half maximum) 2.35σ , the filter function
is W" = exp(−"2 σ 2 /2).
For the large-scale temperature anisotropy, we have already seen that
what matters is the Sachs–Wolfe effect, for which we have derived the spatial
anisotropy power spectrum. The spherical harmonic coefficients for a spherical
slice through such a field can be deduced using the results for large-angle galaxy

Cosmic background fluctuations

95

clustering, in the limit of a selection function that goes to a delta function in
radius:

dk
SW
C" = 16π (k DLS )−4 2k (z LS ) j"2 (k RH ) ,
k
where the j" are spherical Bessel functions (see chapter 10 of Abramowitz and
Stegun 1965). This formula, derived by Peebles (1982), strictly applies only to
spatially flat models, since the Fourier expansion of the density field is invalid in
an open model. Nevertheless, since the curvature radius R0 subtends an angle of
/[2(1 − )1/2], even the lowest few multipoles are not seriously affected by this
point, provided  & 0.1.
For simple mass spectra, the integral for the C" can be performed
analytically. The case of most practical interest is a scale-invariant spectrum
(2k ∝ k 4 ), for which the integral scales as
C" =

6
C2
"(" + 1)

(see equation (6.574.2) of Gradshteyn and Ryzhik 1980). The direct relation
between the mass fluctuation spectrum and the multipole coefficients of CMB
fluctuations mean that either can be used as a measure of the normalization of the
spectrum.
2.8.5 Predictions of CMB anisotropies
We are now in a position to understand the characteristic angular structure
of CMB fluctuations. The change-over from scale-invariant Sachs–Wolfe
fluctuations to fluctuations dominated by Doppler scattering has been shown
to occur at k  DLS . This is one critical angle (call it θ1 ); its definition is
θ1 = DLS /RH , and for a matter-only model it takes the value
θ1 = 1.81/2 degrees.
For flat low-density models with significant vacuum density, RH is smaller; θ1
and all subsequent angles would then be larger by about a factor −0.6 (i.e. θ1 is
roughly independent of  in flat -dominated models).
The second dominant scale is the scale of last-scattering smearing set by
σr = 7(h 2 )−1/2 Mpc. This subtends an angle
θ2 = 41/2 arcmin.
Finally, a characteristic scale in many density power spectra is set by the horizon
at z eq . This is 16(h 2)−1 Mpc and subtends
θ3 = 9h −1 arcmin,
independent of . This is quite close to θ2 , so that alterations in the transfer
function are an effect of secondary importance in most models.

96

An introduction to the physics of cosmology

We therefore expect that all scale-invariant models will have similar CMB
power spectra: a flat Sachs–Wolfe portion down to K  1 deg−1 , followed by a
bump where Doppler and adiabatic effects come in, which turns over on arcminute
scales through damping and smearing. This is illustrated well in figure 2.22,
which shows some detailed calculations of 2D power spectra, generated with
the CMBFAST package. From these plots, the key feature of the anisotropy
spectrum is clearly the peak at " ∼ 100. This is often referred to as the Doppler
peak, but it is not so clear that this name is accurate. Our simplified analysis
suggests that Sachs–Wolfe anisotropy should dominate for θ > θ1 , with Doppler
and adiabatic terms becoming of comparable importance at θ1 , and adiabatic
effects dominating at smaller scales. There are various effects that cause the
simple estimate of adiabatic effects to be too large, but they clearly cannot be
neglected for θ < θ1 . A better name, which is starting to gain currency, is the
acoustic peak. In any case, it is clear that the peak is the key diagnostic feature
of the CMB anisotropy spectrum: its height above the SW ‘plateau’ is sensitive
to B and its angular location depends on  and . It is therefore no surprise
that many experiments are currently attempting accurate measurements of this
feature. Furthermore, it is apparent that sufficiently accurate experiments will be
able to detect higher ‘harmonics’ of the peak, in the form of smaller oscillations
of amplitude perhaps 20% in power, around "  500–1000. These features arise
because the matter–radiation fluid undergoes small-scale oscillations, the phase
of which at last scattering depends on wavelength, since the density oscillation
varies roughly as δ ∝ exp(icS kτ ). Accurate measurement of these oscillations
would pin down the sound speed at last scattering, and help give an independent
measurement of the baryon density.
Since large-scale CMB fluctuations are expected to be dominated by
gravitational potential fluctuations, it was possible to make relatively clear
predictions of the likely level of CMB anisotropies, even in advance of the
first detections. What was required was a measurement of the typical depth of
large-scale potential wells in the universe, and many lines of argument pointed
inevitably to numbers of order 10−5 . This was already clear from the existence of
massive clusters of galaxies with velocity dispersions of up to 1000 km s−1 :
v2 ∼


GM
v2
⇒ 2 ∼ 2,
r
c
c

so the potential well of a cluster is of order 10−5 deep. More exactly, the
abundance of rich clusters is determined by the amplitude σ8 , which measures
[2 (k)]1/2 at an effective wavenumber of very nearly 0.17h Mpc−1 . If we assume
that this is a large enough scale so that what we are measuring is the amplitude
of any scale-invariant spectrum, then the earlier expression for the temperature
power spectrum gives
2  10−5.7 σ [g()]−1 .
TSW
8

Cosmic background fluctuations

97

There were thus strong grounds to expect that large-scale fluctuations would be
present at about the 10−5 level, and it was a significant boost to the credibility of
the gravitational-instability model that such fluctuations were eventually seen.
In more detail, it is possible to relate the COBE anisotropy to the large-scale
portion of the power spectrum. Górski et al (1995), Bunn et al (1995), and White
and Bunn (1995) discuss the large-scale normalization from the two-year COBE
data in the context of CDM-like models. The final four-year COBE data favour
very slightly lower results, and we scale to these in what follows. For scaleinvariant spectra and  = 1, the best normalization is

4
k
2
.
COBE ⇒  (k) =
0.0737h Mpc−1
Translated into other common notation for the normalization, this is equivalent to
Q rms−ps = 18.0 µK, or δH = 2.05 × 10−5 (see e.g. Peacock and Dodds 1994).
For low-density models, the earlier discussion suggests that the power
spectrum should depend on  and the growth factor g as P ∝ g 2 /2 . Because of
the time dependence of the gravitational potential (integrated Sachs–Wolfe effect)
and because of spatial curvature, this expression is not exact, although it captures
the main effect. From the data of White and Bunn (1995), a better approximation
is
g2
2 (k) ∝ 2 g 0.7 .

This applies for low- models both with and without vacuum energy, with a
maximum error of 2% in density fluctuation provided  > 0.2. Since the
rough power-law dependence of g is g()  0.65 and 0.23 for open and
flat models respectively, we see that the implied density fluctuation amplitude
scales approximately as −0.12 and −0.69 respectively for these two cases. The
dependence is weak for open models, but vacuum energy implies much larger
fluctuations for low .
Within the CDM model, it is always possible to satisfy both the large-scale
COBE normalization and the small-scale σ8 constraint, by appropriate choice of
 and n. This is illustrated in figure 2.20. Note that the vacuum energy affects the
answer; for reasonable values of h and reasonable baryon content, flat models
require m  0.3, whereas open models require m  0.5 in order to be
consistent with scale-invariant primordial fluctuations.
2.8.6 Geometrical degeneracy
The statistics of CMB fluctuations depend on a large number of parameters, and
it can be difficult to understand what the effect of changing each one will be.
Furthermore, the effects of some parameters tend to change things in opposite
directions, so that there are degenerate directions in the parameter space, along
which changes leave the CMB unaffected. These were analysed comprehensively
by Efstathiou and Bond (1999), and we now summarize the main results.

98

An introduction to the physics of cosmology

Figure 2.20. For 10% baryons, the value of n needed to reconcile COBE and the cluster
normalization in CDM models.

The usual expression for the comoving angular-diameter distance is
 z

|1 − |1/2 dz
c
,
|1 − |−1/2 Sk
R0 Sk (r ) =
H0
0
(1 − )(1 + z )2 + v + m (1 + z )3
where  = m + v . Defining ωi ≡ i h 2 , this can be rewritten in a way that
has no explicit h dependence:
 z

|ωk |1/2 dz
3000 Mpc
,
R0 Sk (r ) =
Sk
|ωk |1/2
0
ωk (1 + z )2 + ωv + ωm (1 + z )3
where ωk ≡ (1 − m − v )h 2 . This parameter describes the curvature of the
universe, treating it effectively as a physical density that scales as ρ ∝ a −2 .
This is convenient for the present formalism, but it is important to appreciate that
curvature differs fundamentally from a hypothetical fluid with such an equation
of state: the value of ωk also sets the curvature index k.
−1/2
Mpc. Similarly, other
The horizon distance at last scattering is 184ωm
critical length scales such as the sound horizon are governed by the relevant
physical density, ωb . Thus, if ωm and ωb are given, the shape of the spatial power
spectrum is determined. The translation of this into an angular spectrum depends
on the angular-diameter distance, which is a function of these parameters, plus ωk
1/2
and ωv . Models in which ωm R0 Sk (r ) is a constant have the same angular horizon
size. There is therefore a degeneracy between curvature (ωk ) and vacuum (ωv ):
these two parameters can be varied simultaneously to keep the same apparent
distance, as illustrated in figure 2.21.

Cosmic background fluctuations

99

Figure 2.21. The geometrical degeneracy in the CMB means that models with fixed m h 2
and b h 2 can be made to look identical by varying the curvature against vacuum energy,
while also varying the Hubble parameter. This degeneracy is illustrated here for the case
ωm ≡ m h 2 = 0.2. Models along a given line are equivalent from a CMB point of view;
corresponding lines in the upper and lower panels have the same line style. The sensitivity
to curvature is strong: if the universe appears to be flat, then it really must be so, unless
it is very heavily vacuum dominated. Note that supplying external information about h
breaks the degeneracy. This figure assumes scalar fluctuations only; allowing tensor modes
introduces additional degeneracies—mainly between the tensor fraction and tilt.

100

An introduction to the physics of cosmology

The physical degree of freedom here can be thought of as the Hubble
constant. This is involved via the relation
h 2 = ωm + ωv + ωk ,
so specifying h in addition to the physical matter density fixes ωv + ωk and
removes the degeneracy.
2.8.7 Small-scale data and outlook
The study of large-scale CMB anisotropies had a huge impact on cosmology in
the 1990s, and the field seems likely to be of increasing importance over the next
decade. This school was held at a particularly exciting time, as major new data
on the CMB power spectrum arrived during the lectures (de Bernardis et al 2000,
Hanany et al 2000). Although these developments are very recent, the situation
already seems a good deal clearer than previously, and it is interesting to try to
guess where the field is heading.
One immediate conclusion is that it increasingly seems that the relevant
models are ones in which the primordial fluctuations were close to being adiabatic
and Gaussian. Isocurvature models suffer from the high amplitude of the largescale perturbations, and do not become any more attractive when modelled in
detail (Hu et al 1995). Topological defects were for a long time hard to assess,
since accurate predictions of their CMB properties were difficult to make. Recent
progress does, however, indicate that these theories may have difficulty matching
the main details of CMB anisotropies, even as they are presently known (Pen et
al 1997).
We shall therefore concentrate on interpreting the data in terms of the
simplest gravitational-instability models. Many of the features of these models
are generic, although they are often spoken of as ‘the inflationary predictions’.
This statement needs to be examined carefully, since one of the possible prizes
from a study of the CMB may be a test of inflation. CMB anisotropies in
theories where structure forms via gravitational collapse were calculated in
largely the modern way well before inflation was ever considered, by Peebles
and Yu (1970). The difficulty in these calculations is the issue of superhorizon fluctuations. In a conventional hot big bang, these must be generated by
some acausal process—indeed, an acausal origin is required even for large-scale
homogeneity. Inflation is so far the only theory that generates such superhorizon
modes at all naturally. Nevertheless, it is not acceptable to claim that detection
of super-horizon modes amounts to a proof of inflation. Rather, we need some
more characteristic signature of the specific process used by inflation: amplified
quantum fluctuations.
We should thus review the predictions that simple models of inflation make
for CMB anisotropies (see, e.g., chapter 11 of Peacock 1999 or Liddle and Lyth
2000 for more details). Inflation is driven by a scalar field φ, with a potential

Cosmic background fluctuations

101

V (φ). As well as the characteristic energy density of inflation, V , this can be
characterized by two dimensionless parameters
m 2P
(V /V )2
16π
m2
η ≡ P (V /V ),
8π
≡

where m P is the Planck mass, V = dV /dφ, and all quantities are evaluated
towards the end of inflation, when the present large-scale structure modes were
comparable in size to the inflationary horizon. Prior to transfer-function effects,
the primordial fluctuation spectrum is specified by a horizon-scale amplitude
(extrapolated to the present) δH and a slope n:
 3+n
ck
2
2
 (k) = δH
.
H0
The inflationary predictions for these numbers are
V 1/2
m 2P  1/2
n = 1 − 6 + 2η,

δH ∼

which leaves us in the unsatisfactory position of having two observables and three
parameters.
The critical ingredient for testing inflation by making further predictions is
the possibility that, in addition to scalar modes, the CMB could also be affected
by gravitational waves (following the original insight of Starobinsky 1985). We
therefore distinguish explicitly between scalar and tensor contributions to the
CMB fluctuations by using appropriate subscripts. The former category are those
described by the Sachs–Wolfe effect, and are gravitational potential fluctuations
that relate directly to mass fluctuations. The relative amplitude of tensor and
scalar contributions depended on the inflationary parameter  alone:
C"T
C"S

 12.4  6(1 − n).

The second relation to the tilt (which is defined to be 1 − n) is less general,
as it assumes a polynomial-like potential, so that η is related to . If we make
this assumption, inflation can be tested by measuring the tilt and the tensor
contribution. For simple models, this test should be feasible: V = λφ 4 implies
n  0.95 and C"T /C"S  0.3. To be safe, we need one further observation, and
this is potentially provided by the spectrum of C"T . Suppose we write separate
power-law index definitions for the scalar and tensor anisotropies:
C"S ∝ "nS −3 ,

C"T ∝ "nT −3 .

102

An introduction to the physics of cosmology

From the discussion of the Sachs–Wolfe effect, we know that, on large scales,
the scalar index is the same as index in the matter power spectrum: n S = n =
1 − 6 + 2η. By the same method, it is easily shown that n T = 1 − 2 (although
different definitions of n T are in use in the literature; the convention here is that
n = 1 always corresponds to a constant T 2 (")). Finally, then, we can write the
inflationary consistency equation:
C"T
C"S

= 6.2(1 − n T ).

The slope of the scalar perturbation spectrum is the only quantity that contains η,
and so n S is not involved in a consistency equation, since there is no independent
measure of η with which to compare it.
From the point of view of an inflationary purist, the scalar spectrum is
therefore an annoying distraction from the important business of measuring the
tensor contribution to the CMB anisotropies. A certain degree of degeneracy
exists here (see Bond et al 1994), since the tensor contribution has no acoustic
peak; C"T is roughly constant up to the horizon scale and then falls. A spectrum
with a large tensor contribution therefore closely resembles a scalar-only spectrum
with smaller b (and hence a relatively lower peak). One way in which this
degeneracy may be lifted is through polarization of the CMB fluctuations. A nonzero polarization is inevitable because the electrons at last scattering experience
an anisotropic radiation field. Thomson scattering from an anisotropic source
will yield polarization, and the practical size of the fractional polarization P is
of the order of the quadrupole radiation anisotropy at last scattering: P & 1%.
Furthermore, the polarization signature of tensor perturbations differs from that
of scalar perturbations (e.g. Seljak 1997, Hu and White 1997); the different
contributions to the total unpolarized C" can in principle be disentangled, allowing
the inflationary test to be carried out.
How do these theoretical expectations match with the recent data, shown in
figure 2.22? In many ways, the match to prediction is startlingly good: there
is a very clear acoustic peak at "  220, which has very much the height and
width expected for the principal peak in adiabatic models. As we have seen, the
location of this peak is sensitive to , since it measures directly the angular size
of the horizon at last scattering, which scales as " ∝ −1/2 for open models.
The cut-off at "  1000 caused by last-scattering smearing also moves to higher
" for low ; if  were small enough, the smearing cut-off would be carried to
large ", where it would be inconsistent with the upper limits to anisotropies on
10-arcminute scales. This tendency for open models to violate the upper limits
to arcminute-scale anisotropies is in fact a long-standing problem, which allowed
Bond and Efstathiou (1984) to deduce the following limit on CDM universes:
 & 0.3h −4/3.
The known lack of a CMB peak at high " was thus already a very strong argument
for a flat universe (with the caveats expressed in the earlier section on geometrical

Cosmic background fluctuations

103

Figure 2.22. Angular power spectra T 2 (") = "(" + 1)C" /2π for the CMB, plotted
against angular wavenumber " in rad−1 . The experimental data are an updated version
of the compilation described in White et al (1994), communicated by M White; see
also Hancock et al (1997) and Jaffe et al (2000). Various model predictions for
adiabatic scale-invariant CDM fluctuations are shown. The two full curves correspond
to (, B , h) = (1, 0.05, 0.5) and (1,0.1,0.5), with the higher B increasing power
by about 20% at the peak. The dotted line shows a flat -dominated model with
(, B , h) = (0.3, 0.05, 0.65); the broken curve shows an open model with the same
parameters. Note the very similar shapes of all the curves. The normalization has been set
to the large-scale amplitude, and so any dependence on  is quite modest. The main effects
are that open models shift the peak to the right, and that the height of the peak increases
with B and h.

degeneracy). Now that we have a direct detection of a peak at low ", this argument
for a flat universe is even stronger.
If the basic adiabatic CDM paradigm is adopted, then we can move
beyond generic statements about flatness to attempt to use the CMB to measure
cosmological parameters. In a recent analysis (Jaffe et al 2000), the following
best-fitting values for the densities in collisionless matter (c), baryons (b) and
vacuum (v) were obtained, together with tight constraints on the power-spectrum
index:
c + b + v = 1.11 ± 0.07
c h 2 = 0.14 ± 0.06
b h 2 = 0.032 ± 0.005

104

An introduction to the physics of cosmology
n = 1.01 ± 0.09.

The only parameter left undetermined by the CMB is the Hubble constant, h.
Recent work (e.g. Mould et al 2000) suggests that this is now determined to an
rms accuracy of 10%, and we adopt a central value of h = 0.70. This completes
the cosmological model, requiring a total matter density parameter c + b =
0.35 ± 0.14, very nicely consistent with what is required to be consistent with
σ8 for exactly scale-invariant fluctuations. The predicted fluctuation shape is also
very sensible for this model:  = 0.18.
The fact that such a ‘vanilla’ model matches the main cosmological data
so well is a striking achievement, but it raises a number of issues. One is that
the baryon density inferred from the data exceeds that determined via primordial
nucleosynthesis by about a factor 1.5. This may sound like good agreement, but
both the CMB and nucleosynthesis are now impressively precise areas of science,
and neither can easily accommodate the other’s figure. The boring solution is that
small systematics will eventually be identified that allow a compromise figure.
Alternatively, this inconsistency could be the first sign that something is rotten
in the basic framework. However, it is too early to make strong claims in this
direction.
Of potentially greater significance is the fact that this successful fit has been
achieved using scalar fluctuations alone; indeed, the tensor modes are not even
mentioned by Jaffe et al (2000). To a certain extent, the presence of tensor modes
can be hidden by adjustments in the other parameters. There can be no acoustic
peak in the tensor contribution, so that the addition of tensors would require larger
peak in the scalar component to compensate, pushing in the direction of universes
that are of lower density, with larger baryon fractions. However, this would make
it harder to keep higher harmonics of the acoustic oscillations low – and it is
the lack of detection of any second and third peaks that forces the high baryon
density in this solution. There would also be the danger of spoiling the very good
agreement with other constraints, such as the σ8 normalization. We therefore have
to face the unpalatable fact that there is as yet no sign of the two generic signatures
expected from inflationary models: tilt and a significant tensor contribution. It
may be that the next generation of CMB experiments will detect such features
at a low level. If they do not, and the initial conditions for structure formation
remain as they presently seem to be (scale-invariant adiabatic scalar modes), then
the heroic vision of using cosmology to probe physics near the Planck scale may
not be achieved. The stakes are high.

References
Abramowitz M and Stegun I A 1965 Handbook of Mathematical Functions (New York:
Dover)
Adelberger K, Steidel C, Giavalisco M, Dickinson M, Pettini M and Kellogg M 1998
Astrophys. J. 505 18

References

105

Ballinger W E, Peacock J A and Heavens A F 1996 Mon. Not. R. Astron. Soc. 282 877
Bardeen J M, Bond J R, Kaiser N and Szalay A S 1986 Astrophys. J. 304 15
Baugh C M and Efstathiou G 1993 Mon. Not. R. Astron. Soc. 265 145
——1994 Mon. Not. R. Astron. Soc. 267 323
Benson A J, Cole S, Frenk C S, Baugh C M and Lacey C G 2000a Mon. Not. R. Astron.
Soc. 311 793
Benson A J, Baugh C M, Cole S, Frenk C S, Lacey C G 2000b Mon. Not. R. Astron. Soc.
316 107
Bond J R 1995 Phys. Rev. Lett. 74 4369
——1997 Cosmology and large-scale structure Proc. 60th Les Houches School ed
R Schaeffer et al (Amsterdam: Elsevier) p 469
Bond J R, Cole S, Efstathiou G and Kaiser N 1991 Astrophys. J. 379 440
Bond J R, Crittenden R, Davis R L, Efstathiou G and Steinhardt P J 1994 Phys. Rev. Lett.
72 13
Bond J R and Efstathiou G 1984 Astrophys. J. 285 L45
Bunn E F 1995 PhD Thesis University of California, Berkeley
Bunn E F, Scott D and White M 1995 Astrophys. J. 441 9
Carlberg R G, Yee H K C, Morris S L, Lin H, Hall P B, Patton D, Sawicki M and Shepherd
C W 2000 Astrophys. J. 542 57
Carroll S M, Press W H and Turner E L 1992 Annu. Rev. Astron. Astrophys. 30 499
Cole S, Aragón-Salamanca A, Frenk C S, Navarro J F and Zepf S E 1994 Mon. Not. R.
Astron. Soc. 271 781
Colless M 1999 Phil. Trans. R. Soc. A 357 105
Davis M and Peebles P J E 1983 Astrophys. J. 267 465
de Bernardis P et al 2000 Nature 404 955
Efstathiou G 1990 Physics of the early Universe Proc. 36th Scottish Universities Summer
School in Physics ed J A Peacock, A F Heavens and A T Davies (Bristol: Adam
Hilger) p 361
——1995 Mon. Not. R. Astron. Soc. 274 L73
Efstathiou G, Bernstein G, Katz N, Tyson T and Guhathakurta P 1991 Astrophys. J. 380 47
Efstathiou G and Bond J R 1999 Mon. Not. R. Astron. Soc. 304 75
Efstathiou G, Bond J R and White S D M 1992 Mon. Not. R. Astron. Soc. 258 1P
Efstathiou G, Sutherland W and Maddox S J 1990 Nature 348 705
Eisenstein D J and Hu W 1998 Astrophys. J. 496 605
Eisenstein D J and Zaldarriaga M 1999 Astrophys. J. 546 2
Eke V R, Cole S and Frenk C S 1996 Mon. Not. R. Astron. Soc. 282 263
Feldman H A, Kaiser N and Peacock J A 1994 Astrophys. J. 426 23
Felten J E and Isaacman R 1986 Rev. Mod. Phys. 58 689
Fixsen D J, Cheng E S, Gales J M, Mather J C, Shafer R A and Wright E L 1996 Astrophys.
J. 473 576
Gaztañaga E and Baugh C M 1998 Mon. Not. R. Astron. Soc. 294 229
Górski K M, Ratra B, Sugiyama N and Banday A J 1995 Astrophys. J. 444 L65
Gradshteyn I S and Ryzhik I M 1980 Table of Integrals, Series and Products (New York:
Academic Press)
Hamilton A J S 1992 Astrophys. J. 385 L5
——1997a Mon. Not. R. Astron. Soc. 289 285
——1997b Mon. Not. R. Astron. Soc. 289 295
——1998 The Evolving Universe ed D Hamilton (Dordrecht: Kluwer) pp 185–275

106

An introduction to the physics of cosmology

Hamilton A J S, Kumar P, Lu E and Matthews A 1991 Astrophys. J. 374 L1
Hanany S et al 2000 Astrophys. J. 545 L5
Hancock S et al 1997 Mon. Not. R. Astron. Soc. 289 505
Heath D 1977 Mon. Not. R. Astron. Soc. 179 351
Hu W, Bunn E F and Sugiyama N 1995 Astrophys. J. 447 L59
Hu W and Sugiyama N 1995 Astrophys. J. 444 489
Hu W and White M 1997 New Astronomy 2 323
Huchra J P, Geller M J, de Lapparant V and Corwin H G 1990 Astrophys. J. Suppl. 72 433
Jaffe A et al 2000 Preprint astro-ph/0007333
Jenkins A, Frenk C S, Pearce F R, Thomas P A, Colberg J M, White S D M, Couchman H
M P, Peacock J A, Efstathiou G and Nelson A H 1998 Astrophys. J. 499 20
Kaiser N 1987 Mon. Not. R. Astron. Soc. 227 1
Kauffmann G, Colberg J M, Diaferio A and White S D M 1999 Mon. Not. R. Astron. Soc.
303 188
Kauffmann G, White S D M and Guiderdoni B 1993 Mon. Not. R. Astron. Soc. 264 201
Klypin A, Primak J and Holtzman J 1996 Astrophys. J. 466 13
Lahav O, Lilje P B, Primack J R and Rees M J 1991 Mon. Not. R. Astron. Soc. 251 128
Le Fèvre O et al 1996 Astrophys. J. 461 534
Liddle A R and Lyth D 1993 Phys. Rep. 231 1
——2000 Cosmological Inflation & Large-Scale Structure (Cambridge: Cambridge
University Press)
Liddle A R and Scherrer R J 1999 Phys. Rev. D 59 023509 (astro-ph/9809272)
Mészáros P 1974 Astron. Astrophys. 37 225
Maddox S J, Efstathiou G, Sutherland W J 1996 Mon. Not. R. Astron. Soc. 283 1227
Margon B 1999 Phil. Trans. R. Soc. A 357 93
Mather J C et al 1990 Astrophys. J. 354 L37
Matsubara T, Szalay A S and Landy S D 2000 Astrophys. J. 535 1
McClelland J and Silk J 1977 Astrophys. J. 217 331
Meiksin A A, White M 1999 Mon. Not. R. Astron. Soc. 308 1179
Meiksin A A, White M and Peacock J A 1999 Mon. Not. R. Astron. Soc. 304 851
Moore B, Frenk C S and White S D M 1993 Mon. Not. R. Astron. Soc. 261 827
Moore B, Quinn T, Governato F, Stadel J and Lake G 1999 Mon. Not. R. Astron. Soc. 310
1147
Mould J R et al 2000 Astrophys. J. 529 786
Navarro J F, Frenk C S and White S D M 1996 Astrophys. J. 462 563
Neyman J, Scott E L and Shane C D 1953 Astrophys. J. 117 92
Padmanabhan N, Tegmark M and Hamilton A J S 1999 Astrophys. J. 550 52
Partridge R B 1995 3K: The Cosmic Microwave Background (Cambridge: Cambridge
University Press)
Peacock J A 1997 Mon. Not. R. Astron. Soc. 284 885
——1999 Cosmological Physics (Cambridge: Cambridge University Press)
Peacock J A and Dodds S J 1994 Mon. Not. R. Astron. Soc. 267 1020
——1996 Mon. Not. R. Astron. Soc. 280 L19
Peacock J A and Smith R E 2000 Mon. Not. R. Astron. Soc. 318 1144
Pearce F R et al 1999 Astrophys. J. 521 L99
Peebles P J E 1973 Astrophys. J. 185 413
——1974 Astrophys. J. 32 197

References

107

——1980 The Large-Scale Structure of the Universe (Princeton, NJ: Princeton University
Press)
——1982 Astrophys. J. 263 L1
Peebles P J E and Yu J T 1970 Astrophys. J. 162 815
Pen U-L, Seljak U and Turok N 1997 Phys. Rev. Lett. 79 1611
Penzias A A and Wilson R W 1965 Astrophys. J. 142 419
Perlmutter S et al 1998 Astrophys. J. 517 565
Pogosyan D and Starobinsky A A 1995 Astrophys. J. 447 465
Press W H, Teukolsky S A, Vetterling W T and Flannery B P 1992 Numerical Recipes
2nd edn (Cambridge: Cambridge University Press)
Ratra B and Peebles P J E 1988 Phys. Rev. D 37 3406
Riess A G et al 1998 Astron. J. 116 1009
Sachs R K and Wolfe A M 1967 Astrophys. J. 147 73
Saunders W et al 2000 Mon. Not. R. Astron. Soc. 317 55
Saunders W, Rowan-Robinson M and Lawrence A 1992 Mon. Not. R. Astron. Soc. 258 134
Scoccimarro R, Zaldarriaga M and Hui L 1999 Astrophys. J. 527 1
Seljak U 1997 Astrophys. J. 482 6
——2000 Mon. Not. R. Astron. Soc. 318 203
Seljak U and Zaldarriaga M 1996 Astrophys. J. 469 437
Shanks T and Boyle B J 1994 Mon. Not. R. Astron. Soc. 271 753
Shanks T, Fong R, Boyle B J and Peterson B A 1987 Mon. Not. R. Astron. Soc. 227 739
Shectman S A, Landy S D, Oemler A, Tucker D L, Lin H, Kirshner R P and Schechter P L
1996 Astrophys. J. 470 172
Sheth R K and Tormen G 1999 Mon. Not. R. Astron. Soc. 308 11
Somerville R S and Primack J R 1999 Mon. Not. R. Astron. Soc. 310 1087
Starobinsky A A 1985 Sov. Astron. Lett. 11 133
Strauss M A and Willick J A 1995 Phys. Rep. 261 271
Sugiyama N 1995 Astrophys. J. Suppl. 100 281
Tegmark M 1996 Mon. Not. R. Astron. Soc. 280 299
Tegmark M, Taylor A N and Heavens A F 1997 Astrophys. J. 480 22
van Kampen E, Jimenez R and Peacock J A 1999 Mon. Not. R. Astron. Soc. 310 43
Viana P T and Liddle A R 1996 Mon. Not. R. Astron. Soc. 281 323
Vittorio N and Silk J 1991 Astrophys. J. 385 L9
Vogeley M S and Szalay A S 1996 Astrophys. J. 465 34
Weinberg S 1972 Gravitation & Cosmology (New York: Wiley)
——1989 Rev. Mod. Phys. 61 1
White M and Bunn E F 1995 Astrophys. J. 450 477
White M, Scott D and Silk J 1994 Annu. Rev. Astron. Astrophys. 32 319
White S D M, Efstathiou G and Frenk C S 1993 Mon. Not. R. Astron. Soc. 262 1023
White S D M and Rees M 1978 Mon. Not. R. Astron. Soc. 183 341
Zlatev I, Wang L and Steinhardt P J 1999 Phys. Rev. Lett. 82 896

Chapter 3
Cosmological models
George F R Ellis
Mathematics Department, University of Cape Town, South Africa

3.1 Introduction
The current standard models of the universe are the Friedmann–Lemaı̂tre (FL)
family of models, based on the Robertson–Walker (RW) spatially homogeneous
and isotropic geometries but with a much more complex set of matter constituents
than originally envisaged by Friedmann and Lemaı̂tre. It is appropriate then to
ask whether the universe is indeed well described by an RW geometry. There
is reasonable evidence supporting these models on the largest observable scales,
but at smaller scales they are clearly a bad description. Thus a better form of
the question is: On what scales and in what domains is the universe’s geometry
nearly RW? What are the best-fit RW parameters in the observable domain?
Given that the universe is apparently well described by the RW geometry
on the largest scales in the observable domain, the next question is: Why is it
RW? How did the universe come to have such an improbable geometry? The
predominant answer to this question at present is that it results from a very early
epoch when inflation took place (a period of accelerating expansion through many
e-folds of the scale of the universe). It is important to consider how good an
answer this is. One can only do so by considering alternatives to RW geometries,
as well as the models based on those geometries.
The third question is: How did astronomical structure come to exist on
smaller scales? Given a smooth structure on the largest scales, how was
that smoothness broken on smaller scales? Again, inflationary theory applied
to perturbed FL models gives a general answer to that question: quantum
fluctuations in the very early universe formed the seeds of inhomogeneities that
could then grow, on scales bigger than the (time-dependent) Jeans’ scale, by
gravitational attraction. It is important to note, however, that not only do structureformation effects depend in important ways on the background model, but also
108

Introduction

109

(and indeed, in consequence of this remark) many of the ways of estimating the
model parameters depend on models of structure formation. Thus the previous
questions and this one interact in a number of ways.
This review will look at the first two questions in some depth, and only
briefly consider the third (which is covered in depth in Peacock’s chapter). To
examine these questions, we need to consider the family of cosmological solutions
with observational properties like those of the real universe at some stage of their
histories. Thus we are interested in the full state space of solutions, allowing
us to see how realistic (lumpy) models are related to each other and to higher
symmetry models, including, in particular, the FL models. This chapter develops
general techniques for examining this family of models, and describes some
specific models of interest. The first part looks at exact general relations valid in
all cosmological models, the second part examines exact cosmological solutions
of the field equations and the third part looks at the observational properties of
these models and then returns to considering the previous questions. The chapter
concludes by emphasizing some of the fundamental issues that make it difficult
to obtain definitive answers if one tries to pursue the chain of cause and effect to
extremely early times.
3.1.1 Spacetime
We will make the standard assumption that on large scales, physics is dominated
by gravity, which is well described by general relativity (see, e.g. d’Inverno
[19], Wald [129], Hawking and Ellis [68] or Stephani [117]), with gravitational
effects resulting from spacetime curvature. The starting point for describing
a spacetime is an atlas of local coordinates {x i } covering the four-dimensional
spacetime manifold M, and a Lorentzian metric tensor gi j (x k ) at each point of
M, representing the spacetime geometry near the point on a particular scale. This
then determines the connection components  ij k (x s ), and, hence, the spacetime
curvature tensor Ri j kl , at that scale. The curvature tensor can be decomposed into
its trace-free part (the Weyl tensor Ci j kl : C i j il = 0) and its trace (the Ricci tensor
Rik ≡ R sisk ) by the relation
Ri j kl = Ci j kl − 12 (Rik g j l +R j l gik −Ril g j k −R j k gil )+ 16 R(gik g j l −gil g j k ), (3.1)
where R ≡ R aa is the Ricci scalar. The coordinates may be chosen arbitrarily in
each neighbourhood in M. To be useful in an explanatory role, a cosmological
model must be easy to describe—this means they have symmetries or special
properties of some kind or other.
3.1.2 Field equations
The metric tensor is determined, at the relevant averaging scale, by the Einstein
gravitational field equations (‘EFEs’)
(Ri j − 12 Rgi j ) + λgi j = κ Ti j ⇔ Ri j = λgi j + κ(Ti j − 12 T gi j )

(3.2)

110

Cosmological models

where λ is the cosmological constant and κ the gravitational constant. Here
Ti j (with trace T = T aa ) is the total energy–momentum–stress tensor for all
the matter and fields present, described at the relevant averaging scale. This
covariant equation (a set of second-order nonlinear equations for the metric tensor
components) shows that the Ricci tensor is determined pointwise by the matter
present at each point, but the Weyl tensor is not so determined; rather it is
fixed by suitable boundary conditions, together with the Bianchi identities for
the curvature tensor:
e
=0
∇[e Rab]cd = 0 ⇔ ∇[e Rab]cd

(3.3)

(the equivalence of the full equations on the left with the first contracted equations
on the right holding only for four dimensions or less). Consequently it is this
tensor that enables gravitational ‘action at a distance’ (gravitational radiation, tidal
forces, and so on). Contracting the right-hand of equation (3.3) and substituting
into the divergence of equation (3.2) shows Ti j necessarily obeys the energy–
momentum conservation equations
∇ j T ij = 0

(3.4)

(the divergence of λgi j vanishes provided λ is indeed constant, as we assume).
Thus matter determines the geometry which, in turn, determines the motion of
the matter (see e.g. [132]). We can look for exact solutions of these equations,
or approximate solutions obtained by suitable linearization of the equations; and
one can also consider how the solutions relate to Newtonian theory solutions.
Care must be exercised in the latter two cases, both because of the nonlinearity
of the theory, and because there is no fixed background spacetime available in
general relativity theory. This makes it essentially different from both Newtonian
theory and special relativity.
3.1.3 Matter description
The total stress tensor Ti j is the sum of the N stress tensors Tni j for the various
matter components labelled by index n (baryons, radiation, neutrinos, etc):
Ti j = &n Tni j

(3.5)

each component being described by suitable equations of state which encapsulate
their physics. The most common forms of matter in the cosmological context will
often to a good approximation, each have a ‘perfect fluid’ stress tensor;
Tni j = (µn + pn )u ni u n j + pn gi j

(3.6)

with unit 4-velocity u in ( u ni u in = −1), energy density µn and pressure pn , with
suitable equations of state relating µn and pn . In simple cases, they will be related
by a barotropic relation pn = pn (µn ); for example, for baryons, pb = 0 and for

Introduction

111

radiation, e.g. the cosmic background radiation (‘CBR’), pr = µr /3,. However,
in more complex cases there will be further variables determining pn and µn ; for
example, in the case of a massless scalar field φ with potential V (φ), on choosing
u i as the unit vector normal to spacelike surfaces φ = constant, the stress tensor
takes the form (3.6) with
4π pφ = 12 φ̇ 2 − V (φ),

4πµφ = 12 φ̇ 2 + V (φ).

(3.7)

It must be noted that, in general, different matter components will each have
a different 4-velocity u in , and the total stress tensor (3.5) of perfect fluid stress
tensors (3.6) itself has the perfect fluid form if and only if the 4-velocities of all
contributing matter components are the same, i.e. u in = u i for all n; in that case,
Ti j = (µ + p)u i u j + pgi j ,

µ ≡ &n µn ,

p ≡ &n pn

(3.8)

where µ is the total energy density and p the total pressure.
The individual matter components will each separately satisfy the
conservation equation (3.4) if they are non-interacting with the other components;
however this will no longer be the case if interactions lead to exchanges of
energy and momentum between the different components. The key to a physically
realistic cosmological model is the representation of suitable matter components,
with realistic equations of state for each matter component and equations
describing the interactions between the components. For reasonable behaviour
of matter, irrespective of its constitution we require the ‘energy condition’
µ+ p >0

(3.9)

on cosmological averaging scales (the vacuum case µ + p = 0 can apply only
to regions described on averaging scales less than or equal to that of clusters of
galaxies).
3.1.4 Cosmology
A key feature of cosmological models, as contrasted with general solutions of the
EFEs, is that in them, at each point a unique 4-velocity u a is defined representing
the preferred motion of matter there on a cosmological scale. Whenever the matter
present is well described by the perfect fluid stress tensor (3.8), because of (3.9)
there will be a unique timelike eigenvector of this tensor that can be used to define
the vector u, representing the average motion of the matter, and conventionally
referred to as defining the fundamental world-lines of the cosmology. Unless
stated otherwise, we will assume that observers move with this 4-velocity. At
late times, a unique frame is defined by choosing a 4-velocity such that the CBR
anisotropy dipole vanishes; the usual assumption is that this is the same frame as
defined locally by the average motion of matter [26]; indeed this assumption is
what underlies studies of large-scale motions and the ‘Great Attractor’.

112

Cosmological models

The description of matter and radiation in a cosmological model must be
sufficiently complete to determine the observational relations predicted by the
model for both discrete sources and the background radiation, implying a welldeveloped theory of structure growth for very small and for very large physical
scales (i.e. for light atomic nuclei and for galaxies and clusters of galaxies), and
of radiation absorbtion and emission. Clearly an essential requirement for a
viable cosmological model is that it should be able to reproduce current largescale astronomical observations accurately.
I will deal with both the 1 + 3 covariant approach [21, 26, 28, 91] and the
orthonormal tetrad approach, which serves as a completion to the 1 + 3 covariant
approach [41].

3.2 1 + 3 covariant description: variables
3.2.1 Average 4-velocity of matter
The preferred 4-velocity is
ua =

dx a
,
dτ

u a u a = −1,

(3.10)

where τ is the proper time measured along the fundamental world-lines. Given
u a , unique projection tensors can be defined:
U a b = −u a u b ⇒ U a c U c b = U a b , U a a = 1, Uab u b = u a ,
h ab = gab + u a u b ⇒ h a c h c b = h a b , h a a = 3, h ab u b = 0.

(3.11)

The first projects parallel to the velocity vector u a , and the second determines
the metric properties of the (orthogonal) instantaneous rest-spaces of observers
moving with 4-velocity u a . A volume element for the rest spaces:
ηabc = u d ηdabc ⇒ ηabc = η[abc] , ηabc u c = 0,

(3.12)

where
ηabcd is the four-dimensional volume element (ηabcd = η[abcd] , η0123 =
√
| det gab |) is also defined.
Furthermore, two derivatives are defined: the covariant time derivative ‘˙’
along the fundamental world-lines, where for any tensor T ab cd
Ṫ ab cd = u e ∇e T ab cd ,

(3.13)

and the fully orthogonally projected covariant derivative ∇˜ where, for any tensor
T ab cd ,
(3.14)
∇˜ e T ab cd = h a f h b g h p c h q d h r e ∇r T f g pq ,
with total projection on all free indices. The tilde serves as a reminder that if u a
has non-zero vorticity, ∇˜ is not a proper three-dimensional covariant derivative

1 + 3 covariant description: variables

113

(see equation (3.20)). The projected time and space derivatives of Uab , h ab and
ηabc all vanish. Finally, following [91] we use angle brackets to denote orthogonal
projections of vectors and the orthogonally projected symmetric trace-free part of
tensors:
T ab = [h (a c h b) d − 13 h ab h cd ]T cd ;
(3.15)
v a = h a b v b ,
for convenience the angle brackets are also used to denote othogonal projections
of covariant time derivatives along u a (‘Fermi derivatives’):
v̇ a = h a b v̇ b ,

Ṫ ab = [h (a c h b) d − 13 h ab h cd ]Ṫ cd .

(3.16)

3.2.2 Kinematic quantities
The orthogonal vector u̇ a = u b ∇b u a is the acceleration vector, representing the
degree to which the matter moves under forces other than gravity plus inertia
(which cannot be covariantly separated from each other in general relativity). The
acceleration vanishes for matter in free fall (i.e. moving under gravity plus inertia
alone).
We split the first covariant derivative of u a into its irreducible parts, defined
by their symmetry properties:
∇a u b = −u a u̇ b + ∇˜ a u b = −u a u̇ b + 13 'h ab + σab + ωab

(3.17)

where the trace ' = ∇˜ a u a is the (volume) rate of expansion of the fluid (with
H = '/3 the Hubble parameter); σab = ∇˜ a u b is the trace-free symmetric
shear tensor (σab = σ(ab) , σab u b = 0, σ a a = 0), describing the rate of
distortion of the matter flow; and ωab = ∇˜ [a u b] is the skew-symmetric vorticity
tensor (ωab = ω[ab] , ωab u b = 0), describing the rotation of the matter relative
to a non-rotating (Fermi-propagated) frame. The meaning of these quantities
a = h a ηb ,
follows from the evolution equation for a relative position vector η⊥
b
a
where η is a deviation vector for the family of fundamental world-lines, i.e.
a
u b ∇b ηa = ηb ∇b u a . Writing η⊥
= δ"ea , ea ea = 1, we find the relative distance
δ" obeys the propagation equation
(δ").
= 13 ' + (σab ea eb ),
δ"

(3.18)

(the generalized Hubble law), and the relative direction vector ea the propagation
equation
(3.19)
ėa = (σ a b − (σcd ec ed )h a b − ωa b )eb ,
giving the observed rate of change of position in the sky of distant galaxies
[21, 26].
˜
Each function f satisfies the important commutation relation for the ∇derivative [40]
∇˜ [a ∇˜ b] f = ηabc ωc f˙.
(3.20)

114

Cosmological models

Applying this to the energy density µ shows that if ωa µ̇ = 0 in an open set
then ∇˜ a µ = 0 there, so non-zero vorticity implies anisotropic number counts in
an expanding universe [61] (this is because there are then no 3-surfaces orthogonal
to the fluid flow; see [21, 26]).
3.2.2.1 Auxiliary quantities
It is useful to define some associated kinematical quantities:
•
•

the vorticity vector ωa = 12 ηabc ωbc ⇒ ωa u a = 0, ωab ωb = 0,
the magnitudes ω2 = 12 (ωab ωab ) ≥ 0, σ 2 = 12 (σab σ ab ) ≥ 0, and

•

the average length scale S determined by ṠS = 13 ', so the volume of a fluid
element varies along the fluid flow lines as S 3 .

3.2.3 Matter tensor
Both the total matter energy–momentum tensor Tab and each of its components
can be decomposed relative to u a in the form
Tab = µu a u b + qa u b + u a qb + ph ab + πab ,

(3.21)

where µ = (Tab u a u b ) is the relativistic energy density relative to u a , q a =
−Tbc u b h ca is the relativistic momentum density (qa u a = 0), which is also
the energy flux relative to u a , p = 13 (Tab h ab ) is the isotropic pressure, and
πab = Tcd h c a h d b is the trace-free anisotropic pressure (π a a = 0, πab = π(ab),
πab u b = 0). A different choice of u a will result in a different splitting. The
physics of the situation is in the equations of state relating these quantities; for
example, the commonly imposed restrictions
q a = πab = 0 ⇔ Tab = µu a u b + ph ab

(3.22)

characterize a ‘perfect fluid’ moving with the chosen 4-velocity u a as in
equation (3.8) with, in general, an equation of state p = p(µ, s) where s is the
entropy [21, 26].
3.2.4 Electromagnetic field
The Maxwell field tensor Fab of an electromagnetic field is split relative to u a into
electric and magnetic parts by the relations (see [28])
E a = Fab u b ⇒ E a u a = 0,
Ha = 12 ηabc F bc ⇒ Ha u a = 0.
Again, a different choice of u a will result in a different split.

(3.23)
(3.24)

1 + 3 Covariant description: equations

115

3.2.5 Weyl tensor
In analogy to Fab , the Weyl conformal curvature tensor Cabcd defined by
equation (3.1) is split relative to u a into ‘electric’ and ‘magnetic’ Weyl curvature
parts according to
E ab = Cacbd u c u d ⇒ E a a = 0, E ab = E (ab), E ab u b = 0,
Hab =

1
de
c
2 ηade C bc u

⇒H

a

a

= 0, Hab = H(ab), Hab u = 0.
b

(3.25)
(3.26)

These influence the motion of matter and radiation through the geodesic deviation
equation for timelike and null vectors, see, respectively, [107] and [120].

3.3 1 + 3 Covariant description: equations
There are three sets of equations to be considered, resulting from EFE (3.2) and
its associated integrability conditions.
3.3.1 Energy–momentum conservation equations
We obtain from the conservation equations (3.4), on projecting parallel and
perpendicular to u a and using (3.21), the propagation equations

q̇

a

µ̇ + ∇˜ a q a = −'(µ + p) − 2(u̇ a q a ) − (σ a b π b a ),
(3.27)
+ ∇ p + ∇˜ b π ab = − 43 'q a − σ a b q b − (µ + p)u̇ a − u̇ b π ab − ηabc ωb qc .
(3.28)
˜a

For perfect fluids, characterized by equation (3.8), these reduce to
µ̇ = −'(µ + p),

(3.29)

the energy conservation equation, and the momentum conservation equation
0 = ∇˜ a p + (µ + p)u̇ a

(3.30)

(which because of the perfect fluid assumption, has changed from a timederivative equation for q a to an algebraic equation for u̇ a , and thus a timederivative equation for u a ). These equations show that (µ + p) is both the inertial
mass density and that it governs the conservation of energy. It is clear that if this
quantity is zero (the case of an effective cosmological constant) or negative, the
behaviour of matter will be anomalous; in particular velocities will be unstable
if µ + p → 0, because the acceleration generated by a given force will diverge
in this limit. If we assume a perfect fluid with a (linear) γ -law equation of state,
then (3.29) shows that
p = (γ − 1)µ, γ̇ = 0 ⇒ µ = M/S 3γ , Ṁ = 0.

(3.31)

116

Cosmological models

One can approximate ordinary matter in this way, with 1 ≤ γ ≤ 2 in order
that the causality and energy conditions are valid. Radiation corresponds to
γ = 43 ⇒ µ = M/S 4 , so from Stefan’s law (µ ∝ T 4 ) we find that T ∝ 1/S.
Another useful case is pressure-free matter (often described as ‘baryonic’ or ‘cold
dark matter (CDM)’); the momentum conservation: (3.30) shows that such matter
moves geodesically (as expected from the equivalence principle):
γ = 1 ⇔ p = 0 ⇒ u̇ a = 0, µ = M/S 3 .

(3.32)

This is the case of pure gravitation, without fluid dynamical effects. Another
important case is that of a scalar field, see (3.7).
3.3.2 Ricci identities
The second set of equations arise from the Ricci identities for the vector field u a ,
i.e.
2∇[a ∇b] u c = Rab c d u d .

(3.33)

On substituting from (3.17), using (3.2), and separating out the parallelly
and orthogonally projected parts into a trace, symmetric trace-free and skew
symmetric part, we obtain three propagation equations and three constraint
equations. The propagation equations are the Raychaudhuri equation, the
vorticity propagation equation and the shear propagation equation.
3.3.2.1 The Raychaudhuri equation
This equation
˙ = − 1 '2 + ∇a u̇ a − 2σ 2 + 2ω2 − 1 (µ + 3 p) + λ,
'
3
2

(3.34)

the basic equation of gravitational attraction [21, 26, 28], shows the repulsive
nature of a positive cosmological constant and leads to the identification of
(µ + 3 p) as the active gravitational mass density. Rewriting it in terms of the
average scale factor S, this equation can be rewritten in the form
3

S̈
1
= −2(σ 2 − ω2 ) + ∇a u̇ a − (µ + 3 p) + λ,
S
2

(3.35)

showing how the curvature of the curve S(τ ) along each world-line (in terms
of proper time τ along that world-line) is determined by the shear, vorticity and
acceleration; the total energy density and pressure in terms of the combination
(µ + 3 p)—the active gravitational mass; and the cosmological constant λ. This
gives the basic singularity theorem.

1 + 3 Covariant description: equations

117

Singularity theorem. [21, 26, 28] In a universe where the active gravitational
mass is positive at all times,
(µ + 3 p) > 0,

(3.36)

the cosmological constant vanishes (or is negative); λ ≤ 0, and the vorticity
and acceleration vanish; u̇ a = ωa = 0 at all times, at any instant when
H0 = 13 '0 > 0, there must have been a time t0 < 1/H0 ago such that S → 0 as
t → t0 ; a spacetime singularity occurs there, where µ → ∞ and T → ∞.
The further singularity theorems of Hawking and Penrose [68,69,124] utilize
this result or its null version as an essential part of their proofs.
Closely related to this are three other results:
(1) a static universe model containing ordinary matter requires λ > 0 (Einstein’s
discovery of 1917);
(2) the Einstein static universe is unstable (Eddington’s discovery of 1930);
(3) in a universe satisfying the requirements of the singularity theorem, at each
instant t the age of the universe is less that 1/H (t), so for example the hot
early stage of the universe takes place extremely rapidly.
Proofs follow directly from (3.35). The energy condition (µ + 3 p) > 0 will
be satisfied by all ordinary matter but will not, in general, be satisfied by a scalar
field, see (3.7).
3.3.2.2 The vorticity propagation equation
ω̇a − 12 ηabc ∇˜ b u̇ c = − 23 'ωa + σ a b ωb .

(3.37)

If we have a barotropic perfect fluid:
q a = πab = 0, p = p(µ) ⇒ ηabc ∇˜ b u̇ c = 0,

(3.38)

then ωa = 0 is involutive: i.e. the statement
ωa = 0 initially ⇒ ω̇a = 0 ⇒ ωa = 0 at later times
follows from the vorticity conservation equation (3.37) (and it is also true in the
special case p = 0). Thus non-trivial entropy dependence or an imperfect fluid is
required to create vorticity.
When the vorticity vanishes ⇔ ω = 0:
(1) The fluid flow is hypersurface-orthogonal, and there exists a cosmic time
function t such that u a = −g(x b )∇a t, allowing synchronization of the
clocks of fundamental observers. If, in addition, the acceleration vanishes,
we can set g = 1 and the time function can be proper time for all of
them (whereas if the acceleration is non-zero, the coordinate time t will
necessarily correspond to different proper times along different world-lines).

Cosmological models

118

(2) The metric of the orthogonal 3-spaces t = constant formed by meshing
together the tangent spaces orthogonal to u a is h ab .
(3) From the Gauss equation and the Ricci identities for u a , the Ricci tensor of
these 3-spaces is given by [21, 26]
3

Rab = −σ̇ab − 'σab + ∇˜ a u̇ b + u̇ a u̇ b + πab + 13 h ab 3 R,

(3.39)

and their Ricci scalar is given by
3

R = 2µ − 23 '2 + 2σ 2 + 2λ,

(3.40)

which is a generalized Friedmann equation, showing how the matter tensor
determines the 3-space average curvature. These equations fully determine
the curvature tensor 3 Rabcd of the orthogonal 3-spaces, and so show how the
EFEs result in spatial curvature (as well as spacetime curvature) [21, 26].
3.3.2.3 The shear propagation equation
σ̇ ab − ∇˜ a u̇ b = − 23 'σ ab + u̇ a u̇ b −σ a c σ bc −ωa ωb −(E ab − 12 π ab ). (3.41)
This shows how the tidal gravitational field E ab directly induces shear (which
then feeds into the Raychaudhuri and vorticity propagation equations, thereby
changing the nature of the fluid flow), and that the anisotropic pressure term πab
also generates shear in an imperfect fluid situation. Shear-free solutions are very
special solutions, because (in contrast to the case of vorticity) a conspiracy of
terms is required to maintain the shear zero if it is zero at any initial time (see
later for a specific example).
The constraint equations are as follows:
(1) The (0α)-equation
0 = (C1 )a = ∇˜ b σ ab − 23 ∇˜ a ' + ηabc [∇˜ b ωc + 2u̇ b ωc ] + q a ,

(3.42)

shows how the momentum flux q a (zero for a comoving perfect fluid) relates
to the spatial inhomogeneity of the expansion.
(2) The vorticity divergence identity
0 = (C2 ) = ∇˜ a ωa − (u̇ a ωa ),

(3.43)

follows because ωa is a curl.
(3) The Hab -equation
0 = (C3 )ab = H ab + 2u̇ a ωb + (curl σ )ωab − (curl σ )ab ,

(3.44)

characterizes the magnetic part of the Weyl tensor as being constructed from
the ‘curls’ of the vorticity and shear tensors: (curl ω)ab = ηcda ∇˜ c ωb d ,
(curl σ )ab = ηcda ∇˜ c σ b d .

1 + 3 Covariant description: equations

119

3.3.3 Bianchi identities
The third set of equations arises from the Bianchi identities (3.3). On using
the splitting of Rabcd into Rab and Cabcd , the 1 + 3 splitting, (3.21),(3.25) of
those quantities, and the EFE (3.2), these identities give two further propagation
equations and two further constraint equations, which are similar in form to the
Maxwell field equations for the electromagnetic field in an expanding universe
(see [28]).
The propagation equations are:
( Ė ab + 12 π̇ ab ) = (curl H )ab − 12 ∇˜ a q b − 12 (µ + p)σ ab − '(E ab + 16 π ab )
+ 3σ a c (E bc − 16 π bc ) − u̇ a q b
+ ηcda [2u̇ c H bd + ωc (E b d + 12 π b d )],

(3.45)

the Ė-equation, and
Ḣ ab = − (curl E)ab + 12 (curl π)ab − 'H ab + 3σ a c H bc
+ 32 ωa q b − ηcda [2u̇ c E b d − 12 σ b c qd − ωc H bd ],

(3.46)

the Ḣ -equation, where we have defined the ‘curls’:
(curl H )ab = ηcda ∇˜ c H b d ,

(curl E)ab = ηcda ∇˜ c E b d .

(3.47)

These equations show how gravitational radiation arises: as in the electromagnetic
case, taking the time derivative of the Ė-equation gives a term of the form
(curl H )˙; commuting the derivatives and substituting from the Ḣ -equation
eliminates H , and results in a term in Ë and a term of the form (curl curl E),
which together give the wave operator acting on E [20, 66]. Similarly the time
derivative of the Ḣ -equation gives a wave equation for H, and associated with
these is a wave equation for the shear σ .
The constraint equations are
0 = (C4 )a = ∇˜ b (E ab + 12 π ab ) − 13 ∇˜ a µ + 13 'q a
− 12 σ a b q b − 3ωb H ab − ηabc [σbd H d c − 32 ωb qc ],

(3.48)

the (div E)-equation with its source the spatial gradient of the energy density and
0 = (C5 )a = ∇˜ b H ab + (µ + p)ωa + 3ωb (E ab − 16 π ab )
+ ηabc [ 1 ∇˜ b qc + σbd (E d c + 1 π d c )],
2

2

(3.49)

the (div H )-equation, with its source the fluid vorticity. The (div E)-equation
can be regarded as a (vector) analogue of the Newtonian Poisson equation [52],
leading to the Newtonian limit and enabling tidal action at a distance. These
equations respectively show that, generically, scalar modes will result in a nonzero divergence of E ab (and hence a non-zero E-field) and vector modes in a
non-zero divergence of Hab (and hence a non-zero H -field).

120

Cosmological models

3.3.4 Implications
Altogether, we have six propagation equations and six constraint equations;
considered as a set of evolution equations for the 1+3 covariant variables, they are
a first-order system of equations. This set is determinate once the fluid equations
of state are given; together they then form a dynamical system (the set closes up,
but is essentially an infinite dimensional dynamical system because of the spatial
derivatives that occur).
The key issue that arises is consistency of the constraints with the evolution
equations. It is believed that they are generally consistent for physically
reasonable and well-defined equations of state, i.e. they are consistent if no
restrictions are placed on their evolution other than those implied by the constraint
equations and the equations of state (this has been shown for irrotational dust
[91]). It is this that makes consistent the overall hyperbolic nature of the equations
with the ‘instantaneous’ action at a distance implicit in the Gauss-like equations
(specifically, the (div E)-equation), the point being that the ‘action at a distance’
nature of the solutions to these equations is built into the initial data, which must
be chosen so that the constraints are satisfied initially, and they then remain
satisfied thereafter because the time evolution preserves these constraints (cf
[49]).

3.3.5 Shear-free dust
One must be very cautious with imposing simplifying assumptions in order to
obtain solutions: this can lead to major restrictions on the possible flows, and
one can be badly misled if their consistency is not investigated carefully. A case
of particular interest is shear-free dust, that is perfect-fluid solutions for which
σab = 0, p = 0 ⇒ u̇ a = 0. In this case, careful study of the consistency
conditions between all the equations [25] shows that necessarily ω' = 0: the
solutions either do not rotate, or do not expand. This conclusion is of considerable
importance, because if it were not true, there would be shear-free expanding and
rotating solutions which would violate the Hawking–Penrose singularity theorems
for cosmology [68,69] (integrating the vorticity equation along the fluid flow lines
(3.37) gives ω = ω0 /S 2 ; substituting in the Raychaudhuri equation (3.34) and
integrating, using the conservation equation (3.29), gives a first integral which is a
generalized Friedmann equation, in which vorticity dominates expansion at early
times and allows a bounce and singularity avoidance). The interesting point then
is that this result does not hold in Newtonian theory [113], in which case there
do indeed exist such solutions when suitable boundary conditions are imposed.
If one uses these solutions as an argument against the singularity theorems, the
argument is invalid; what they really do is point out the dangers of the Newtonian
limit of cosmological equations.

Tetrad description

121

3.4 Tetrad description
The 1+3 covariant equations are immediately transparent in terms of representing
relations between 1 + 3 covariantly defined quantities with clear geometrical
and/or physical significance. However, they do not form a complete set of
equations guaranteeing the existence of a corresponding metric and connection.
For that we need to use a full tetrad description. The equations determined will
then form a complete set, which will contain as a subset all the 1 + 3 covariant
equations just derived (albeit presented in a slightly different form) [53, 55]. First
we summarize a generic tetrad formalism, and then describe its application to
cosmological models (cf [25, 92]).
3.4.1 General tetrad formalism
A tetrad is a set of four linearly independent vector fields {ea }, a = 0, 1, 2, 3,
which serves as a basis for spacetime vectors and tensors. It can be written in
terms of a local coordinate basis by means of the tetrad components ea i (x j ):
ea = ea i (x j )

∂
∂f
⇔ ea ( f ) = ea i (x j ) i ,
i
∂x
∂x

ea i ≡ ea (x i ),

(3.50)

(the latter stating that the i th component of the ath tetrad vector is just the
directional derivative of the i th coordinate x i in the direction ea ). This relation
can be thought of as just a change of vector basis, leading to a change of tensor
components of the standard tensorial form:
T ab cd = ea i eb j ec k ed l T i j kl
with an obvious inverse, where the inverse components ea i (x j ) (note the placing
of the indices!) are defined by
ea i e a j = δ i j ⇔ ea i e b i = δ b a .

(3.51)

However, this is a change from an integrable basis to a non-integrable one, so
the non-tensorial relations (specifically the form of the metric and connection
components) differ slightly from when coordinate bases are used. A change of
one tetrad basis to another will also lead to transformations of the standard tensor
form for all tensorial quantities: if ea = λa a (x i )ea is a change of tetrad basis
with inverse ea = λa a (x i )ea (in the case of orthonormal bases, each of these
matrices representing a Lorentz transformation), then
T ab cd = λa a λb b λc c λd d T a b c d .
Again the inverse is obvious. The commutation functions related to the tetrad are
the quantities γ a bc (x i ) defined by the commutators [ea , eb ] of the basis vectors:
[ea , eb ] = γ c ab (x i )ec ⇒ γ a bc (x i ) = −γ a cb (x i ).

(3.52)

122

Cosmological models

It follows (apply this relation to the coordinate x i ) that in terms of the tetrad
components,
γ a bc (x i ) = ea i (eb j ∂ j ec i − ec j ∂ j eb i ) = −2eb i ec j ∇[i ea j ] .

(3.53)

These quantities vanish iff the basis {ea } is a coordinate basis: that is, there exist
coordinates x i such that ea = δa i ∂/∂ x i , iff
[ea , eb ] = 0 ⇔ γ a bc = 0.
The metric tensor components in the tetrad form are given by
gab = gi j ea i eb j = ea · eb .

(3.54)

gi j (x k ) = gab ea i (x k )eb j (x k )

(3.55)

The inverse equation

explicitly constructs the coordinate components of the metric from the (inverse)
tetrad components ea i (x j ). We can raise and lower tetrad indices by use of the
metric gab and its inverse g ab . In the case of an orthonormal tetrad,
gab = diag(−1, +1, +1, +1) = g ab ,

(3.56)

showing by (3.54) that the basis vectors are unit vectors orthogonal to each
other. Such a tetrad is defined up to an arbitrary position-dependent Lorentz
transformation.
The connection components  a bc for the tetrad are defined by the relations
∇eb ea =  c ab ec ⇔  c ab = ec i eb j ∇ j ea i ,

(3.57)

i.e. it is the c-component of the covariant derivative in the b-direction of the
a-vector. It follows that all covariant derivatives can be written out in tetrad
components in a way completely analogous to the usual tensor form, for example
∇a Tbc = ea (Tbc ) −  d ba Tdc −  d ca Tbd ,
where for any function f , ea ( f ) = ea i ∂ f /∂ x i is the derivative of f in the
direction ea . In the case of an orthonormal tetrad, (3.56) shows that ea (gbc ) = 0;
hence applying this relation to the metric tensor,
∇a gbc = 0 ⇔ (ab)c = 0,

(3.58)

—the connection components are skew in their first two indices, when we use
the metric to raise and lower the first indices only, and are called ‘Ricci rotation
coefficients’ or just rotation coefficients. We obtain from this and the assumption

Tetrad description

123

of vanishing torsion the relations for an orthonormal tetrad that are the analogues
of the usual Christoffel relation:
γ a bc = −( a bc − a cb ),

abc = 12 (gad γ d cb − gbd γ d ca + gcd γ d ab ). (3.59)

This shows that the rotation coefficients and the commutation functions are each
just linear combinations of the other.
Any set of vectors however must satisfy the Jacobi identities:
[X, [Y, Z ]] + [Y, [Z , X]] + [Z , [X, Y ]] = 0,
which follows from the definition of a commutator. Applying this to the basis
vectors ea , eb and ec gives the identities
e[a (γ d bc] ) + γ e [ab γ d c]e = 0,

(3.60)

which are the integrability conditions that the γ a bc (x i ) are the commutation
functions for the set of vectors ea .
If we apply the Ricci identities to the tetrad basis vectors ea , we obtain the
Riemann curvature tensor components in the form
R a bcd = ∂c ( a bd ) − ∂d ( a bc ) +  a ec  e bd −  a ed  e bc −  a be γ e cd .

(3.61)

Contracting this on a and c, one obtains the EFE in the form
Rbd = ∂a ( a bd ) − ∂d ( a ba ) +  a ea  e bd −  a de  e ba = Tbd − 12 T gbd + λgbd .
(3.62)
It is not immediately obvious that this is symmetric, but this follows because
(3.60) implies Ra[bcd] = 0 ⇒ Rab = R(ab) .
3.4.2 Tetrad formalism in cosmology
In detailed studies of families of exact non-vacuum solutions, it will usually be
advantageous to use an orthonormal tetrad basis, because the tetrad vectors can be
chosen in physically preferred directions. For a cosmological model we choose an
orthonormal tetrad with the timelike vector e0 chosen to be either the fundamental
4-velocity field u a , or the normals n a to surfaces of homogeneity when they
exist. This fixing implies that the initial six-parameter freedom of using Lorentz
transformations has been reduced to a three-parameter freedom of rotations of
the spatial frame {eα }. The 24 algebraically independent rotation coefficients can
then be split into (see [25, 45, 53]):
α00 = u̇ α ,

α0β = 13 'δαβ + σαβ − αβγ ωγ ,

αβγ = 2a[α δβ]γ + γ δ[α n δ β] + 12 αβδ n δ γ .

αβ0 = αβγ γ (3.63)
(3.64)

The first two sets contain the kinematical variables for the chosen vector field.
The third is the rate of rotation α of the spatial frame {eα } with respect to

124

Cosmological models

a Fermi-propagated (physically non-rotating) basis along the fundamental flow
lines. Finally, the quantities a α and n αβ = n (αβ) determine the nine spatial
rotation coefficients. In terms of these quantities, the commutator equations (3.52)
applied to any function f take the form
[e0, eα ]( f ) = u̇ α e0 ( f ) − [ 13 'δα β + σα β + α β γ (ωγ + γ )]eβ ( f ),
γ

[eα , eβ ]( f ) = 2αβγ ω e0 ( f ) + [2a[α δ

γ

β]

δγ

+ αβδ n ]eγ ( f ).

(3.65)
(3.66)

3.4.3 Complete set
The full set of equations for a gravitating fluid can be written in tetrad form, using
the matter variables, the rotation coefficients (3.57) and the tetrad components
(3.50) as the primary variables. The equations needed are the conservation
equations (3.27), (3.28) and all the Ricci equations (3.61) and Jacobi identities
(3.60) for the tetrad basis vectors, together with the tetrad equations (3.50) and
the commutator equations (3.53). This gives a set of constraints and a set of firstorder evolution equations, which include the tetrad form of all the 1 + 3 covariant
equations given earlier, based on the chosen vector field. For a prescribed set of
equations of state, this gives the complete set of relations needed to determine the
spacetime structure. One has the option of including or not including the tetrad
components of the Weyl tensor as variables in this set; whether it is better to
include them or not depends on the problem to be solved (if they are included,
there will be more equations in the corresponding complete set, for we must then
include the full Bianchi identities). The full set of equations is given in [41, 55],
and see [25, 118] for the use of tetrads to study locally rotationally symmetric
spacetimes, and [45, 128] for the case of Bianchi universes.
Finally, when tetrad vectors are chosen uniquely in an invariant way
(e.g. as eigenvectors of a non-degenerate shear tensor), then—because they are
uniquely defined from 1 + 3 covariant quantities—all the rotation coefficients
are covariantly defined scalars, so these equations are all equations for scalar
invariants. The only times when it is not possible to define unique tetrads in
this way is when the spacetimes are isotropic or locally rotationally symmetric
(these concepts are discussed later).

3.5 Models and symmetries
3.5.1 Symmetries of cosmologies
Symmetries of a space or a spacetime (generically, ‘space’) are transformations of
the space into itself that leave the metric tensor and all physical and geometrical
properties invariant. We deal here only with continuous symmetries, characterized
by a continuous group of transformations and associated vector fields [24].

Models and symmetries

125

3.5.1.1 Killing vectors
A space or spacetime symmetry, or isometry, is a transformation that drags the
metric along a certain congruence of curves into itself. The generating vector field
ξi of such curves is called a Killing vector (field) (or ‘KV’), and obeys Killing’s
equations,
(L ξ g)i j = 0 ⇔ ∇(i ξ j ) = 0 ⇔ ∇i ξ j = −∇ j ξi ,
(3.67)
where L X is the Lie derivative. By the Ricci identities for a KV, this implies the
curvature equation:
∇i ∇ j ξk = R m i j k ξm ,
(3.68)
and hence the infinite series of further equations that follows by taking covariant
derivatives of this one, e.g.
∇l ∇i ∇ j ξk = (∇l R m i j k )ξm + R m i j k ∇l ξm .

(3.69)

The set of all KVs forms a Lie algebra with a basis {ξa }, a = 1, 2, . . . , r , of
dimension r ≤ 12 n(n − 1). ξai denotes the components with respect to a local
coordinate basis, a, b and c label the KV basis and i , j and k the coordinate
components. Any KV can be written in terms of this basis, with constant
coefficients. Hence, if we take the commutator [ξa , ξb ] of two of the basis KVs,
this is also a KV, and so can be written in terms of its components relative to the
KV basis, which will be constants. We can write the constants as C c ab , obtaining
[ξa , ξb ] = C c ab ξc ,

C a bc = C a [bc] .

(3.70)

By the Jacobi identities for the basis vectors, these structure constants must satisfy
C a e[b C e cd] = 0

(3.71)

(which is just equation (3.60) specialized to a set of vectors with constant
commutation functions). These are the integrability conditions that must
be satisfied in order that the Lie algebra exist in a consistent way. The
transformations generated by the Lie algebra form a Lie group of the same
dimension (see Eisenhart [24] or Cohn [11]).
Arbitrariness of the basis: We can change the basis of KVs in the usual way;
ξa = λa a ξa ⇔ ξai = λa a ξai ,

(3.72)

where the λa a are constants with det(λa a ) = 0, so unique inverse matrices λa a
exist. Then the structure constants transform as tensors:
C c a b = λc c λa a λb b C c ab .

(3.73)

Thus the possible equivalence of two Lie algebras is not obvious, as they may be
given in different bases.

126

Cosmological models

3.5.1.2 Groups of isometries
The isometries of a space of dimension n must be a group, as the identity is
an isometry, the inverse of an isometry is an isometry, and the composition of
two isometries is an isometry. Continuous isometries are generated by the Lie
algebra of KVs. The group structure is determined locally by the Lie algebra,
in turn characterized by the structure constants [11]. The action of the group is
characterized by the nature of its orbits in space; this is only partially determined
by the group structure (indeed the same group can act as a spacetime symmetry
group in quite different ways).
3.5.1.3 Dimensionality of groups and orbits
Most spaces have no KVs, but special spaces (with symmetries) have some. The
group action defines orbits in the space where it acts and the dimensionality of
these orbits determines the kind of symmetry that is present.
The orbit of a point p is the set of all points into which p can be moved
by the action of the isometries of a space. Orbits are necessarily homogeneous
(all physical quantities are the same at each point). An invariant variety is a
set of points moved into itself by the group. This will be bigger than (or equal
to) all orbits it contains. The orbits are necessarily invariant varieties; indeed
they are sometimes called minimum invariant varieties, because they are the
smallest subspaces that are always moved into themselves by all the isometries
in the group. Fixed points of a group of isometries are those points which are
left invariant by the isometries (thus the orbit of such a point is just the point
itself). These are the points where all KVs vanish (however, the derivatives of
the KVs there are non-zero; the KVs generate isotropies about these points).
General points are those where the dimension of the space spanned by the KVs
(that is, the dimension of the orbit through the point) takes the value it has almost
everywhere; special points are those where it has a lower dimension (e.g. fixed
points). Consequently, the dimension of the orbits through special points is lower
than that of orbits through general points. The dimension of the orbit and isotropy
group is the same at each point of an orbit, because of the equivalence of the group
action at all points on each orbit.
The group is transitive on a surface S (of whatever dimension) if it can move
any point of S into any other point of S. Orbits are the largest surfaces through
each point on which the group is transitive; they are therefore sometimes referred
to as surfaces of transitivity. We define their dimension as follows, and determine
limits from the maximal possible initial data for KVs: dimension of the surface of
transitivity = s, where in a space of dimension n, s ≤ n.
At each point we can also consider the dimension of the isotropy group
(the group of isometries leaving that point fixed), generated by all those KVs
that vanish at that point: dimension of an isotropy group = q, where q ≤
1
2 n(n − 1).

Models and symmetries

127

The dimension r of the group of symmetries of a space of dimension n is
r = s + q (translations plus rotations). The dimension q of the isotropy group
can vary over the space (but not over an orbit): it can be greater at special points
(e.g. an axis centre of symmetry) where the dimension s of the orbit is less, but
r (the dimension of the total symmetry group) must stay the same everywhere.
From these limits , 0 ≤ r ≤ n + 12 n(n − 1) = 12 n(n + 1) (the maximal number
of translations and of rotations). This shows the Lie algebra of KVs is finite
dimensional.
Maximal dimensions: If r = 12 n(n + 1), we have a space(time) of constant
curvature (maximal symmetry for a space of dimension n). In this case,
Ri j kl = K (gik g j l − gil g j k ),

(3.74)

with K a constant. One cannot get q = 12 n(n − 1) − 1 so r = 12 n(n + 1) − 1.
A group is simply transitive if r = s ⇔ q = 0 (no redundancy:
dimensionality of group of isometries is just sufficient to move each point in a
surface of transitivity into each other point). There is no continuous isotropy
group.
A group is multiply transitive if r > s ⇔ q > 0 (there is redundancy in that
the dimension of the group of isometries is larger than is necessary to move each
point in an orbit into each other point). There exist non-trivial isotropies.
3.5.2 Classification of cosmological symmetries
We consider non-empty perfect fluid models, i.e. (3.6) holds with (µ + p) > 0,
implying u a is the uniquely defined timelike eigenvector of the Ricci tensor.
Spacetime is four-dimensional, so the possibilities for the dimension of the
surface of transitivity are s = 0, 1, 2, 3, 4. Because u a is invariant, the isotropy
group at each point has to be a sub-group of the rotations O(3) acting orthogonally
to u a , but there is no two-dimensional subgroup of O(3). Thus the possibilities
for isotropy at a general point are:
(1) Isotropic: q = 3, the matter is a perfect fluid, the Weyl tensor vanishes, all
kinematical quantities vanish except '. All observations (at every point) are
isotropic. This is the RW family of geometries.
(2) Local rotational symmetry (‘LRS’): q = 1, the Weyl tensor is of algebraic
Petrov type D, kinematical quantities are rotationally symmetric about a
preferred spatial direction. All observations at every general point are
rotationally symmetric about this direction. All metrics are known in the
case of dust [25] and a perfect fluid [51, 118].
(3) Anisotropic: q = 0; there are no rotational symmetries. Observations in each
direction are different from observations in each other direction.
Putting this together with the possibilities for the dimensions of the surfaces of
transitivity, we have the following possibilities (see table 3.1).

Cosmological models

128

Table 3.1. Classification of cosmological models (with (µ + p) > 0) by isotropy and
homogeneity.
Dim invariant variety
Dimension,
Isotropy
group

s=2
Inhomogeneous

s=3
Spatially
homogeneous

s=4
Spacetime
homogeneous

q=0
anisotropic

Generic metric form known.
Spatially self-similar,
Abelian G 2 on 2D
spacelike surfaces,
non-Abelian G 2

Bianchi:
orthogonal,
tilted

Osvath/Kerr

q=1
LRS

Lemaı̂tre–Tolman–
Bondi family

Kantowski–Sachs,
LRS Bianchi

Gödel

q=3
isotropic

None
(cannot happen)

Friedmann

Einstein
static

Two non-ignorable
coordinates

One non-ignorable
coordinate

Algebraic EFE
(no redshift)

Dim invariant variety
Dimension
Isotropy
group

s=0
Inhomogeneous

s=1
Inhomogeneous/
no isotropy group

q=0

Szekeres–Szafron,
Stephani–Barnes,
Oleson type N

General metric
form independent
of one coord;
KV h.s.o./not h.s.o.

The real universe!

3.5.2.1 Spacetime homogeneous models
These models with s = 4 are unchanging in space and time, hence µ is a constant,
so by the energy conservation equation (3.29) they cannot expand: ' = 0.
They cannot produce an almost isotropic redshift, and are not useful as models
of the real universe. Nevertheless they are of some interest for their geometric
properties.
The isotropic case q = 3 (⇒ r = 7) is the Einstein static universe, the nonexpanding FL model that was the first relativistic cosmological model found. It is

Models and symmetries

129

not a viable cosmology because it has no redshifts, but it laid the foundation for
the discovery of the expanding FLRW models.
The LRS case q = 1 (⇒ r = 5) is the Gödel stationary rotating
universe [60], also with no redshifts. This model was important because of
the new understanding it brought as to the nature of time in general relativity
(see [68, 124]). It is a model in which causality is violated (there exist closed
timelike lines through each spacetime point) and there exists no cosmic time
function whatsoever.
The anisotropic models q = 0 (⇒ r = 4) are all known, but are interesting
only for the light they shed on Mach’s principle; see [101].
3.5.2.2 Spatially homogeneous universes
These models with s = 3 are the major models of theoretical cosmology, because
they express mathematically the idea of the ‘cosmological principle’: all points
of space at the same time are equivalent to each other [6].
The isotropic case q = 3 (⇒ r = 6) is the family of FL models, the standard
models of cosmology, with the comoving RW metric form:
ds 2 = −dt 2 + S 2 (t)(dr 2 + f 2 (r )(dθ 2 + sin2 θ dφ 2 )),

u a = δ0a .

(3.75)

Here the space sections are of constant curvature K = k/S 2 and
f (r ) = sin r, r, sinh r

(3.76)

if the normalized spatial curvature k is +1, 0, −1 respectively. The space sections
are necessarily closed if k = +1.
The LRS case q = 1 (⇒ r = 4) is the family of Kantowski–Sachs universes
[13,80] plus the LRS orthogonal [45] and tilted [77] Bianchi models. The simplest
are the Kantowski–Sachs family, with comoving metric form
ds 2 = −dt 2 + A2 (t) dr 2 + B 2 (t)(dθ 2 + f 2 (θ ) dφ 2 ),

u a = δ0a ,

(3.77)

where f (θ ) is given by (3.76).
The anisotropic case q = 0 (⇒ r = 3) is the family of Bianchi universes
with a group of isometries G 3 acting simply transitively on spacelike surfaces.
They can be orthogonal or tilted. The simplest class is the Bianchi type I family,
with an Abelian isometry group and metric form:
ds 2 = −dt 2 + A2 (t) dx 2 + B 2 (t) dy 2 + C 2 (t) dz 2 ,

u a = δ0a .

(3.78)

The family as a whole has quite complex properties; these models are discussed
in the following section.

130

Cosmological models

3.5.2.3 Spatially inhomogeneous universes
These models have s ≤ 2. The LRS cases (q = 1 ⇒ s = 2, r = 3) are the
spherically symmetric family with metric form:
ds 2 = −C 2 (t, r ) dt 2 + A2 (t, r ) dr 2 + B 2 (t, r )(dθ 2 + f 2 (θ ) dφ 2 ),

u a = δ0a ,
(3.79)
where f (θ ) is given by (3.76). In the dust case, we can set C(t, r ) = 1 and
can integrate the EFE analytically; for k = +1, these are the (‘LTB’) spherically
symmetric models [5,87]. They may have a centre of symmetry (a timelike worldline), and can even allow two such centres, but they cannot be isotropic about a
general point (because isotropy everywhere implies spatial homogeneity).
Solutions with no symmetries at all have r = 0 ⇒ s = 0, q =
0. The real universe, of course, belongs to this class; all the others are
intended as approximations to this unique universe. Remarkably, we know
some exact solutions without any symmetries, specifically (a) the Szekeres quasispherical models [121, 122], (b) Stephani’s conformally flat models [84, 116],
and (c) Oleson’s type-N solutions (for a discussion of these and all the other
inhomogeneous models, see Krasiński [85] and Kramer et al [83]). One further
interesting family without global symmetries are the ‘Swiss-cheese’ models,
made by cutting and pasting segments of spherically symmetric models [23,112].
Because of the nonlinearity of the equations, it is helpful to have exact
solutions at hand as models of structure formation as well as studies of linearly
perturbed FL models (briefly discussed later). The dust (Tolman–Bondi) and
perfect fluid spherically symmetric models are useful here, in particular in terms
of relating the time evolution to self-similar models. However, in the fully
nonlinear regime numerical solutions of the full equations are needed.

3.6 Friedmann–Lemaı̂tre models
The FL models are discussed in detail in other chapters, so here I will only briefly
mention some interesting properties of these solutions (and see also [33]). These
models are perfect fluid solutions with metric form (3.75), characterized by
u̇ = 0 = ω = σ = 0,
⇒ ∇˜ e µ = ∇˜ e p = ∇˜ e θ = 0,

Ṡ
S
= Hab = 0.

θ =3

(3.80)

E ab

(3.81)

They are isotropic about every point (q = 3) and consequently are spatially
homogeneous (s = 3). The equations that apply are the covariant equations
(3.80), (3.83) with restrictions (3.80). The dynamical equations are the energy
equation (3.29)
Ṡ
(3.82)
µ̇ = −3 (µ + p),
S

Friedmann–Lemaı̂tre models

131

the Raychaudhuri equation (3.34):
3

S̈
= − 12 (µ + 3 p) + λ
S

(3.83)

and the Friedmann equation (3.40), where 3 R = 6k/S 2 ,
3

3k
Ṡ 2
− κµ − λ = − 2 .
S2
S

(3.84)

The Friedmann equation is a first integral of the other two when Ṡ = 0. The
solutions, of course, depend on the equation of state; for the currently favoured
universe models, going backward in time there will be
(1)
(2)
(3)
(4)
(5)

a cosmological-constant-dominated phase,
a matter-dominated phase,
a radiation-dominated phase,
a scalar-field-dominated inflationary phase and
a pre-inflationary phase where the physics is speculative (see the last section
of this chapter). The normalized density parameter is  ≡ κµ/3H 2, where
as usual H = Ṡ/S.

3.6.1 Phase planes and evolutionary paths
From these equations, one can obtain phase planes
(i) for the density parameter  against the deceleration parameter q, see [115];
(ii) for the density parameter  against the Hubble parameter H , see [128] for
the case λ = 0; and
(iii) for the density parameter  against the scale parameter S, see [94], showing
how  changes in inflationary and non-inflationary universes.
It is a consequence of the equations that the spatial curvature parameter k
is a constant of the motion. In particular, flatness cannot change as the universe
evolves: either k = 0 or not, depending on the initial conditions, and this is
independent of any inflation that may take place. Thus while inflation can drive
the spatial curvature K = k/S 2 very close indeed to zero, it cannot set K = 0.
If one has a scalar field matter source φ with potential V (φ), one can obtain
essentially arbitrary functional forms for the scale function S(t) by using the
arbitrariness in the function V (φ) and running the field equations backwards,
see [46].
3.6.2 Spatial topology
The Einstein field equations determine the time evolution of the metric and its
spatial curvature, but they do not determine its spatial topology. Spatially closed

132

Cosmological models

FL models can occur even if k = 0 or k = −1, for example with a toroidal
topology [27]. These universes can be closed on a small enough spatial scale that
we could have seen all the matter in the universe already, and indeed could have
seen it many times over; see the discussion on ‘small universes’ later.
3.6.3 Growth of inhomogeneity
This is studied by looking at linear perturbations of the FL models, as well as
by examining inhomogeneous models. The geometry and dynamics of perturbed
FL models is described in detail in other talks, so I will again just make a few
remarks. In dealing with perturbed FL models, one runs into the gauge issue:
the background model is not uniquely defined by a realistic (lumpy) model and
the definition of the perturbations depends on the choice of background model
(the gauge chosen). Consequently it is advisable to use gauge-invariant variables,
either coordinate-based [2] or covariant [39]. When dealing with multiple matter
components, it is important to take carefully into account the separate velocities
needed for each matter component, and their associated conservation equations.
The CBR can best be described by kinetic theory, which again can be presented
in a covariant and gauge invariant way [10, 59].

3.7 Bianchi universes (s = 3)
These are the models in which there is a group of isometries G 3 simply transitive
on spacelike surfaces, so they are spatially homogeneous. There is only one
essential dynamical coordinate (the time t) and the EFE reduce to ordinary
differential equations, because the inhomogeneous degrees of freedom have been
‘frozen out’. They are thus quite special in geometrical terms; nevertheless, they
form a rich set of models where one can study the exact dynamics of the full
nonlinear field equations. The solutions to the EFE will depend on the matter in
the spacetime. In the case of a fluid (with uniquely defined flow lines), we have
two different kinds of models:
(1) Orthogonal models, with the fluid flow lines orthogonal to the surfaces
of homogeneity (Ellis and MacCallum [45], see also [128]). In this case the fluid
4-velocity u a is parallel to the normal vectors n a so the matter variables will be
just the fluid density and pressure. The fluid flow is necessarily irrotational and
geodesic.
(2) Tilted models, with the fluid flow lines not orthogonal to the surfaces of
homogeneity. Thus the fluid 4-velocity is not parallel to the normals, and the
components of the fluid peculiar velocity enter as further variables (King and
Ellis [15, 77]). They determine the fluid energy–momentum tensor components
relative to the normal vectors (a perfect fluid will appear as an imperfect fluid in
that frame). Rotating models must be tilted, and are much more complex than
non-rotating models.

Bianchi universes (s = 3)

133

3.7.1 Constructing Bianchi universes
The approach of Ellis and MacCallum [45]) uses an orthonormal tetrad based on
the normals to the surfaces of homogeneity (i.e. e0 = n, the unit normal vector to
these surfaces). The tetrad is chosen to be invariant under the group of isometries,
i.e. the tetrad vectors commute with the KVs. Then we have an orthonormal basis
ea , a = 0, 1, 2, 3, such that equation (3.52) becomes
[ea , eb] = γ c ab (t)ec

(3.85)

and all dynamic variables are function of time t only. The matter variables—
µ(t), p(t), and u α (t) in the case of tilted models—and the commutation functions
γ a bc (t), which by (3.59) are equivalent to the rotation coefficients, are chosen to
be these variables. The EFE (3.2) are first-order equations for these quantities,
supplemented by the Jacobi identities for the γ a bc (t), which are also first-order
equations. Thus the equations needed are just the tetrad equations mentioned in
section 3.3, for the case
u̇ α = ωα = 0 = eα (γ a bc ).

(3.86)

The spatial commutation functions γ α βγ (t) can be decomposed into a timedependent matrix n αβ (t) and vector a α (t), see (3.66), and are equivalent to the
structure constants C α βγ of the symmetry group at each point. In view of (3.86),
the Jacobi identities (3.60) for the spatial vectors now take the simple form
n αβ aβ = 0.

(3.87)

The tetrad basis can be chosen to diagonalize n αβ at all times, to attain n αβ =
diag(n 1 , n 2 , n 3 ), a α = (a, 0, 0), so that the Jacobi identities are then simply
n 1 a = 0. Consequently we define two major classes of structure constants (and
so Lie algebras):
Class A: a = 0; and
Class B: a = 0.
Following Schücking’s extension of Bianchi’s work, the classification of G 3
group types used is as in table 3.2. Given a specific group type at one instant,
this type will be preserved by the evolution equations for the quantities n α (t) and
a(t). This is a consequence of a generic property of the EFE: they will always
preserve symmetries in initial data (within the Cauchy development of that data);
see Hawking and Ellis [68].
In some cases, the Bianchi groups allow higher symmetry subcases, i.e. they
are compatible with isotropic (FL) or LRS models, see [45] for details. For us the
interesting point is that k = 0 FL models are compatible with groups of type I
and VII0 , k = −1 models with groups of types V and VIIh , and k = +1 models
with groups of type IX.

134

Cosmological models

Table 3.2. Canonical structure constants for different Bianchi types. The parameter
h = a 2 /n 2 n 3 .
Class

Type

n1

n2

n3

a

A

I

0

0

0

0

II
VI0
VII0
VIII
IX

+ve
0
0
−ve
+ve

0
+ve
+ve
+ve
+ve

0
−ve
+ve
+ve
+ve

0
0
0
0
0

V

0

0

0

+ve

IV
VIh
III
VIIh

0
0
0
0

0
+ve
+ve
+ve

+ve
−ve
−ve
+ve

+ve
+ve
n2n3
+ve

B

Abelian

h<0
same as VI1
h>0

The set of tetrad equations (section 3.3) with restrictions (3.86) will
determine the evolution of all the commutation functions and matter variables and,
hence, determine the metric and also the evolution of the Weyl tensor. One can
relate these equations to variational principles and a Hamiltonian, thus expressing
them in terms of a potential formalism that gives an intuitive feel for what the
evolution will be like [92, 93]. They are also the basis of dynamical systems
analyses.
3.7.2 Dynamical systems approach
The most illuminating description of the evolution of families of Bianchi models
is a dynamical systems approach based on the use of orthonormal tetrads,
presented in detail in Wainwright and Ellis [128]. The main variables used
are essentially the commutation functions mentioned earlier, but rescaled by a
common time-dependent factor.
3.7.2.1 Reduced differential equations
The basic idea [12, 126] is to write the EFE in a way that enables one to study the
evolution of the various physical and geometrical quantities relative to the overall
rate of expansion of the universe, as described by the rate of expansion scalar '
or, equivalently, the Hubble parameter H = 13 '. The remaining freedom in the
choice of orthonormal tetrad needs to be eliminated by specifying the variables
α implicitly or explicitly (for example by specifying the basis as eigenvectors of

Bianchi universes (s = 3)

135

the σαβ ). This also simplifies other quantities (for example the choice of a shear
eigenframe will result in the tensor σαβ being represented by two diagonal terms).
One hence obtains a reduced set of variables, consisting of H and the remaining
commutation functions, which we denote symbolically by x = (γ a bc |reduced).
The physical state of the model is thus described by the vector (H, x). The details
of this reduction differ for classes A and B in the latter case, there is an algebraic
constraint of the form g(x) = 0, where g is a homogeneous polynomial.
The idea is now to normalize x with the Hubble parameter H . Denoting the
resulting variables by a vector y ∈ R n , we write
y=

x
.
H

(3.88)

These new variables are dimensionless, and will be referred to as expansionnormalized variables. It is clear that each dimensionless state y determines a
one-parameter family of physical states (x, H ). The evolution equations for the
γ a bc lead to evolution equations for H and x and hence for y. In order that the
evolution equations define a flow, it is necessary, in conjunction with the rescaling
of the variables, to introduce a dimensionless time variable τ according to
S = S0 eτ ,

(3.89)

where S0 is the value of the scale factor at some arbitrary reference time. Since
S assumes values 0 < S < +∞ in an ever-expanding model, τ assumes all real
values, with τ → −∞ at the initial singularity and τ → +∞ at late times. It
follows that
1
dt
=
(3.90)
dτ
H
and the evolution equation for H can be written
dH
= −(1 + q)H,
dτ

(3.91)

where the deceleration parameter q is defined by q = − S̈S/ Ṡ 2 , and is related to
Ḣ by Ḣ = −(1 + q)H 2. Since the right-hand side of the evolution equations for
the γ a bc are homogeneous of degree 2 in the γ a bc , the change (3.90) of the time
variable results in H cancelling out of the evolution equation for y, yielding an
autonomous differential equation (DE):
dy
= f ( y),
dτ

y ∈ Rn .

(3.92)

The constraint g(x) = 0 translates into a constraint
g( y) = 0,

(3.93)

which is preserved by the DE. The functions f : R n → R n and g : R n → R are
polynomial functions in y. An essential feature of this process is that the evolution

136

Cosmological models

equation for H , namely (3.91), decouples from the remaining equations (3.92)
and (3.93). Thus the DE (3.92) describes the evolution of the non-tilted Bianchi
cosmologies, the transformation of variables essentially scaling away the effects
of the overall expansion. An important consequence is that the new variables are
bounded near the initial singularity.
3.7.2.2 Equations and orbits
Since τ assumes all real values (for models which expand indefinitely), the
solutions of (3.92) are defined for all τ and hence define a flow {φτ } on R n . The
evolution of the cosmological models can thus be analysed by studying the orbits
of this flow in the physical region of state space, which is a subset of R n defined
by the requirement that the matter energy density µ be non-negative, i.e.
κµ
( y) =
≥ 0,
(3.94)
3H 2
where the density parameter  is a dimensionless measure of µ.
The vacuum boundary, defined by ( y) = 0, describes the evolution of
vacuum Bianchi models, and is an invariant set which plays an important role
in the qualitative analysis because vacuum models can be asymptotic states for
perfect fluid models near the big bang or at late times. There are other invariant
sets which are also specified by simple restrictions on y which play a special
role: the subsets representing each Bianchi type (table 3.2), and the subsets
representing higher-symmetry models, specifically the FLRW models and the
LRS Bianchi models (table 3.1).
It is desirable that the dimensionless state space D in R n is a compact
set. In this case each orbit will have non-empty future and past limit sets, and
hence there will exist a past attractor and a future attractor in state space. When
using expansion-normalized variables, compactness of the state space has a direct
physical meaning for ever-expanding models: if the state space is compact, then
at the big bang no physical or geometrical quantity diverges more rapidly than
the appropriate power of H , and at late times no such quantity tends to zero less
rapidly than the appropriate power of H . This will happen for many models;
however, the state space for Bianchi type VII0 and type VIII models is noncompact. This lack of compactness manifests itself in the behaviour of the Weyl
tensor at late times.
3.7.2.3 Equilibrium points and self-similar cosmologies
Each ordinary orbit in the dimensionless state space corresponds to a oneparameter family of physical universes, which are conformally related by a
constant rescaling of the metric. However, for an equilibrium point y∗ of the
DE (3.92), which satisfies f ( y∗ ) = 0, the deceleration parameter q is a constant,
i.e. q( y∗ ) = q ∗ , and we find
H (τ ) = H0e(1+q

∗ )τ

.

Bianchi universes (s = 3)

137

In this case the parameter H0 is no longer essential, since it can be set to unity by
a translation of τ , τ → τ + constant; then (3.90) implies that
Ht =

1
,
1 + q∗

(3.95)

so that the commutation functions are of the form (constant) × t −1 . It follows
that the resulting cosmological model is self-similar. It thus turns out that to
each equilibrium point of the DE (3.92) there corresponds a unique self-similar
cosmological model. In such a model the physical states at different times differ
only by an overall change in the length scale. Such models are expanding, but
in such a way that their dimensionless state does not change. They include the
flat FLRW model ( = 1) and the Milne model ( = 0). All vacuum and
non-tilted perfect fluid self-similar Bianchi solutions have been given by Hsu and
Wainwright [73]. The equilibrium points determine the asymptotic behaviour of
other more general models.
3.7.2.4 Phase planes
Many phase planes can be constructed explicitly. The reader is referred to
Wainright and Ellis [128] for a comprehensive presentation and survey of results.
Several interesting points emerge
(1) Variety of singularities. Various types of singularity can occur in Bianchi
universes: cigar, pancake and oscillatory in the orthogonal case. In the case
of tilted models, one can, in addition get non-scalar singularities, associated
with a change in the nature of the spacetime symmetries—a horizon occurs
where the surfaces of homogeneity change from being timelike to being
spacelike, so the model changes from being spatially homogeneous to
spatially inhomogeneous [15, 42]. The fluid can then run into timelike
singularities, quite unlike the spacelike singularities in FL models. Thus the
singularity structure can be quite unlike that in a FL model, even in models
that are arbitrarily similar to a FL model today and indeed since the time of
decoupling.
(2) Relation to lower dimensional spaces. It seems that the lower dimensional
spaces, delineating higher symmetry models, may be skeletons guiding the
development of the higher dimensional spaces (the more generic models).
This is one reason why study of the exact higher symmetry models is of
significance.
(3) Identification of models in state space. The analysis of the phase planes for
Bianchi models shows that the procedure sometimes adopted of identifying
all points in state space corresponding to the same model, is not a good idea.
For example the Kasner ring that serves as a framework for evolution of
many other Bianchi models contains multiple realizations of the same Kasner
model. To identify them as the same point in state space would make the

138

Cosmological models
evolution patterns very difficult to follow. It is better to keep them separate,
but to learn to identify where multiple realizations of the same model occur
(which is just the equivalence problem for cosmological models).

3.7.3 Isotropization properties
An issue of importance is whether these models tend to isotropy at early or
late times. An important paper by Collins and Hawking [16] shows that for
ordinary matter, at late times, types I, V, VII, isotropize but other Bianchi models
become anisotropic at very late times, even if they are very nearly isotropic at
present. Thus isotropy is unstable in this case. However, a paper by Wald [130]
showed that Bianchi models will tend to isotropize at late times if there is a
positive cosmological constant present, implying that an inflationary era can
cause anisotropies to die away. The latter work, however, while applicable to
models with non-zero tilt angle, did not show this angle dies away, and indeed it
does not do so in general (Goliath and Ellis [62]). Inflation also only occurs in
Bianchi models if there is not too much anisotropy to begin with (Rothman and
Ellis [111]), and it is not clear that shear and spatial curvature are in fact removed
in all cases [109]. Hence, some Bianchi models isotropize due to inflation, but
not all.
An important idea that arises out of this study is that of intermediate
isotropization: namely, models that become very like a FLRW model for a period
of their evolution but start and end quite unlike these models. It turns out that
many Bianchi types allow intermediate isotropization, because the FLRW models
are saddle points in the relevant phase planes. This leads to the following two
interesting results:
Bianchi evolution theorem 1. Consider a family of Bianchi models that allow
intermediate isotropization. Define an -neighbourhood of a FLRW model as a
region in state space where all geometrical and physical quantities are closer
than  to their values in a FLRW model. Choose a time scale L. Then no matter
how small  and how large L, there is an open set of Bianchi models in the state
space such that each model spends longer than L within the corresponding neighbourhood of the FLRW model.
This follows because the saddle point is a fixed point of the phase flow;
consequently the phase flow vector becomes arbitrarily close to zero at all
points in a small enough open region around the FLRW point in state space.
Consequently, although these models are quite unlike FLRW models at very
early and very late times, there is an open set of them that are observationally
indistinguishable from a FLRW model (choose L long enough to encompass
from today to last coupling or nucleosynthesis, and  to correspond to current
observational bounds). Thus there exist many such models that are viable
as models of the real universe in terms of compatibility with astronomical
observations.

Observations and horizons

139

Bianchi evolution theorem 2. In each set of Bianchi models of a type admitting
intermediate isotropization, there will be spatially homogeneous models that are
linearizations of these Bianchi models about FLRW models. These perturbation
modes will occur in any almost-FLRW model that is generic rather than finetuned; however, the exact models approximated by these linearizations will be
quite unlike FLRW models at very early and very late times.
Proof is by linearizing the previous equations (see the following section)
to obtain the Bianchi equations linearized about the FLRW models that occur
at the saddle point leading to the intermediate isotropisation. These modes will
be the solutions in a small neighbourhood about the saddle point permitted by
the linearized equations (given existence of solutions to the nonlinear equations,
linearization will not prevent corresponding linearized solutions existing).
The point is that these modes can exist as linearizations of the FLRW model;
if they do not occur, then the initial data have been chosen to set these modes
precisely to zero (rather than being made very small), which requires very special
initial conditions. Thus these modes will occur in almost all almost-FLRW
universes. Hence, if one believes in generality arguments, they will occur in the
real universe. When they occur, they will, at early and late times grow until the
model is very far from a FLRW geometry (while being arbitrarily close to an
FLRW model for a very long time, as per the previous theorem).

3.8 Observations and horizons
The basic observational problem is that, because of the enormous scale of the
universe, we can effectively only see it from one spacetime point, ‘here and
now’ [26, 29]. Consequently what we are able to see is a projection onto a 2sphere (‘the sky’) of all the objects in the universe, and our fundamental problem
is determining the distances of the various objects we see in the images we
obtain. In the standard universe models, redshift is a reliable zero-order distance
indicator, but is unreliable at first order because of local velocity perturbations.
Thus we need the array of other distance indicators (Tully–Fisher for example).
Furthermore, to test cosmological models we need at least two reliable measurable
properties of the objects we see, that we can plot against each other (magnitude
and redshift, for example), and most of them are unreliable both because of
intrinsic variation in source properties, and because of evolutionary effects
associated with the inevitable lookback-time involved when we observe distant
objects.
3.8.1 Observational variables and relations: FL models
The basic variables underlying direct observations of objects in the spatially
homegenous and isotropic FL models are:

140

Cosmological models

(1) the redshift, basically a time-dilation effect for all measurements of the
source (determined by the radial component of velocity);
(2) the area distance, equivalent to the angular diameter distance in RW
geometries, and also equivalent (up to a redshift factor) to the luminosity
distance in all relativistic cosmological models, because of the reciprocity
theorem [26]—this can best be calculated from the geodesic deviation
equation [54]; and
(3) number counts, determined by (i) the number of objects in a given
volume, (ii) the relation of that volume to increments in distance measures
(determined by the spacetime geometry) and (iii) the selection and detection
effects that determine which sources we can actually identify and measure
(difficulties in detection being acute in the case of dark matter).
Thus to determine the spacetime geometry in these models, we need to
correlate at least two of these variables against each other. Further observational
data which must be consistent with the other observations comes from the
following sources:
(4) background radiation spectra at all wavelengths particularly the 3K
blackbody relic radiation (‘CBR’); and
(5) the ‘relics’ of processes taking place in the hot big-bang era, for
example the primeval element abundances resulting from baryosynthesis and
nucleosynthesis in the hot early universe (and the CBR anisotropies and
present large-scale structures can also be regarded in this light, for they are
evidence about density fluctuations, which are one form of such relic).
The observational relations in FL models are covered in depth in other
reports to this meeting (and see also [26, 33]), so I will just comment on two
aspects here.
Selection/detection effects: The way we detect objects from available images
depends on both their surface brightness and their apparent size. Thus
we essentially need two variables to adequately characterize selection and
detection effects; it simply is not adequate to discuss such effects on the
basis of apparent magnitude or flux alone [47]. Hence one should regard with
caution any catalogues that claim to be magnitude limited, for that cannot be
an adequate criteria for detection limits; such catalogues may well be missing
out many low surface-brightness objects.
Minimum angles and trapping surfaces: For ordinary matter, there is a redshift
z ( such that apparent sizes of objects of fixed linear size reach a minimum at
z = z ( , and for larger redshift look larger again. What is happening here is
that the universe as a whole is acting as a giant gravitational lens, refocusing
our past light cone as a whole [26]; in an Einstein–de-Sitter universe, this
happens at z ( = 5/4; in a low density universe, it happens at about z = 4.
This refocusing means that closed trapped surfaces occur in the universe, and

Observations and horizons

141

hence via the Hawking–Penrose singularity theorems, leads to the prediction
of the existence of a spacetime singularity in our past [68].
3.8.2 Particle horizons and visual horizons
For ordinary equations of state, because causal influences can travel at most at
the speed of light, there is both a particle horizon [110, 124], limiting causal
communication since the origin of the universe and a visual horizon [50], limiting
visual communication since the decoupling of matter and radiation. The former
depends on the equation of state of matter at early times, and can be changed
drastically by an early period of inflation; however the latter depends only on
the equation of state since decoupling, and is unaffected by whether inflation
took place or not. From (3.75), at an arbitrary time of observation t0 , the radial
comoving coordinate values corresponding to the particle and event horizons,
respectively, of an observer at the origin of coordinates are:
 t0
 t0
dt
dt
,
u vh (t0 ) =
,
(3.96)
u ph (t0 ) =
S(t)
S(t)
0
td
where we have assumed the initial singularity occurred at t = 0 and decoupling
at t = td . We cannot have had causal contact with objects lying at a coordinate
value r greater than u ph (t0 ), and cannot have received any type of electromagnetic
radiation from objects lying at a coordinate value r greater than u vh (t0 ).
It is fundamental to note, then, that no object can leave either of these
horizons once it has entered it: once two objects are in causal or visual
contact, that contact cannot be broken, regardless of whether inflation or an
accelerated expansion takes place or not. This follows immediately from (3.96):
t1 > t0 ⇒ u ph (t1 ) > u ph (t0 ) (the integrand between t0 and t1 is positive, so
du ph (t)/dt = 1/S(t) > 0.) Furthermore the physical scales associated with these
horizons cannot decrease while the universe is expanding. These are
Dph (t) = S(t)u ph (t),

Dvh (t) = S(t)u vh (t)

respectively, at time t; hence for example d(Dph (t))/dt = 1 + H (t)Dph (t) > 0.
Much of the literature on inflation is misleading in this regard.
3.8.3 Small universes
The one case where visual horizons do not occur is when the universe has compact
spatial sections whose physical size is less than the Hubble radius; consider, for
example, the case of a k = 0 model universe of toroidal topology, with a length
scale of identification of, say, 300 Mpc. In that case we can see right round
the universe, with many images of each galaxy, and indeed many images of our
own galaxy [48]. There are some philosophical advantages in such models [32],
but they may or may not correspond to physical reality. If this is indeed the

142

Cosmological models

case, it would show up in multiple images of the same objects [48, 81], identical
circles in the CBR anisotropy pattern across the sky [18], and altered CBR power
spectra predictions [17]. A complete cosmological observational programme
should test for the possibility of such small alternative universe topologies, as
well as determining the fundamental cosmological parameters.
3.8.4 Observations in anisotropic and inhomogeneous models
In anisotropic models, new kinds of observations become possible. First, each of
these relations will be anisotropic and so will vary with direction in the sky. In
particular,
(6) background radiation anisotropies will occur and provide important
information on the global spacetime geometry [100] as well as on local
inhomogeneities [10, 59, 82] and gravitational waves [9];
(7) image distortion effects (strong and weak lensing) are caused by the Weyl
tensor, which in turn is generated by local matter inhomogeneities through
the ‘div E’ equation (3.48).
Finally, to fully determine the spacetime geometry [44, 86] we should also
measure
(8) transverse velocities, corresponding to proper motions in the sky. However,
these are so small as to be undetectable and so measurements only give weak
upper limits in this case.
To evaluate the limits put on inhomogenity and anisotropy by observations,
one must calculate observational relations predicted in anisotropic and
inhomogenous models.
3.8.4.1 Bianchi observations
One can examine observational relations in the spatially homogeneous class of
models, for example determining predicted Hubble expansion anisotropy, CBR
anisotropy patterns, and nucleosynthesis results in Bianchi universes. These
enable one to put strong limits on the anisotropy of these universe models
since decoupling, and limits on the deviation from FL expansion rates during
nucleosynthesis. However although these analyses put strong limits on the shear
and vorticity in such models today, nevertheless they could have been very
anisotropic at very early times—in particular, before nucleosynthesis—without
violating the observational limits, and they could become anisotropic again at
very late times. Also these limits are derived for specific spatially homogeneous
models of particular Bianchi type, and there are others where they do not apply.
For example, there exist Bianchi models in which rapid oscillations take place in
the shear at late times, and these oscillations prevent a build up of CBR anisotropy,
even though the universe is quite anisotropic at many times.

Observations and horizons

143

3.8.4.2 Inhomogeneity and observations
Similarly, one can examine observational relations in specific inhomogeneous
models, for example the Tolman–Bondi spherically symmetric models and
hierarchical Swiss-cheese models. We can then use these models to investigate
the spatial homegenity of the universe (cf the next subsection).
The observational relations in linearly perturbed FL models, particularly (a)
gravitational lensing properties and (b) CBR anisotropies have been the subject
of intense theoretical study as well as observational exploration. A crucial issue
that arises is on what scale we are representing the universe, for both its dynamic
and observational properties may be quite different on small and large scales, and
then the issue arises of how averaging over the small-scale behaviour can lead to
the correct observational behaviour on large scales [32]. It seems that this will
work out correctly, but really clear and compelling arguments that this is so are
still lacking.
3.8.4.3 Perturbed FL models and FL parameters
As explained in detail in other chapters, the CBR anisotropies in perturbed FL
models, in conjunction with studies of large-scale structure and models of the
growth of inhomogeneities in such models, also using large-scale structure and
supernovae observations, enables us to tie down the parameters of viable FL
background models to a striking degree [8, 75].
3.8.5 Proof of almost-FL geometry
On a cosmological scale, observations appear almost isotropic about us (in
particular number counts of many kinds of objects on the one hand, and the CBR
temperature on the other). From this we may deduce that the observable region
of the universe is, to a good approximation, also isotropic about us. A particular
substantial issue, then, is how we can additionally prove the universe is spatially
homogeneous, and so has an RW geometry, as is assumed in the standards models
of cosmology.
3.8.5.1 Direct proof
Direct proof of spatial homogeneity would follow if we could show that the
universe has precisely the relation between both area distance r0 (z) and number
counts N(z) with redshift z that is predicted by the FL family of models. However,
proving this observationally is not easily possible. Current supernova-based
observations are indicate a non-zero cosmological constant rather than the relation
predicted by the FL models with zero λ, and we are not able to test the r0 (z)
relationship accurately enough to show it takes a FL form with non-zero λ [95].
Furthermore number counts are only compatible with the FL models if we assume
just the right source evolution takes place to make the observations compatible

144

Cosmological models

with spatial homogeneity; but once we take evolution into account, we can fit
almost any observational relations by almost any spherically symmetric model
(see [98] for exact theorems making this statement precise). Recent statistical
observations of distant sources support spatial homogeneity on intermediate
scales (between 30 and 400 Mpc [102]), but do not extend to larger scales because
of sample limits.
3.8.5.2 Uniform thermal histories
A strong indication of spatial homogeneity is the fact that we see the same
kinds of object, more or less, at high z as nearby. This suggests that they
must have experienced more or less the same thermal history as nearby objects,
as otherwise their structure would have come out different; and this, in turn,
suggests that the spacetime geometry must have been rather similar near those
objects as near to us, else (through the field equations) the thermal history
would have come out different. This idea can be formulated in the Postulate
of Uniform Thermal Histories (PUTH), stating that uniform thermal histories can
occur only if the geometry is spatially homogeneous. Unfortunately, counterexamples to this conjecture have been found [7]. These are, however, probably
exceptional cases and this remains a strong observationally-based argument for
spatial homogeneity, indeed probably the most compelling at an intuitive level.
However, relating the idea to observations also involves untangling the effects of
time evolution, and it cannot be considered a formal proof of homogeneity.
3.8.5.3 Almost-EGS theorem
The most compelling precisely formulated argument is a based on our
observations of the high degree of CBR anisotropy around us. If we assume
we are not special observers, others will see the same high degree of anisotropy;
and then that shows spatial homogeneity: exactly, in the case of exact isotropy
(the Ehlers–Geren–Sachs (EGS) theorem [22]) and approximately in the case of
almost-isotropy:
Almost-EGS-theorem. [119]. If the Einstein–Liouville equations are satisfied in
an expanding universe, where there is pressure-free matter with 4-velocity vector
field u a (u a u a = −1) such that (freely-propagating) background radiation is
everywhere almost-isotropic relative to u a in some domain U , then spacetime is
almost-FLRW in U .
This description is intended to represent the situation since decoupling to
the present day. The pressure-free matter represents the galaxies on which
fundamental observers live, who measure the radiation to be almost isotropic.
This deduction is very plausible, particularly because of the argument just
mentioned in the last subsection: conditions there look more or less the same,

Observations and horizons

145

so there is no reason to think they are very different. Nevertheless, in the end this
argument rests on an unproved philosophical assumption (that other observers
see more or less what we do), and so is highly suggestive rather than a full
observational proof. In addition, there is a technical issue of substance, namely
what derivatives of the CBR temperature should be included in this formulation
(remembering here that there are Bianchi models where the matter shear remains
small but its time derivative can be large; these can have a large Weyl tensor but
small CBR anisotropy [99]).
3.8.5.4 Theoretical arguments
Given the observational difficulties, one can propose theoretical rather than
observational arguments for spatial homogeneity. Traditionally this was done by
appeal to a cosmological principle [6,131]; however, this is no longer fashionable.
Still some kinds of theoretical argument remain in vogue.
One can try to argue for spatial homogeneity on the basis of probability: this
is more likely than the case of a spherically symmetric inhomogeneous universe,
where we are near the centre (see [43] for detailed development of such a model).
However, that argument is flawed [30], because spatially homogeneous universe
models are intrinsically less likely than spherically symmetric inhomogeneous
ones (as the latter have more degrees of freedom, and so are more general). In
additionally, it is unclear that any probability arguments at all can be applied to
the universe, because of its uniqueness [37].
Alternatively, one can argue that inflation guarantees that the universe must
be spatially homogeneous. If we accept that argument, then the implication is that
we are giving preference to a theoretically based analysis over what can, in fact, be
established from observational data. In addition, it provides a partial rather than
complete solution to the issues it addresses (see the discussion in the next section).
Nevertheless it is an important argument that many find fully convincing.
Perhaps the most important argument in the end is that from cumulative
evidence: none of these approaches by themselves proves spatial homogeneity,
but taken together they give a sound cumulative argument that this is indeed the
case—within the domain previously specified above.
3.8.5.5 Domains of plausibility
Accepting that argument, to what spacetime regions does it apply? We may take
it as applying to the observable region of the universe V , that is, the region both
inside our visual horizon, and lying between the epoch of decoupling and the
present day. It will then also hold in some larger neigbourhood of this region, but
there is no reason to believe it will hold elsewhere; specifically, it need not hold
(i) very far out from us (say, 1000 Hubble radii away), hence chaotic inflation is
a possibility; nor (ii) at very early times (say, before nucleosynthesis), so Bianchi
anisotropic modes are possible at these early times; nor (iii) at very late times (say

146

Cosmological models

in another 50 Hubble times), so late-time anisotropic modes which are presently
negligble could come to dominate (cf the discussion in the section on evolution
of Bianchi models above). Thus we can observationally support the supposition
of spatial homegeneity and isotropy within the domain V , but not too far outside
of it.
3.8.6 Importance of consistency checks
Because we have no knock-out observational proof of spatial homogeneity, it is
important to consider all the possible observationally based consistency checks
on the standard model geometry. The most important are as follows:
(1) Ages. This has been one of the oldest worries for expanding universe models:
the requirement that the age of the universe must be greater than the ages of
all objects in it. However with present estimates of the ages of stars on
the one hand, and of the value of the Hubble constant on the other, this
no longer seems problematic, particularly if current evidence for a positive
cosmological constant turn out to be correct.
(2) Anisotropic number counts. If our interpretation of the CBR dipole as
due to our motion relative to the FL model is correct, then this must also
be accompanied by a dipole in all cosmological number counts at the 2%
level [38]. Observationally verifying that this is so is a difficult task, but it is
a crucial check on the validity of the standard model of cosmology.
(3) High-z observations. The best check on spatial homogeneity is to try to
check the physical state of the universe at high redshifts and hence at great
distances from us, and to compare the observations with theory. This can
be done in particular (a) for the CBR, whose temperature can be measured
via excited states of particular molecules; this can then be compared with the
predicted temperature T = T0 (1+z), where T0 is the present day temperature
of 2.75 K. It can also be done (b) for element abundances in distant objects,
specifically helium abundances. This is particularly useful as it tests the
thermal history of the universe at very early times of regions that are far out
from us [34].

3.9 Explaining homogeneity and structure
This is the unique core business of physical cosmology: explaining both why
the universe has the very improbable high-symmetry FL geometry on the largest
scales, and how structures come into existence on all smaller scales. Clearly
only cosmology itself can ask the first question; and it uniquely sets the initial
conditions underlying the astrophysical and physical processes that are the key to
the second, underlying all studies of origins.There is a creative tension between
two aims: smoothing processes, on the one hand, and structure growth, on the
other. Present day cosmology handles this tension by suggesting a change of

Explaining homogeneity and structure

147

equation of state: at early enough times, the equation of state was such as to
cause smoothing on all scales; but at later times, it was such as to cause structure
growth on particular scales. The inflationary scenario, and the models that build
on it, are remarkably successful in this regard, particularly through predicting the
CBR anisotropy patterns (the ‘Doppler peaks’) which seem to have been found
now (but significant problems remain, particularly as regards compatibility with
the well-established nucleosynthesis arguments).
Given these astrophysical and physical processes, explanation of the largescale isotropy and homogeneity of the universe together with the creation
of smaller-scale structures means determining the dynamical evolutionary
trajectories relating initial to final conditions, and then essentially either (a)
explaining initial conditions or (b) showing they are irrelevant.
3.9.1 Showing initial conditions are irrelevant
This can be attempted in a number of different ways.
3.9.1.1 Initial conditions are irrelevant because they are forgotten
Demonstrating minimal dependence of the large-scale final state on the initial
conditions has been the aim of
•
•

the chaotic cosmology programme of Misner, where physical processes such
a viscosity wipe out memories of previous conditions [97]; and
the inflationary family of theories, where the rapid exponential expansion
driven by a scalar field smooths out the universe and so results in similar
memory loss [79].

The (effective) scalar field is slow-rolling, so the energy condition (3.36) is
violated and a period of accelerating expansion can take place through many efoldings, until the scalar field decays into radiation at the end of inflation. This
drives the universe model towards flatness, and is commonly believed to predict
that the universe must be very close indeed to flatness today, even though this is
an unstable situation, see the phase planes of  against S [94]. It can also damp
out both anisotropy, as previously explained and inhomogeneity, if the initial
situation is close enough to a FL model of that inflation can in fact start. In
a chaotic inflationary scenario, with random initial conditions occurring at some
initial time, inflation will not succeed in starting in most places, but those domains
where it does start will expand so much that they will soon be the dominant feature
of the universe: there will be many vast FL-like domains, each with different
parameter values and perhaps even different physics, separated from each other
by highly inhomogeneous transition regions (where physics may be very strange).
In the almost-FL domains, quantum fluctuations are expanded to a very large scale
in the inflationary era, and form the seeds for structure formation at later times.
Inflation then goes on to provide a causal theory of initial structure formation

148

Cosmological models

from an essentially homogeneous early state (via amplification of initial quantum
fluctuations)—a major success if the all the details can be sorted out.
This is an attractive scenario, particularly because it ties in the large-scale
structure of the universe with high-energy physics. It works fine for those regions
that start off close enough to FL models, and, as noted earlier this suffices to
explain the existence of large FL-like domains, such as the one we inhabit. It does
not necessarily rule out the early and late anisotropic modes that were discussed
in the section on Bianchi models. It fits the observations provided one has
enough auxiliary functions and parameters available to mediate between the basic
theory and the observations (specifically, evolution functions, a bias parameter
or function, a dark matter component, a cosmological constant or ‘quintessence’
component at late times). However, it is not at present a specific physical model,
rather it is a family of models (see e.g. [78]), with many competing explanations
for the origin of the inflaton, which is not yet identified with any specific matter
component or field. It will become a well-defined physical theory when one or
other of these competing possibilities is identified as the real physical driver of an
inflationary early epoch.
There are three other issues to note here. First, the issue of probability:
inflation is intended as a means of showing the observed region of the universe is
in fact probable. But we have no proper measure of probability on the family
of universe models, so this has not been demonstrated in a convincing way.
Second, the Trans-Planckian problem [96]: inflation is generally very successful
in generating a vast expansion of the universe. The consequence is that the
spacetime region that has been expanded to macroscopic scales today is deep
in the Planck (quantum-gravity) era, so the nature of what is predicted depends
crucially on our assumptions about that era; but we do not know what conditions
were like there, and indeed even lack proper tools to describe that epoch, which
may have been of the nature of a spacetime foam, for example. Thus the results of
inflation for large-scale structure depend on specific assumptions about the nature
of spacetime in the strong quantum gravity regime, and we do not know what
that nature is. Penrose suggests it was very inhomogeneous at that time, in which
case inflation will amplify that inhomogeneous nature rather than creating spatial
homgeneity. As in the previous case, whether or not the process succeeds will
depend on the initial conditions for the expansion of the universe as it emerges
from the Planck (quantum gravity) era. Thirdly, there are still unsolved problems
regarding the end of inflation. These relate to the fact that if one has a very slow
rolling field as is often claimed, then the inertial mass density is very close to zero
so velocities are unstable.
It must be emphasized that in order to investigate this issue of isotropisation
properly, one must examine the dynamical behaviour of very anisotropic and
inhomogeneous cosmologies. This is seldom done—for example, almost all of
the literature on inflation examines only its effects in RW geometries, which
is precisely when there is no need for inflation take place in order to explain
the smooth geometry—for then a smooth geometry has been assumed a priori.

Explaining homogeneity and structure

149

When the full range of inhomogeneities and anisotropies is taken into account
(e.g. [128]), it appears that both approaches are partially successful: with or
without inflation one can explain a considerable degree of isotropization and
homogenization of the physical universe (see e.g. [127]), but this will not work in
all circumstances [105,106]. It can only be guaranteed to work if initial conditions
are somewhat restricted—so in order for the programme to succeed, we have to go
back to the former issue of somehow explaining why it is probable for a restricted
set of initial data to occur.
3.9.1.2 Initial conditions are irrelevant because they never happened
Some attempts involve avoiding a true beginning by going back to some form
of eternal or cyclic state, so that the universe existed forever. Initial conditions
are pushed back into the infinite past, and thus were never set. Examples are as
follows.
•
•
•

•
•

The original steady state universe proposal of Bondi [6], and its updated
form as the quasi-steady state universe of Hoyle, Burbidge and Narlikar
[71, 72].
Linde’s eternal chaotic inflation, where ever-forming new bubbles of
expansion arising within old ones exist forever; this can prevent the universe
from ever entering the quantum gravity regime [90].
The Hartle–Hawking ‘no-boundary’ proposal (cf [67]) avoids the initial
singularity through a change of spacetime signature at very early times,
thereby entering a positive-definite (‘space–space’) regime where the
singularity theorems do not apply (the physical singularity of the big bang
gets replaced by the coordinate singularity at the south pole of a sphere).
There is no singularity and no boundary, and so there are no boundary
conditions. This gets round the issue of a creation event in an ingenious
way: there is no unique start to the universe, but there is a beginning of time.
The Hawking–Turok initial instanton proposal is a variant of this scenario,
where there is a weak singularity to start with, and one is then able to enter a
low-density inflationary phase.
Gott and Liu’s causality violation in the early universe does the same kind of
thing in a different way: causality violation takes place in the early universe,
enabling the universe to ‘create itself’ [63]. Like the chaotic inflation picture,
new expanding universe bubbles are forming all the time; but one of them is
the universe region where the bubble was formed, this being possible because
closed timelike lines are allowed, so ‘the universe is its own mother’. This
region of closed timelike lines is separated from the later causally regular
regions by a Cauchy horizon.

There are thus a variety of ingenious and intriguing options which, in a sense,
allow avoidance of setting initial conditions. But this is really a technicality: the
issue still arises as to why in each case one particular initial state ‘existed’ or

150

Cosmological models

came into being rather than any of the other options. Some particular solutions of
the equations have been implemented rather than the other possibilities; boundary
conditions choosing one set of solutions over others have still been set, even if
they are not technically initial conditions set at a finite time in the past.
3.9.1.3 Initial conditions are irrelevant because they all happened
The idea of an ensemble of universes, mentioned earlier, is one approach that
sidesteps the problem of choice of specific initial data, because by hypothesis
all that can occur has then occurred. Anthropic arguments select the particular
universe in which we live from all those in this vast family (see e.g. [57,70]). This
is again an intriguing and ingenious idea, extending to a vast scale the Feynman
approach to quantum theory. However, there are several problems.
First, it is not clear that the selection of universes from this vast family by
anthropic arguments will necessarily result in as large and as isotropic a universe
as we see today; here one runs up against the unsolved problem of justifying a
choice of probabilities in this family of universes. Second, this proposal suffers
from complete lack of verifiability. In my view, this means this is a metaphysical
rather than scientific proposal, because it is completely untestable. And in the
end, this suggestion does not solve the basic issue in any case, because then one
can ask: Why does this particular ensemble exist, rather than a different ensemble
with different properties?; and the whole series of fundamental questions arises
all over again, in an even more unverifiable form than before.
3.9.2 The explanation of initial conditions
The explanation of initial conditions has been the aim of the family of theories
one can label collectively as quantum cosmology and the more recent studies of
string cosmology.
3.9.2.1 Explanation of initial conditions from a previous state of a different
nature
One option has been explaining the universe as we see it as arising from some
completely different initial state, for example:
•
•

proposals for creation of the universe as a bubble formed in a flat spacetime
or de Sitter spacetime, for example Tryon’s vacuum fluctuations and Gott’s
open bubble universes; or
Vilenkin’s tunnelling universe which arises from a state with no classical
analogue (described as ‘creation of the universe from nothing’, but this is
inaccurate).

These proposals (like the proposals by Hartle and Hawking, Hawking and
Turok, and Gott and Liu previously mentioned; for a comparative discussion and

Explaining homogeneity and structure

151

references, see Gott and Liu [63]) are based on the quantum cosmology idea of
the wavefunction of the universe, taken to obey the Wheeler–de Witt equation
(a generalization to the cosmological context of the Schrödinger equation) (see
e.g. [67]). This approach faces considerable technical problems, related to
•
•
•
•
•
•

the meaning of time, because vanishing of the Hamiltonian of general
relativity means that the wavefunction appears to be explicitly independent
of time;
divergences in the path-integrals often used to formulate the solutions to the
Wheeler–de-Witt equation;
the meaning of the wavefunction of the universe, in a context where
probabilities are ill defined [56];
the fundamentally important issue of the meaning of measurement in
quantum theory (when does ‘collapse of the wavefunction’ take place, in
a context where a classical ‘observer’ does not exist);
the conditions which will lead to these quantum equations having classicallike behaviour at some stage in the history of the universe [65]; and
the way in which this reduced set of equations, taken to be valid irrespective
of the nature of the full quantum theory of gravity, relates to that as yet
unknown theory.

The alternative is to work with the best current proposal for such a theory,
taken by many to be M-theory, which aims to unite the previously disparate
superstring theories into a single theory, with the previously separate theories
related to each other by a series of symmetries called dualities. There is a rapidly
growing literature on superstring cosmology, relating this theory to cosmology
[89]. In particular, much work is taking place on two approaches:
•

The pre big-bang proposal, where a ‘pre big-bang’ branch of the universe is
related to a ‘post big-bang’ era by a duality: a(t) → 1/a(t), t → −t, and
dimensional reduction results in a scalar field (a ‘dilaton’) occurring in the
field equations (see Gasperini [58] for updated references).

This approach has major technical difficulties to solve, particularly related
to the transition from the ‘pre big-bang’ phase to the ‘post big-bang’ phase,
and to the transition from that phase to a standard cosmological expansion. In
additionally it faces fine-tuning problems related to its initial conditions. So this
too is very much a theory in the course of development, rather than a fully viable
proposal.
•

The brane cosmology proposal, where the physical universe is confined to a
four-dimensional ‘brane’ in a five-dimensional universe. The physics of this
proposal are very speculative, and issues arise as to why the initial conditions
in the 5D space had the precise nature so as to confine matter to this lowerdimensional subspace; and then the confinement problem is why they remain
there.

152

Cosmological models

Supposing these technical difficulties can be overcome in each case, it is still
unclear that these proposals avoid the real problem of origins. It can be claimed
they simply postpone facing it, for one now has to ask all the same questions of
origins and uniqueness about the supposed prior state to the present hot big bang
expansion phase: Why did this previous state have the properties it had? (whether
or not it had a classical analogue)? This ‘pre-state’ should be added to one’s
cosmology, and then the same basic questions as before now arise regarding this
completed model.
3.9.2.2 Explanation of initial conditions from ‘nothing’
Attempts at an ‘explanation’ of a true origin, i.e. not arising from some preexisting state (whether it has a classical analogue or not), are difficult even to
formulate.
They may depend on assuming a pre-existing set of physical laws that are
similar to those that exist once spacetime exists, for they rely on an array of
properties of quantum field theory and of fields (existence of Hilbert spaces
and operators, validity of variational principles and symmetry principles, and
so on) that seem to hold sway independently of the existence of the universe
and of space and time (for the universe itself, and so space and time, is to arise
out of their validity). This issue arises, for example, in the case of Vilenkin’s
tunnelling universes: not only do they come from a pre-existent state, as remarked
previously, but they also take the whole apparatus of quantum theory for granted.
This is far from ‘nothing’—it is a very complex structure; but there is no clear
locus for those laws to exist in or material for them to act on. The manner of
their existence or other grounds for their validity in this context are unclear—and
we run into the problems noted before: there are problems with the concepts of
‘occurred’, ‘circumstances’ and even ‘when’—for we are talking inter alia about
the existence of spacetime. Our language can hardly deal with this. Given the
feature that no spacetime exists before such a beginning, brave attempts to define
a ‘physics of creation’ stretch the meaning of ‘physics’. There cannot be a prior
physical explanation, precisely because physics and the causality associated with
physics does not exist there/then.
Perhaps the most radical proposal is that
order arises out of nothing: all order, including the laws of physics,
somehow arises out of chaos,
in the true sense of that word—namely a total lack of order and structure of any
kind (e.g. [1]). However, this does not seem fully coherent as a proposal. If
the pre-ordered state is truly chaotic and without form, I do not see how order
can arise therefrom when physical action is as yet unable to take place, or even
how we can meaningfully contemplate that situation. We cannot assume any
statistical properties would hold in that regime, for example; even formulating
a description of states seems well nigh impossible, for that can only be done in

Explaining homogeneity and structure

153

terms of concepts that have a meaning only in a situation of some stability and
underlying order such as is characterized by physical laws.
3.9.3 The irremovable problem
Thus a great variety of possibilities is being investigated. However, the same
problem arises in every approach: even if a literal creation does not take place, as
is the case in various of the present proposals, this does not resolve the underlying
issue. Apart from all the technical difficulties, and the lack of experimental
support for these proposals, none of these can get around the basic problem: given
any specific proposal,
How was it decided that this particular kind of universe would be the
one that was actually instantiated and what fixed its parameters?
A choice between different contingent possibilities has somehow occurred; the
fundamental issue is what underlies this choice. Why does the universe have
one specific form rather than another, when other forms seem perfectly possible?
Why should any one of these approaches have occurred if all the others are
possibilities? This issue arises even if we assume an ensemble of universes exists:
for then we can ask why this particular ensemble, and not another one?
All approaches face major problems of verifiability, for the underlying
dynamics relevant to these times can never be tested. Here we inevitably
reach the limits to what the scientific study of the cosmos can ever say—if we
assume that such studies must of necessity involve an ability to observationally
or experimentally check the relevant physical theories. However we can attain
some checks on these theories by examining their predictions for the present state
of the universe—its large-scale structure, smaller scale structure and observable
features such as gravitational waves emitted at very early times. These are
important restrictions, and are very much under investigation at the present time;
we need to push our observations as far as we can, and this is indeed happening
at present (particularly through deep galactic observations; much improved CBR
observations; and the prospect of new generation gravitational wave detectors
coming on line).
If it could be shown that only one of all these options was compatible with
observations of the present day universe, this would be a major step forward:
it would select one dynamical evolution from all the possibilities. However,
this does not seem likely, particularly because of the proliferation of auxiliary
functions that can be used to fit the data to the models, as noted before. In addition,
even if this was achieved, it would not show why that one had occurred rather
than any of the others. This would be achieved if it could be eventually shown
that only one of these possibilities is self-consistent: that, in fact, fatal flaws in
all the others reduce the field of possibilities to one. We are nowhere near this
situation at present, indeed possibilities are proliferating rather than reducing.

154

Cosmological models

Given these problems, any progress is of necessity based on specific
philosophical positions, which decide which of the many possible physical and
metaphysical approaches is to be preferred. These philosophical positions should
be identified as such and made explicit [37, 88]. As explained earlier, no
experimental test can determine the nature of any mechanisms that may be in
operation in circumstances where even the concepts of cause and effect are
suspect. Initial conditions cannot be determined by the laws of physics alone—
for if they were so determined they would no longer be contingent conditions, the
essential feature of initial data, but rather would be necessary. A purely scientific
approach cannot succeed in explaining this specific nature of the universe.
Consequent on this situation, it follows that unavoidably, whatever approach
one may take to issues of cosmological origins, metaphysical issues inevitably
arise in cosmology: philosophical choices are needed in order to shape the theory.
That feature should be explicitly recognized, and then sensibly developed in the
optimal way by carefully examining the best way to make such choices.

3.10 Conclusion
There is a tension between theory and observation in cosmology. The issue we
have considered here is, Which family of models is consistent with observations?
To answer this demands an equal sophistication of geometry and physics, whereas
in the usual approaches there is a major imbalance: very sophisticated physics and
very simple geometry. We have looked here at tools to deal with the geometry
in a resaonably sophisticated way, and summarized some of the results that are
obtained by using them. This remains an interesting area of study, particularly in
terms of relating realistic inhomogeneous models to the smoothed out standard
FL models of cosmology.
Further problems arise in considering the physics of the extremely early
universe, and any pre-physics determining initial conditions for the universe.
We will need to develop approaches to these topics that explicitly recognizes
the limitations of the scientific method—assuming that this method implies the
possibility of verification of our theories.

References
[1] Anandan J 1998 Preprint quant-phy/9808045
[2] Bardeen J 1980 Phys. Rev. D 22 1882
[3] Barrow J and Tipler F J 1986 The Anthropic Cosmological Principle (Oxford:
Oxford University Press)
[4] Boerner G and Gottlober S (ed) 1997 The Evolution of the Universe (New York:
Wiley)
[5] Bondi H 1947 Mon. Not. R. Astron. Soc. 107 410
[6] Bondi H 1960 Cosmology 1960 (Cambridge: Cambridge University Press)
[7] Bonnor W B and Ellis G F R 1986 Mon. Not. R. Astron. Soc. 218 605

References

155

[8] Bridle S L, Zehavi I, Dekel A, Lahav O, Hobson M P and Lasenby A N 2000 Mon.
Not. R. Astron. Soc. 321 333
[9] Challinor A 2000 Class. Quantum Grav. 17 871 (astro-ph/9906474)
[10] Challinor A and Lasenby A 1998 Phys. Rev. D 58 023001
[11] Cohn P M 1961 Lie Algebras (Cambridge: Cambridge University Press).
[12] Collins C B 1971 Commun. Math. Phys. 23 137
[13] Collins C B 1977 J. Math. Phys. 18 2116
[14] Collins C B 1985 J. Math. Phys. 26 2009
[15] Collins C B and Ellis G F R 1979 Phys. Rep. 56 63
[16] Collins C B and Hawking S W 1973 Astrophys. J. 180 317
[17] Cornish N J and Spergel D N 1999 Phys. Rev. D 92 087304
[18] Cornish N J, Spergel D N and Starkman G 1966 Phys. Rev. Lett. 77 215
[19] d’Inverno R 1992 Introducing Einstein’s Relativity (Oxford: Oxford Univerity
Press)
[20] Dunsby P K S, Bassett B A C and Ellis G F R 1996 Class. Quantum Grav. 14 1215
[21] Ehlers J 1961 Akad. Wiss. Lit. Mainz, Abhandl. Math.-Nat. Kl. 11 793 (Engl. transl.
1993 Gen. Rel. Grav. 25 1225)
[22] Ehlers J, Geren P and Sachs R K 1968 J. Math. Phys. 9 1344
[23] Einstein A and Straus E G 1945 Rev. Mod. Phys. 17 120
[24] Eisenhart L P 1933 Continuous Groups of Transformations (Princeton, NJ:
Princeton University Press) reprinted: 1961 (New York: Dover)
[25] Ellis G F R 1967 J. Math. Phys. 8 1171
[26] Ellis G F R 1971 General relativity and cosmology Proc. XLVII Enrico Fermi
Summer School ed R K Sachs (New York: Academic Press)
[27] Ellis G F R 1971 Gen. Rel. Grav. 2 7
[28] Ellis G F R 1973 Cargèse Lectures in Physics vol 6, ed E Schatzman (New York:
Gordon and Breach)
[29] Ellis G F R 1975 Q. J. R. Astron. Soc. 16 245
[30] Ellis G F R 1979 Gen. Rel. Grav. 11 281
[31] Ellis G F R 1980 Ann. New York Acad. Sci. 336 130
[32] Ellis G F R 1984 General Relativity and Gravitation ed B Bertotti et al (Dordrecht:
Reidel) p 215
[33] Ellis G F R 1987 Vth Brazilian School on Cosmology and Gravitation ed M Novello
(Singapore: World Scientific)
[34] Ellis G F R 1987 Theory and Observational Limits in Cosmology ed W Stoeger
(Vatican Observatory) pp 43–72
Ellis G F R 1995 Galaxies and the Young Universe ed H von Hippelein,
K Meisenheimer and J H Roser (Berlin: Springer) p 51
[35] Ellis G F R 1990 Modern Cosmology in Retrospect ed B Bertotti et al (Cambridge:
Cambridge University Press) p 97
[36] Ellis G F R 1991 Mem. Ital. Ast. Soc. 62 553–605
[37] Ellis G F R 1999 Astron. Geophys. 40 4.20
Ellis G F R 2000 Toward a New Millenium in Galaxy Morphology ed D Block et al
(Dordrecht: Kluwer)
Ellis G F R 1999 Astrophysics and Space Science 269–279 693
[38] Ellis G F R and Baldwin J 1984 Mon. Not. R. Astron. Soc. 206 377–81
[39] Ellis G F R and Bruni M 1989 Phys. Rev. D 40 1804
[40] Ellis G F R, Bruni M and Hwang J C 1990 Phys. Rev. D 42 1035

156

Cosmological models

[41] Ellis G F R and van Elst H 1999 Theoretical and Observational Cosmology (Nato
Science Series C, 541) ed M Lachieze-Rey (Dordrecht: Kluwer) p 1
[42] Ellis G F R and King A R 1974 Commun. Math. Phys. 38 119
[43] Ellis G F R, Maartens R and Nel S D 1978 Mon. Not. R. Astron. Soc. 184 439–65
[44] Ellis G F R, Nel S D, Stoeger W, Maartens R and Whitman A P 1985 Phys. Rep.
124 315
[45] Ellis G F R and MacCallum M A H 1969 Commun. Math. Phys. 12 108
[46] Ellis G F R and Madsen M S 1991 Class. Quantum Grav. 8 667
[47] Ellis G F R, Perry J J and Sievers A 1984 Astron. J. 89 1124
[48] Ellis G F R and Schreiber G 1986 Phys. Lett. A 115 97–107
[49] Ellis G F R and Sciama D W 1972 General Relativity (Synge Festschrift) ed
L O’Raifeartaigh (Oxford: Oxford University Press)
[50] Ellis G F R and Stoeger W R 1988 Class. Quantum Grav. 5 207
[51] van Elst H and Ellis G F R 1996 Class. Quantum Grav. 13 1099
[52] van Elst H and Ellis G F R 1998 Class. Quantum Grav. 15 3545
[53] van Elst H and Ellis G F R 1999 Phys. Rev. D 59 024013
[54] Ellis G F R and van Elst H 1999 On Einstein’s Path: Essays in Honour of Englebert
Schucking ed A Harvey (Berlin: Springer) pp 203–26
[55] van Elst H and Uggla C 1997 Class. Quantum Grav. 14 2673
[56] Fink H and Lesche H 2000 Found. Phys. Lett. 13 345
[57] Garriga J and Vilenkin A 2000 Phys. Rev. D 61 083502
[58] Gasperini M 1999 String Cosmology http://www.to.infin.it/∼gasperini
[59] Gebbie T and Ellis G F R 2000 Ann. Phys. 282 285
Gebbie T, Dunsby P K S and Ellis G F R 2000 Ann. Phys. 282 321
[60] Gödel K 1949 Rev. Mod. Phys. 21 447
[61] Gödel K 1952 Proc. Int. Cong. Math. (Am. Math. Soc.) 175
[62] Goliath M and Ellis G F R 1999 Phys. Rev. D 60 023502 (gr-qc/9811068)
[63] Gott J R and Liu L 1999 Phys. Rev. D 58 023501 (astro-ph/9712344)
[64] Harrison E R 1981 Cosmology: The Science of the Universe (Cambridge:
Cambridge University Press)
[65] Hartle J B 1996 Quantum mechanics at the Planck scale Physics at the Planck Scale
ed J Maharana, A Khare and M Kumar (Singapore: World Scientific)
[66] Hawking S W 1966 Astrophys. J. 145 544
[67] Hawking S W 1993 Hawking on the Big Bang and Black Holes (Singapore: World
Scientific)
[68] Hawking S W and Ellis G F R 1973 The Large Scale Structure of Spacetime
(Cambridge: Cambridge University Press)
[69] Hawking S W and Penrose R 1970 Proc. R. Soc. A 314 529
[70] Hogan C J 1999 Preprint astro-ph/9909295
[71] Hoyle F, Burbidge G and Narlikar J 1993 Astrophys. J. 410 437
[72] Hoyle F, Burbidge G and Narlikar J 1995 Proc. R. Soc. A 448 191
[73] Hsu L and Wainwright J 1986 Class. Quantum Grav. 3 1105
[74] Isham C J 1997 Lectures on Quantum Theory: Mathematical and Structural
Foundations (London: Imperial College Press, Singapore: World Scientific)
[75] Jaffe A H et al 2001 Phys. Rev. Lett. 86 3475
[76] Kantowski R and Sachs R K 1966 J. Math. Phys. 7 443
[77] King A R and Ellis G F R 1973 Commun. Math. Phys. 31 209
[78] Kinney W H, Melchiorri A and Riotto A 2001 Phys. Rev. D 63 023505

References
[79]
[80]
[81]
[82]
[83]
[84]
[85]
[86]
[87]
[88]
[89]
[90]
[91]
[92]
[93]
[94]
[95]
[96]
[97]
[98]
[99]
[100]
[101]
[102]
[103]
[104]
[105]
[106]
[107]
[108]
[109]
[110]
[111]
[112]
[113]

157

Kolb E W and Turner M S 1990 The Early Universe (New York: Wiley)
Kompaneets A S and Chernov A S 1965 Sov. Phys.–JETP 20 1303
Lachieze R M and Luminet J P 1995 Phys. Rep. 254 136
Lewis A, Challinor A and Lasenby A 2000 Astrophys. J. 538 3273 (astroph/9911177)
Kramer D, Stephani H, MacCallum M A H and Herlt E 1980 Exact Solutions of
Einstein’s Field Equations (Cambridge: Cambridge University Press)
Krasiński A 1983 Gen. Rel. Grav. 15 673
Krasiński A 1996 Physics in an Inhomogeneous Universe (Cambridge: Cambridge
University Press)
Kristian J and Sachs R K 1966 Astrophys. J. 143 379
Lemaı̂tre G 1933 Ann. Soc. Sci. Bruxelles I A 53 51 (Engl. transl. 1997 Gen. Rel.
Grav. 29 641)
Leslie J (ed) 1998 Modern Cosmology and Philosophy (Amherst, NY: Prometheus
Books)
Lidsey J E, Wands D and Copeland E J 2000 Superstring cosmology Phys. Rep. 337
343–492
Linde A D 1990 Particle Physics and Inflationary Cosmology (Chur, Switzerland:
Harwood Academic)
Maartens R 1997 Phys. Rev. D 55 463
MacCallum M A H 1973 Cargèse Lectures in Physics vol 6, ed E Schatzman (New
York: Gordon and Breach)
MacCallum M A H 1979 General Relativity, An Einstein Centenary Survey ed
S W Hawking and W Israel (Cambridge: Cambridge University Press)
Madsen M and Ellis G F R 1988 Mon. Not. R. Astron. Soc. 234 67
Maor I, Brustein R and Steinhardt P J 2001 Phys. Rev. Lett. 86 6
Martin J and Brandenberger R H 2001 Phys. Rev. D 63 123501
Misner C W 1968 Astrophys. J. 151 431
Mustapha N, Hellaby C and Ellis G F R 1999 Mon. Not. R. Astron. Soc. 292 817
Nilsson U S, Uggla C and Wainwright J 1999 Astrophys. J. Lett. 522 L1 (grqc/9904252)
Nilsson U S, Uggla C and Wainwright J 2000 Gen. Rel. Grav. 32 1319 (grqc/9908062)
Oszvath I and Schücking E 1962 Nature 193 1168
Pan J and Coles P 2000 Mon. Not. R. Astron. Soc. 318 L51
Peacocke J R 1999 Cosmological Physics (Cambridge: Cambridge University
Press)
Peebles P J E, Schramm D N, Turner E L and Kron R G 1991 Nature 352 769
Penrose R 1989 Proc. 14th Texas Symposium on Relativistic Astrophysics (Ann. New
York Acad. Sci.) ed E Fenves
Penrose R 1989 The Emperor’s New Mind (Oxford: Oxford University Press) ch 7
Pirani F A E 1956 Acta Phys. Polon. 15 389
Pirani F A E 1957 Phys. Rev. 105 1089
Raychaudhuri A and Modak B 1988 Class. Quantum Grav. 5 225
Rindler W 1956 Mon. Not. R. Astron. Soc. 116 662
Rothman A and Ellis G F R 1986 Phys. Lett. B 180 19
Schucking E 1954 Z. Phys. 137 595
Senovilla J M, Sopuerta C and Szekeres P 1998 Gen. Rel. Grav. 30 389

158
[114]
[115]
[116]
[117]
[118]
[119]
[120]
[121]
[122]
[123]
[124]

[125]
[126]
[127]
[128]
[129]
[130]
[131]
[132]

Cosmological models
Smolin L 1992 Class. Quantum Grav. 9 173
Stabell R and Refsdal S 1966 Mon. Not. R. Astron. Soc. 132 379
Stephani H 1987 Class. Quantum Grav. 4 125
Stephani H 1990 General Relativity (Cambridge: Cambridge University Press)
Stewart J M and Ellis G F R 1968 J. Math. Phys. 9 1072
Stoeger W, Maartens R and Ellis G F R 1995 Astrophys. J. 443 1
Szekeres P 1965 J. Math. Phys. 6 1387
Szekeres P 1975 Commun. Math. Phys. 41 55
Szekeres P 1975 Phys. Rev. D 12 2941
Tegmark M 1998 Ann. Phys., NY 270 1
Tipler F J, Clarke C J S and Ellis G F R 1980 General Relativity and Gravitation:
One Hundred Years after the Birth of Albert Einstein vol 2, ed A Held (New York:
Plenum)
Tolman R C 1934 Proc. Natl Acad. Sci., USA 20 69
Wainwright J 1988 Relativity Today ed Z Perjes (Singapore: World Scientific)
Wainright J, Coley A A, Ellis G F R and Hancock M 1998 Class. Quantum Grav.
15 331
Wainwright J and Ellis G F R (ed) 1997 Dynamical Systems in Cosmology
(Cambridge: Cambridge University Press)
Wald R M 1984 General Relativity (Chicago, IL: University of Chicago Press)
Wald R M 1983 Phys. Rev. D 28 2118
Weinberg S W 1972 Gravitation and Cosmology (New York: Wiley)
Wheeler J A 1968 Einstein’s Vision (Berlin: Springer)

Chapter 4
Inflationary cosmology and creation of
matter in the universe
Andrei D Linde
Department of Physics, Stanford University, Stanford, USA

4.1 Introduction
The typical lifetime of a new trend in high-energy physics and cosmology is
nowadays about 5–10 years. If it has survived for a longer time, the chances
are that it will be with us for quite a while. Inflationary theory by now is 20
years old, and it is still very much alive. It is the only theory which explains
why our universe is so homogeneous, flat and isotropic, and why its different
parts began their expansion simultaneously. It provides a mechanism explaining
galaxy formation and solves numerous different problems at the intersection
between cosmology and particle physics. It seems to be in a good agreement
with observational data and it does not have any competitors. Thus we have some
reasons for optimism.
According to the standard textbook description, inflation is a stage of
exponential expansion in a supercooled false vacuum state formed as a result of
high-temperature phase transitions in Grand Unified Theories (GUTs). However,
during the last 20 years inflationary theory has changed quite substantially. New
versions of inflationary theory typically do not require any assumptions about
initial thermal equilibrium in the early universe, supercooling and exponential
expansion in the false vacuum state. Instead of this, we are thinking about chaotic
initial conditions, quantum cosmology and the theory of a self-reproducing
universe.
Inflationary theory was proposed as an attempt to resolve problems of the
big bang theory. In particular, inflation provides a simple explanation of the
extraordinary homogeneity of the observable part of the universe. But it can make
the universe extremely inhomogeneous on a much greater scale. Now we believe
that instead of being a single, expanding ball of fire produced in the big bang, the
159

160

Inflationary cosmology and creation of matter in the universe

universe looks like a huge growing fractal. It consists of many inflating balls that
produce new balls, which in turn produce more new balls, ad infinitum. Even now
we continue learning new things about inflationary cosmology, especially about
the stage of reheating of the universe after inflation.
In this chapter we will briefly describe the history of inflationary cosmology
and then we will give a review of some recent developments.

4.2 Brief history of inflation
The first inflationary model was proposed by Alexei Starobinsky in 1979 [1].
It was based on investigation of conformal anomaly in quantum gravity. This
model was rather complicated, it did not aim on solving homogeneity, horizon
and monopole problems, and it was not easy to understand the beginning of
inflation in this model. However, it did not suffer from the graceful exit problem
and, in this sense, it can be considered the first working model of inflation.
The theory of density perturbations in this model was developed in 1981 by
Mukhanov and Chibisov [2]. This theory does not differ much from the theory
of density perturbations in new inflation, which was proposed later by Hawking,
Starobinsky, Guth, Pi, Bardeen, Steinhardt, Turner and Mukhanov [3, 4].
A much simpler model with a very clear physical motivation was proposed
by Alan Guth in 1981 [5]. His model, which is now called ‘old inflation’, was
based on the theory of supercooling during the cosmological phase transitions [6].
It was so attractive that even now all textbooks on astronomy and most of the
popular books on cosmology describe inflation as exponential expansion of the
universe in a supercooled false vacuum state. It is seductively easy to explain the
nature of inflation in this scenario. False vacuum is a metastable state without any
fields or particles but with a large energy density. Imagine a universe filled with
such ‘heavy nothing’. When the universe expands, empty space remains empty,
so its energy density does not change. The universe with a constant energy density
expands exponentially, thus we have inflation in the false vacuum.
Unfortunately this explanation is somewhat misleading. Expansion in the
false vacuum in a certain sense is false: de Sitter space with a constant vacuum
energy density can be considered either expanding, or contracting, or static,
depending on the choice of a coordinate system [7]. The absence of a preferable
hypersurface of decay of the false vacuum is the main reason why the universe
after inflation in this scenario becomes very inhomogeneous [5]. After many
attempts to overcome this problem, it was concluded that the old inflation scenario
cannot be improved [8].
Fortunately, this problem was resolved with the invention of the new
inflationary theory [9]. In this theory, just as in the Starobinsky model, inflation
may begin in the false vacuum. This stage of inflation is not very useful, but
it prepares a stage for the next stage, which occurs when the inflaton field φ
driving inflation moves away from the false vacuum and slowly rolls down to the

Brief history of inflation

161

minimum of its effective potential. The motion of the field away from the false
vacuum is of crucial importance: density perturbations produced during inflation
are inversely proportional to φ̇ [2, 3]. Thus the key difference between the new
inflationary scenario and the old one is that the useful part of inflation in the new
scenario, which is responsible for homogeneity of our universe, does not occur in
the false vacuum state.
The new inflation scenario was plagued by its own problems. This scenario
works only if the effective potential of the field φ has a very flat plateau near
φ = 0, which is somewhat artificial. In most versions of this scenario the inflaton
field originally could not be in a thermal equilibrium with other matter fields. The
theory of cosmological phase transitions, which was the basis for old and new
inflation, simply did not work in such a situation. Moreover, thermal equilibrium
requires many particles interacting with each other. This means that new inflation
could explain why our universe was so large only if it was very large and contained
many particles from the very beginning. Finally, inflation in this theory begins
very late, and during the preceding epoch the universe could easily collapse or
become so inhomogeneous that inflation may never happen [7]. Because of all
these difficulties no realistic versions of the new inflationary universe scenario
have been proposed so far.
From a more general perspective, old and new inflation represented a
substantial but incomplete modification of the big bang theory. It was still
assumed that the universe was in a state of thermal equilibrium from the very
beginning, that it was relatively homogeneous and large enough to survive until
the beginning of inflation, and that the stage of inflation was just an intermediate
stage of the evolution of the universe. At the beginning of the 1980s these
assumptions seemed most natural and practically unavoidable. That is why it
was so difficult to overcome a certain psychological barrier and abandon all of
these assumptions. This was done with the invention of the chaotic inflation
scenario [10]. This scenario resolved all the problems of old and new inflation.
According to this scenario, inflation may occur even in the theories with simplest
potentials such as V (φ) ∼ φ n . Inflation may begin even if there was no thermal
equilibrium in the early universe, and it may start even at the Planckian density,
in which case the problem of initial conditions for inflation can be easily resolved
[7].
4.2.1 Chaotic inflation
To explain the basic idea of chaotic inflation, let us consider the simplest model
of a scalar field φ with a mass m and with the potential energy density V (φ) =
(m 2 /2)φ 2 , see figure 4.1. Since this function has a minimum at φ = 0, one may
expect that the scalar field φ should oscillate near this minimum. This is indeed
the case if the universe does not expand. However, one can show that in a rapidly
expanding universe the scalar field moves down very slowly, as a ball in a viscous
liquid, viscosity being proportional to the speed of expansion.

162

Inflationary cosmology and creation of matter in the universe

Figure 4.1. Motion of the scalar field in the theory with V (φ) = 12 m 2 φ 2 . Several different
regimes are possible, depending on the value of the field φ. If the potential energy density
of the field is greater than the Planck density MP4 ∼ 1094 g cm−3 , quantum fluctuations
of spacetime are so strong that one cannot describe it in usual terms. Such a state is called
spacetime foam. At a somewhat smaller energy density (region A: m MP3 < V (φ) < MP4 )
quantum fluctuations of spacetime are small, but quantum fluctuations of the scalar field
φ may be large. Jumps of the scalar field due to quantum fluctuations lead to a process of
eternal self-reproduction of inflationary universe which we are going to discuss later. At
even smaller values of V (φ) (region B: m 2 MP2 < V (φ) < m MP3 ) fluctuations of the field
φ are small; it slowly moves down as a ball in a viscous liquid. Inflation occurs both in
the region A and region B. Finally, near the minimum of V (φ) (region C) the scalar field
rapidly oscillates, creates pairs of elementary particles, and the universe becomes hot.

There are two equations which describe evolution of a homogeneous scalar
field in our model, the field equation
φ̈ + 3H φ̇ = −V ‘(φ),

(4.1)

and the Einstein equation
k
8π
H + 2 =
a
3MP2
2




1 2
φ̇ + V (φ) .
2

(4.2)

Here H = ȧ/a is the Hubble parameter in the universe with a scale factor a(t),
k = −1, 0, 1 for an open, flat or closed universe respectively, MP is the Planck
mass. In the case V = m 2 φ 2 /2, the first equation becomes similar to the equation
of motion for a harmonic oscillator, where instead of x(t) we have φ(t), with a
friction term 3H φ̇:
(4.3)
φ̈ + 3H φ̇ = −m 2 φ.
If the scalar field φ initially was large, the Hubble parameter H was large
too, according to the second equation. This means that the friction term in the first

Brief history of inflation

163

equation was very large, and therefore the scalar field was moving very slowly,
as a ball in a viscous liquid. Therefore at this stage the energy density of the
scalar field, unlike the density of ordinary matter, remained almost constant, and
expansion of the universe continued with a much greater speed than in the old
cosmological theory. Due to the rapid growth of the scale of the universe and
a slow motion of the field φ, soon after the beginning of this regime one has
φ̈  3H φ̇, H 2  (k/a 2 ), φ̇ 2  m 2 φ 2 , so the system of equations can be
simplified:

2mφ π
ȧ
ȧ
2
=
.
(4.4)
3 φ̇ = −m φ,
a
a
MP 3
The last equation shows that the size of the universe in this regime grows
approximately as e H t , where

2mφ π
.
H=
MP 3
More exactly, these equations lead to following solutions for φ and a:
m MP t
,
φ(t) = φ0 − √
12π
2π
a(t) = a0 exp 2 (φ02 − φ 2 (t)).
MP

(4.5)
(4.6)

This stage of exponentially rapid expansion of the universe is called inflation.
In realistic versions of inflationary theory its duration could be as short as 10−35 s.
When the field φ becomes sufficiently small, viscosity becomes small, inflation
ends, and the scalar field φ begins to oscillate near the minimum of V (φ). As any
rapidly oscillating classical field, it loses its energy by creating pairs of elementary
particles. These particles interact with each other and come to a state of thermal
equilibrium with some temperature T . From this time on, the corresponding part
of the universe can be described by the standard hot universe theory.
The main difference between inflationary theory and the old cosmology
becomes clear when one calculates the size of a typical inflationary domain at
the end of inflation. Investigation of this question shows that even if the initial
size of inflationary universe was as small as the Plank size lP ∼ 10−33 cm, after
12
10−35 s of inflation the universe acquires a huge size of l ∼ 1010 cm!
This number is model-dependent, but in all realistic models the size of the
universe after inflation appears to be many orders of magnitude greater than the
size of the part of the universe which we can see now, l ∼ 1028 cm. This
immediately solves most of the problems of the old cosmological theory.
Our universe is almost exactly homogeneous on large scale because all
12
inhomogeneities were stretched by a factor of 1010 . The density of primordial
monopoles and other undesirable ‘defects’ becomes exponentially diluted by
inflation. The universe becomes enormously large. Even if it was a closed

164

Inflationary cosmology and creation of matter in the universe

universe of a size ∼ 10−33 cm, after inflation the distance between its ‘South’
and ‘North’ poles becomes many orders of magnitude greater than 1028 cm. We
see only a tiny part of the huge cosmic balloon. That is why nobody has ever seen
how parallel lines cross. That is why the universe looks so flat.
If one considers a universe which initially consisted of many domains with
chaotically distributed scalar field φ (or if one considers different universes with
different values of the field), then domains in which the scalar field was too
small never inflated. The main contribution to the total volume of the universe
will be given by those domains which originally contained large scalar field φ.
Inflation of such domains creates huge homogeneous islands out of initial chaos.
Each homogeneous domain in this scenario is much greater than the size of the
observable part of the universe.
The first models of chaotic inflation were based on the theories with
polynomial potentials, such as
V (φ) = ±

m2 2 λ 4
φ + φ .
2
4

But the main idea of this scenario is quite generic. One should consider
any particular potential V (φ), polynomial or not, with or without spontaneous
symmetry breaking, and study all possible initial conditions without assuming
that the universe was in a state of thermal equilibrium, and that the field φ was
in the minimum of its effective potential from the very beginning [10]. This
scenario strongly deviated from the standard lore of the hot big bang theory and
was psychologically difficult to accept. Therefore during the first few years after
invention of chaotic inflation many authors claimed that the idea of chaotic initial
conditions is unnatural, and made attempts to realize the new inflation scenario
based on the theory of high-temperature phase transitions, despite numerous
problems associated with it. Gradually, however, it became clear that the idea
of chaotic initial conditions is most general, and it is much easier to construct a
consistent cosmological theory without making unnecessary assumptions about
thermal equilibrium and high temperature phase transitions in the early universe.
Many other versions of inflationary cosmology have been proposed since
1983. Most of them are based not on the theory of high-temperature phase
transitions, as in old and new inflation, but on the idea of chaotic initial conditions,
which is the definitive feature of the chaotic inflation scenario.

4.3 Quantum fluctuations in the inflationary universe
The vacuum structure in the exponentially expanding universe is much more
complicated than in ordinary Minkowski space. The wavelengths of all vacuum
fluctuations of the scalar field φ grow exponentially during inflation. When
the wavelength of any particular fluctuation becomes greater than H −1, this
fluctuation stops oscillating, and its amplitude freezes at some non-zero value

Quantum fluctuations in the inflationary universe

165

δφ(x) because of the large friction term 3H φ̇ in the equation of motion of the field
φ. The amplitude of this fluctuation then remains almost unchanged for a very
long time, whereas its wavelength grows exponentially. Therefore, the appearance
of such a frozen fluctuation is equivalent to the appearance of a classical field
δφ(x) that does not vanish after averaging over macroscopic intervals of space
and time.
Because the vacuum contains fluctuations of all wavelengths, inflation
leads to the continuous creation of new perturbations of the classical field with
wavelengths greater than H −1 , i.e. with momentum k smaller than H . One
can easily understand on dimensional grounds that the average amplitude of
perturbations with momentum k ∼ H is O(H ). A more accurate investigation
shows that the average amplitude of perturbations generated during a time interval
H −1 (in which the universe expands by a factor of e) is given by [7]
|δφ(x)| ≈

H
.
2π

(4.7)

Some of the most important features of inflationary cosmology can be
understood only with an account taken of these quantum fluctuations. That is
why in this section we will discuss this issue. We will begin this discussion on a
rather formal level, and then we will suggest a simple interpretation of our results.
First of all, we will describe inflationary universe with the help of the metric
of a flat de Sitter space,
(4.8)
ds 2 = dt 2 − e2H t dx 2.
We will assume that the Hubble constant H practically does not change during
the process, and for simplicity we will begin with investigation of a massless field
φ.
To quantize the massless scalar field φ in de Sitter space in the coordinates
(4.8) in much the same way as in Minkowski space [11]. The scalar field operator
φ(x) can be represented in the form

i px
∗
−i px
+ a−
],
(4.9)
φ(x, t) = (2π)−3/2 d3 p [a +
p ψ p (t)e
p ψ p (t)e
where ψ p (t) satisfies the equation
ψ̈ p (t) + 3H ψ̇ p (t) + p2 e−2H t ψ p (t) = 0.

(4.10)

The term 3H ψ̇ p (t) originates from the term 3H φ̇ in equation (4.1), the last term
appears because of the gradient term in the Klein–Gordon equation for the field
φ. Note, that p is a comoving momentum, which, just like the coordinates x, does
not change when the universe expands.
In Minkowski space, ψ p (t) √12 p e−i pt , where p = p2 . In de Sitter space
(4.8), the general solution of (4.10) takes the form
√
π
(1)
(2)
H η3/2[C1 ( p)H3/2
ψ p (t) =
( pη) + C2 ( p)H3/2
( pη)],
(4.11)
2

166

Inflationary cosmology and creation of matter in the universe

(i)
where η = −H −1e−H t is the conformal time, and the H3/2
are Hankel functions:


(2)
H3/2(x)

=

(1)
[H3/2(x)]∗

=−



2 −ix
1
e
1+
.
πx
ix

(4.12)

Quantization in de Sitter space and Minkowski space should be identical in the
high-frequency limit, i.e. C1 ( p) → 0, C2 ( p) → −1 as p → ∞. In particular,
this condition is satisfied† for C1 ≡ 0, C2 ≡ −1. In that case,


i p −H t
p −H t 
iH 
exp
e
e
1+
ψ p (t) = √
.
(4.13)
iH
H
p 2p
Note that at sufficiently √
large t (when pe−H t < H ), ψ p (t) ceases to oscillate, and
becomes equal to iH / p 2 p.
The quantity φ 2  may be simply expressed in terms of ψ p :


  −2H t
H2
1
1
e
2
2 3
+ 3 d3 p.
(4.14)
|ψ p | d p =
φ  =
(2π)3
(2π)3
2p
2p
The physical meaning of this result becomes clear when one transforms from the
conformal momentum p, which is time-independent, to the conventional physical
momentum k = pe−H t , which decreases as the universe expands:

 3 
H2
1
d k 1
2
.
(4.15)
+
φ  =
k
2 2k 2
(2π)3
The first term is the usual contribution of vacuum fluctuations in Minkowski space
with H = 0. This contribution can be eliminated by renormalization. The second
term, however, is directly related to inflation. Looked at from the standpoint of
quantization in Minkowski space, this term arises because of the fact that de Sitter
space, apart from the usual quantum fluctuations that are present when H = 0,
also contains φ-particles with occupation numbers
nk =

H2
.
2k 2

(4.16)

It can be seen from (4.15) that the contribution to φ 2  from long-wave
fluctuations of the φ field diverges.
However, the value of φ 2  for a massless field φ is infinite only in eternally
existing de Sitter space with H = constant, and not in the inflationary universe,
which expands (quasi)exponentially starting at some time t = 0 (for example,
when the density of the universe becomes smaller than the Planck density).
† It is important that if the inflationary stage is long enough, all physical results are independent of
the specific choice of functions C1 ( p) and C2 ( p) if C1 ( p) → 0, C2 ( p) → −1 as p → ∞.

Quantum fluctuations in the inflationary universe

167

Indeed, the spectrum of vacuum fluctuations (4.15) strongly differs from the
spectrum in Minkowski space when k  H . If the fluctuation spectrum before
inflation has a cut-off at k ≤ k0 ∼ T resulting from high-temperature effects,
or at k ≤ k0 ∼ H due to a small initial size ∼H −1 of an inflationary region,
then the spectrum will change at the time of inflation, due to exponential growth
in the wavelength of vacuum fluctuations. The spectrum (4.15) will gradually
be established, but only at momenta k ≥ k0 e−H t . There will then be a cut-off
in the integral (4.14). Restricting our attention to contributions made by longwave fluctuations with k ≤ H , which are the only ones that will subsequently be
important for us, and assuming that k0 = O(H ), we obtain
 H

H2 0
d3 k
H2
k
=
d ln
3
2
H
2(2π) H e−H t k
4π −H t
 Ht
2
3
H
p
H
=
d ln
t.
≡
4π 2 0
H
4π 2

φ 2  ≈

(4.17)

A similar result is obtained for a massive scalar field φ. In that case, longwave fluctuations with m 2  H 2 behave as



4
2
2m
3H
1 − exp −
t
.
(4.18)
φ 2  =
3H
8π 2 m 2
When t ≤ 3H /m 2, the term φ 2  grows linearly, just as in the case of the massless
field (4.17), and it then tends to its asymptotic value
φ 2  =

3H 4
.
8π 2 m 2

(4.19)

Let us now try to provide an intuitive physical interpretation of these results.
First, note that the main contribution to φ 2  (4.17) comes from integrating over
exponentially small k (with k ∼ H exp(−H t)). The corresponding occupation
numbers n k (4.16) are then exponentially large. One can show that for large
l = |x − y|e H t , the correlation function φ(x)φ(y) for the massless field φ
is


1
(4.20)
ln H l .
φ(x, t)φ( y, t) ≈ φ 2 (x, t) 1 −
Ht
This means that the magnitudes of the fields φ(x) and φ(y) will be highly
correlated out to exponentially large separations l ∼ H −1 exp(H t), and the
corresponding occupation numbers will be exponentially large. By all these
criteria, long-wave quantum fluctuations of the field φ with k  H −1 behave like
a weakly inhomogeneous (quasi)classical field φ generated during the inflationary
stage.
Analogous results also hold for a massive field with m 2  H 2. There,
the principal contribution to φ 2  comes from modes with exponentially small

168

Inflationary cosmology and creation of matter in the universe

momenta k ∼ H exp(−3H 2/2 m 2 ), and the correlation length is of order
H −1 exp(3H 2/2m 2 ).
Later on we will develop a stochastic formalism which will allow us to
describe various properties of the motion of the scalar field.

4.4 Quantum fluctuations and density perturbations
Fluctuations of the field φ lead to adiabatic density perturbations δρ ∼ V (φ)δφ,
which grow after inflation. The theory of inflationary density perturbations
is rather complicated, but one can make an estimate of their post-inflationary
magnitude in the following intuitively simple way: Fluctuations of the scalar field
lead to a local delay of the end of inflation by the time δt ∼ δφ/φ̇. Density of the
universe after inflation decreases as t −2 , so the local time delay δt leads to density
contrast |δρ/ρ| ∼ |2δt/t|. If one takes into account that δφ ∼ H /2π and that at
the end of inflation t −1 ∼ H , one obtains an estimate
δρ
H2
∼
.
ρ
2π φ̇

(4.21)

Needless to say, this is a very rough estimate. Fortunately, however, it gives a
very good approximation to the correct result which can be obtained by much
more complicated methods [2–4, 7]:
H2
δρ
=C
,
ρ
2π φ̇

(4.22)

where the parameter C depends on equation of state of the universe. For example,
C = 6/5 for the universe dominated by cold dark matter [4]. Then equations
3H φ̇ = V and H 2 = 8π V /3MP2 imply that
√
16 6π V 3/2
δρ
.
(4.23)
=
ρ
5
V
Here φ is the value of the classical field φ(t) (4), at which the fluctuation
we consider has the wavelength l ∼ k −1 ∼ H −1(φ) and becomes frozen in
amplitude. In the simplest theory of the massive scalar field with V (φ) = 12 m 2 φ 2
one has
√
8 3π
δρ
=
mφ 2 .
(4.24)
ρ
5
Taking into account (4.4) and also the expansion of the universe by about
1030 times after the end of inflation, one can obtain the following result for
the density perturbations with the wavelength l (cm) at the moment when these
perturbations begin growing and the process of the galaxy formation starts:
δρ
∼ m ln l (cm).
ρ

(4.25)

From the big bang theory to the theory of eternal inflation

169

The definition of δρ/ρ used in [7] corresponds to COBE data for δρ/ρ ∼
5 × 10−5 . This gives m ∼ 10−6 , in Planck units, which is equivalent to 1013 GeV.
An important feature of the spectrum of density perturbations is its flatness:
δρ/ρ in our model depends on the scale l only logarithmically. For the theories
with exponential potentials, the spectrum can be represented as
δρ
∼ l (1−n)/2 .
ρ

(4.26)

This representation is often used for the phenomenological description of various
inflationary models. Exact flatness of the spectrum implies n = 1. Usually n < 1,
but the models with n > 1 are also possible. In most of the realistic models of
inflation one has n = 1 ± 0.2.
Flatness of the spectrum of δρ/ρ together with flatness of the universe
( = 1) constitute the two most robust predictions of inflationary cosmology.
It is possible to construct models where δρ/ρ changes in a very peculiar way, and
it is also possible to construct theories where  = 1, but it is extremely difficult
to do so.

4.5 From the big bang theory to the theory of eternal inflation
A significant step in the development of inflationary theory which I would like to
discuss here is the discovery of the process of self-reproduction of inflationary
universe. This process was known to exist in old inflationary theory [5] and
in the new one [12], but it is especially surprising and leads to most profound
consequences in the context of the chaotic inflation scenario [13]. It appears
that in many models large scalar field during inflation produces large quantum
fluctuations which may locally increase the value of the scalar field in some parts
of the universe. These regions expand at a greater rate than their parent domains,
and quantum fluctuations inside them lead to the production of new inflationary
domains which expand even faster. This surprising behaviour leads to an eternal
process of self-reproduction of the universe.
To understand the mechanism of self-reproduction one should remember that
the processes separated by distances l greater than H −1 proceed independently
of one another. This is so because during exponential expansion the distance
between any two objects separated by more than H −1 is growing with a speed
exceeding the speed of light. As a result, an observer in the inflationary universe
can see only the processes occurring inside the horizon of the radius H −1.
An important consequence of this general result is that the process of
inflation in any spatial domain of radius H −1 occurs independently of any events
outside it. In this sense any inflationary domain of initial radius exceeding H −1
can be considered as a separate mini-universe.
To investigate the behaviour of such a mini-universe, with an account taken
of quantum fluctuations, let us consider an inflationary domain of initial radius

170

Inflationary cosmology and creation of matter in the universe

H −1 containing sufficiently homogeneous field with initial value φ  MP .
Equation (4.4) implies that during a typical time interval t = H −1 the field
inside this domain will be reduced by φ = MP2 /4πφ. By comparison this
expression with

2V (φ)
mφ
H
∼
,
=
|δφ(x)| ≈
2
2π
3M
3π MP
P
one can easily see that if φ is much less than
MP
φ ∼
3
∗



MP
,
m

then the decrease of the field φ due to its classical motion is much greater
than the average amplitude of the quantum fluctuations δφ generated during
the same time. But for φ  φ ∗ one has δφ(x)  φ. Because the typical
wavelength of the fluctuations δφ(x) generated during the time is H −1 , the whole
domain after t = H −1 effectively becomes divided into e3 ∼ 20 separate
domains (mini-universes) of radius H −1 , each containing almost homogeneous
field φ − φ + δφ. In almost a half of these domains the field φ grows by
|δφ(x)| − φ ≈ |δφ(x)| = H /2π, rather than decreases. This means that the
total volume of the universe containing growing field φ increases 10 times. During
the next time interval t = H −1 the situates repeats. Thus, after the two time
intervals H −1 the total volume of the universe containing the growing scalar field
increases 100 times, etc. The universe enters eternal process of self-reproduction.
This effect is very unusual. Its investigation still brings us new unexpected
results. For example, for a long time it was believed that self-reproduction in
the chaotic inflation scenario can occur only if the scalar field φ is greater than
φ ∗ [13]. However, it was shown in [14] that if the size of the initial inflationary
domain is large enough, then the process of self-reproduction of the universe
begins for all values of the field φ for which inflation is possible (for φ > MP
in the theory 2m 2 φ 2 ). This result is based on the investigation of quantum jumps
with amplitude δφ  H /2π.
Until now we have considered the simplest inflationary model with only one
scalar field, which had only one minimum of its potential energy. Meanwhile,
realistic models of elementary particles propound many kinds of scalar fields. For
example, in the unified theories of weak, strong and electromagnetic interactions,
at least two other scalar fields exist. The potential energy of these scalar fields
may have several different minima. This means that the same theory may have
different ‘vacuum states’, corresponding to different types of symmetry breaking
between fundamental interactions, and, as a result, to different laws of low-energy
physics.
As a result of quantum jumps of the scalar fields during inflation, the universe
may become divided into infinitely many exponentially large domains that have
different laws of low-energy physics. Note that this division occurs even if the

From the big bang theory to the theory of eternal inflation

171

whole universe originally began in the same state, corresponding to one particular
minimum of potential energy.
To illustrate this scenario, we present here the results of computer
simulations of the evolution of a system of two scalar fields during inflation.
The field φ is the inflaton field driving inflation; it is shown by the height of the
distribution of the field φ(x, y) in a two-dimensional slice of the universe. The
field χ determines the type of spontaneous symmetry breaking which may occur
in the theory. We paint the surface black if this field is in a state corresponding
to one of the two minima of its effective potential; we paint it white if it is in the
second minimum corresponding to a different type of symmetry breaking, and
therefore to a different set of laws of low-energy physics.
In the beginning of the process the whole inflationary domain was black, and
the distribution of both fields was very homogeneous. Then the domain became
exponentially large (but it has the same size in comoving coordinates, as shown in
figure 4.1). Each peak of the mountains corresponds to nearly Planckian density
and can be interpreted as a beginning of a new ‘big bang’. The laws of physics are
rapidly changing there, but they become fixed in the parts of the universe where
the field φ becomes small. These parts correspond to valleys in figure 4.2. Thus
quantum fluctuations of the scalar fields divide the universe into exponentially
large domains with different laws of low-energy physics, and with different values
of energy density.
If this scenario is correct, then physics alone cannot provide a complete
explanation for all the properties of our part of the universe. The same
physical theory may yield large parts of the universe that have diverse properties.
According to this scenario, we find ourselves inside a four-dimensional domain
with our kind of physical laws not because domains with different dimensionality
and with alternate properties are impossible or improbable, but simply because
our kind of life cannot exist in other domains.
This consideration is based on the anthropic principle, which was not very
popular among physicists for two main reasons. First of all, it was based on
the assumption that the universe was created many times until the final success.
Second, it would be much easier (and quite sufficient) to achieve this success in a
small vicinity of the solar system rather than in the whole observable part of our
universe.
Both objections can be answered in the context of the theory of eternal
inflation. First of all, the universe indeed reproduces itself in all its possible
versions. Second, if the conditions suitable for the existence of life appear in
a small vicinity of the solar system, then because of inflation the same conditions
will exist in a domain much greater than the observable part of the universe. This
means that inflationary theory for the first time provides real physical justification
of the anthropic principle.

172

Inflationary cosmology and creation of matter in the universe

Figure 4.2. Evolution of scalar fields φ and χ during the process of self-reproduction of the
universe. The height of the distribution shows the value of the field φ which drives inflation.
The surface is painted black in those parts of the universe where the scalar field χ is in the
first minimum of its effective potential, and white where it is in the second minimum. The
laws of low-energy physics are different in the regions of different colour. The peaks of the
‘mountains’ correspond to places where quantum fluctuations bring the scalar fields back
to the Planck density. Each such place in a certain sense can be considered as the beginning
of a new big bang.

4.6 (P)reheating after inflation
The theory of the universe reheating after inflation is the most important
application of the quantum theory of particle creation, since almost all matter
constituting the universe was created during this process.

(P)reheating after inflation

173

At the stage of inflation all energy is concentrated in a classical slowly
moving inflaton field φ. Soon after the end of inflation this field begins to
oscillate near the minimum of its effective potential. Eventually it produces many
elementary particles, they interact with each other and come to a state of thermal
equilibrium with some temperature Tr .
Elementary theory of this process was developed many years ago [15]. It was
based on the assumption that the oscillating inflaton field can be considered as a
collection of non-interacting scalar particles, each of which decays separately in
accordance with perturbation theory of particle decay. However, it was recently
understood that in many inflationary models the first stages of reheating occur
in a regime of a broad parametric resonance. To distinguish this stage from
the subsequent stages of slow reheating and thermalization, it was called preheating [16]. The energy transfer from the inflaton field to other bose fields and
particles during pre-heating is extremely efficient.
To explain the main idea of the new scenario we will consider first the
simplest model of chaotic inflation with the effective potential V (φ) = 12 m 2 φ 2 ,
and with the interaction Lagrangian − 12 g 2 φ 2 χ 2 . We will take m = 10−6 MP , as
required by microwave background anisotropy [7] and, in the beginning, we will
assume for simplicity that χ particles do not have a bare mass, i.e. m χ (φ) = g|φ|.
In this model inflation occurs at |φ| > 0.3MP [7]. Suppose for definiteness
that initially φ is large and negative, and inflation ends at φ ∼ −0.3MP . After
that the field φ rolls to φ = 0, and then it oscillates about φ = 0 with a gradually
decreasing amplitude.
For the quadratic potential V (φ) = 12 mφ 2 the amplitude after the first
oscillation becomes only 0.04MP , i.e. it drops by a factor of ten during the first
oscillation. Later on, the solution for the scalar field φ asymptotically approaches
the regime
φ(t) = (t) sin mt
MP
MP
(t) = √
∼
√
.
(4.27)
3πmt
2π 3π N
Here (t) is the amplitude of oscillations, N is the number of oscillations since
the end of inflation. For simple estimates which we will make later one may use
MP
MP
(t) ≈
≈
.
(4.28)
3mt
20N
The scale factor averaged over several oscillations grows asa(t) ≈ a0 (t/t0 )2/3 .
Oscillations of φ in this theory are sinusoidal, with the decreasing amplitude


a0 3/2
MP
.
(t) =
3
a(t)
The energy density of the field φ decreases in the same way as the density of
non-relativistic particles of mass m:
ρφ = 12 φ̇ 2 + 12 m 2 φ 2 ∼ a −3 .

174

Inflationary cosmology and creation of matter in the universe

Hence the coherent oscillations of the homogeneous scalar field correspond to the
matter-dominated effective equation of state with vanishing pressure.
We will assume that g > 10−5 [16], which implies g MP > 102m for the
realistic value of the mass m ∼ 10−6 MP . Thus, immediately after the end of
inflation, when φ ∼ MP /3, the effective mass g|φ| of the field χ is much greater
than m. It decreases when the field φ moves down, but initially this process
remains adiabatic, |ṁ χ |  m 2χ .
Particle production occurs at the time when the adiabaticity condition
becomes violated, i.e. when |ṁ χ | ∼ g|φ̇| becomes greater than m 2χ = g 2 φ 2 .
This happens only when the field φ rolls close to φ = 0. The velocity of the field
at that time was |φ̇0 | ≈ m MP /10 ≈ 10−7 MP . The process becomes non-adiabatic
for g 2 φ 2 < g|φ̇0 |, i.e. for −φ∗ < φ < φ∗ , where φ∗ ∼ |φ̇0 |/g [16]. Note that
for g  10−5 the interval −φ∗ < φ < φ∗ is very narrow: φ∗  MP /10. As a
result, the process of particle production occurs nearly instantaneously, within the
time
φ∗
(4.29)
∼ (g|φ̇0|)−1/2 .
t∗ ∼
|φ̇0 |
This time interval is much smaller than the age of the universe, so all effects
related to the expansion of the universe can be neglected during the process of
particle production. The uncertainty principle implies in this case that the created
particles will have typical momenta k ∼ (t∗ )−1 ∼ (g|φ̇0 |)1/2 . The occupation
number n k of χ particles with momentum k is equal to zero all the time when it
moves toward φ = 0. When it reaches φ = 0 (or, more exactly, after it moves
through the small region −φ∗ < φ < φ∗ ) the occupation number suddenly (within
the time t∗ ) acquires the value [16]


πk 2
n k = exp −
,
(4.30)
g|φ̇0|
and this value does not change until the field φ rolls to the point φ = 0 again.
To derive this equation one should first represent quantum fluctuations of the
scalar field χ̂ minimally interacting with gravity in the following way:

1
(4.31)
d3 k (âk χk (t)e−ik x + âk+ χk∗ (t)eik x ),
χ̂(t, x) =
(2π)3/2
where âk and âk+ are annihilation and creation operators. In general, one
should write equations for these fluctuations taking into account expansion of
the universe. However, in the beginning we will neglect expansion. Then the
functions χk obey the following equation:
χ̈k + (k2 + g 2 φ 2 (t))χk = 0.

(4.32)

Equation (4.32) describes an oscillator with a variable frequency ωk2 = k 2 +
g 2 φ 2 (t). If φ does not change in time, then one has the usual solution χk =

(P)reheating after inflation
175
√
e−iωk t / 2ωk . However, when the field φ changes, the solution becomes different,
and this difference can be interpreted in terms of creation of particles χ.
The number of created particles is equal to the energy of particles 12 |χ̇k |2 +
1 2
2
2 ωk |χk | divided by the energy ωk of each particle:
ωk
nk =
2




1
|χ̇k |2
2
+ |χk | − .
2
2
ωk

(4.33)

The subtraction 12 is needed to eliminate vacuum fluctuations from the counting.
To calculate this number, one should solve equation (4.32) and substitute the
solutions to equation (4.33).√ One can easily check that for the usual quantum
fluctuations χk = e−iωk t / 2ωk one finds n k = 0. In the case described
earlier, when the particles are created by the rapidly changed field φ in the
regime of strong violation of adiabaticity condition, one can solve equation (4.32)
analytically and find the number of produced particles given by equation (4.30).
One can also solve equations for quantum fluctuations and calculate n k
numerically. In figure 4.3 we show the growth of fluctuations of the field χ and
the number of particles χ produced by the oscillating field φ in the case when the
mass of the field φ (i.e. the frequency of its oscillations) is much smaller than the
average mass of the field χ given by gφ.
The time evolution in figure 4.3 is shown in units m/2π, which corresponds
to the number of oscillations N of the inflaton field φ. The oscillating field
φ(t) ∼  sin mt is zero at integer and half-integer values of the variable mt/2π.
This allows us to identify particle production with time intervals when φ(t) is
very small.
During each oscillation of the inflaton field φ, the field χ oscillates many
times. Indeed, the effective mass m χ (t) = gφ(t) is much greater than the inflaton
mass m for the main part of the period of oscillation of the field φ in the broad
resonance regime with q 1/2 = g/2m  1. As a result, the typical frequency of
oscillation ω(t) = k 2 + g 2 φ 2 (t) of the field χ is much higher than that of the
field φ. That is why during the most of this interval it is possible to talk about an
adiabatically changing effective mass m χ (t). But this condition breaks at small
φ, and particles χ are produced there.
Each time the field φ approaches the point φ = 0, new χ particles are being
produced. Bose statistics implies that the number of particles produced each time
will be proportional to the number of particles produced before. This leads to
explosive process of particle production out of the state of thermal equilibrium.
We called this process pre-heating [16].
This process does not occur for all momenta. It is most efficient if the field
φ comes to the point φ = 0 in phase with the field χk , which depends on k; see
phases of the field χk for some particular values of k for which the process is
most efficient on the upper panel of figure 4.3. Thus we deal with the effect of the
exponential growth of the number of particles χ due to parametric resonance.

176

Inflationary cosmology and creation of matter in the universe

Figure 4.3. Broad parametric resonance for the field χ in Minkowski space in the theory
1 m 2 φ 2 . For each oscillation of the field φ(t) the field χ oscillates many times. Each peak
k
2
in the amplitude of the oscillations of the field χ corresponds to a place where φ(t) = 0.
At this time the occupation number n k is not well defined, but soon after that time it
stabilizes at a new, higher level, and remains constant until the next jump. A comparison
of the two parts of this figure demonstrates the importance of using proper variables for the
description of pre-heating. Both χk and the integrated dispersion χ 2  behave erratically
in the process of parametric resonance. Meanwhile n k is an adiabatic invariant. Therefore,
the behaviour of n k is relatively simple and predictable everywhere except at the short
intervals of time when φ(t) is very small and the particle production occurs.

Expansion of the universe modifies this picture for many reasons. First of all,
expansion of the universe’s redshifts produced particles, making their momenta
smaller. More importantly, the amplitude of oscillations of the field φ decreases
because of the expansion. Therefore the frequency of oscillations of the field χ
also decreases. This may destroy the parametric resonance because it changes,
in an unpredictable way, the phase of the oscillations of the field χ each moment
that φ becomes close to zero.
That is why the number of created particles χ may either increase or decrease
each time when the field φ becomes zero. However, a more detailed investigation

(P)reheating after inflation

177

Figure 4.4. Early stages of parametric resonance in the theory 12 m 2 φ 2 2 in an expanding
universe with scale factor a ∼ t 2/3 for g = 5 × 10−4 , m = 10−6 MP . Note that the
number of particles n k in this process typically increases, but it may occasionally decrease
as well. This is a distinctive feature of stochastic resonance in an expanding universe. A
decrease in the number of particles is a purely quantum mechanical effect which would be
impossible if these particles were in a state of thermal equilibrium.

shows that it increases three times more often than it decreases, so the total
number of produced particles grows exponentially, though in a rather specific
way, see figure 4.4. We called this regime stochastic resonance.
In the course of time the amplitude of the oscillations of the field φ decreases,
and when gφ becomes smaller than m, particle production becomes inefficient
and their number stops growing.
In reality the situation is even more complicated. First of all, created particles
change the frequency of oscillations of the field φ because they give a contribution
∼ g 2 χ 2  to the effective mass squared of the inflaton field [16]. Also, these
particles scatter on each other and on the oscillating scalar field φ, which leads
to additional particle production. As a result, it becomes extremely difficult to
describe analytically the last stages of the process of the parametric resonance,

178

Inflationary cosmology and creation of matter in the universe

Figure 4.5. Development of the resonance in the theory 12 m 2 φ 2 + 14 λφ 4 + 12 g 2 φ 2 χ 2
for g 2 /λ = 5200. The upper curve corresponds to the massless theory, the lower curve
describes stochastic
resonance with a theory with a mass m which is chosen to be much
√
smaller than λφ during the whole period of calculations. Nevertheless, the presence of a
small mass term completely changes the development of the resonance.

even though in many cases it is possible to estimate the final results. In particular,
one can show that the whole process of parametric resonance typically takes only
few dozen of oscillations, and the final occupation numbers of particles grow up
to n k ∼ 102 g −2 [16]. But a detailed description of the last stages of pre-heating
requires lattice simulations, as proposed by Khlebnikov and Tkachev [18].
The theory of pre-heating is very sensitive to the choice of the model. For
example, in the theory 14 λφ 4 + 12 g 2 φ 2 χ 2 the resonance does not become stochastic
despite expansion of the universe. However, if one adds to this theory even a very
small term m 2 φ 2 , the resonance becomes stochastic [17].
This conclusion is illustrated by figure 4.5, where we show the development
of the resonance both for the massless theory with g 2 /λ ∼ 5200, and for the
theory with a small mass m. As we see, in the purely massless theory the
logarithm of the number density n k for the leading growing mode increases
linearly in time
√ x, whereas in the presence of a mass m, which we took to be much
smaller than λφ during the whole process, the resonance becomes stochastic.
In fact, the development of the resonance is rather complicated
even for
√
smaller g 2 /λ. The resonance for a massive field with m  λφ in this case
is not stochastic, but it may consist of stages of regular resonance separated by
the stages without any resonance, see figure 4.6.
Thus we see that the presence of the mass term 12 m 2 φ 2 can modify the
nature of the resonance even if this term is much smaller than 14 λφ 4 . This is a
rather unexpected conclusion, which is an additional manifestation of the nonperturbative nature of pre-heating.

(P)reheating after inflation

179

Figure 4.6. Development of the resonance in the theory 12 m 2 φ 2 + 14 λφ 4 + 12 g 2 φ 2 χ 2 with
m 2  λφ 2 for g 2 /λ = 240. In this particular case the resonance is not stochastic. As time
x grows, the relative contribution of the mass term to the equation describing the resonance
also grows. This shifts the mode from one instability band to another.

Different regimes of parametric resonance in the theory
1 2 2
2m φ

+ 14 λφ 4 + 12 g 2 φ 2 χ 2

are shown in figure 4.7. We suppose that immediately after inflation
amplitude
√ the√
 of the oscillating inflaton field is greater than m/sqr tλ. If g/ λ < λMP /m,
the χ-particles are produced in√the regular stable resonance regime until the
amplitude (t) decreases to m/ λ, after which the resonance occurs as in the
theory 12 m 2 φ 2 + 12 g 2 φ 2 χ 2 [16]. The resonance never becomes stochastic.
√
√
λMP /m, the resonance originally develops as in the
If g /λ >
conformally invariant theory 14 λφ 4 + 12 g 2 φ 2 χ 2 , but with a decrease of (t) the
√
resonance becomes stochastic. Again, for (t) < m/ λ the resonance occurs as
in the theory 12 m 2 φ 2 + 12 g 2 φ 2 χ 2 . In all cases the resonance eventually disappears
when the field (t) becomes sufficiently small. Reheating in this class of models
can be complete only if there is a symmetry breaking in the theory, i.e. m 2 < 0, or
if one adds interaction of the field φ with fermions. In both cases the last stages
of reheating are described by perturbation theory [17].
Adding fermions does not alter substantially the description of the stage
of parametric resonance. Meanwhile the change of sign of m 2 does lead to
substantial changes in the theory of pre-heating, see figure 4.8. Here we will
briefly describe the structure of the resonance in the theory − 12 m 2 φ 2 + 14 λφ 4 +
1 2 2 2
2
effects of backreaction.
2 g φ χ for various g and λ neglecting
√
First of all, at   m/ λ the field φ oscillates in the same way as in
the massless theory 14 λφ 4 + 12 g 2 φ 2 χ 2 . The condition for the resonance to be

180

Inflationary cosmology and creation of matter in the universe

Figure 4.7. Schematic representation of
√ different regimes which are possible in the theory
1 m 2 φ 2 + 1 λφ 4 + 1 g 2 φ 2 χ 2 for m/ λ  10−1 M and for various relations between
P
2
4
2
in this chapter describes the
g 2 and λ in an expanding universe. The theory developed
√
resonance in the white area above the line  = m/ λ. The theory of pre-heating for
√
 < m/ λ is given in [16]. A complete decay of the inflaton is possible only if additional
interactions are present in the theory which allow one inflaton particle to decay to several
other particles, for example, an interaction with fermions ψ̄ψφ.

Figure 4.8. Schematic representation of different regimes which are possible in the theory
− 12 m 2 φ 2 + 14 λφ 4 + 12 g 2 φ 2 χ 2 . White regions correspond to the regime of a regular stable
resonance, a small dark region in the left-hand corner near the origin corresponds to the
perturbative decay φ → χχ. Unless additional interactions are included (see figure 4.7), a
complete decay of the inflaton field is possible only in this small area.

stochastic is
g π 2m 2
< √
.
λ 3λMP

(P)reheating after inflation
181
√
However, as soon as the amplitude  drops down to m/ λ, the situation
changes dramatically. First of all, depending on the values of parameters
the
√
field rolls to one of the minima of its effective potential at φ = ±m/ λ. The
description of this process is rather complicated. Depending on the values
of
√
parameters and on the relation between φ 2 , χ 2  and σ ≡ m/ λ, the
universe may become divided into domains with φ = ±σ , or it may end up in
a single state with a definite sign of φ. After this transitional period√the field φ
oscillates near the minimum
√ of the effective potential at φ = ±m/ λ with an
amplitude   σ = m/ λ. These oscillations lead to parametric resonance
with χ-particle production. For definiteness we will consider here the regime
λ3/2 MP < m  λ1/2 MP . The resonance in this case is possible only if g 2 /λ < 12 .
Using the results of [16] one can show that the resonance is possible only for
g
√ >
λ



m

√
λMP

1/4
.

(The resonance may terminate somewhat earlier if the particles produced by
the parametric resonance give a considerable contribution to the energy density of
the universe.) However, this is not the end of reheating, because the perturbative
decay of the inflaton field remains possible. It occurs with the decay rate
(φ → χχ) = g 4 m/8πλ. This is the process which is responsible for the last
stages of the decay of the inflaton field. It occurs only if one φ-particle can decay
into two χ-particles, which implies that g 2 /λ < 12 .
Thus we see that pre-heating is an incredibly rich phenomenon. Interestingly,
complete decay of the inflaton field is not by any means guaranteed. In most
of the models not involving fermions the decay never completes. Efficiency
of pre-heating and, consequently, efficiency of baryogenesis, depends in a very
non-monotonic way on the parameters of the theory. This may lead to a certain
‘unnatural selection’ of the theories where all necessary conditions for creation of
matter and the subsequent emergence of life are satisfied.
Bosons produced at that stage are far away from thermal equilibrium and
have enormously large occupation numbers. Explosive reheating leads to many
interesting effects. For example, specific non-thermal phase transitions may occur
soon after pre-heating, which are capable of restoring symmetry even in the
theories with symmetry breaking on the scale ∼1016 GeV [19]. These phase
transitions are capable of producing topological defects such as strings, domain
walls and monopoles [20]. Strong deviation from thermal equilibrium and the
possibility of production of superheavy particles by oscillations of a relatively
light inflaton field may resurrect the theory of GUT baryogenesis [21] and may
considerably change the way baryons are produced in the Affleck–Dine scenario
[22], and in the electroweak theory [23].
Usually only a small fraction of the energy of the inflaton field ∼ 10−2 g 2
is transferred to the particles χ when the field φ approaches the point φ = 0 for
the first time [24]. The role of the parametric resonance is to increase this energy

182

Inflationary cosmology and creation of matter in the universe

exponentially within several oscillations of the inflaton field. But suppose that the
particles χ interact with fermions ψ with the coupling h ψ̄ψχ. If this coupling
is strong enough, then χ particles may decay to fermions before the oscillating
field φ returns back to the minimum of the effective potential. If this happens,
parametric resonance does not occur. However, something equally interesting
may occur instead of it: the energy density of the χ particles at the moment of
their decay may become much greater than their energy density at the moment of
their creation. This may be sufficient for a complete reheating.
Indeed, prior to their decay the number density of χ particles produced at
the point φ = 0 remains practically constant [16], whereas the effective mass of
each χ particle grows as m χ = gφ when the field φ rolls up from the minimum
of the effective potential. Therefore their total energy density grows. One may
say that χ particles are ‘fattened’, being fed by the energy of the rolling field φ.
The fattened χ particles tend to decay to fermions at the moment when they have
the greatest mass, i.e. when φ reaches its maximal value ∼10−1 MP , just before it
begins rolling back to φ = 0.
At that moment χ particles can decay to two fermions with mass up to
m ψ ∼ 12 g10−1 MP , which can be as large as 5 × 1017 GeV for g ∼ 1. This
is five orders of magnitude greater than the masses of the particles which can
be produced by the usual decay of φ particles. As a result, the chain reaction
φ → χ → ψ considerably enhances the efficiency of transfer of energy of the
inflaton field to matter.
More importantly, superheavy particles ψ (or the products of their decay)
may eventually dominate the total energy density of matter even if in the
beginning their energy density was relatively small. For example, the energy
density of the oscillating inflaton field in the theory with the effective potential
1
4
−4 in an expanding universe with a scale factor a(t).
4 λφ decreases as a
Meanwhile the energy density stored in the non-relativistic particles ψ (prior
to their decay) decreases only as a −3 . Therefore their energy density rapidly
becomes dominant even if originally it was small. A subsequent decay of such
particles leads to a complete reheating of the universe.
Thus in this scenario the process of particle production occurs within less
than one oscillation of the inflaton field. We called it instant pre-heating [24].
This mechanism is very efficient even in the situation when all other mechanisms
fail. Consider, for example, models where the post-inflationary motion of the
inflaton field occurs along a flat direction of the effective potential. In such
theories the standard scenario of reheating does not work because the field φ
does not oscillate. Until the invention of the instant pre-heating scenario the only
mechanism of reheating discussed in the context of such models was based on
the gravitational production of particles [25]. The mechanism of instant preheating in such models is typically much more efficient. After the moment
when χ particles are produced their energy density grows due to the growth of
the field φ. Meanwhile the energy density of the field φ moving along a flat
direction of V (φ) decreases extremely rapidly, as a −6 (t). Therefore very soon all

Conclusions

183

energy becomes concentrated in the particles produced at the end of inflation, and
reheating completes.
As we see, the theory of creation of matter in the universe is much more
interesting and complicated than we expected few years ago.

4.7 Conclusions
During the last 20 years inflationary theory gradually became the standard
paradigm of modern cosmology. In addition to resolving many problems of
the standard big bang theory, inflation made several important predictions. In
particular:
(1) The universe must be flat. In most models total = 1 ± 10−4 .
(2) Perturbations of the metric produced during inflation are adiabatic.
(Topological defects produce isocurvature perturbations.)
(3) These perturbations should have flat spectrum. In most of the models one
has n = 1 ± 0.2.
(4) These perturbations should be Gaussian. (Topological defects produce nonGaussian perturbations.)
(5) There should be no (or almost no) vector perturbations after inflation. (They
may appear in the theory of topological defects.)
At the moment all of these predictions seem to be in a good agreement with
observational data [26], and no other theory is available that makes all of these
predictions.
This does not mean that all difficulties are over and we can relax. First of all,
inflation is still a scenario which changes with every new idea in particle theory.
Do we really know that inflation began at Planck density 1094 g cm− 3? What if
our space has large internal dimensions, and energy density could never rise above
the electroweak density 1025 g cm− 3? Was there any stage before inflation? Is it
possible to implement inflation in string theory/M-theory?
We do not know which version of inflationary theory will survive ten years
from now. It is absolutely clear than new observational data are going to rule
out 99% of all inflationary models. But it does not seem likely that they will
rule out the basic idea of inflation. Inflationary scenario is very versatile, and
now, after 20 years of persistent attempts of many physicists to propose an
alternative to inflation, we still do not know any other way to construct a consistent
cosmological theory. For the time being, we are taking the position suggested long
ago by Sherlock Holmes: ‘When you have eliminated the impossible, whatever
remains, however improbable, must be the truth’ [27]. Did we really eliminate
the impossible? Do we really know the truth? It is for you to find the answer.

References
[1] Starobinsky A A 1979 JETP Lett. 30 682

184

Inflationary cosmology and creation of matter in the universe

Starobinsky A A 1980 Phys. Lett. B 91 99
[2] Mukhanov V F and Chibisov G V 1981 JETP Lett. 33 532
[3] Hawking S W 1982 Phys. Lett. B 115 295
Starobinsky A A 1982 Phys. Lett. B 117 175
Guth A H and Pi S-Y 1982 Phys. Rev. Lett. 49 1110
Bardeen J, Steinhardt P J and Turner M S 1983 Phys. Rev. D 28 679
[4] Mukhanov V F 1985 JETP Lett. 41 493
Mukhanov V F, Feldman H A and Brandenberger R H 1992 Phys. Rep. 215 203
[5] Guth A H 1981 Phys. Rev. D 23 347
[6] Kirzhnits D A 1972 JETP Lett. 15 529
Kirzhnits D A and Linde A D 1972 Phys. Lett. B 42 471
Kirzhnits D A and Linde A D 1974 Sov. Phys.–JETP 40 628
Kirzhnits D A and Linde A D 1976 Ann. Phys. 101 195
Weinberg S 1974 Phys. Rev. D 9 3320
Dolan L and Jackiw R 1974 Phys. Rev. D 9 3357
[7] Linde A D 1990 Particle Physics and Inflationary Cosmology (Chur, Switzerland:
Harwood)
[8] Guth A H and Weinberg E J 1983 Nucl. Phys. B 212 321
[9] Linde A D 1982 Phys. Lett. B 108 389
Linde A D 1982 Phys. Lett. B 114 431
Linde A D 1982 Phys. Lett. B 116 335, 340
Albrecht A and Steinhardt P J 1982 Phys. Rev. Lett. 48 1220
[10] Linde A D 1983 Phys. Lett. B 129 177
[11] Vilenkin A and Ford L H 1982 Phys. Rev. D 26 1231
Linde A D 1982 Phys. Lett. B 116 335
Starobinsky A A 1982 Phys. Lett. B 117 175
[12] Steinhardt P J 1982 The Very Early Universe ed G W Gibbons, S W Hawking and
S Siklos (Cambridge: Cambridge University Press) p 251
Linde A D 1982 Nonsingular Regenerating Inflationary Universe (Cambridge:
Cambridge University Press)
Vilenkin A 1983 Phys. Rev. D 27 2848
[13] Linde A D 1986 Phys. Lett. B 175 395
Goncharov A S, Linde A and Mukhanov V F 1987 Int. J. Mod. Phys. A 2 561
[14] Linde A D, Linde D A and Mezhlumian A 1994 Phys. Rev. D 49 1783
[15] Dolgov A D and Linde A D 1982 Phys. Lett. B 116 329
Abbott L F, Fahri E and Wise M 1982 Phys. Lett. B 117 29
[16] Kofman L A, Linde A D and Starobinsky A A 1994 Phys. Rev. Lett. 73 3195
Kofman L, Linde A and Starobinsky A A 1997 Phys. Rev. D 56 3258–95
[17] Greene P B, Kofman L, Linde A D and Starobinsky A A 1997 Phys. Rev. D 56 6175–
92 (hep-ph/9705347)
[18] Khlebnikov S and Tkachev I 1996 Phys. Rev. Lett. 77 219
Khlebnikov S and Tkachev I 1997 Phys. Lett. B 390 80
Khlebnikov S and Tkachev I 1997 Phys. Rev. Lett. 79 1607
Khlebnikov S and Tkachev I 1997 Phys. Rev. D 56 653
Prokopec T and Roos T G 1997 Phys. Rev. D 55 3768
Greene B R, Prokopec T and Roos T G 1997 Phys. Rev. D 56 6484
[19] Kofman L A, Linde A D and Starobinsky A A 1996 Phys. Rev. Lett. 76 1011
Tkachev I 1996 Phys. Lett. B 376 35

References

185

[20] Tkachev I, Kofman L A, Linde A D, Khlebnikov S and Starobinsky A A in
preparation
[21] Kolb E W, Linde A and Riotto A 1996 Phys. Rev. Lett. 77 4960
[22] Anderson G W, Linde A D and Riotto A 1996 Phys. Rev. Lett. 77 3716
[23] Garcı́a-Bellido J, Grigorev D, Kusenko A and Shaposhnikov M 1999 Phys. Rev. D
60 123504
Garcı́a-Bellido J and Linde A 1998 Phys. Rev. D 57 6075
[24] Felder G, Kofman L A and LindeA D 1999 Phys. Rev. D 59 123523
[25] Ford L H 1987 Phys. Rev. D 35 2955
Spokoiny B 1993 Phys. Lett. B 315 40
Joyce M 1997 Phys. Rev. D 55 1875
Joyce M and Prokopec T 1998 Phys. Rev. D 57 6022
Peebles P J E and Vilenkin A 1999 Phys. Rev. D 59 063505
[26] Jaffe A H et al 2001 Cosmology from Maxima-1, Boomerang and COBE/DMR CMB
observations Phys. Rev. Lett. 86 3475
[27] Conan Doyle A The Sign of Four ch 6 (http://www.litrix.com/sign4/signo006.htm)

Chapter 5
Dark matter and particle physics
Antonio Masiero and Silvia Pascoli
SISSA, Trieste, Italy

Dark matter constitutes a key problem at the interface between particle physics,
astrophysics and cosmology. Indeed, the observational facts which have been
accumulated in the last years on dark matter point to the existence of an amount
of non-baryonic dark matter. Since the Standard Model (SM) of particle physics
does not possess any candidate for such non-baryonic dark matter, this problem
constitutes a major indication for new physics beyond the SM.
We analyse the most important candidates for non-baryonic dark matter
in the context of extensions of the SM (in particular supersymmetric models).
Recent hints of the presence of a large amount of unclustered ‘vacuum’ energy
(cosmological constant?) are discussed from the astrophysical and particle
physics perspective.

5.1 Introduction
The electroweak SM is now approximately 30 years old and it enjoys a full
maturity with an extraordinary success in reproducing the many electroweak tests
which have been going on since its birth. Not only have its characteristic gauge
bosons, W and Z, been discovered but also the top quark has been found in
the mass range expected by the electroweak radiative corrections, but the SM
has been able to account for an impressively long and very accurate series of
measurements. Indeed, in particular at LEP, some of the electroweak observables
have been tested with precisions reaching the per mille level without finding any
discrepancy with the SM predictions. At the same time, the SM has successfully
passed another very challenging class of exams, namely it has so far accounted
for all the very suppressed or forbidden processes where flavour-changing neutral
currents (FCNC) are present.
186

Introduction

187

By now we can firmly state that no matter what physics should lies beyond
the SM, necessarily such new physics will necessarily have to reproduce the SM
with great accuracy at energies of the order of 100 GeV.
And, yet, in spite of all this glamorous success of the SM in reproducing an
impressive set of experimental electroweak results, we are deeply convinced of
the existence of new physics beyond this model. We see two main motivations
pushing us beyond the SM.
First, we have theoretical ‘particle physics’ reasons to believe that the SM is
not the whole story. The SM does not truly unify the elementary interactions (if
nothing else, gravity is left out of the game), it leaves the problem of fermion
masses and mixings completely unsolved and it exhibits the gauge hierarchy
problem in the scalar sector (namely, the scalar Higgs mass is not protected by
any symmetry and, hence, it would tend to acquire large values of the order of the
energy scale at which the new physics sets in). This first class of motivation for
new physics is well known to particle physicists. Less familiar is a second class of
reasons which finds its origin in some relevant issues of astroparticle physics. We
refer to the problems of the solar and atmospheric neutrino deficits, baryogenesis,
inflation and dark matter (DM). In a sense these aspects (or at least some of
them, in particular the solar and atmospheric neutrino problems and DM) may
be considered as the only ‘observational’ evidence that we have at the moment
for physics beyond the SM.
As for baryogenesis, if it is true that in the SM it is not possible to give
rise to a sufficient amount of baryon–antibaryon asymmetry, still one may debate
whether baryogenesis should have a dynamical origin and, indeed, whether
primordial antimatter is absent. Coming to inflation, again one has to admit that
in the SM there seems to be no room for an inflationary epoch in its scalar sector,
but, as nice as inflation is in coping with several crucial cosmological problems,
its presence in the history of the universe is still debatable. Finally, let me come
to the main topic of this chapter, namely the relation between the DM issue and
physics beyond the SM.
There exists little doubt that a conspicuous amount of the DM has to be in
non-baryonic nature. This is supported both by the upper bound on the amount
of baryonic matter from nucleosynthesis and by studies of galaxy formation. The
SM does not have any viable candidate for such non-baryonic DM. Hence the DM
issue constitutes a powerful probe in our search for new physics beyond the SM.
In this chapter we will briefly review the following aspects.
•
•
•

The main features of the SM such as its spectrum, the Lagrangian and its
symmetries, the Higgs mechanism, the successes and shortcomings of the
SM.
The experimental evidence for the existence of DM.
Two major particle physics candidates for DM: massive (light) neutrinos
and the lightest supersymmetric (SUSY) particle in SUSY extensions of the
SM with R parity (to be defined later on). Light neutrinos and the lightest

188

•

•

Dark matter and particle physics
sparticle are ‘canonical’ examples of the hot and cold DM, respectively. This
choice does not mean that these are the only interesting particle physics
candidates for DM. For instance axions are still of great interest as CDM
candidates and the experimental search for them is proceeding at full steam.
The possibility of warm dark matter which has recently attracted much
attention in relation to the possibility of light gravitinos (as WDM
candidates) in a particular class of SUSY models known as gauge-mediated
SUSY breaking schemes.
Finally the problem of the cosmological constant  in relation to the
structure formation in the universe as in the CDM or QCDM models.

5.2 The SM of particle physics
In particle physics the fundamental interactions are described by the Glashow–
Weinberg–Salam standard theory (GSW) for the electroweak interactions [1–3]
(for a recent review see [4]) and QCD for the strong one. GWS and QCD are
gauge theories based, respectively, on the gauge groups SU (2) L × U (1)Y and
SU (3)c where L refers to left, Y to hypercharge and c to colour. We recall that
a gauge theory is invariant under a local symmetry and requires the existence of
vector gauge fields living in the adjoint representation of the group. Therefore in
our case we have:
(i) three gauge fields Wµ1 , Wµ2 , Wµ3 for SU (2) L ;
(ii) one gauge field Bµ for U (1)Y ; and
(iii) eight gauge bosons λaµ for SU (3)c .
The SM fermions live in the irreducible representations of the gauge group
and are reported in table 5.1: the indices L and R indicate, respectively, the left
and right fields, b = 1, 2, 3 the generation, the colour is not shown.
The Lagrangian of the SM is dictated by the invariance under the Lorentz
group and the gauge group and the request of renormalizability. It is given
by the sum of the kinetic fermionic part LK mat and the gauge one LK gauge:
L = LK mat + LK gauge. The fermionic part reads for one generation:




LK mat = iQ L γ µ ∂µ + igWµa Ta + i g6 Bµ Q L + id R γ µ ∂µ − i g3 Bµ d R




2g
g
Bµ u R + iE L γ µ ∂µ + igWµa Ta − i Bµ E L
+ iu R γ µ ∂µ + i
3
2


µ
+ ie R γ ∂µ − ig Bµ e R
(5.1)
where the matrices Ta = σa /2, σa are the Pauli matrices, g and g are the coupling
constants of the groups SU (2) L and U (1)Y respectively. The Dirac matrices
γ µ are defined as usual. The colour and generation indeces are not specified.
This Lagrangian LK mat is invariant under two global accidental symmetries, the

The SM of particle physics

189

Table 5.1. The fermionic spectrum of the SM.
Generations
Fermions


νb
E bL ≡
eb− L



I



νe
e− L



II



νµ
µ− L



III



SU (2) L ⊗ U (1)Y

ντ
τ− L

(2, −1)

e−
R

µ−
R

τ R−

(1, −2)

 
u
d L

 
c
s L

 
t
b L

(2, 1/3)

ub R

uR

cR

tR

(1, 4/3)

db R

dR

sR

bR

(1, −2/3)

eb R

Q bL ≡


ub
db L

leptonic number and the baryonic one: the fermions belonging to the fields E bL
and eb R are called leptons and trasform under the leptonic symmetry U (1) L while
the ones belonging to Q bL , u b R and db R baryons and trasform under U (1) B .
The Lagrangian for the gauge fields reads:

LK gauge =

− 14 (∂µ Wνa − ∂ν Wµa +  abc Wµb Wνc )
× (∂ µ W νa − ∂ ν W µa +  ab c Wνb Wµc )
− 14 (∂µ Bν − ∂ν Bµ )(∂ µ B ν − ∂ ν B µ ).

(5.2)

5.2.1 The Higgs mechanism and vector boson masses
The gauge symmetry protects the gauge bosons from having mass. Unfortunately
the weak interactions require massive gauge bosons in order to explain the
experimental behaviour. However, adding a direct mass term for gauge bosons
breaks explicitly the gauge symmetry and spoils renormalizability. To preserve
such nice feature of gauge theories, it is necessary to break spontaneously the
symmetry. This is achieved through the Higgs mechanism. We introduce in the
spectrum a scalar field H , which transforms as a doublet under SU (2) L , carries
hypercharge while it is colourless. The Higgs doublet has the following potential
VHiggs, kinetic terms LKH and Yukawa couplings with the fermions LHf :
VHiggs = − µ2 H † H + λ(H † H )2

† 

g
g
a
a
LKH = − ∂µ H + igWµ Ta H + i 2 Bµ H ∂µ H + igWµ Ta H + i 2 Bµ H

Dark matter and particle physics

190

LHf =



gener.

−

 Rc + λe E Lb H E Rc ) + h.c. (5.3)
(λdbc Q Lb H D Rc + λubc Q Lb HU
bc

b,c

where the parameters µ and λ are real constants, λdbc , λubc and λebc are 3 × 3
 indicates the charge conjugate of H :
matrices on the generation space. H
†
a
ab

H =  Hb .
While the Lagrangian is invariant under gauge symmetry the vacuum is not
and the neutral component of the doublet H develops a vacuum expectation value
(vev):
 
0
0
.
(5.4)
H  =
v
This breaks the symmetry SU (2) L ⊗U (1)Y down to U (1) E M . We recall that
when a global symmetry is spontaneously broken, a massless Goldstone boson
appears in the theory; if the symmetry is local (gauge) these Goldstone bosons
become the longitudinal components of the vector bosons (it is said that they are
eaten up by the gauge bosons). The gauge bosons relative to the broken symmetry
acquire a mass as shown in LM gauge :

LM gauge = − 21 v4 [g2(Wµ1 )2 + g2(Wµ2 )2 + (−gWµ3 + g Bµ)2].
2

(5.5)

Therefore there are three massive vectors Wµ± and Z µ0 :
1
Wµ± = √ (Wµ1 ∓ iWµ2 ),
2
1
(gWµ3 − g Bµ ),
Z µ0 =
2
g +g2

(5.6)
(5.7)

whose masses are given by
v
mW = g ,
2

(5.8)

mZ =

(5.9)

v
(g 2 + g 2 ) ,
2

while the gauge boson
Aµ ≡

1
g2 + g 2

(gWµ3 + g Bµ ),

relative to U (1) E M , remains massless as imposed by the gauge symmetry. Such
a mechanism is called the Higgs mechanism and preserves renormalizability.

The SM of particle physics

191

5.2.2 Fermion masses
Fermions are spinors with respect to the Lorentz group SU (2) ⊗ SU (2). Weyl
spinors are two-component spinors which transform under the Lorentz group:
χL

as ( 12 , 0)

(5.10)

ηR

1
2)

(5.11)

as (0,

and therefore are said to be left-handed and right-handed respectively.
A fermion mass term must be invariant under the Lorentz group. We have
two possibilities:
(i) A Majorana mass term couples just one spinor with itself:
χ α χ β αβ

or

ηα̇ ηβ̇ α̇ β̇ .

(5.12)

It is not invariant under any local or global symmetry under which the field
transforms not trivially;
(ii) A Dirac mass term involves two different spinors χ L and η R :
χ α η̄β αβ

or

χ̄ α̇ ηβ̇ α̇ β̇ .

(5.13)

This can be present even if the fields carry quantum numbers.
In the SM Majorana masses are forbidden by the gauge symmetry; in fact
we have that, for example,
e L e L ⇒ Q = 0
ν L ν L ⇒ SU (2) L =
and SU (2) L forbids Dirac mass terms:
e L e R ⇒ SU (2) L = .

(5.14)

Therefore no direct mass term can be present for fermions in the SM.
However, when the gauge symmetry breaks spontaneously the Yukawa
couplings provide Dirac mass terms to fermions which read:

LM mat = + √1

1
1
λe v ē L e R + √ λu v ū L u R + √ λd v d̄ L d R + h.c.
2
2
2

(5.15)

with masses:
1
m e = √ λe v
2

1
m u = √ λu v
2

1
m d = √ λd v.
2

(5.16)

We note that neutrinos are massless and so remain at any order in
perturbation theory:

192

Dark matter and particle physics

(i) lacking of the right component they cannot have a Dirac mass term; and
(ii) belonging to a SU (2) L doublet, they cannot have a Majorana mass term.
However, from experimental data we can infer that neutrinos are massive and
that their mass is very small compared to the other mass scales in the SM. The SM
cannot provide such a mass to neutrinos and hence this consitutes a proof of the
existence of a physics beyond the SM. The problem of ν masses will be addressed
in more detail in section 5.4.2.
5.2.3 Successes and difficulties of the SM
The SM has been tested widely at accelerators receiving strong confirmations of
its validity. Up to now there are no appreciable deviations from its expectations
even if some observables have been tested at the per mille level. In the future
LHC will reach higher energies and will have the possibility to discover new
physics beyond the SM if this one lies at the TeV scale. Another promising class
of experiments to detect deviations from the SM predictions are rare processes
which are very suppressed or forbidden in the SM such as flavour-changing
neutral currents phenomena or C P-violation ones. All these tests up to now are
compatible with SM expectations.
However, we see good reasons to expect the existence of physics beyond
the SM. From a theoretical point of view the SM cannot give an explanation of
the existence of three families, of the hierarchy present among their masses, of
the fine tuning of some of its parameters, of the lack of unification of the three
fundamental interactions (considering the behaviour of the coupling constants,
we see that they unify at a scale MX ∼ 1015 GeV where a unified simple group
might arise), of the hierarchy problem of the scalar masses which tend to become
as large as the highest mass scale in the theory. From an experimental point of
view, the measured neutrino masses are a proof of a physics beyond SM even if
what the type of physics is still an open question to be addressed. Cosmology also
gives strong hints in favour of a physics beyond the SM: in particular baryogenesis
cannot find a satisfactory explanation in the SM, inflation is not predicted by SM
and finally we have the dark matter problem.

5.3 The dark matter problem: experimental evidence
Let us define  (for a review see [5] and [6]) as the ratio between the density ρ
and the critical density
ρcr =

3H02
= 1.88h 20 × 10−29 g cm−3
8π G

where H0 is the Hubble constant, G the gravitational constant:
=

ρ
.
ρcr

(5.17)

The dark matter problem: experimental evidence

193

The lum due to the contribution of the luminous matter (stars, emitting
clouds of gases) is given by
lum ≤ 0.01.
(5.18)
The first evidence for dark matter (DM) comes from observations of galactic
rotation curves (circular orbital velocity versus radial distance from the galactic
centre) using stars and clouds of neutral hydrogen. These curves show an
increasing profile for small values of the radial distance r while for larger ones it
becomes flat, finally decreasing again. According to Newtonian mechanics this
behaviour can be explained if the enclosed mass rises linearly with galactocentric
distance. However, the light falls off more rapidly and therefore we are forced
to assume that the main part of the matter in the galaxies is made of non-shining
matter or DM which extends for a much bigger region than the luminous one. The
limit on galactic which can be inferred from the study of these curves is
galactic ≥ 0.1.

(5.19)

The simplest idea is to suppose that the DM is due to baryonic objects which
do not shine. However big-bang nucleosynthesis (BBN) and, in particular, a
precise determination of the primeval aboundance of deuterium provide strong
limits on the value of the baryon density [7] B = ρB /ρcr :
B = (0.019 ± 0.001)h −2
0  0.045 ± 0.005.

(5.20)

One-third of the BBN baryon density is given by stars and the cold and warm
gas present in galaxies. The other two-thirds are probably in hot intergalactic gas,
warm gas in galaxies and dark stars such as low-mass objects which do not shine
(brown dwarfs and planets) or the result of stellar evolution (neutron stars, black
holes, white dwarfs). The latter ones are called MACHOS (MAssive Compact
Halo Objects) and can be detected in our galaxy through microlensing.
Anyway from cluster observations the ratio of baryons to total mass is
−3/2
f = (0.075 ± 0.002)h 0 ; assuming that clusters provide a good sample of
the universe, from f and B in (5.20) we can infer that:
M ∼ 0.35 ± 0.07.

(5.21)

Such a value for M is supported by evidence, from the evolution, of the
abundance of clusters and measurements of the power spectrum of large-scale
structures.
Hence the major part of DM is non-baryonic [8]. The crucial point is that
the SM does not possess any candidate for such non-baryonic relics of the early
universe. Hence the demand for non-baryonic DM implies the existence of a new
physics beyond the SM. Non-baryonic DM divides into two classes [23, 26]: cold
DM (CDM), made of neutral heavy particles called WIMPS (Weakly Interacting
Massive Particles) or very light ones such as axions, hot DM (HDM) made
of relativistic particles as neutrinos or even warm dark matter (WDM) with
intermediate characteristics such as gravitinos.

194

Dark matter and particle physics

5.4 Lepton number violation and neutrinos as HDM
candidates
Neutrinos are the first candidate for DM we review which can account for
HDM [30]: particles that were relativistic at their decoupling from the thermal
bath when their rate of interaction became smaller than the expansion rate and
they froze out (or, to be more precise, at the time galaxy formation starts at
T ∼ 300 eV). The SM has no candidate for HDM; however, it is now well
established from experimental data that neutrinos are massive and very light.
Therefore they can account for HDM. We briefly discuss their characteristics.
5.4.1 Experimental limits on neutrino masses
The recent atmospheric neutrino data from Super-Kamiokande provide strong
evidence of neutrino oscillations which can take place only if neutrinos are
massive. The parameters relevant in ν-oscillations are the mixing angle θ and the
mass-squared differences which can be measured in atmospheric neutrinos, solar
neutrinos, short-baseline and long-baseline experiments (for a review see [9,10]):
(i) In atmospheric neutrino experiments, to account for the deficit of the νµ flux
expected towards the νe one from cosmic rays and its zenith dependence, it
is necessary to call for νµ → ντ oscillations with
sin2 2θatm ≥ 0.82
m 2atm  (1.5–8.0) × 10−3 eV2

(5.22)
(5.23)

from Super-Kamiokande data at 99% C.L. [11].
(ii) The solar ν anomaly arises from the fact that the νe flux coming from the Sun
is sensibly less than the one predicted by the solar SM: this problem can also
be explained in terms of neutrino oscillations. The recent Super-Kamiokande
data [12] favour the LMA (large mixing angle) solution with
tan2 θ  0.15–4
m 2  (1.5–10) × 10−5 eV 2

(5.24)
(5.25)

at 99% C.L., even if the small mixing angle (tan2 θ ∼ 10−4 ) and the LOW
(tan2 θ ∼ 0.4–4) solutions cannot be excluded and the oscillations into
sterile neutrinos are strongly disfavoured.
(iii) Reactor [13] and short- and long-baseline experiments constrain further the
parameters and, in particular, the mixing angles.
(iv) Finally the LSND experiment has evidence of ν µ → ν e oscillations with
m 2LSND  0.1–2 eV2 , the Karmen experiment has no positive results for
the same oscillation and then restricts the LSND allowed region [14].
In the near future several long-baseline experiments will be held to test νoscillations directly and measure the relevant parameters: K2K in Japan is already

Lepton number violation and neutrinos as HDM candidates

195

looking for missing νµ in νµ → ντ oscillations, MINOS (in US) and OPERA
(with neutrino beam from CERN to Gran Sasso) are long-baseline experiments
devoted to this aim, which are under construction.
The tritium beta-decay experiments are searching directly for the effective
electron neutrino mass m β and the present Troitzk [15] and Mainz [16] limits
give m β ≤ 2.5–2.9 eV; there are perspectives to increase the sensitivity down to
1 eV.
The ββ0ν decay predicted if neutrinos are Majorana particles will indicate
the value of the effective mass |m|, the present Heidelberg–Moscow bound is
(see, for example, [17]):


 2 

(5.26)
|m| ≡ 
Uei m i  ≤ 0.2–1 eV
i

but in the near future there are perpectives to reach |m| ∼ 0.01 eV.
Finally the direct search for m ν at accelerators has so far given negative
results leading to upper bounds [18]:
m νµ < 0.19 MeV,

m ντ < 18.2 MeV

(5.27)

from LEP at 90% C.L. and 95% C.L. respectively.
From all these experiments we can conclude that neutrinos have masses and
that their values must be much lower than the other mass scales in the SM.
5.4.2 Neutrino masses in the SM and beyond
The SM cannot account for neutrino masses: we cannot construct either a Dirac
mass term as there is only a left-handed neutrino and no right-handed component,
or a Majorana mass term because such a mass would violate the lepton number
and the gauge symmetry.
To overcome this problem, many possibilities have been suggested:
(1) Within the SM spectrum we can form an SU (2) L singlet with ν L using
a triplet formed by two Higgs field H as ν L ν L H H . When the Higgs field H
develops a vev, this term gives rise to a Majorana mass term. However, this
term is not renormalizable, breaks the leptonic symmetry and does not give an
explanation of the smallness of the neutrino masses.
(2) We can introduce a new Higgs triplet  and produce a Majorana mass
term as in the previous case when  acquires a vev.
(3) However, the most economical way to extend the SM is to introduce
a right-handed component N R , a singlet under the gauge group, which couples
with the left-handed neutrinos. The lepton number L can be either conserved
or violated. In the former option neutrinos acquire a ‘regular’ Dirac mass like
all other charged fermions of the SM. The left- and right-handed components of
the neutrino combine together to give rise to a massive four-component Dirac
fermion. The problem is that the extreme lightness of the neutrinos (in particular

196

Dark matter and particle physics

of the electron-neutrino) requires an exceedingly small neutrino Yukawa coupling
of O(10−11 ) or so. Although quite economical, we do not consider this option
particularly satisfactory.
(4) The other possibility is to link the presence of neutrino masses to the
violation of L. In this case one introduces a new mass scale, in addition to the
electroweak Fermi scale, into the problem. Indeed, lepton number can be violated
at a very high or a very low mass scale. The former choice represents, in our view,
the most satisfactory way to have massive neutrinos with a very small mass. The
idea (see-saw mechanism [19,20]) is to introduce a right-handed neutrino into the
fermion mass spectrum with a Majorana mass M much larger than MW . Indeed,
being the right-handed neutrino, a singlet under the electroweak symmetry group,
its mass is not chirally protected. The simultaneous presence of a very large
chirally unprotected Majorana mass for the right-handed component together with
a ‘regular’ Dirac mass term (which can be at most of O(100 GeV) gives rise to
two Majorana eigenstates with masses very far apart.
The Lagrangian for neutrino masses is given by

 c 
1
c
0 mD
Lmass = − 2 (ν L N L ) m D M Nν RR + h.c.
(5.28)
where ν cR is the C P-conjugated of ν L and N Lc of N R . It holds that m D  M.
Diagonalizing the mass matrix we find two Majorana eigenstates n 1 and n 2 with
masses very far apart:
m2
m 2  M.
m1  D ,
M
The light eigenstate n 1 is mainly in the ν L direction and is the neutrino that we
‘observe’ experimentally while the heavy one n 2 is in the N R one. The key point
is that the smallness of its mass (in comparison with all the other fermion masses
in the SM) finds a ‘natural’ explanation in the appearance of a new, large mass
scale where L is violated explicitly (by two units) in the right-handed neutrino
mass term.
5.4.3 Thermal history of neutrinos
Let us consider a stable massive neutrino (of mass less than 1 MeV) (see for
example [5]). If its mass is less than 10−4 eV it is still relativistic today and
its contribution to M is negligible. In the opposite case it is non-relativistic
and its contribution to the energy density of the universe is simply given by its
number density multiplied by its mass. The number density is determined by the
temperature at which the neutrino decouples and, hence, by the strength of the
weak interactions. Neutrinos decouple when their mean free path exceeds the
horizon size or equivalently  < H . Using natural units (c = h̄ = 1), we have
that
(5.29)
 ∼ σν n e± ∼ G 2F T 5

Lepton number violation and neutrinos as HDM candidates
and
H∼
so that

−1/3

Tνd ∼ MP

T2
MP

−2/3

GF

197

(5.30)
∼ 1 MeV,

(5.31)

where G F is the Fermi constant, T denotes the temperature, MP is the Planck
mass. Since this decoupling temperature Tνd is higher than the electron mass, then
the relic neutrinos are slightly colder than the relic photons which are ‘heated’ by
the energy released in the electron–positron annihilation. The neutrino number
density turns out to be linked to the number density of relic photons n γ by the
relation:
3
gν n γ ,
(5.32)
n ν = 22
where gν = 2 or 4 according to the Majorana or Dirac nature of the neutrino,
respectively.
Then one readily obtains the ν contribution to M :
ν = 0.01 × m ν (eV)h −2
0

gν
2



T0
2.7

3
.

(5.33)

Imposing ν h 20 to be less than one (which comes from the lower bound on the
lifetime of the universe), one obtains the famous upper bound of 200(gν )−1 eV
on the sum of the masses of the light and stable neutrinos:

m νi ≤ 200(gν )−1 eV.
(5.34)
i

Clearly from equation (5.33) one easily sees that it is enough to have one
neutrino with a mass in the 1–20 eV range to obtain ν in the 0.1–1 range of
interest for the DM problem.
5.4.4 HDM and structure formation
Hence massive neutrinos with mass in the eV range are very natural candidates
to contribute to an M larger than 0.1. The actual problem for neutrinos as
viable DM candidates concerns their role in the process of large-scale structure
formation. The crucial feature of HDM is the erasure of small fluctuations by
free-streaming: neutrinos stream relativistically for quite a long time until their
temperature drops to T ∼ m ν . Therefore a neutrino fluctuation in order to
be preserved must be larger than the distance dν travelled by neutrinos during
such an interval. The mass contained in that space volume is of the order of the
supercluster masses:
M J,ν ∼ dν3 m ν n ν (T = m ν ) ∼ 1015 M ,

(5.35)

198

Dark matter and particle physics

where n ν is the number density of the relic neutrinos, M is the solar mass.
Therefore the first structures to form are superclusters and smaller structures
such as galaxies arise from fragmentation in a typical top-down scenario.
Unfortunately in these schemes one obtains too many structures at superlarge
scales. The possibility of improving the situation by adding the seeds for
small-scale structure formation using topological defects (cosmic strings) are
essentially ruled out at present [21,22]. Hence schemes of pure HDM are strongly
disfavoured by the demand of a viable mechanism for large-structure formation.

5.5 Low-energy SUSY and DM
Another kind of DM, widely studied, called cold DM (CDM) is made of particles
which were non-relativistic at their decoupling. Natural candidates for such
DM are Weakly Interacting Massive Particles (WIMPs), which are very heavy
if compared to neutrinos. The SM does not have non-baryonic neutral particles
which can account for CDM and therefore we need to consider extensions of the
SM as supersymmetric SM in which there are heavy neutral particles remnants of
annichilations such as neutralinos (for a review see [36]).
5.5.1 Neutralinos as the LSP in SUSY models
One of the major shortcomings of the SM concerns the protection of the scalar
masses once the SM is embedded into some underlying theory (and at least at
the Planck scale such new physics should set in to incorporate gravity into the
game). Since there is no typical symmetry protecting the scalar masses (while
for fermions there is the chiral symmetry and for gauge bosons there are gauge
symmetries), the clever idea which was introduced in the early 1980s to prevent
scalar masses from becoming too large was to have a supersymmetry (SUSY)
unbroken down to the weak scale. Since fermion masses are chirally protected
and as long as SUSY is unbroken there must be a degeneracy between the fermion
and scalar components of a SUSY multiplet; then, having a low-energy SUSY,
it is possible to have an ‘induced protection’ on scalar masses (for a review
see [34, 35]).
However, the mere supersymmetrization of the SM faces an immediate
problem. The most general Lagrangian contains terms which violate baryon and
lepton numbers producing a proton decay which is too rapid. To prevent this
catastrophic result we have to add some symmetry which forbids all or part of
these dangerous terms with L or B violations. The most familiar solution is the
imposition of a discrete symmetry, called R matter parity, which forbids all these
dangerous terms. It reads over the fields contained in the theory:
R = (−1)3(B−L)+2s .

(5.36)

R is a multiplicative quantum number reading −1 over the SUSY particles and
+1 over the ordinary particles. Clearly in models with R parity the lightest

Low-energy SUSY and DM

199

SUSY particle can never decay. This is the famous LSP (lightest SUSY particle)
candidate for CDM.
Note that proton decay does not call directly for R parity. Indeed this decay
entails the violation of both B and L. Hence, to prevent a fast proton decay
one may impose a discrete symmetry which forbids all the B violating terms in
the SUSY Lagrangian, while allowing for terms with L violation (the reverse is
also viable). Models with such alternative discrete symmetries are called SUSY
models with broken R parity. In such models the stability of the LSP is no longer
present and the LSP cannot be a candidate for stable CDM. We will comment
later on these alternative models in relation to the DM problem, but we turn now
to the more ‘orthodox’ situation with R parity. The favourite LSP is the lightest
neutralino.
5.5.2 Neutralinos in the minimal supersymmetric SM
If we extend the SM in the minimal way, adding for each SM particle a
supersymmetric partner with the same quantum numbers, we obtain the so
called Minimal Supersymmetric Standard Model (MSSM). In this context the
neutralinos are the eigenvectors of the mass matrix of the four neutral fermions
partners of the W3 , B, H01 and H02 called, respectively, wino W̃3 , bino B̃, higgsinos
H̃01 and H̃02 . There are four parameters entering the mass matrix, M1 , M2 , µ and
tan β:

M=

M2
0

m Z cos θw cos β
−m Z cos θw sin β

0
M1
−m Z sin θw cos β
m Z sin θw sin β

m Z cos θw cos β
−m Z sin θw cos β
0
−µ



−m Z cos θw sin β
m Z sin θw sin β 
−µ
0

(5.37)
where m Z = 91.19±0.002 GeV is the mass of the Z boson, θw is the weak mixing
angle, tan β ≡ v2 /v1 with v1 , v2 vevs of the scalar fields H10 and H20 respectively.
In general M1 and M2 are two independent parameters, but if one assumes
that a grand unification scale takes place, then at grand unification M1 = M2 =
M3 , where M3 is the gluino mass at that scale. Then at the MW scale one obtains:
M1 =

5
3

tan2 θw M2  12 M2 ,

M2 =

g22
m g̃
g32

 m g̃ /3,

(5.38)
(5.39)

where g2 and g3 are the SU (2) and SU (3) gauge coupling constants, respectively,
and m g̃ is the gluino mass.
The relation (5.38) between M1 and M2 reduces to three the number of
independent parameters which determine the lightest neutralino composition and
mass: tan β, µ and M2 . The neutralino eigenstates are usually denoted by χ̃i0 , χ̃10
being the lightest one.

200

Dark matter and particle physics

If |µ| > M1 , M2 then χ̃10 is mainly a gaugino and, in particular, a bino if
M1 > m Z , if M1 , M2 > |µ| then χ̃10 is mainly a higgsino. The corresponding
phenomenology is drastically different leading to different predictions for CDM.
For fixed values of tan β one can study the neutralino spectrum in the (µ, M2 )
plane. The major experimental inputs to exclude regions in this plane are the
request that the lightest chargino be heavier than m Z /2 and the limits on the
invisible width of the Z hence limiting the possible decays Z → χ̃10 χ̃10 , χ̃10 χ̃20 .
Moreover, if the GUT assumption is made, then the relation (5.38) between M2
and m g̃ implies a severe bound on M2 from the experimental lower bound on m g̃
of CDF (roughly m g̃ > 220 GeV, hence implying M2 > 70 GeV); the theoretical
demand that the electroweak symmetry be broken radiatively, i.e. due to the
renormalization effects on the Higgs masses when going from the superlarge scale
of supergravity breaking down to MW , further constrains the available (µ, M2 )
region. The first important outcome of this analysis is that the lightest neutralino
mass exhibits a lower bound of roughly 30 GeV. The actual bound on the mass of
the lightest neutralino χ̃10 from LEP2 is:
m χ̃ 0 ≥ 40 GeV

(5.40)

1

for any value of tan β. This bound becomes stronger if we put further constraints
on the MSSM. The strongest limit is obtained in the so-called Constrained MSSM
(CMSSM) where we have only four independent SUSY parameters plus the sign
of the µ parameter (see equation (5.37)): m χ̃ 0 ≥ 95 GeV [29].
1
There are many experiments already running or approved to detect WIMPS;
however, they rely on different techniques:
(i) DAMA and CDMS use the scattering of WIMPS on nuclei measuring the
recoil energy; in particular DAMA [31] exploits an annual modulation of the
signal which could be explained in terms of WIMPS;
(ii) ν-telescopes (AMANDA) are held to detect ν fluxes coming from the
annihilation of WIMPS which accumulate in celestial bodies such as the
Earth or the Sun;
(iii) experiments (AMS, PAMELA) which detect low-energy antiprotons and γ rays from χ̃10 annihilation in the galactic halo.
5.5.3 Thermal history of neutralinos and CDM
Let us focus now on the role played by χ̃10 as a source of CDM. The lightest
neutralino χ̃10 is kept in thermal equilibrium through its electroweak interactions
not only for T > m χ̃ 0 , but even when T is below m χ̃ 0 . However for T < m χ̃ 0
1

1

1

the number of χ̃10 s rapidly decreases because of the appearance of the typical
Boltzmann suppression factor exp(−m χ̃ 0 /T ). When T is roughly m χ̃ 0 /20 the
1

1

number of χ̃10 diminishes so much so that they no longer interact, i.e. they
decouple. Hence their contribution to CDM of χ̃10 is determined by two

Low-energy SUSY and DM

201

parameters: m χ̃ 0 and the temperature at which χ̃10 decouples (Tχd ) which fixes
1

the number of surviving χ̃10 s. As for the determination of Tχd itself, one has to
compute the χ̃10 annihilation rate and compare it with the cosmic expansion rate.
Several annihilation channels are possible with the exchange of different
SUSY or ordinary particles, f̃, H, Z, etc. Obviously the relative importance of
the channels depends on the composition of χ̃10 :
(i) If χ̃10 is mainly a gaugino (say at least at the 90% level) then the annihilation
goes through f̃ or l̃ R exchange and the sfermion mass m f̃ plays a crucial role.
The actual limits from LEP2 are roughly:
m ν̃ ≥ 43 GeV

m ẽ , m q̃ ≥ 90 GeV.

and

(5.41)

The contribution to  due to neutralinos χ̃ 0 is given by
1

χ̃ 0 h 20
1



m 2 0 + m 2˜
χ̃1

(1

lR

TeV)2 m 2 0
χ̃1


1−

1
2

m2 0
χ̃
1

m 2 0 +m 2˜
χ̃
1

+

(5.42)

χ̃
1

(m 2 0 +m 2˜ )2
χ̃
1

lR

.

m4 0
lR

If sfermions are light the χ̃10 annihilation rate is fast and χ̃ 0 is negligible.
1

However, if f̃ (and hence l̃, in particular) is heavier than 150 GeV, the
annihilation rate of χ̃10 is sufficiently suppressed so that χ̃ 0 can be in the
1

right ball park for CDM . In fact if all the f̃s are heavy, say above 500 GeV
and for m χ̃ 0  m f̃ , then the suppression of the annihilation rate can become
1
too efficient yielding χ̃ 0 unacceptably large.
1

(ii) If χ̃10 / is mainly a combination of H̃10 and H̃20 it means that M1 and M2 have
to be much larger than µ. Invoking the relation (5.38) one concludes that,
in this case, we expect heavy gluinos, typically in the TeV range. As for
the number of surviving χ̃10 s in this case, what is crucial is whether m χ̃ 0
1
is larger or smaller than MW . Indeed, for m χ̃ 0 > MW the annihilation
1

channels χ̃10 χ̃10 → WW, ZZ, tt̄ reduce χ̃ 0 too much. If m χ̃ 0 < MW then
1

1

acceptable contributions of χ̃10 to CDM are obtainable in rather wide areas
of the (µ − m Z ) parameter space;
(iii) Finally it turns out that if χ̃10 results from a large mixing of the gaugino (W̃3
and B̃) and higgsino (H̃01 and H̃02 ) components, then the annihilation is too
efficient to allow the surviving χ̃10 to provide a large enough χ̃ 0 . Typically
1

in this case χ̃ 0 < 10−2 and hence χ̃10 is not a good CDM candidate.
1

In the minimal SUSY standard model there are five new parameters in
addition to those already present in the non-SUSY case. Imposing the electroweak

202

Dark matter and particle physics

radiative breaking further reduces this number to four. Finally, in simple
supergravity realizations the soft parameters A and B are related. Hence we end
up with only three new, independent parameters. One can use the constraint that
the relic χ̃10 abundance provides a correct CDM to restrict the allowed area in
this three-dimensional space. Or, at least, one can eliminate points of this space
which would lead to χ̃ 0 > 1, hence overclosing the universe. For χ̃10 masses
1
up to 150 GeV it is possible to find sizable regions in the SUSY parameter space
where χ̃ 0 acquires interesting values for the DM problem. The interested reader
1
can find a thorough analysis in the review [36] and the original papers therein
quoted.
Finally a comment on models without R parity. From the point of view
of DM, the major implication is that in this context the LSP is no longer a viable
CDM candidate since it decays. There are very special circumstances under which
this decay may be so slow that the LSP can still constitute a CDM candidate. The
very slow decay of χ̃10 may have testable consequences. For instance in some
schemes the LSP could decay emitting a neutrino and a photon. The negative
result of the search for such neutrino line at Kamiokande resulted in an improved
lower bound on the χ̃10 lifetime.
5.5.4 CDM models and structure formation
In the pure CDM model, almost all of the energy density needed to reach the
critical one (the remaining few percent being given by the baryons) was provided
by CDM alone. However, some observational facts (in particular the results of
COBE) put this model into trouble, showing that it cannot correctly reproduce the
power spectrum of density perturbations at all scales. At the same time it became
clear that some CDM was needed anyway in order to obtain a successful scheme
for large-scale structure formation.
A popular option is that of a flat universe realized with the total energy
density mostly provided by two different matter components, CDM and HDM in
a convenient fraction. These models, which have been called mixed DM (MDM)
[33], succeeded in fitting the entire power spectrum quite well. A little amount
of HDM has a dramatic effect on CDM models because the free-streaming of
relativistic neutrinos washes out any inhomogeneities in their spatial distribution
which will become galaxies. Therefore their presence slows the growth rates of
the density inhomogeneities which will lead to galaxies.
Another interesting possibility for improving CDM models consists in the
introduction of some late-time decaying particle [50]. The injection of nonthermal radiation due to such decays and the consequent increase in the horizon
length at the equivalence time could lead to a convenient suppression of the
excessive power at small scales (hence curing the major disease of the pure
CDM SM). As appealing as this proposal may be from the cosmological point of
view, its concrete realization in particle physics models meets several difficulties.
Indeed, after considering cosmological and astrophysical bounds on such late

Warm dark matter

203

decays, it turns out that only a few candidates survive as viable solutions.
These schemes beyond pure CDM which presently enjoy most scientific
favour accompany CDM with a conspicous amount of ‘vacuum’ energy density, a
form of unclustered energy which could be due to the presence of a cosmological
constant . We will deal with this interesting class of DM models, called CDM
models, in the final part of this report.

5.6 Warm dark matter
Another route which has been followed in the attempt to go beyond the pure
CDM proposal is the possibility of having some form of warm DM (WDM).
The implementation of this idea is quite attractive in SUSY models where the
breaking of SUSY is conveyed by gauge interactions instead of gravity (these
are the so-called gauge-mediated SUSY breaking (GMSB) models, for a review
see [32]). This scenario had already been critically considered in the old days
of the early constructions of SUSY models and was subject to renewed interest
with the proposal in [37–39], where some guidelines for the realization of lowenergy SUSY breaking are provided. In these schemes, the gravitino mass (m 3/2)
loses its role of fixing the typical size of soft breaking terms and we expect it
to be much smaller than that in models with a hidden sector. Indeed, given
√ the
well-known relation [34] between m 3/2 and the scale of SUSY breaking F, i.e.
m 3/2 = O(F/M), where
√ M is the6 reduced Planck scale, we expect m 3/2 in the
keV range for a scale F of O(10 GeV) that has been proposed in models with
low-energy SUSY breaking in a visible sector.
In the following we briefly report some implications of SUSY models with a
light gravitino (in the keV range) in relation with the dark matter (DM) problem.
We anticipate that a gravitino of that mass behaves as a warm dark matter (WDM)
particle [24, 25, 27], that is, a particle whose free-streaming scale involves a mass
comparable to that of a galaxy, ∼1011−12 M .
5.6.1 Thermal history of light gravitinos and WDM models
Suppose that the gravitinos were once in thermal equilibrium and were frozen out
at the temperature Tψµ d during the cosmic expansion. It can be shown that the
density parameter ψµ contributed by relic thermal gravitinos is:
ψµ h 20

m
  g∗ (Tψ d ) −1
3/2
µ
= 1.17
,
1 keV
100

(5.43)

where g∗ (Tψµ d ) represents the effective massless degrees of freedom at the
temperature Tψµ d . Therefore, a gravitino in the previously mentioned keV range
provides a significant portion of the mass density of the present universe.

204

Dark matter and particle physics

As for the redshift Z NR at which gravitinos become non-relativistic, it
corresponds to the epoch at which their temperature becomes m 3/2 /3, that is:

Z NR 

g∗ (Tψµ d )

1/3

g∗S (T0 )



= 4.14 × 106 ×

m 3/2 /3
T0
g∗ (Tψµ d )

1/3 

100

m 3/2 
,
1 keV

(5.44)

where T0 = 2.726 K is the temperature of the CMB at present time. Once Z NR is
known, one can estimate the free-streaming length until the epoch of the matter–
radiation equality, λFS , which represents a quantity of crucial relevance for the
formation of large-scale cosmic structures.
The free-streaming length for the thermal gravitinos is about 1 Mpc (for
Z NR ∼ 4 × 106 ) which, in turn, corresponds to ∼1012 M , if it is required
to provide a density parameter close to unity. This explicitly shows that light
gravitinos are actually WDM candidates. We also note that, taking h = 0.5, the
requirement of not overclosing the universe turns into m 3/2 ≤ 200 eV.
However, critical density models with pure WDM are known to suffer from
serious troubles [41]. Indeed, a WDM scenario behaves much like CDM on scales
above λFS . Therefore, we expect in the light gravitino scenario that the level of
cosmological density fluctuations on the scale of galaxy clusters (∼10h −1
0 Mpc)
to be almost the same as in CDM. As a consequence, the resulting number density
of galaxy clusters is predicted to be much larger than what is observed [42].
This is potentially a critical test for any WDM-dominated scheme, the
abundance of high-redshift galaxies having been already recognized as a nontrivial constraint for several DM models. It is, however, clear that quantitative
conclusions on this point would at least require the explicit computation of the
fluctuation power spectrum for the whole class of WDM scenarios.

5.7 Dark energy, CDM and xCDM or QCDM
The expansion of the universe is described by two parameters, the Hubble
constant H0 and the deceleration parameter q0 :
(i)

H0 ≡ Ṙ(t0 )/R(t0 ), where R(t0 ) is the scale factor, t0 the age of the universe
at present epoch, and we have
H0 = 65 ± 5 km s−1 Mpc−1

(h = 0.65 ± 0.05);

(5.45)

(ii) q0 ≡ −( R̈(t0 )/H02)R(t0 ) states whether the universe is accelerating or
decelerating. q0 is related to 0 as follows
q0 =

3
0
+
i wi
2
2
i

(5.46)

Dark energy, CDM and xCDM or QCDM
where 0 ≡

i

205

ρi /ρcr , i is the fraction of critical density due to the

component i , pi = wi ρi is the pressure of the component i , ρcr =
1.88h 2 × 10−29 g cm−3 .

3H02
8π G

=

Measurements of q0 from high-Z Type Ia SuperNovae (SNeIa) [44, 45]
give strong indications in favour of an accelerating universe. CMB data [46]
and cluster mass distribution [47] seem to favour models in which the energy
density contributed by the negative pressure component should be roughly twice
as much as the energy of the matter, thus leading to a flat universe (tot = 1)
with  M ∼ 0.4 and  ∼ 0.6. Therefore the universe should be presently
dominated by a smooth component with effective negative pressure; this is, in
fact, the most general requirement in order to explain the observed accelerated
expansion. The most straightforward candidate for that is, of course, a ‘true’
cosmological constant [48]. A plausible alternative that has recently received
much attention is a dynamical vacuum energy given by a scalar field rolling down
its potential: a cosmological scalar field, depending on its dynamics, can easily
fulfil the condition of an equation of state wQ = pQ /ρQ between −1 (which
corresponds to the cosmological constant case) and 0 (that is the equation of state
of matter). Since it is useful to have a short name for the rather long definition
of this dynamical vacuum energy, we follow the literature in calling it briefly
‘quintessence’ [49].
5.7.1 CDM models
At the moment models with  ∼ 0.6 seem to be favoured (see for example [28]).
 is given by
8π G
 ≡
(5.47)
3H02
where  is the cosmological constant, which appears in the most general form of
the Einstein equation. The equation of state for  is p = −ρ or, equivalently,
w = −1. In order to have  ∼ O(1),  has to be:
 ∼ (2 × 10−3 eV)4 .

(5.48)

Being a constant there is no reason in particle physics why this constant
should be so small and not receive corrections at the highest mass scale present in
the theory. This constitutes the most severe hierarchy problem in particle physics
and there are no hints as to how to solve it.
If  = 0, in the early universe the density of energy and matter is dominant
over the vacuum energy contribution, while the universe expands the average
matter density decreases and at low redshifts the  term becomes important. At
the end the universe starts inflating under the influence of the  term.
At present there are models based on the presence of  called CDM
models or CHDM if we allow the presence of a small amount of HDM. Such

206

Dark matter and particle physics

models provide a good fit of the observed universe even if they need further study
and more data confirmations.
5.7.2 Scalar field cosmology and quintessence
The role of the cosmological constant in accelerating the universe expansion
could be played by any smooth component with a negative equation of state
pQ /ρQ = wQ − 0.6 [49, 52], as in the so-called ‘quintessence’ models (QCDM)
[49], otherwise known as xCDM models [51].
A natural candidate for quintessence is given by a rolling scalar field Q with
potential V (Q) and equation of state:
wQ =

Q̇ 2 /2 − V (Q)
,
Q̇ 2 /2 + V (Q)

(5.49)

which—depending on the amount of kinetic energy—could, in principle, take any
value from −1 to +1. Study of scalar field cosmologies has shown [53, 54] that,
for certain potentials, there exist attractor solutions that can be of the ‘scaling’
[55–57] or ‘tracker’ [58, 59] type; this means that for a wide range of initial
conditions the scalar field will rapidly join a well-defined late-time behaviour.
In the case of an exponential potential, V (Q) ∼ exp (−Q), the solution
Q ∼ ln t is, under very general conditions, a ‘scaling’ attractor in a phase space
characterized by ρQ /ρB ∼ constant [55–57]. This could potentially solve the
so called ‘cosmic coincidence’ problem, providing a dynamical explanation for
the order of magnitude equality between matter and scalar field energy today.
Unfortunately, the equation of state for this attractor is wQ = wB , which cannot
explain the acceleration of the universe neither during radiation domination
(wrad = 1/3) nor during matter domination (wm = 0). Moreover, BBNS
constrains the field energy density to values much smaller than the required
∼ 2/3 [54, 56, 57].
If, instead, an inverse power-law potential is considered, V (Q) =
M 4+α Q −α , with α > 0, the attractor solution is Q ∼ t 1−n/m , where n =
3(wQ + 1), m = 3(wB + 1); and the equation of state turns out to be wQ =
(wB α − 2)/(α + 2), which is always negative during matter domination. The ratio
of the energies is no longer constant but scales as ρQ /ρB ∼ a m−n thus growing
during the cosmological evolution, since n < m. ρQ could then have been
safely small during nucleosynthesis and grown later into the phenomenologically
interesting values. These solutions are then good candidates for quintessence and
have been called ‘tracker’ solutions in the literature [54, 58, 59].
The inverse power-law potential does not improve the cosmic coincidence
problem with respect to the cosmological constant case. Indeed, the scale M has
to be fixed from the requirement that the scalar energy density today is exactly
what is needed. This corresponds to choosing the desired tracker path. An
important difference exists in this case though. The initial conditions for the
physical variable ρQ can vary between the present critical energy density ρcr

References

207

and the background energy density ρB at the time of beginning (this range can
span many tens of orders of magnitude, depending on the initial time), and will
anyway end on the tracker path before the present epoch, due to the presence of
an attractor in the phase space. In contrast, in the cosmological constant case, the
physical variable ρ is fixed once and for all at the beginning. This allows us to
state that in the quintessence case the fine-tuning issue, even if still far from being
solved, is at least weakened.
Much effort has recently been devoted to finding ways to constrain such
models with present and future cosmological data in order to distinguish
quintessence from  models [60, 61]. An even more ambitious goal is the partial
reconstruction of the scalar field potential from measuring the variation of the
equation of state with increasing redshift [62].
Natural candidates for these scalar fields are pseudo-Goldstone bosons,
axions, e.g. scalar fields with a scalar potential decreasing to zero for an infinite
value of the fields. Such behaviour occurs naturally in models of dynamical
SUSY breaking: in SUSY models scalar potentials have many flat directions, that
is directions in the field’s space where the potential vanishes. After dynamical
SUSY breaking the degeneracy of the flat potential is lifted but it is restored for
infinite values of the scalar fields.
However, the investigation of quintessence models from the particle physics
point of view is just in a preliminary stage and a realistic model is not yet
available (see, for example, [63–66]). There are two classes of problems: the
construction of a field theory model with the required scalar potential and the
interaction of the quintessence field with SM fields [67]. The former problem
has already been considered by Binétruy [63], who pointed out that scalar inverse
power law potentials appear in supersymmetric QCD theories (SQCD) [68] with
Nc colours and N f < Nc flavours. The latter seems the toughest. Indeed the
quintessence field today has typically a mass of order Q 0 ∼ 10−33 eV. Then, in
general, it would mediate long range interactions of gravitational strength, which
are phenomenologically unacceptable.

References
[1] Salam A 1967 Elementary Particle Theory ed N Svartholm (Stockholm: Almquist
and Wiksells)
[2] Weinberg S 1967 Phys. Rev. Lett. 19 1264
[3] Glashow S L 1961 Nucl. Phys. 22 579
[4] Peskin M E and Schroeder D V 1995 An Introduction to Quantum Field Theory
(Reading, MA: Addison-Wesley)
[5] For an introduction to the DM problem, see, for instance: Kolb R and Turner S 1990
The Early Universe (New York: Addison-Wesley)
Srednicki M (ed) 1989 Dark Matter (Amsterdam: North-Holland)
Primack J, Seckel D and Sadoulet B 1988 Annu. Rev. Nucl. Part. Sci. 38 751
[6] For a recent review see: Primack J 2000 Preprint astro-ph/0007187

208

Dark matter and particle physics

[7] Burles S et al 1999 Phys. Rev. Lett. 82 4176
[8] Freese K, Fields B D and Graff D S 2000 Proc. MPA/ESO Workshop on the First
Stars (Garching, Germany, August 4–6, 1999) (astro-ph/0002058)
[9] For a review see: Ellis J 2000 Nucl. Phys. Proc. Suppl. 91 903
[10] See http://www.hep.anl.gov/NDK/Hypertext/nuindustry.html
[11] Kajita T 2000 Now2000: Europhysics Neutrino Oscillation Workshop (Otranto, Italy,
September 9–16)
[12] Suzuki Y 2000 Neutrino2000: XIX Int. Conf. on Neutrino Physics and Astrophysics
(Sudbury, Canada, June 16–21, 2000)
[13] Apollonio M et al (CHOOZ Collaboration) 1999 Phys. Lett. B 466 415
[14] Mills G 2000 Neutrino2000: XIX Int. Conf. on Neutrino Physics and Astrophysics
(Sudbury, Canada, June 16–21, 2000)
[15] Lobashev V M et al 1999 Phys. Lett. B 460 227
[16] Weinheimer C et al 1999 Phys. Lett. B 460 219
[17] See, for example, Bilenky S M et al 1999 Phys. Lett. B 465 193
[18] Particle Data Book, Caso C et al 2000 Eur. Phys. J. C 15 1
[19] Tanagida T 1979 The Unified Theories and the Baryon Number in the Universe ed
O Sawada and A Sugamoto (Tsukuba: KEK)
[20] Gell-Mann M, Ramond P and Slansky R 1979 Supergravity ed P Van Nieuwenhuizen
and D Z Freedman
[21] Pen U L, Seljak U and Turok N 1997 Phys. Rev. Lett. 79 1611
[22] Albrecht A, Battye R A and Robinson J 1999 Phys. Rev. D 59 023508
[23] Bond J R, Centrella J, Szalay A S and Wilson J R 1984 Formation and Evolution of
Galaxies and Large Structures in the Universe ed J Audouze, J Tran Thanh Van
(Dordrecht: Reidel) pp 87–99
[24] Pagels H and Primack J R 1982 Phys. Rev. Lett. 48 223
[25] Blumenthal G R, Pagels H and Primack J R 1982 Nature 299 37
[26] Blumenthal G R and Primack J R 1984 Formation and Evolution of Galaxies and
Large Structures in the Universe ed J Audouze and J Tran Thanh Van (Dordrecht:
Reidel) pp 163–83
[27] Bond J R, Szalay A S and Turner M S 1982 Phys. Rev. Lett. 48 1636
[28] Blumenthal G R, Faber S M, Primack J R and Rees M J 1984 Nature 311 517
[29] Ellis J et al 2001 Phys. Lett. B 502 171
[30] Primack J R and Gross M A K 2000 Current Aspect of Neutrino Physics ed
D O Caldwell (Berlin: Springer)
[31] Bernabei R et al (DAMA Collaboration) 2000 Phys. Lett. B 480 23
Bottino A, Donato F, Fornengo N and Scopel S 2000 Phys. Rev. D 62 056006
[32] Giudice G and Rattazzi R 1999 Phys. Rep. 322 419
[33] Shafi Q and Stecker F W 1984 Phys. Lett. B 53 1292
Bonometto S A and Valdarnini R 1985 Astrophys. J. 299 L71
Achilli S, Occhionero F and Scaramella R 1985 Astrophys. J.299 577
Holtzman J A 1981 Astrophys. J. Suppl. 71 1
Taylor A N and Rowan-Robinson M 1992 Nature 359 396
Holtzman J A and Primach J 1992 Astrophys. J. 396 113
Pogosyan D Yu and Starobinoski A A 1993 Preprint
Klypin A, Holtzman J, Primach J and Regös E 1993 Astrophys. J. 415 1
[34] For a review, see: Nilles H P 1984 Phys. Rep. C 110 1 (1984);
Haber H and Kane G 1985 Phys. Rep. C 117 1

References

209

[35] Cremmer E, Ferrara S, Girardello L and van Proeyen A 1982 Phys. Lett. B 116 231
Cremmer E, Ferrara S, Girardello L and van Proeyen A 1983 Nucl. Phys. B 212 413
[36] Jungman G, Kamionkowski M and Griest K 1996 Phys. Rep. 267 195 and references
therein
Bottino A and Fornengo N 1999 Preprint hep-ph/9904469
[37] Dine M, Nelson A, Nir Y and Shirman Y 1996 Phys. Rev. D 53 2658
[38] Dine M and Nelson A E 1993 Phys. Rev. D 48 1277
Dine M, Nelson A E and Shirman Y 1995 Phys. Rev. D 51 1362
[39] Dvali G, Giudice G F and Pomarol A 1996 Nucl. Phys. B 478 31
[40] Ambrosanio S, Kane G L, Kribs G D, Martin S P and Mrenna S 1996 Phys. Rev. Lett.
76 3498
[41] Colombi S, Dodelson S and Widrow L M 1996 Astrophys. J. 458 1
Pierpaoli E, Borgani S, Masiero A and Yamaguchi M 1998 Phys. Rev. D 57 2089
[42] White S D M, Efstathiou G and Frenk C S 1993 Mon. Not. R. Astron. Soc. 262 1023
Biviano A, Girardi M, Giuricin G, Mardirossian F and Mezzetti M 1993 Astrophys.
J. 411 L13
Viana T P V and Liddle A R 1996 Mon. Not. R. Astron. Soc. 281 323
[43] White M, Viana P T P, Liddle A R and Scott D 1996 Mon. Not. R. Astron. Soc. 283
107
[44] Perlmutter S et al 1999 Astrophys. J. 517 565
Perlmutter S et al 1997 Bull. Am. Astron. Soc. 29 1351
Perlmutter S et al 1998 Nature 391 51
See also http://www-super-nova.LBL.gov/
[45] Riess A G et al 1998 Astron. J. 116 1009
Filippenko A V and Riess A G 1998 Phys. Rep. 307 31
Filippenko A V and Riess A G 1999 Preprint astro-ph/9905049
Garnavich P M et al 1998 Astrophys. J. 501 74
Leibundgut B, Contardo G, Woudt P and Spyromilio J 1998 Dark ’98 ed H KlapolozKleingzothaus and L Baudis (Singapore: World Scientific)
See also http://cfa-www.harvard.edu/cfa/oir/Research/supernova/HighZ.html
[46] Bartlett J G, Blanchard A, Le Dour M, Douspis M and Barbosa D 1998 Preprint
astro-ph/9804158
Efstathiou G 1999 Preprint astro-ph/9904356
Efstathiou G, Bridle S L, Lasenby A N, Hobson M P and Ellis R S 1999 Mon. Not.
R. Astron. Soc. 303 L47
Lineweaver C 1998 Astrophys. J. 505 L69
[47] Carlberg R G, Yee H K C and Ellingson E 1997 Astrophys. J. 478 462
Carlstrom J 1999 Phys. Scr. in press
[48] See, for example: Carroll S M, Press W H and Turner E L 1992 Annu. Rev. Astron.
Astrophys. 30 499
[49] Caldwell R R, Dave R and Steinhardt P J 1998 Phys. Rev. Lett. 80 1582
[50] Kim H B and Kim J E 1995 Nucl. Phys. B 433 421
Masiero A, Montanino D and Peloso M 2000 Astropart. Phys. 12 351
[51] Turner M S and White M 1997 Phys. Rev. D 56 4439
Chiba T, Sugiyama N and Nakamura T 1997 Mon. Not. R. Astron. Soc. 289 L5
[52] Frieman J A and Waga I 1998 Phys. Rev. D 57 4642
[53] Peebles P J E and Ratra B 1988 Astrophys. J. 325 L17
Ratra B and Peebles P J E 1988 Phys. Rev. D 37 3406

210
[54]
[55]
[56]
[57]
[58]
[59]
[60]

[61]

[62]

[63]
[64]

[65]
[66]
[67]
[68]

Dark matter and particle physics
Liddle A R and Scherrer R J 1999 Phys. Rev. D 59 023509
Wetterich C 1988 Nucl. Phys. B 302 668
Copeland E J, Liddle A R and Wands D 1998 Phys. Rev. D 57 4686
Ferreira P G and Joyce M 1997 Phys. Rev. Lett. 79 4740
Ferreira P G and Joyce M 1998 Phys. Rev. D 58 023503
Zlatev I, Wang L and Steinhardt P J 1999 Phys. Rev. Lett. 82 896
Steinhardt P J, Wang L and Zlatev I 1999 Phys. Rev. D 59 123504
Baccigalupi C and Perrotta F 1999 Phys. Rev. D 59 123508
Hu W, Eisenstein D J, Tegmark M and White M 1999 Phys. Rev. D 59 023512
Cooray A R and Huterer D 1999 Astrophys. J. 513 L95
Wang L and Steinhardt P J 1998 Astrophys. J. 508 483
Hui L 1999 Astrophys. J. 519 L9
Ratra B, Stompor R, Ganga K, Rocha G, Sugiyama N and Górski K M 1999
Astrophys. J. 517 549
van de Bruck C and Priester W 1998 Preprint astro-ph/9810340
Alcaniz J S and Lima J A S 1999 Astrophys. J. 521 L87
Wang L, Caldwell R R, Ostriker J P and Steinhardt P J 2000 Astrophys. J. 530 17
Huey G, Wang L, Dave R, Caldwell R R and Steinhardt P J 1999 Phys. Rev. D 59
063005
Perlmutter S, Turner M S and White M 1999 Phys. Rev. Lett. 83 670
Chiba T, Sugiyama N and Nakamura T 1998 Mon. Not. R. Astron. Soc. 301 72
Huterer D and Turner M S 2000 Phys. Rev. D 60 081301
Nakamura T and Chiba T 1999 Mon. Not. R. Astron. Soc. 306 696
Chiba T and Nakamura T 1998 Prog. Theor. Phys. 100 1077
Binétruy P 1999 Phys. Rev. D 60 063502
Masiero A, Pietroni M and Rosati F 2000 Phys. Rev. D 61 023504
Frieman J A, Hill C T, Stebbins A and Waga I 1995 Phys. Rev. Lett. 75 2077
Choi K 2000 Phys. Rev. D 62 043509
Kim J E 1999 JHEP 9905 022
Kolda C and Lyth D H 1999 Phys. Lett. B 458 197
Brax P and Martin J 1999 Phys. Lett. B 468 40
Carroll S M 1998 Phys. Rev. Lett. 81 3067
Taylor T R, Veneziano G and Yankielowicz S 1983 Nucl. Phys. B 218 493
Affleck I, Dine M and Seiberg N 1983 Phys. Rev. Lett. 51 1026
Affleck I, Dine M and Seiberg N 1984 Nucl. Phys. B 241 493
For a pedagogical introduction, see also: Peskin M E 1997 Preprint hep-th/9702094,
TASI 96 lectures

Chapter 6
Supergravity and cosmology
Renata Kallosh
Department of Physics, Stanford University, Stanford, USA

6.1 M/string theory and supergravity
Supergravity is a low-energy limit of a fundamental M/string theory. At present
there is no well-established M/string theory cosmology. However, there are
some urgent issues in cosmology which require a knowledge of the fundamental
theory. Those issues are related to expanding universe, dark matter, inflation,
creation of particles after inflation, etc. The basic problem is that general
relativity which is required for explanation of the cosmology and an expanding
universe is not yet combined with any relativistic quantum theory and particle
physics to the extent in which a full description of the early universe would
be possible. Superstring theory offers a consistent theory of quantum gravity
at least at the level of the string theory perturbation theory in ten-dimensional
target space. The non-perturbative string theory which includes the D-branes is
much less understood, since these objects are charged under so-called Ramond–
Ramond charges which can be incorporated only at the non-perturbative level.
The main attempts during the last few years have been focused on understanding
the M-theory, which represents a string theory at strong coupling, when an
additional dimension is decompactified. M-theory has as a low-energy limit the
11-dimensional supergravity and has two types of extended objects: two-branes
and five-branes.
The radical aspect of major attempts to construct quantum gravity is the
concept that the spacetime x µ = {t, x} is not fundamental. The coordinates x µ are
not labels but fields which are defined by the dynamics of the the world-volume of
a p-brane so that they depend on world-volume coordinates, x µ (σ 0 , σ 1 , . . . , σ p ).
A two-dimensional object, a string is an one-brane with x µ (σ 0 , σ 1 ), a two-brane
is a three-dimensional object with x µ (σ 0 , σ 1 , σ 2 ), a four-dimensional object
called a three-brane and has x µ (σ 0 , σ 1 , σ 2 , σ 3 ), etc. M-theory/string theory
211

212

Supergravity and cosmology

includes a theory of branes of various dimensions. The fields x µ (σ ) have their
own dynamics. The zero modes of the excitations of such extended objects are
µ
coordinates of spacetime, x µ (σ ) = x constant + · · ·. Thus the concept of spacetime
is an approximation to a full quantum theory of gravity!
Supergravity (gravity + supersymmetry) may be viewed as an approximate
effective description of a fundamental theory when the dependence on coordinates
of the world-volume is ignored. The smallest theory of supergravity includes
two types of fields, the graviton and the gravitino. Supergravity interacting with
matter multiplets includes also scalars, spinors and vectors. All these fields are
functions of the usual spacetime coordinates t, x in a four-dimensional spacetime.
The fundamental M-theory, which should encompass both supergravity and string
theory, at present experiences rapid changes. Over the last few years M-theory
and string theory focused its main attention on the superconformal theories
and adS/CFT (anti-de Sitter/conformal field theory) correspondence [1]. It has
been discovered that IIB string theory on ad S5 × S 5 is related to SU (2, 2|4)
superconformal symmetry. In particular, one finds the SU (2, 2|1) superconformal
algebra from the anti-de Sitter compactification of the string theory with onequarter of the unbroken supersymmetry. These recent developments in M-theory
and non-perturbative string theory suggest that we should take a fresh look at the
superconformal formulation underlying the supergravity.
The ‘phenomenological supergravity’ based on the most general N = 1
supergravity [2] has an underlying superconformal structure. This has been
known for a long time but only recently the complete most general N = 1
gauge theory superconformally coupled to supergravity was introduced [4]. The
theory has local SU (2, 2|1) symmetry and no dimensional parameters. The phase
of this theory with spontaneously broken conformal symmetry gives various
formulations of N = 1 supergravity interacting with matter, depending on the
choice of the R-symmetry fixing.
The relevance of supergravity to cosmology is that it gives a framework of
an effective field theory in the background of the expanding universe and timedependent scalar fields. Let us remind here that the early universe is described by
an FRW metric which can be written in a form which is conformal to a flat metric:
ds 2 = a 2(η)[−dη2 + γi j dx i dx j ].

(6.1)

This fact leads to an interest in the superconformal properties of supergravity.

6.2 Superconformal symmetry, supergravity and cosmology
The most general four-dimensional N = 1 supergravity [2] describes a
supersymmetric theory of gravity interacting with scalars, spinors and vectors
of a supersymmetric gauge theory. It is completely defined by the choice of the
three functions: the superpotential W [φ] and the vector coupling fab [φ] which
are holomorphic functions of the scalar fields (depend on φ i and do not depend

Superconformal symmetry, supergravity and cosmology

213

on φi∗ ) and the Kähler potential K [φ, φ∗]. These functions from the perspective
of supergravity are arbitrary. One may hope that they will be defined eventually
from the fundamental M/string theory.
The potential V of the scalar fields is given by
MP−2 e K [−3W W ∗ + (Di W )g −1 i j (D j W ∗ )] + 12 (Re( f )αβ )D α D β ,

(6.2)

here D α are the D-components of the vector superfields, which may take some
non-vanishing values. The metric of the Kähler space, gi j which depends on
φ, φ∗, is the metric of the moduli space which defines the kinetic term for the
scalar fields:
(6.3)
gi j ∂µ φ i ∂ µ φ ∗j .
The properties of the Kähler space in M/string theory are related to the Calabi–
Yau spaces on which the theory is compactified to four dimensions.
One of the problems related to the gravitino is the issue of the conformal
invariance of the gravitino and the possibility of non-thermal gravitino production
in the early universe.
Many observable properties of the universe are, to a large extent, determined
by the underlying conformal properties of the fields. One may consider inflaton
scalar field(s) φ which drive inflation, inflaton fluctuations which generate
cosmological metric fluctuations, gravitational waves generated during inflation,
photons in the cosmic microwave background (CMB) radiation which (almost)
freely propagate from the last scattering surface, etc. If the conformal properties
of any of these fields were different, the universe would also look quite different.
For example, the theory of the usual massless electromagnetic field is conformally
invariant. This implies, in particular, that the strength of the magnetic field in the
universe decreases as a −2 (η). As a result, all vector fields become exponentially
small after inflation. Meanwhile the theory of the inflaton field(s) should not be
conformally invariant, because otherwise these fields would rapidly disappear and
inflation would never happen.
Superconformal supergravity is particularly suitable to study the conformal
properties of various fields, because in this framework all fields initially are
conformally covariant; this invariance becomes spontaneously broken only when
one uses a particular gauge which requires that some combination of scalar fields
becomes equal to MP2 .
The issue of conformal invariance of the gravitino remained rather obscure
for a long time. One could argue that a massless gravitino should be conformally
invariant. Once we introduce a scalar field driving inflation, the gravitino acquires
a mass m 3/2 = e K /2 |W |/MP2 . Thus, one could expect that the conformal
invariance of gravitino equations should be broken only by the small gravitino
mass m 3/2, which is suppressed by the small gravitational coupling constant
MP−2 . This is indeed the case for the gravitino component with helicity ±3/2.
However, breaking of conformal invariance for the gravitino component with
helicity ±1/2, which appears due to the super-Higgs effect, is much stronger.

214

Supergravity and cosmology

In the first approximation in the weak gravitational coupling, it is related to the
chiral fermion mass scale [3].
This locally superconformal theory is useful for describing the physics of the
early universe with a conformally flat FRW metric.
Superconformal theory underlying supergravity has no dimensional
parameters and one extra chiral superfield, the conformon. This superfield can
be gauged away using local conformal symmetry and S-supersymmetry. The
mechanism can be explained using a simple example: an arbitrary gauge theory
with Yang–Mills fields Wµ coupled to fermions λ and gravity:

√
1 2
S conf =
d4 x g( 12 (∂µ φ)(∂ν φ)g µν − 12
φ R
−

1
4

Tr Fµν g µρ g νσ Fρσ − 12 λ̄γ µ Dµ λ).

(6.4)

The field φ is a conformon. The last two terms in the action represent superYang–Mills theory coupled to gravity. The action is conformal invariant under
the following local transformations:
gµν = e−2σ (x) gµν ,

φ = eσ (x)φ,

Wµ = Wµ ,

λ = e 2 σ (x) λ. (6.5)
3

The gauge symmetry (6.5)
√ with one local gauge parameter can be gauge fixed.
If we choose the φ = 6MP gauge, the φ-terms in (6.4) reduce to the Einstein
action, which is no longer conformally invariant:

√
conf
Sg.f. ∼ d4 x g(− 12 MP2 R − 14 Fµν g µρ g νσ Fρσ + 12 λ̄γ µ Dµ λ).
(6.6)
√
Here MP ≡ MPlanck / 8π ∼ 2 × 1018 GeV. In this action, the transformation
(6.5) no longer leaves the Einstein action invariant. The R-term transforms
with derivatives of σ (x), which in the action (6.4) were compensated by the
kinetic term of the compensator field. However, the actions of the Yang–
Mills sector of the theory, i.e. spin- 12 and spin-1 fields interacting with gravity,
remain conformally invariant. Only the conformal properties of the gravitons are
affected by the removal of the compensator field. A supersymmetric version of
this mechanism requires adding a few more symmetries, so that the SU (2, 2|1)
symmetric theory is constructed. The non-conformal properties of the gravitino
can be followed from this starting point, as shown in [4].
Few applications of superconformal theory to cosmology include the study
of (i) particle production after inflation, in particular the study of the nonconformal helicity ±1/2 states of gravitino; (ii) the super-Higgs effect in
cosmology and the derivation of the equations for the gravitino interacting with
any number of chiral and vector multiplets in the gravitational background with
varying scalar fields; and (iii) the weak coupling limit of supergravity MP → ∞
and gravitino–goldstino equivalence. This explains why gravitino production in
the early universe in general is not suppressed in the limit of weak gravitational
coupling.

Gravitino production after inflation

215

6.3 Gravitino production after inflation
During the last couple of years there has been a growing interest in understanding
gravitino production in the early universe [3, 14]. The general consensus is that
gravitinos can be produced during pre-heating after inflation due to a combined
effect of interactions with an oscillating inflaton field and because the helicity
±1/2 gravitino have equations of motion which break conformal invariance. In
general the probability of gravitino production is not suppressed by the small
gravitational coupling. This may lead to a copious production of gravitinos
after inflation. The efficiency of the new non-thermal mechanism of gravitino
production is very sensitive to the choice of the underlying theory. This may put
strong constraints on certain classes of inflationary models.
A formal reason why the effect may be strong even at MP → ∞ is
the following: in Minkowski space the constraint which the massive gravitino
satisfies has the form
γ µ ψµ = 0.

(6.7)

In an expanding universe, the analogue of equation (6.7) looks as follows:
γ 0 ψ0 − Âγ i ψi = 0

(6.8)

where, in the limit MP → ∞,
Â =

p
2 Ẇ
+ γ0
,
ρ
ρ

| Â|2 = 1.

(6.9)

Matrix Â rotates twice during each oscillation of the field φ. The non-adiabaticity
of the gravitino field ψ0 (related to helicity ±1/2 is determined not by the mass
of the gravitino but by the mass of the chiral fermion µ = Wφφ . This equation
was obtained in the framework of a simple model of the supergravity theory
interacting with one chiral multiplet. The gauge-fixing of the spontaneously
broken supersymmetry was relatively easy, the only one available in the model
chiral fermion, a goldstino field, was chosen to vanish and the massive gravitino
was described by helicity ±3/2 as well as helicity ±1/2 states.
A physical reason for gravitino production is a gravitino–goldstino
equivalence theorem which, however, had to be properly understood in the
cosmological context.
One of the major problems with studies of gravitino production after inflation
was to consider the theories with few chiral multiplets. It become clear that one
cannot simply apply the well-known super-Higgs mechanism of supergravity in
the flat background to the situation in which we have a curved metric of the early
universe.

216

Supergravity and cosmology

6.4 Super-Higgs effect in cosmology
We would like to choose a gauge in which a goldstino equals zero. The question
is which field is this goldstino: we start with the gravitino ψµ and some number
of left- and right-handed chiral fermions χ i , χi . In the past, this has been sought
for constant backgrounds [2], but in cosmological applications the scalar fields
are time-dependent in the background. Therefore we need a modification.
In the action there are a few terms where gravitinos mix with the other
fermions, and these as well as the supersymmetry transformations should give us
the possibility of finding the correct goldstino in the cosmological time-dependent
background. We want to obtain a combination whose variation is always non-zero
for spontaneously broken supersymmetry. This leads to the following definition
of a goldstino:
(6.10)
υ = ξ †i χi + ξi† χ i + 12 iγ5 Dα λα ,
where the λα are gauginos, the Dα are auxiliary fields from the vector multiplets
and
ξ †i ≡ e K /2 D i W − γ0 g j i φ̇ j ,

ξi† ≡ e K /2 Di W − γ0 gi j φ̇ j .

(6.11)

The goldstino defined here differs from the one in the flat background by the
presence of the time-dependent derivatives of the scalar fields.
Goldstino is non-vanishing in the vacuum supersymmetry transformation:
δυ = − 32 (H 2 + m 23/2).
Here H is the Hubble ‘constant’:
 2
ρ
ȧ
= H2 =
.
a
3MP2

(6.12)

(6.13)

This has important implications.
First of all, it shows that, in a
conformally flat universe (6.1), the parameter α is strictly positive. To avoid
misunderstandings, we should note that, in general, one may consider situations in
which the energy density ρ is negative. The famous example is anti-de Sitter space
with a negative cosmological constant. However, in the context of inflationary
cosmology, the energy density never can turn negative, so anti-de Sitter space
cannot appear. The reason is that inflation makes the universe almost exactly flat.
As a result, the term k/a 2 drops out from the Einstein equation for the scale factor
independently of whether the universe is closed, open or flat. Then gradually the
energy density decreases, but it can never become negative even if a negative
cosmological constant is present, as in anti-de Sitter space. Indeed, the equation
 2
ρ
ȧ
=
a
3MP2

MP → ∞ limit

217

implies that as soon as the energy density becomes zero, expansion stops.
Then the universe recollapses, and the energy density becomes positive again.
This implies that supersymmetry is always broken. The symmetry breaking is
associated, to an equal extent, with the expansion of the universe and with the
non-vanishing gravitino mass (the term (H 2 + m 23/2 ). This is an interesting result
because usually supersymmetry breaking is associated with the existence of the
gravitino mass. Here we see that, in an expanding universe, the Hubble parameter
H plays an equally important role.
The progress achieved in understanding the super-Higgs effect in an
expanding universe has allowed us to find the equations for the gravitino in the
most general theory of supergravity interacting with chiral and vector multiplets
[4]. Analysis of these equations in various inflationary models and the estimates
of the scale of gravitino production remains to be done.
Consider, for example, the hybrid inflation model. In this model all coupling
constants are of order 10−1 , so there should be no suppression of the production
of chiral fermions as compared to the other particles. One can expect, therefore,
that
n 3/2
(6.14)
∼ 10−1 –10−2 .
s
This would violate the cosmological bound by 13 orders of magnitude! However,
one should check whether these gravitinos will survive until the end or turn into
the usual fermions.
Thus supergravity theory and its underlying superconformal structures
provide the framework for studies of the production of particles in
supersymmetric theories in the early universe.

6.5

MP → ∞ limit

The complete equations of motion for the gravitino in a cosmological background
were derived in [4] with an account of the gravitational effects. However,
in [11] some part of these equations, corresponding to the vanishing Hubble
constant and vanishing gravitino mass, was derived in the framework of a gauge
theory, i.e. from rigid supersymmetric theory without gravity. To find the
relation between these two equations one has to understand how to take the limit
MP → ∞ in supergravity. This is a very subtle issue, if one starts with the
fields of phenomenological supergravity. One has to do various rescaling of the
fields with different powers of the MP to be able to compare these two sets of
equations. Surprisingly, the full set of rescalings reproduces exactly the fields of
the underlying superconformal theory. These are the fields which survive in the
weak coupling limit of supergravity.
Thus at present there are indications that a description of the cosmology
of the early universe may be achieved in the framework of superconformal
theory only after the gauge-fixing of conformal symmetry is equivalent to

218

Supergravity and cosmology

supergravity. The super-Higgs mechanism in cosmology and the goldstino–
gravitino equivalence theorem have a clear origin in this SU (2, 2|1) symmetric
theory of gravity.

References
[1] Maldacena J 1998 The large N limit of superconformal field theories and
supergravity Adv. Theor. Math. Phys. 2 231 (hep-th/9711200)
[2] Cremmer E, Ferrara S, Girardello L and Van Proeyen A 1983 Nucl. Phys. B 212 413
[3] Kallosh R, Kofman L, Linde A and Van Proeyen A 2000 Gravitino production after
inflation Phys. Rev. D 61 103503 (hep-th/9907124)
[4] Kallosh R, Kofman L, Linde A and Van Proeyen A 2000 Superconformal symmetry,
supergravity and cosmology Class. Quantum Grav. 17 4269
[5] Moroi T 1995 Effects of the gravitino on the inflationary universe PhD Thesis
Tohoku, Japan (hep-ph/9503210)
[6] Maroto A L and Mazumdar A 2000 Production of spin 3/2 particles from vacuum
fluctuations Phys. Rev. Lett. 84 1655 (hep-ph/9904206)
[7] Lemoine M 1999 Gravitational production of gravitinos Phys. Rev. D 60 103522
(hep-ph/9908333)
[8] Giudice G F, Tkachev I and Riotto A 1999 Non-thermal production of dangerous
relics in the early universe JHEP 9908 009 (hep-ph/9907510)
[9] Lyth D H 1999 Abundance of moduli, modulini and gravitinos produced by the
vacuum fluctuation Phys. Lett. B 469 69 (hep-ph/9909387)
[10] Lyth D H 2000 The gravitino abundance in supersymmetric ‘new’ inflation models
Phys. Lett. B 488 417
[11] Giudice G F, Riotto A and Tkachev I 1999 Thermal and non-thermal production of
gravitinos in the early universe JHEP 9911 036 (hep-ph/9911302)
[12] Maroto A L and Pelaez J R 2000 The equivalence theorem and the production of
gravitinos after inflation Phys. Rev. D 62 023518
[13] Lyth D H 2000 Late-time creation of gravitinos from the vacuum Phys. Lett. B 476
356 (hep-ph/9912313)
[14] Bastero-Gil M and Mazumdar A 2000 Gravitino production in hybrid inflationary
models Phys. Rev. D 62 083510

Chapter 7
The cosmic microwave background
Arthur Kosowsky
Rutgers University, Piscataway, New Jersey, USA

It is widely accepted that the field of cosmology is entering an era dubbed
‘precision cosmology’. Data directly relevant to the properties and evolution of
the universe are flooding in by the terabyte (or soon will be). Such vast quantities
of data were the purview only of high-energy physics just a few years ago; now
expertise from this area is being coopted by some astronomers to help deal with
our wealth of information. In the past decade, cosmology has gone from a datastarved science in which often highly speculative theories went unconstrained to
a data-driven pursuit where many models have been ruled out and the remaining
‘standard cosmology’ will be tested with stringent precision.
The cosmic microwave background (CMB) radiation is at the centre of this
revolution. The radiation present today as a 2.7 K thermal background originated
when the universe was denser by a factor of 109 and younger by a factor of
around 5 × 104 . The radiation provides the most distant direct image of the
universe we can hope to see, at least until gravitational radiation becomes a useful
astronomical data source. The microwave background radiation is extremely
uniform, varying in temperature by only a few parts in 105 over the sky (apart
from an overall dipole variation arising from our peculiar motion through the
microwave background’s rest frame); its departure from a perfect blackbody
spectrum has yet to be detected.
The very existence of the microwave background provides crucial support
for the hot big bang cosmological model: the universe began in a very hot, dense
state from which it expanded and cooled. The microwave background visible
today was once in thermal equilibrium with the primordial plasma of the universe,
and the universe at that time was highly uniform. Crucially, the universe could
not have been perfectly uniform at that time or no structures would have formed
subsequently. The study of small temperature and polarization fluctuations in
the microwave background, reflecting small variations in density and velocity
219

220

The cosmic microwave background

in the early universe, have the potential to provide the most precise constraints
on the overall properties of the universe of any data source. The reasons are
that (1) the universe was very simple at the time imaged by the microwave
background and is extremely well described by linear perturbation theory around
a completely homogeneous and isotropic cosmological spacetime; and (2) the
physical processes relevant at that time are all simple and very well understood.
The microwave background is essentially unique among astrophysical systems in
these regards.
The goal behind this chapter is to provide a qualitative description of
the physics of the microwave background, an appreciation for the microwave
background’s cosmological importance, and an understanding of what kinds of
constraints may be placed on cosmological models. It is not intended to be a
definitive technical reference to the microwave background. Unfortunately, such
a reference does not really exist at this time, but I have attempted to provide
pedagogically useful references to other literature. I have also not attempted to
give a complete bibliography; please do not consider this article to give definitive
references to any topics mentioned. A recent review of the microwave background
with a focus on potential particle physics constraints is Kamionkowski and
Kosowsky (1999). A more general review of the microwave background and
large-scale structure with references to many early microwave background articles
is White et al (1994).

7.1 A brief historical perspective
The story of the serendipidous discovery of the microwave background in 1965
is widely known, so I will only briefly summarize it here. A recent book by the
historian of science Helge Kragh (1996) is a careful and authoritative reference
on the history of cosmology, from which much of the information in this section
was obtained. Arno Penzias and Robert Wilson, two radio astronomers at Bell
Laboratories in Crawford, New Jersey, were using a sensitive microwave horn
radiometer originally intended for talking to the early Telstar telecommunications
satellites. When Bell Laboratories decided to get out of the communications
satellite business in 1963, Penzias and Wilson began to use the radiometer to
measure radio emission from the Cassiopeia A supernova remnant. They detected
a uniform noise source, which was assumed to come from the apparatus. But after
many months of checking the antenna and the electronics (including removal of a
bird’s nest from the horn), they gradually concluded that the signal might actually
be coming from the sky. When they heard about a talk given by P J E Peebles
of Princeton predicting a 10 K blackbody cosmological background, they got
in touch with the group at Princeton and realized that they had detected the
cosmological radiation. At the time, Peebles was collaborating with Dicke,
Roll and Wilkinson in a concerted effort to detect the microwave background.
The Princeton group wound up confirming the Bell Laboratories discovery a

A brief historical perspective

221

few months later. Penzias and Wilson published their result in a brief paper
with the unassuming title of ‘A Measurement of Excess Antenna Temperature
at λ = 7.3 cm’ (Penzias and Wilson 1965); a companion paper by the Princeton
group explained the cosmological significance of the measurement (Dicke et al
1965). The microwave background detection was a stunning success of the hot
big bang model, which to that point had been well outside the mainstream of
theoretical physics. The following years saw an explosion of work related to the
big bang model of the expanding universe. To the best of my knowledge, the
Penzias and Wilson paper was the second-shortest ever to garner a Nobel Prize,
awarded in 1978. (Watson and Crick’s renowned double helix paper wins by a
few lines.)
Less well known is the history of earlier probable detections of the
microwave background which were not recognized as such. Tolman’s classic
monograph on thermodynamics in an expanding universe was written in 1934, but
a blackbody relic of the early universe was not predicted theoretically until 1948
by Alpher and Herman, a by-product of their pioneering work on nucleosynthesis
in the early universe. Prior to this, Andrew McKellar (1940) had observed the
population of excited rotational states of CN molecules in interstellar absorption
lines, concluding that it was consistent with being in thermal equilibrium with
a temperature of around 2.3 K. Walter Adams also made similar measurements
(1941). Its significance was unappreciated and the result essentially forgotten,
possibly because the Second World War had begun to divert much of the world’s
physics talent towards military problems.
Alpher and Herman’s prediction of a 5 K background contained no
suggestion of its detectability with available technology and had little impact.
Over the next decade, George Gamow and collaborators, including Alpher and
Herman, made a variety of estimates of the background temperature which
fluctuated between 3 and 50 K (e.g. Gamow 1956). This lack of a definitive
temperature might have contributed to an impression that the prediction was less
certain than it actually was, because it aroused little interest among experimenters
even though microwave technology had been highly developed through radar
work during the war. At the same time, the incipient field of radio astronomy
was getting started. In 1955, Emile Le Roux undertook an all-sky survey at
a wavelength of λ = 33 cm, finding an isotropic emission corresponding to
a blackbody temperature of T = 3 ± 2 K (Denisse et al 1957). This was
almost certainly a detection of the microwave background, but its significance
was unrealized. Two years later, T A Shmaonov observed a signal at λ = 3.2 cm
corresponding to a blackbody temperature of 4 ± 3 K independent of direction
(see Sharov and Novikov 1993, p 148). The significance of this measurement was
not realized, amazingly, until 1983! (Kragh 1996). Finally in the early 1960s the
pieces began to fall into place: Doroshkevich and Novikov (1964) emphasized
the detectability of a microwave blackbody as a basic test of Gamow’s hot big
bang model. Simultaneously, Dicke and collaborators began searching for the
radiation, prompted by Dicke’s investigations of the physical consequences of

222

The cosmic microwave background

the Brans–Dicke theory of gravitation. They were soon scooped by Penzias and
Wilson’s discovery.
As soon as the microwave background was discovered, theorists quickly
realized that fluctuations in its temperature would have fundamental significance
as a reflection of the initial perturbations which grew into galaxies and clusters.
Initial estimates of the amplitude of temperature fluctuations were a part in
a hundred; this level of sensitivity was attained by experimenters after a few
years with no observed fluctuations. Thus began a quarter-century chase
after temperature anisotropies in which the theorists continually revised their
estimates of the fluctuation amplitude downwards, staying one step ahead of
the experimenters’ increasingly stringent upper limits. Once the temperature
fluctuations were shown to be less than a part in a thousand, baryonic density
fluctuations did not have time to evolve freely into the nonlinear structures visible
today, so theorists invoked a gravitationally dominant DM component (structure
formation remains one of the strongest arguments in favour of non-baryonic DM).
By the end of the 1980s, limits on temperature fluctuations were well below a part
in 104 and theorists scrambled to reconcile standard cosmology with this small
level of primordial fluctuations. Ideas like late-time phase transitions at redshifts
less than z = 1000 were taken seriously as a possible way to evade the microwave
background limits (see, e.g., Jaffe et al 1990). Finally, the COBE satellite detected
fluctuations at the level of a few parts in 105 (Smoot et al 1990), just consistent
with structure formation in inflation-motivated Cold Dark Matter cosmological
models. The COBE results were soon confirmed by numerous ground-based and
balloon measurements, sparking the intense theoretical and experimental interest
in the microwave background over the past decade.

7.2 Physics of temperature fluctuations
The minute temperature fluctuations present in the microwave background
contain a wealth of information about the fundamental properties of the universe.
In order to understand the reasons for this and the kinds of information available,
an appreciation of the underlying physical processes generating temperature
and polarization fluctuations is required. This section and the following one
give a general description of all basic physics processes involved in producing
microwave background fluctuations.
First, one practical matter. Throughout this chapter, common cosmological
units will be employed in which h̄ = c = kb = 1. All dimensionful quantities
can then be expressed as powers of an energy scale, commonly taken as GeV.
In particular, length and time both have units of [GeV]−1 , while Newton’s
constant G has units of [GeV]−2 since it is defined as equal to the square of
the inverse Planck mass. These units are very convenient for cosmology, because
many problems deal with widely varying scales simultaneously. For example,
any computation of relic particle abundances (e.g. primordial nucleosynthesis)

Physics of temperature fluctuations

223

involves both a quantum mechanical scale (the interaction cross section) and a
cosmological scale (the time scale for the expansion of the universe). Conversion
between these cosmological units and physical (cgs) units can be achieved by
inserting needed factors of h̄, c, and kb . The standard textbook by Kolb and
Turner (1990) contains an extremely useful appendix on units.
7.2.1 Causes of temperature fluctuations
Blackbody radiation in a perfectly homogeneous and isotropic universe, which
is always adopted as a zeroth-order approximation, must be at a uniform
temperature, by assumption. When perturbations are introduced, three elementary
physical processes can produce a shift in the apparent blackbody temperature of
the radiation emitted from a particular point in space. All temperature fluctuations
in the microwave background are due to one of the following three effects.
The first is simply a change in the intrinsic temperature of the radiation
at a given point in space. This will occur if the radiation density increases
via adiabatic compression, just as with the behaviour of an ideal gas. The
fractional temperature perturbation in the radiation just equals the fractional
density perturbation.
The second is equally simple: a Doppler shift if the radiation at a particular
point is moving with respect to the observer. Any density perturbations within the
horizon scale will necessarily be accompanied by velocity perturbations. The
induced temperature perturbation in the radiation equals the peculiar velocity
(in units of c, of course), with motion towards the observer corresponding to a
positive temperature perturbation.
The third is a bit more subtle: a difference in gravitational potential between
a particular point in space and an observer will result in a temperature shift of
the radiation propagating between the point and the observer due to gravitational
redshifting. This is known as the Sachs–Wolfe effect, after the original paper
describing it (Sachs and Wolfe, 1967). This paper contains a completely
straightforward general relativistic calculation of the effect, but the details are
lengthy and complicated. A far simpler and more intuitive derivation has been
given by Hu and White (1997) making use of gauge transformations. The Sachs–
Wolfe effect is often broken into two parts, the usual effect and the so-called
Integrated Sachs–Wolfe effect. The latter arises when gravitational potentials are
evolving with time: radiation propagates into a potential well, gaining energy
and blueshifting in the process. As it climbs out, it loses energy and redshifts,
but if the depth of the potential well has increased during the time the radiation
propagates through it, the redshift on exiting will be larger than the blueshift on
entering, and the radiation will gain a net redshift, appearing cooler than it started
out. Gravitational potentials remain constant in time in a matter–dominated
universe, so to the extent the universe is matter dominated during the time the
microwave background radiation freely propagates, the Integrated Sachs–Wolfe
effect is zero. In models with significantly less than critical density in matter (i.e.

224

The cosmic microwave background

the currently popular CDM models), the redshift of matter–radiation equality
occurs late enough that the gravitational potentials are still evolving significantly
when the microwave background radiation decouples, leading to a non-negligible
Integrated Sachs–Wolfe effect. The same situation also occurs at late times in
these models; gravitational potentials begin to evolve again as the universe makes
a transition from matter domination to either vacuum energy domination or a
significantly curved background spatial metric, giving an additional Integrated
Sachs–Wolfe contribution.
7.2.2 A formal description
The early universe at the epoch when the microwave background radiation begins
propagating freely, around a redshift of z = 1100, is a conceptually simple place.
Its constituents are ‘baryons’ (including protons, helium nuclei and electrons,
even though electrons are not baryons), neutrinos, photons and DM particles.
The neutrinos and DM can be treated as interacting only gravitationally since
their weak interaction cross sections are too small at this energy scale to be
dynamically or thermodynamically relevant. The photons and baryons interact
electromagnetically, primarily via Compton scattering of the radiation from the
electrons. The typical interaction energies are low enough for the scattering to
be well approximated by the simple Thomson cross section. All other scattering
processes (e.g. Thomson scattering from protons, Rayleigh scattering of radiation
from neutral hydrogen) have small enough cross-sections to be insignificant, so
we have four species of matter with only one relevant (and simple) interaction
process among them. The universe is also very close to being homogeneous and
isotropic, with small perturbations in density and velocity on the order of a part in
105 . The tiny size of the perturbations guarantees that linear perturbation theory
around a homogeneous and isotropic background universe will be an excellent
approximation.
Conceptually, the formal description of the universe at this epoch is quite
simple. The unperturbed background cosmology is described by the Friedmann–
Robertson–Walker (FRW) metric, and the evolution of the cosmological scale
factor a(t) in this metric is given by the Friedmann equation (see the lectures
by Peacock in this volume). The evolution of the free electron density n e is
determined by the detailed atomic physics describing the recombination of neutral
hydrogen and helium; see Seager et al (2000) for a detailed discussion. At a
temperature of around 0.5 eV, the electrons combine with the protons and helium
nuclei to make neutral atoms. As a result, the photons cease Thomson scattering
and propagate freely to us. The microwave background is essentially an image of
the ‘surface of last scattering’. Recombination must be calculated quite precisely
because the temperature and thickness of this surface depend sensitively on the
ionization history through the recombination process.
The evolution of first-order perturbations in the various energy density
components and the metric are described with the following sets of equations:

Physics of temperature fluctuations
•

•

•

225

The photons and neutrinos are described by distribution functions f (x, p, t).
A fundamental simplifying assumption is that the energy dependence of
both is given by the blackbody distribution. The space dependence is
generally Fourier transformed, so the distribution functions can be written as
'(k, n̂, t), where the function has been normalized to the temperature of the
blackbody distribution and n̂ represents the direction in which the radiation
propagates. The time evolution of each is given by the Boltzmann equation.
For neutrinos, collisions are unimportant so the Boltzmann collision term on
the right hand side is zero; for photons, Thomson scattering off electrons
must be included.
The DM and baryons are, in principle, described by Boltzmann equations
as well, but a fluid description incorporating only the lowest two velocity
moments of the distribution functions is adequate. Thus each is described
by the Euler and continuity equations for their densities and velocities. The
baryon Euler equation must include the coupling to photons via Thomson
scattering.
Metric perturbation evolution and the connection of the metric perturbations
to the matter perturbations are both contained in the Einstein equations.
This is where the subtleties arise. A general metric perturbation has 10
degrees of freedom, but four of these are unphysical gauge modes. The
physical perturbations include two degrees of freedom constructed from
scalar functions, two from a vector, and two remaining tensor perturbations
(Mukhanov et al 1992). Physically, the scalar perturbations correspond
to gravitational potential and anisotropic stress perturbations; the vector
perturbations correspond to vorticity and shear perturbations; and the tensor
perturbations are two polarizations of gravitational radiation. Tensor and
vector perturbations do not couple to matter evolving only under gravitation;
in the absence of a ‘stiff source’ of stress energy, like cosmic defects or
magnetic fields, the tensor and vector perturbations decouple from the linear
perturbations in the matter.

A variety of different variable choices and methods for eliminating the gauge
freedom have been developed. The subject can be fairly complicated. A detailed
discussion and comparison between the Newtonian and synchronous gauges,
along with a complete set of equations, can be found in Ma and Bertschinger
(1995); also see Hu et al (1998). An elegant and physically appealing formalism
based on an entirely covariant and gauge-invariant description of all physical
quantities has been developed for the microwave background by Challinor and
Lasenby (1999) and Gebbie et al (2000), based on earlier work by Ehlers (1993)
and Ellis and Bruni (1989). A more conventional gauge-invariant approach was
originated by Bardeen (1980) and developed by Kodama and Sasaki (1984).
The Boltzmann equations are partial differential equations, which can be
converted to hierarchies of ordinary differential equations by expanding their
directional dependence in Legendre polynomials. The result is a large set of

226

The cosmic microwave background

coupled, first-order linear ordinary differential equations which form a well-posed
initial value problem. Initial conditions must be specified. Generally they are
taken to be so-called adiabatic perturbations: initial curvature perturbations with
equal fractional perturbations in each matter species. Such perturbations arise
naturally from the simplest inflationary scenarios. Alternatively, isocurvature
perturbations can also be considered: these initial conditions have fractional
density perturbations in two or more matter species whose total spatial curvature
perturbation cancels. The issue of numerically determining initial conditions is
discussed later in section 7.4.2.
The set of equations are numerically stiff before last scattering, since
they contain the two widely discrepant time scales: the Thomson scattering
time for electrons and photons and the (much longer) Hubble time.
Initial conditions must be set with high accuracy and an appropriate stiff
integrator must be employed.
A variety of numerical techniques have
been developed for evolving the equations. Particularly important is the
line-of-sight algorithm first developed by Seljak and Zaldarriaga (1996) and
then implemented by them in the publicly available CMBFAST code (see
http://www.sns.ias.edu/∼matiasz/CMBFAST/cmbfast.html).
This discussion is intentionally heuristic and somewhat vague because many
of the issues involved are technical and not particularly illuminating. My main
point is an appreciation for the detailed and precise physics which goes into
computing microwave background fluctuations. However, all of this formalism
should not obscure several basic physical processes which determine the ultimate
form of the fluctuations. A widespread understanding of most of the physical
processes detailed have followed from a seminal paper by Hu and Sugiyama
(1996), a classic of the microwave background literature.
7.2.3 Tight coupling
Two basic time scales enter into the evolution of the microwave background.
The first is the photon scattering time scale ts , the mean time between Thomson
scatterings. The other is the expansion time scale of the universe, H −1, where
H = ȧ/a is the Hubble parameter. At temperatures significantly greater than
0.5 eV, hydrogen and helium are completely ionized and ts  H −1. The
Thomson scatterings which couple the electrons and photons occur much more
rapidly than the expansion of the universe; as a result, the baryons and photons
behave as a single ‘tightly coupled’ fluid. During this period, the fluctuations
in the photons mirror the fluctuations in the baryons. (Note that recombination
occurs at around 0.5 eV rather than 13.6 eV because of the huge photon–baryon
ratio; the universe contains somewhere around 109 photons for each baryon, as
we know from primordial nucleosynthesis. It is a useful exercise to work out the
approximate recombination temperature.)
The photon distribution function for scalar perturbations can be written
as '(k, µ, t) where µ = k̂ · n̂ and the scalar character of the fluctuations

Physics of temperature fluctuations

227

guarantees the distribution cannot have any azimuthal directional dependence.
(The azimuthal dependence for vector and tensor perturbations can also be
included in a similar decomposition). The moments of the distribution are defined
as
∞

(−i)l 'l (k, t)Pl (µ);
(7.1)
'(k, µ, t) =
l=0

sometimes other normalizations are used. Tight coupling implies that 'l = 0 for
l > 1. Physically, the l = 0 moment corresponds to the photon energy density
perturbation, while l = 1 corresponds to the bulk velocity. During tight coupling,
these two moments must match the baryon density and velocity perturbations.
Any higher moments rapidly decay due to the isotropizing effect of Thomson
scattering; this follows immediately from the photon Boltzmann equation.
7.2.4 Free-streaming
In the other regime, for temperatures significantly lower than 0.5 eV, ts  H −1
and photons on average never scatter again until the present time. This is known
as the ‘free-streaming’ epoch. Since the radiation is no longer tightly coupled
to the electrons, all higher moments in the radiation field develop as the photons
propagate. In a flat background spacetime, the exact solution is simple to derive.
After scattering ceases, the photons evolve according to the Liouville equation
' + ikµ' = 0

(7.2)

'(k, µ, η) = e−ikµ(η−η∗ ) '(k, µ, η∗ ),

(7.3)

with the trivial solution

where we have converted to conformal time defined by dη = dt/a(t) and η∗
corresponds to the time at which free-streaming begins. Taking moments of both
sides results in
'l (k, η) = (2l + 1)['0(k, η∗ ) jl (kη − kη∗ ) + '1 (k, η∗ ) jl (kη − kη∗ )]

(7.4)

with jl a spherical Bessel function. The process of free-streaming essentially
maps spatial variations in the photon distribution at the last-scattering surface
(wavenumber k) into angular variations on the sky today (moment l).
7.2.5 Diffusion damping
In the intermediate regime during recombination, ts  H −1. Photons propagate
a characteristic distance L D during this time. Since some scattering is still
occurring, baryons experience a drag from the photons as long as the ionization
fraction is appreciable. A second-order perturbation analysis shows that the result

228

The cosmic microwave background

is damping of baryon fluctuations on scales below L D , known as Silk damping or
diffusion damping. This effect can be modelled by the replacement
'0 (k, η∗ ) → '0 (k, η∗ )e−(k L D )

2

(7.5)

although detailed calculations are needed to define L D precisely. As a result of
this damping, microwave background fluctuations are exponentially suppressed
on angular scales significantly smaller than a degree.
7.2.6 The resulting power spectrum
The fluctuations in the universe are assumed to arise from some random statistical
process. We are not interested in the exact pattern of fluctuations we see from our
vantage point, since this is only a single realization of the process. Rather, a
theory of cosmology predicts an underlying distribution, of which our visible sky
is a single statistical realization. The most basic statistic describing fluctuations
is their power spectrum. A temperature map on the sky T (n̂) is conventionally
expanded in spherical harmonics,
∞ 
l

T (n̂)
T
=1+
a(lm)
Y(lm) (n̂)
T0

(7.6)

l=1 m=−l


1
∗
(n̂)
(7.7)
dn̂ T (n̂)Y(lm)
T0
are the temperature multipole coefficients and T0 is the mean CMB temperature.
The l = 1 term in equation (7.6) is indistinguishable from the kinematic dipole
and is normally ignored. The temperature angular power spectrum Cl is then
given by
T∗ T
a(l m )  = ClT δll δmm ,
(7.8)
a(lm)
where

T
a(lm)
=

where the angled brackets represent an average over statistical realizations of the
underlying distribution. Since we have only a single sky to observe, an unbiased
estimator of Cl is constructed as
ĈlT =

l

1
T∗ T
alm
alm .
2l + 1

(7.9)

m=−l

The statistical uncertainty in estimating ClT by a sum of 2l + 1 terms is known as
‘cosmic variance’. The constraints l = l and m = m follow from the assumption
of statistical isotropy: ClT must be independent of the orientation of the coordinate
system used for the harmonic expansion. These conditions can be verified via an
explicit rotation of the coordinate system.
A given cosmological theory will predict ClT as a function of l, which can be
obtained from evolving the temperature distribution function as described earlier.

Physics of polarization fluctuations

229

Figure 7.1. The temperature angular power spectrum for a cosmological model with mass
density 0 = 0.3, vacuum energy density  = 0.7, Hubble parameter h = 0.7, and a
scale-invariant spectrum of primordial adiabatic perturbations.

This prediction can then be compared with data from measured temperature
differences on the sky. Figure 7.1 shows a typical temperature power spectrum
from the inflationary class of models, described in more detail later. The
distinctive sequence of peaks arise from coherent acoustic oscillations in the fluid
during the tight coupling epoch and are of great importance in precision tests of
cosmological models; these peaks will be discussed in section 7.4. The effect
of diffusion damping is clearly visible in the decreasing power above l = 1000.
When viewing angular power spectrum plots in multipole space, keep in mind
that l = 200 corresponds approximately to fluctuations on angular scales of a
degree, and the angular scale is inversely proportional to l. The vertical axis
is conventionally plotted as l(l + 1)ClT because the Sachs–Wolfe temperature
fluctuations from a scale-invariant spectrum of density perturbations appears as
a horizontal line on such a plot.

7.3 Physics of polarization fluctuations
In addition to temperature fluctuations, the simple physics of decoupling
inevitably leads to non-zero polarization of the microwave background radiation

230

The cosmic microwave background

as well, although quite generically the polarization fluctuations are expected to
be significantly smaller than the temperature fluctuations. This section reviews
the physics of polarization generation and its description. For a more detailed
pedagogical discussion of microwave background polarization, see Kosowsky
(1999), from which this section is excerpted.
7.3.1 Stokes parameters
Polarized light is conventionally described in terms of the Stokes parameters,
which are presented in any optics text. If a monochromatic electromagnetic wave
propagating in the z-direction has an electric field vector at a given point in space
given by
E x = ax (t) cos[ω0 t − θx (t)],

E y = a y (t) cos[ω0 t − θ y (t)],

(7.10)

then the Stokes parameters are defined as the following time averages:
I ≡ ax2  + a 2y ;

(7.11)

Q ≡ ax2  − a 2y ;

(7.12)

U ≡ 2ax a y cos(θx − θ y );
V ≡ 2ax a y sin(θx − θ y ).

(7.13)
(7.14)

The averages are over times long compared to the inverse frequency of the wave.
The parameter I gives the intensity of the radiation which is always positive and is
equivalent to the temperature for blackbody radiation. The other three parameters
define the polarization state of the wave and can have either sign. Unpolarized
radiation, or ‘natural light’, is described by Q = U = V = 0.
The parameters I and V are physical observables independent of the
coordinate system, but Q and U depend on the orientation of the x and y axes. If
a given wave is described by the parameters Q and U for a certain orientation of
the coordinate system, then after a rotation of the x–y plane through an angle φ, it
is straightforward to verify that the same wave is now described by the parameters
Q = Q cos(2φ) + U sin(2φ),
U = − Q sin(2φ) + U cos(2φ).

(7.15)

From this transformation it is easy to see that the quantity P 2 ≡ Q 2 + U 2 is
invariant under rotation of the axes, and the angle
α≡

1
U
tan−1
2
Q

(7.16)

defines a constant orientation parallel to the electric field of the wave. The Stokes
parameters are a useful description of polarization because they are additive for
incoherent superposition of radiation; note this is not true for the magnitude or

Physics of polarization fluctuations

231

orientation of polarization. Note that the transformation law in equation (7.15) is
characteristic not of a vector but of the second-rank tensor


1 I + Q U − iV
ρ=
,
(7.17)
2 U + iV I − Q
which also corresponds to the quantum mechanical density matrix for an
ensemble of photons (Kosowsky 1996). In kinetic theory, the photon distribution
function f (x, p, t) discussed in section 7.2.2 must be generalized to ρi j (x, p, t),
corresponding to this density matrix.
7.3.2 Thomson scattering and the quadrupolar source
Non-zero linear polarization in the microwave background is generated around
decoupling because the Thomson scattering which couples the radiation and the
electrons is not isotropic but varies with the scattering angle. The total scattering
cross-section, defined as the radiated intensity per unit solid angle divided by the
incoming intensity per unit area, is given by
2
dσ
3σT 
=
ε̂ · ε̂
d
8π

(7.18)

where σT is the total Thomson cross section and the vectors ε̂ and ε̂ are
unit vectors in the planes perpendicular to the propogation directions which
are aligned with the outgoing and incoming polarization, respectively. This
scattering cross section can give no net circular polarization, so V = 0 for
cosmological perturbations and will not be discussed further. Measurements of
V polarization can be used as a diagnostic of systematic errors or microwave
foreground emission.
It is a straightforward but slightly involved exercise to show that these
relations imply that an incoming unpolarized radiation field with the multipole
expansion equation (7.6) will be Thomson scattered into an outgoing radiation
field with Stokes parameters

π
3σT
a20 sin2 β
(7.19)
Q(n̂) − iU (n̂) =
8πσ B 5
if the incoming radiation field has rotational symmetry around its direction of
propagation, as will hold for individual Fourier modes of scalar perturbations.
Explicit expressions for the general case of no symmetry can be derived in terms
of Wigner D-symbols (Kosowsky 1999).
In simple and general terms, unpolarized incoming radiation will be
Thomson scattered into linearly polarized radiation if and only if the incoming
radiation has a non-zero quadrupolar directional dependence. This single fact
is sufficient to understand the fundamental physics behind polarization of the
microwave background. During the tight-coupling epoch, the radiation field has

232

The cosmic microwave background

only monopole and dipole directional dependences as explained earlier; therefore,
scattering can produce no net polarization and the radiation remains unpolarized.
As tight coupling begins to break down as recombination begins, a quadrupole
moment of the radiation field will begin to grow due to free-streaming of the
photons. Polarization is generated during the brief interval when a significant
quadrupole moment of the radiation has built up, but the scattering electrons have
not yet all recombined. Note that if the universe recombined instantaneously,
the net polarization of the microwave background would be zero. Due to
this competition between the quadrupole source building up and the density of
scatterers declining, the amplitude of polarization in the microwave background
is generically suppressed by an order of magnitude compared to the temperature
fluctuations.
Before polarization generation commences, the temperature fluctuations
have either a monopole dependence, corresponding to density perturbations, or
a dipole dependence, corresponding to velocity perturbations. A straightforward
solution to the photon free-streaming equation (in terms of spherical Bessel
functions) shows that for Fourier modes with wavelengths large compared to a
characteristic thickness of the last-scattering surface, the quadrupole contribution
through the last scattering surface is dominated by the velocity fluctuations
in the temperature, not the density fluctuations. This makes intuitive sense:
the dipole fluctuations can free stream directly into the quadrupole, but the
monopole fluctuations must stream through the dipole first. This conclusion
breaks down on small scales where either monopole or dipole can be the dominant
quadrupole source, but numerical computations show that on scales of interest
for microwave background fluctuations, the dipole temperature fluctuations are
always the dominant source of quadrupole fluctuations at the last scatteringsurface. Therefore, polarization fluctuations reflect mainly velocity perturbations
at last scattering, in contrast to temperature fluctuations which predominantly
reflect density perturbations.
7.3.3 Harmonic expansions and power spectra
Just as the temperature on the sky can be expanded into spherical harmonics,
facilitating the computation of the angular power spectrum, so can the
polarization. The situation is formally parallel, although in practice it is more
complicated: while the temperature is a scalar quantity, the polarization is
a second-rank tensor. We can define a polarization tensor with the correct
transformation properties, equation (7.15), as


1
Q(n̂)
−U (n̂) sin θ
.
(7.20)
Pab (n̂) =
2 −U (n̂) sin θ −Q(n̂) sin2 θ
The dependence on the Stokes parameters is the same as for the density matrix,
equation (7.17); the extra factors are convenient because the usual spherical
coordinate basis is orthogonal but not orthonormal. This tensor quantity must

Physics of polarization fluctuations

233

be expanded in terms of tensor spherical harmonics which preserve the correct
transformation properties. We assume a complete set of orthonormal basis
functions for symmetric trace-free 2 × 2 tensors on the sky,
∞
l
Pab (n̂)   G
G
C
C
=
[a(lm) Y(lm)ab
(n̂) + a(lm)
Y(lm)ab
(n̂)],
T0

(7.21)

l=2 m=−l

where the expansion coefficients are given by

1
G
Gab∗
=
(n̂),
dn̂ Pab (n̂)Y(lm)
a(lm)
T0

1
C
Cab∗
a(lm)
=
(n̂),
dn̂ Pab (n̂)Y(lm)
T0

(7.22)
(7.23)

which follow from the orthonormality properties


G∗
C∗
(n̂)Y(lGab
(
n̂)
=
dn̂ Y(lm)ab
(n̂)Y(lCab
dn̂ Y(lm)ab
m)
m ) (n̂) = δll δmm , (7.24)

G∗
(n̂)Y(lCab
(7.25)
dn̂ Y(lm)ab
m ) (n̂) = 0.
These tensor spherical harmonics are not as exotic as they might sound;
they are used extensively in the theory of gravitational radiation, where they
naturally describe the radiation multipole expansion. Tensor spherical harmonics
are similar to vector spherical harmonics used to represent electromagnetic
radiation fields, familiar from chapter 16 of Jackson (1975). Explicit formulas
for tensor spherical harmonics can be derived via various algebraic and group
theoretic methods; see Thorne (1980) for a complete discussion. A particularly
elegant and useful derivation of the tensor spherical harmonics (along with
the vector spherical harmonics as well) is provided by differential geometry:
the harmonics can be expressed as covariant derivatives of the usual spherical
harmonics with respect to an underlying manifold of a two-sphere (i.e. the sky).
This construction has been carried out explicitly and applied to the microwave
background polarization (Kamionkowski et al 1996).
The existence of two sets of basis functions, labelled here by ‘G’ and ‘C’,
is due to the fact that the symmetric traceless 2 × 2 tensor describing linear
polarization is specified by two independent parameters. In two dimensions, any
symmetric traceless tensor can be uniquely decomposed into a part of the form
A;ab − (1/2)gab A;c c and another part of the form B;ac  c b + B;bc  c a where A
and B are two scalar functions and semicolons indicate covariant derivatives.
This decomposition is quite similar to the decomposition of a vector field into
a part which is the gradient of a scalar field and a part which is the curl of a
vector field; hence we use the notation G for ‘gradient’ and C for ‘curl’. In
fact, this correspondence is more than just cosmetic: if a linear polarization
field is visualized in the usual way with headless ‘vectors’ representing the

234

The cosmic microwave background

amplitude and orientation of the polarization, then the G harmonics describe
the portion of the polarization field which has no handedness associated with it,
while the C harmonics describe the other portion of the field which does have
a handedness (just as with the gradient and curl of a vector field). Note that
Zaldarriaga and Seljak (1997) label these harmonics E and B, with a slightly
different normalization than defined here (see Kamionkowski et al 1996).
T , a G , and a C , which
We now have three sets of multipole moments, a(lm)
(lm)
(lm)
fully describe the temperature/polarization map of the sky. These moments can be
combined quadratically into various power spectra analogous to the temperature
ClT . Statistical isotropy implies that
T∗ T
a(lm)
a(l m )  = ClT δll δmm ,
C∗ C
a(l m )  = ClC δll δmm ,
a(lm)
T∗ C
a(lm)
a(l m )  = ClTC δll δmm ,

G∗ G
a(lm)
a(l m )  = ClG δll δmm ,
T∗ G
a(lm)
a(l m )  = ClTG δll δmm ,
G∗ C
a(lm)
a(l m )  = ClGC δll δmm ,

(7.26)

where the angle brackets are an average over all realizations of the probability
distribution for the cosmological initial conditions. Simple statistical estimators
of the various Cl s can be constructed from maps of the microwave background
temperature and polarization.
For fluctuations with Gaussian random distributions (as predicted by the
simplest inflation models), the statistical properties of a temperature/polarization
map are specified fully by these six sets of multipole moments. In addition,
G
have
the scalar spherical harmonics Y(lm) and the G tensor harmonics Y(lm)ab
C
l
l+1
parity (−1) , but the C harmonics Y(lm)ab have parity (−1) . If the largescale perturbations in the early universe were invariant under parity inversion,
then ClTC = ClGC = 0. So generally, microwave background fluctuations
are characterized by the four power spectra ClT , ClG , ClC , and ClTG . The end
result of the numerical computations described in section 7.2.2 are these power
spectra. Polarization power spectra ClG and ClTG for scalar perturbations in a
typical inflation-like cosmological model, generated with the CMBFAST code
(Seljak and Zaldarriaga 1996), are displayed in figure 7.2. The temperature
power spectrum in figure 7.1 and the polarization power spectra in figure 7.2
come from the same cosmological model. The physical source of the features in
the power spectra is discussed in the next section, followed by a discussion of
how cosmological parameters can be determined to high precision via detailed
measurements of the microwave background power spectra.

7.4 Acoustic oscillations
Before decoupling, the matter in the universe has significant pressure because it
is tightly coupled to radiation. This pressure counteracts any tendency for matter
to collapse gravitationally. Formally, the Jeans mass is greater than the mass
within a horizon volume for times earlier than decoupling. During this epoch,

Acoustic oscillations

235

Figure 7.2. The G polarization power spectrum (full curve) and the cross-power TG
between temperature and polarization (dashed curve), for the same model as in figure 7.1.

density perturbations will set up standing acoustic waves in the plasma. Under
certain conditions, these waves leave a distinctive imprint on the power spectrum
of the microwave background, which in turn provides the basis for precision
constraints on cosmological parameters. This section reviews the basics of the
acoustic oscillations.
7.4.1 An oscillator equation
In their classic 1996 paper, Hu and Sugiyama transformed the basic equations
describing the evolution of perturbations into an oscillator equation. Combining
the zeroth moment of the photon Boltzmann equation with the baryon Euler
equation for a given k-mode in the tight-coupling approximation (mean baryon
velocity equals mean radiation velocity) gives
¨0+H
'

R ˙
¨ −H R 
˙ − 1 k 2 +,
'0 + k 2 cs2 '0 = −
1+ R
1+ R
3

(7.27)

where '0 is the zeroth moment of the temperature distribution function
(proportional to the photon density perturbation), R = 3ρb /4ργ is proportional
to the scale factor a, H = ȧ/a is the conformal Hubble parameter, and the
sound speed is given by cs2 = 1/(3 + 3R). (All overdots are derivatives with

236

The cosmic microwave background

respect to conformal time.)  and + are the scalar metric perturbations in the
Newtonian gauge; if we neglect the anisotropic stress, which is generally small in
conventional cosmological scenarios, then + = −. But the details are not very
important. The equation represents damped, driven oscillations of the radiation
density, and the various physical effects are easily identified. The second term
on the left-hand side is the damping of oscillations due to the expansion of the
universe. The third term on the left-hand side is the restoring force due to the
pressure, since cs2 = dP/dρ. On the right-hand side, the first two terms depend
on the time variation of the gravitational potentials, so these two are the source
of the Integrated Sachs–Wolfe effect. The final term on the right-hand side is the
driving term due to the gravitational potential perturbations. As Hu and Sugiyama
emphasized, these damped, driven acoustic oscillations account for all of the
structure in the microwave background power spectrum.
A WKB approximation to the homogeneous equation with no driving source
terms gives the two oscillation modes (Hu and Sugiyama 1996)

(1 + R)−1/4 cos krs (η)
'0 (k, η) ∝
(7.28)
(1 + R)−1/4 sin krs (η)
where the sound horizon rs is given by

rs (η) ≡

η

cs (η ) dη .

(7.29)

0

Note that at times well before
√ matter–radiation equality, the sound speed is
essentially constant, cs = 1/ 3, and the sound horizon is simply proportional to
the causal horizon. In general, any perturbation with wavenumber k will set up an
oscillatory behaviour in the primordial plasma described by a linear combination
of the two modes in equation (7.28). The relative contribution of the modes will
be determined by the initial conditions describing the perturbation.
Equation (7.27) appears to be simpler than it actually is, because  and +
are the total gravitational potentials due to all matter and radiation, including the
photons which the left-hand side is describing. In other words, the right-hand
side of the equation contains an implicit dependence on '0 . At the expense
of pedagogical transparency, this situation can be remedied by considering
separately the potential from the photon–baryon fluid and the potential from the
truly external sources, the DM and neutrinos. This split has been performed by
Hu and White (1996). The resulting equation, while still an oscillator equation,
is much more complicated, but must be used for a careful physical analysis of
acoustic oscillations.
7.4.2 Initial conditions
The initial conditions for radiation perturbations for a given wavenumber k can
be broken into two categories, according to whether the gravitational potential

Acoustic oscillations

237

perturbation from the baryon–photon fluid, bγ , is non-zero or zero as η → 0.
The former case is known as ‘adiabatic’ (which is somewhat of a misnomer since
adiabatic technically refers to a property of a time-dependent process) and implies
that n b /n γ , the ratio of baryon to photon number densities, is a constant in space.
This case must couple to the cosine oscillation mode since it requires '0 = 0 as
η → 0. The simplest (i.e. single-field) models of inflation produce perturbations
with adiabatic initial conditions.
The other case is termed ‘isocurvature’ since the fluid gravitational potential
perturbation bγ , and hence the perturbations to the spatial curvature, are zero.
In order to arrange such a perturbation, the baryon and photon densities must
vary in such a way that they compensate each other: n b /n γ varies, and thus
these perturbations are in entropy, not curvature. At an early enough time, the
temperature perturbation in a given k mode must arise entirely from the Sachs–
Wolfe effect, and thus isocurvature perturbations couple to the sine oscillation
mode. These perturbations arise from causal processes like phase transitions:
a phase transition cannot change the energy density of the universe from point
to point, but it can alter the relative entropy between various types of matter
depending on the values of the fields involved. The potentially most interesting
cause of isocurvature perturbations is multiple dynamical fields in inflation.
The fields will exchange energy during inflation, and the field values will vary
stochastically between different points in space at the end of the phase transition,
generically giving isocurvature along with adiabatic perturbations (Polarski and
Starobinsky 1994).
The numerical problem of setting initial conditions is somewhat tricky. The
general problem of evolving perturbations involves linear evolution equations for
around a dozen variables, outlined in section 7.2.2. Setting the correct initial
conditions involves specifying the value of each variable in the limit as η → 0.
This is difficult for two reasons: the equations are singular in this limit, and
the equations become increasingly numerically stiff in this limit. Simply using
the leading-order asymptotic behaviour for all of the variables is only valid in
the high-temperature limit. Since the equations are stiff, small departures from
this limiting behaviour in any of the variables can lead to numerical instability
until the equations evolve to a stiff solution, and this numerical solution does not
necessarily correspond to the desired initial conditions. Numerical techniques for
setting the initial conditions to high accuracy at temperaturesare currently being
developed.
7.4.3 Coherent oscillations
The characteristic ‘acoustic peaks’ which appear in figure 7.1 arise from acoustic
oscillations which are phase coherent: at some point in time, the phases of all of
the acoustic oscillations were the same. This requires the same initial condition
for all k-modes, including those with wavelengths longer than the horizon. Such
a condition arises naturally for inflationary models, but is very hard to reproduce

238

The cosmic microwave background

in models producing perturbations causally on scales smaller than the horizon.
Defect models, for example, produce acoustic oscillations, but the oscillations
generically have incoherent phases and thus display no peak structure in their
power spectrum (Seljak et al 1997). Simple models of inflation which produce
only adiabatic perturbations insure that all perturbations have the same phase at
η = 0 because all of the perturbations are in the cosine mode of equation (7.28).
A glance at the k dependence of the adiabatic perturbation mode reveals how
the coherent peaks are produced. The microwave background images the radiation
density at a fixed time; as a function of k, the density varies like cos(krs ), where
rs is fixed. Physically, on scales much larger than the horizon at decoupling, a
perturbation mode has not had enough time to evolve. At a particular smaller
scale, the perturbation mode evolves to its maximum density in potential wells, at
which point decoupling occurs. This is the scale reflected in the first acoustic
peak in the power spectrum. Likewise, at a particular still smaller scale, the
perturbation mode evolves to its maximum density in potential wells and then
turns around, evolving to its minimum density in potential wells; at that point,
decoupling occurs. This scale corresponds to that of the second acoustic peak.
(Since the power spectrum is the square of the temperature fluctuation, both
compressions and rarefactions in potential wells correspond to peaks in the power
spectrum.) Each successive peak represents successive oscillations, with the
scales of odd-numbered peaks corresponding to those perturbation scales which
have ended up compressed in potential wells at the time of decoupling, while the
even-numbered peaks correspond to the perturbation scales which are rarefied in
potential wells at decoupling. If the perturbations are not phase coherent, then
the phase of a given k-mode at decoupling is not well defined, and the power
spectrum just reflects some mean fluctuation power at that scale.
In practice, two additional effects must be considered: a given scale in kspace is mapped to a range of l-values; and radiation velocities as well as densities
contribute to the power spectrum. The first effect broadens out the peaks, while
the second fills in the valleys between the peaks since the velocity extrema will
be exactly out of phase with the density extrema. The amplitudes of the peaks
in the power spectrum are also suppressed by Silk damping, as mentioned in
section 7.2.5.
7.4.4 The effect of baryons
The mass of the baryons creates a distinctive signature in the acoustic oscillations
(Hu and Sugiyama 1996). The zero-point of the oscillations is obtained by setting
'0 constant in equation (7.27): the result is
'0 

1
 = (1 + a).
3cs2

(7.30)

The photon temperature '0 is not itself observable, but must be combined with
the gravitational redshift to form the ‘apparent temperature’ '0 − , which

Cosmological models and constraints

239

oscillates around a. If the oscillation amplitude is much larger than a =
3ρb /4ργ , then the oscillations are effectively about the mean temperature.
The positive and negative oscillations are of the same amplitude, so when the
apparent temperature is squared to form the power spectrum, all of the peaks
have the same height. However, if the baryons contribute a significant mass so
that a is a significant fraction of the oscillation amplitude, then the zero point
of the oscillations are displaced, and when the apparent temperature is squared
to form the power spectrum, the peaks arising from the positive oscillations are
higher than the peaks from the negative oscillations. If a is larger than the
amplitude of the oscillations, then the power spectrum peaks corresponding to
the negative oscillations disappear entirely. The physical interpretation of this
effect is that the baryon mass deepens the potential well in which the baryons are
oscillating, increasing the compression of the plasma compared to the case with
less baryon mass. In short, as the baryon density increases, the power spectrum
peaks corresponding to compressions in potential wells get higher, while the
alternating peaks corresponding to rarefactions get lower. This alternating peak
height signature is a distinctive signature of baryon mass, and allows the precise
determination of the cosmological baryon density with the measurement of the
first several acoustic peak heights.

7.5 Cosmological models and constraints
The cosmological interpretation of a measured microwave background power
spectrum requires, to some extent, the introduction of a particular space of
models. A very simple, broad and well-motivated set of models are motivated
by inflation: a universe described by a homogeneous and isotropic background
with phase-coherent, power-law initial perturbations which evolve freely. This
model space excludes, for example, perturbations caused by topological defects
or other ‘stiff’ sources, arbitrary initial power spectra, or any departures from
the standard background cosmology. This set of models has the twin virtues
of being relatively simple to calculate and best conforming to current power
spectrum measurements. (In fact, most competing cosmological models, like
those employing cosmic defects to make structure, are essentially ruled out by
current microwave background and large-scale structure measurements.) This
section will describe the parameters defining the model space and discuss the
extent to which the parameters can be constrained through the microwave
background.
7.5.1 A space of models
The parameters defining the model space can be broken into three types:
cosmological parameters describing the background spacetime; parameters
describing the initial conditions; and other parameters describing miscellaneous
additional physical effects. Background cosmological parameters are as follows.

240
•

•

•

•

•

•

The cosmic microwave background
, the ratio of the total energy density to the critical density ρcr = 8π/3H 2.
This parameter determines the spatial curvature of the universe:  = 1
is a flat universe with critical density. Smaller values of  correspond
to a negative spatial curvature, while larger values correspond to positive
curvature. Current microwave background measurements constrain  to be
roughly within the range 0.8–1.2, consistent with a critical-density universe.
b , the ratio of the baryon density to the critical density. Observations of
the abundance of deuterium in high redshift gas clouds and comparison with
predictions from primordial nucleosynthesis place strong constraints on this
parameter (Tytler et al 2000).
m , the ratio of the DM density to the critical density. Dynamical
constraints, gravitational lensing, cluster abundances and numerous other
lines of evidence all point to a total matter density in the neighbourhood
of 0 = m + b = 0.3.
 , the ratio of vacuum energy density  to the critical density. This is the
notorious cosmological constant. Several years ago, almost no cosmologist
advocated a cosmological constant; now almost every cosmologist accepts
its existence. The shift was precipitated by the Type Ia supernova Hubble
diagram (Perlmutter et al 1999, Riess et al 1998) which shows an apparent
acceleration in the expansion of the universe. Combined with strong
constraints on , a cosmological constant now seems unavoidable, although
high-energy theorists have a difficult time accepting it. Strong gravitational
lensing of quasars places upper limits on  (Falco et al 1998).
The present Hubble parameter h, in units of 100 km s−1 /Mpc−1 . Distance
ladder measurements (Mould et al 2000) and supernova Ia measurements
(Riess et al 1998) give consistent estimates for h of around 0.70, with
systematic errors on the order of 10%.
Optionally, further parameters describing additional contributions to the
energy density of the universe; for example, the ‘quintessence’ models
(Caldwell et al 1998) which add one or more scalar fields to the universe.

Parameters describing the initial conditions are:
•
•

•

•

The amplitude of fluctuations Q, often defined at the quadrupole scale.
COBE fixed this amplitude to high accuracy (Bennett et al 1996).
The power law index n of initial adiabatic density fluctuations. The scaleinvariant Harrison–Zeldovich spectrum is n = 1. Comparison of microwave
background and large-scale structure measurements shows that n is close to
unity.
The relative contribution of tensor and scalar perturbations r , usually defined
as the ratio of the power at l = 2 from each type of perturbation. The fact
that prominent features are seen in the power spectrum (presumably arising
from scalar density perturbations) limits the power spectrum contribution of
tensor perturbations to roughly 20% of the scalar amplitude.
The power law index n T of tensor perturbations. Unfortunately, tensor power

Cosmological models and constraints

•

241

spectra are generally defined so that n T = 0 corresponds to scale invariant,
in contrast to the scalar case.
Optionally, more parameters describing either departures of the scalar
perturbations from a power law (e.g. Kosowsky and Turner 1995) or a small
admixture of isocurvature perturbations.

Other miscellaneous parameters include:
•
•
•

A significant neutrino mass m ν . None of the current neutrino oscillation
results favour a cosmologically interesting neutrino mass.
The effective number of neutrino species Nν . This quantity includes any
particle species which is relativistic when it decouples or can model entropy
production prior to last scattering.
The redshift of reionization, z r . Spectra of quasars at redshift z = 5 show
that the universe has been reionized at least since then.

A realistic parameter analysis might include at least eight free parameters.
Given a particular microwave background measurement, deciding on a particular
set of parameters and various priors on those parameters is as much art as science.
For the correct model, parameter values should be insensitive to the size of the
parameter space or the particular priors invoked. Several particular parameter
space analyses are mentioned in section 7.5.5.
7.5.2 Physical quantities
While these parameters are useful and conventional for characterizing
cosmological models, the features in the microwave background power spectrum
depend on various physical quantities which can be expressed in terms of the
parameters. Here the physical quantities are summarized, and their dependence
on parameters given. This kind of analysis is important for understanding the
model space of parameters as more than just a black box producing output power
spectra. All of the physical dependences discussed here can be extracted from
Hu and Sugiyama (1996). By comparing numerical solutions with the evolution
equations, Hu and Sugiyama demonstrated that they had accounted for all relevant
physical processes.
Power-law initial conditions are determined in a straightforward way by the
appropriate parameters Q, n, r and n T , if the perturbations are purely adiabatic.
Additional parameters must be used to specify any departure from power-law
spectra or to specify an additional admixture of isocurvature initial conditions
(e.g. Bucher et al 1999). These parameters directly express physical quantities.
However, the physical parameters determining the evolution of the
initial perturbations until decoupling involve a few specific combinations of
cosmological parameters. First, note that the density of radiation is fixed by
the current microwave background temperature which is known from COBE,
as well as the density of the neutrino backgrounds. The gravitational potentials

242

The cosmic microwave background

describing scalar perturbations determine the size of the Sachs–Wolfe effect and
also magnitude of the forces driving the acoustic oscillations. The potentials
are determined by 0 h 2 , the matter density as a fraction of critical density.
The baryon density, b h 2 , determines the degree to which the acoustic peak
amplitudes are modulated as previousy described in section 7.4.4.
The time of matter–radiation equality is obviously determined solely by the
total matter density 0 h 2 . This quantity affects the size of the DM fluctuations,
since DM starts to collapse gravitationally only after matter–radiation equality.
Also, the gravitational potentials evolve in time during radiation domination and
not during matter domination: the later matter–radiation equality occurs, the
greater the time evolution of the potentials at decoupling, increasing the Integrated
Sachs–Wolfe effect. The power spectrum also has a weak dependence on 0 in
models with 0 significantly less than unity, because at late times the evolution
of the background cosmology will be dominated not by matter, but rather by
vacuum energy (for a flat universe with ) or by curvature (for an open universe).
In either case, the gravitational potentials once again begin to evolve with time,
giving an additional late-time integrated Sachs–Wolfe contribution, but this tends
to affect only the largest scales for which the constraints from measurements are
least restrictive due to cosmic variance (see the discussion in section 7.5.4).
The sound speed, which sets the sound horizon and thus affects the
wavelength of the acoustic modes (cf equation (7.28)), is completely determined
by the baryon density b h 2 . The horizon size at recombination, which sets the
overall scale of the acoustic oscillations, depends only on the total mass density
0 h 2 . The damping scale for diffusion damping depends almost solely on the
baryon density b h 2 , although numerical fits give a slight dependence on b
alone (Hu and Sugiyama 1996). Finally, the angular diameter distance to the lastscattering surface is determined by 0 h and h; the angular diameter sets the
angular scale on the sky of the acoustic oscillations.
In summary, the physical dependence of the temperature perturbations at
last scattering depends on 0 h 2 , b h 2 , 0 h, and h instead of the individual
cosmological parameters 0 , b , h and . When analysing constraints on
cosmological models from microwave background power spectra, it may be more
meaningful and powerful to constrain these physical parameters rather than the
cosmological ones.
7.5.3 Power spectrum degeneracies
As might be expected from the previous discussion, not all of the parameters
considered here are independent. In fact, one nearly exact degeneracy exists if
0 , b , h and  are taken as independent parameters. To see this, consider a
shift in 0 . In isolation, such a shift will produce a corresponding stretching
of the power spectrum in l-space. But this effect can be compensated by first
shifting h to keep 0 h 2 constant, then shifting b to keep b h 2 constant, and
finally shifting  to keep the angular diameter distance constant. This set of

Cosmological models and constraints

243

shifted parameters will, in linear perturbation theory, produce almost exactly the
same microwave background power spectra as the original set of parameters. The
universe with shifted parameters will generally not be flat, but the resulting latetime Integrated Sachs–Wolfe effect only weakly break the degeneracy. Likewise,
gravitational lensing has only a very weak effect on the degeneracy.
But all is not lost. The required shift in  is generally something like eight
times larger than the original shift in 0 , so although the degeneracy is nearly
exact, most of the degenerate models represent rather extreme cosmologies.
Good taste requires either that  = 0 or that  = 1, in other words that we
disfavour models which have both a cosmological constant and are not flat. If such
models are disallowed, the degeneracy disappears. Finally, other observables not
associated with the microwave background break the degeneracy: the acceleration
parameter q0 = 0 /2 − , for example, is measured directly by the highredshift supernova experiments. So in practice, this fundamental degeneracy in
the microwave background power spectrum between  and  is not likely to
have a great impact on our ability to constrain cosmological parameters.
Other approximate degeneracies in the temperature power spectrum exist
between Q and r , and between z r and n. The first is illusory: the amplitudes
of the scalar and tensor power spectra can be used in place of their sum and
ratio, which eliminates the degeneracy. The power spectrum of large-scale
structure will lift the latter degeneracy if bias is understood well enough, as will
polarization measurements and small-scale second-order temperature fluctuations
(the Ostriker–Vishniac effect, see Gnedin and Jaffe 2000) which are both sensitive
to z r .
Finally, many claims have been made about the ability of the microwave
background to constrain the effective number of neutrino species or neutrino
masses. The effective number of massless degrees of freedom at decoupling can
be expressed in terms of the effective number of neutrino species Nν (which does
not need to be an integer). This is a convenient way of parameterizing ignorance
about fundamental particle constituents of nature. Contributors to Nν could
include, for example, an extra sterile neutrino sometimes invoked in neutrino
oscillation models, or the thermal background of gravitons which would exist
if inflation did not occur. This parameter can also include the effects of entropy
increases due to decaying or annihilating particles; see chapter 3 of Kolb and
Turner (1990) for a detailed discussion. As far as the microwave background
is concerned, Nν determines the radiation energy density of the universe and
thus modifies the time of matter–radiation equality. It can, in principle, be
distinguished from a change in 0 h 2 because it affects other physical parameters
like the baryon density or the angular diameter distance differently than a shift in
either 0 or h.
Neutrino masses cannot provide the bulk of the DM, because their freestreaming greatly suppresses fluctuation power on galaxy scales, leading to a
drastic mismatch with observed large-scale structure. But models with some
small fraction of dark matter as neutrinos have been advocated to improve

244

The cosmic microwave background

the agreement between the predicted and observed large-scale structure power
spectrum. Massive neutrinos have several small effects on the microwave
background, which have been studied systematically by Dodelson et al (1996).
They can slightly increase the sound horizon at decoupling due to their transition
from relativistic to non-relativistic behaviour as the universe expands. More
importantly, free-streaming of massive neutrinos around the time of last scattering
leads to a faster decay of the gravitational potentials, which in turn means more
forcing of the acoustic oscillations and a resulting increase in the monopole
perturbations. Finally, since matter–radiation equality is slightly delayed for
neutrinos with cosmologically interesting masses of a few eV, the gravitational
potentials are less constant and a larger Integrated Sachs–Wolfe effect is induced.
The change in sound horizon and shift in matter–radiation equality due to massive
neutrinos cannot be distinguished from changes in b h 2 and 0 h 2 , but the
alteration of the gravitational potential’s time dependence due to neutrino freestreaming cannot be mimicked by some other change in parameters. In principle
the effect of neutrino masses can be extracted from the microwave background,
although the effects are very small.
7.5.4 Idealized experiments
Remarkably, the microwave background power spectrum contains enough
information to constrain numerous parameters simultaneously (Jungman et al
1996). We would like to estimate quantitatively just how well the space of
parameters described earlier can be constrained by ideal measurements of the
microwave background. The question has been studied in some detail; this
section outlines the basic methods and results, and discusses how good various
approximations are. For simplicity, only temperature fluctuations are considered
in this section; the corresponding formalism for the polarization power spectra is
developed in Kamionkowski et al (1997a, b).
Given a pixelized map of the microwave sky, we need to determine the
contribution of pixelization noise, detector noise, and beam width to the multipole
moments and power spectrum. Consider a temperature map of the sky T map (n̂)
which is divided into Npix equal-area pixels. The observed temperature in pixel
map
j is due to a cosmological signal plus noise, T j
= T j + T jnoise. The multipole
coefficients of the map can be constructed as

1
T
dlm
=
dn̂ T map (n̂)Ylm (n̂)
T0
Npix
1  4π map

T
Ylm (n̂ j ),
(7.31)
T0
Npix j
j =1

where n̂ j is the direction vector to pixel j . The map moments are written
as dlm to distinguish them from the moments of the cosmological signal alm ;
the former include the effects of noise. The extent to which the second line

Cosmological models and constraints

245

in equations (7.31) is only an approximate equality is the pixelization noise.
Most current experiments oversample the sky with respect to their beam, so
the pixelization noise is negligible. Now assume that the noise is uncorrelated
between pixels and is well represented by a normal distribution. Also, assume that
the map is created with a Gaussian beam with width θb . Then it is straightforward
to show that the variance of the temperature moments is given by (Knox 1995)
T T∗
dl m  = (Cl e−l
dlm

2σ 2
b

+ w−1 )δll δmm ,

(7.32)

where σb = 0.007 42(θb/1◦ ) and
w−1 =

4π (Tinoise)2 
Npix
T02

(7.33)

is the inverse statistical weight per unit solid angle, a measure of experimental
sensitivity independent of the pixel size.
Now the power spectrum can be estimated via equation (7.32) as
ClT = (DlT − w−1 )el
where
DlT =

2σ 2
b

l

1
T T∗
dlm
dlm .
2l + 1

(7.34)

(7.35)

m=−l

T are Gaussian random variables. This means that
The individual coefficients dlm
T
2
distribution, and its variance is (Knox 1995)
Cl is a random variable with a χ2l+1

(ClT )2 =

2
2 2
(Cl + w−1 el σb ).
2l + 1

(7.36)

Note that even for w−1 = 0, corresponding to zero noise, the variance is nonzero. This is the cosmic variance, arising from the fact that we have only one
sky to observe: the estimator in equation (7.35) is the sum of 2l + 1 random
variables, so it has a fundamental fractional variance of (2l + 1)−1/2 simply due
to Poisson statistics. This variance provides a benchmark for experiments: if the
goal is to determine a power spectrum, it makes no sense to improve resolution
or sensitivity beyond the level at which cosmic variance is the dominant source of
error.
Equation (7.36) is extremely useful: it gives an estimate of how well the
power spectrum can be determined by an experiment with a given beam size
and detector noise. If only a portion of the sky is covered, the variance estimate
should be divided by the fraction of the total sky covered. With these variances in
hand, standard statistical techniques can be employed to estimate how well a given
measurement can recover a given set s of cosmological parameters. Approximate
the dependence of ClT on a given parameter as linear in the parameter; this will

246

The cosmic microwave background

always be true for some sufficiently small range of parameter values. Then the
parameter space curvature matrix (also known as the Fisher information matrix)
is specified by
 ∂C T ∂C T
1
l
l
αi j =
.
(7.37)
∂si ∂s j (ClT )2
l
The variance in the determination of the parameter si from a set of ClT with
variances ClT after marginalizing over all other parameters is given by the
diagonal element i of the matrix α −1 .
Estimates of this kind were first made by Jungman et al (1996) and
subsequently refined by Zaldarriaga et al (1997) and Bond et al (1997), among
others. The basic result is that a map with pixels of a few arcminutes in size and a
signal-to-noise ratio of around one per pixel can determine , b h 2 , m h 2 , h 2 ,
Q, n, and z r at the few percent level simultaneously, up to the one degeneracy
mentioned earlier (see the table in Bond et al 1997). Significant constraints
will also be placed on r and Nν . This prospect has been the primary reason
that the microwave background has generated such excitement. Note that , h,
b , and  are the classical cosmological parameters. Decades of painstaking
astronomical observations have been devoted to determining the values of these
parameters. The microwave background offers a completely independent method
of determining them with comparable or significantly greater accuracy, and
with fewer astrophysical systematic effects to worry about. The microwave
background is also the only source of precise information about the spectrum and
character of the primordial perturbations from which we arose. Of course, these
exciting possibilities hold only if the universe is accurately represented by a model
in the assumed model space. The model space is, however, quite broad. Modelindependent constraints which the microwave background provides are discussed
in section 7.6.
The estimates of parameter variances based on the curvature matrix would
be exact if the power spectrum always varied linearly with each parameter. This,
of course, is not true in general. Given a set of power spectrum data, we want
to know two pieces of information about the cosmological parameters: (1) What
parameter values provide the best-fit model? (2) What are the error bars on these
parameters, or more precisely, what is the region of parameter space which defines
a given confidence level? The first question can be answered easily using standard
methods of searching parameter space; generally such a search requires evaluating
the power spectrum for fewer than 100 different models. This shows that the
parameter space is generally without complicated structure or many false minima.
The second question is more difficult. Anything beyond the curvature matrix
analysis requires looking around in parameter space near the best-fit model. A
specific Monte Carlo technique employing a Metropolis algorithm has recently
been advocated (Christensen and Meyer 2000); such techniques will certainly
prove more flexible and efficient than recent brute-force grid searches (Tegmark
and Zaldarriaga 2000). As upcoming data-sets contain more information and

Cosmological models and constraints

247

consequently have greater power to constrain parameters, efficient techniques of
parameter space exploration will become increasingly important.
To this point, the discussion has assumed that the microwave background
power spectrum is perfectly described by linear perturbation theory. Since the
temperature fluctuations are so small, parts in a hundred thousand, linear theory is
a very good approximation. However, on small scales, nonlinear effects become
important and can dominate over the linear contributions. The most important
nonlinear effects are the Ostriker–Vishniac effect coupling velocity and density
perturbations (Jaffe and Kamionkowski 1998, Hu 2000), gravitational lensing by
large-scale structure (Seljak 1996), the Sunyaev–Zeldovich effect which gives
spectral distortions when the microwave background radiation passes through
hot ionized regions (Birkinshaw 1999) and the kinetic Sunyaev–Zeldovich effect
which Doppler shifts radiation passing through plasma with bulk velocity (Gnedin
and Jaffe 2000). All three effects are measurable and give important additional
constraints on cosmology, but more detailed descriptions are outside the scope of
this chapter.
Finally, no discussion of parameter determination would be complete
without mention of galactic foreground sources of microwave emission. Dust
radiates significantly at microwave frequencies, as do free–free and synchrotron
emission; point source microwave emission is also a potential problem. Dust
emission generally has a spectrum which rises with frequency, while free–free and
synchrotron emission have falling frequency spectra. The emission is not uniform
on the sky, but rather concentrated in the galactic plane, with fainter but pervasive
diffuse emission in other parts of the sky. The dust and synchrotron/free–
free emission spectra cross each other at a frequency of around 90 GHz.
Fortunately for cosmologists, the amplitude of the foreground emission at this
frequency is low enough to create a frequency window in which the cosmological
temperature fluctuations dominate the foreground temperature fluctuations. At
other frequencies, the foreground contribution can be effectively separated from
the cosmological blackbody signal by measuring in several different frequencies
and projecting out the portion of the signal with a flat frequency spectrum.
The foreground situation for polarization is less clear, both in amplitude and
spectral index, and could potentially be a serious systematic limit to the quality
of cosmological polarization data. However,, it may be no greater problem for
polarization fluctuations than for temperature fluctuations. For an overview of
issues surrounding foreground emission, see Bouchet and Gispert 1999 or the
WOMBAT web site, http://astro.berkeley.edu/wombat.
7.5.5 Current constraints and upcoming experiments
As the Como School began, results from the high-resolution balloon-born
experiment MAXIMA (Hanany et al 2000) were released, complementing the
week-old data from BOOMERanG (de Bernardis et al 2000) and creating a
considerable buzz at coffee breaks. The derived power spectrum estimates are

248

The cosmic microwave background

Figure 7.3.
Two current measurements of the microwave background radiation
temperature power spectrum. Triangles are BOOMERanG measurements multiplied by
1.21; squares are MAXIMA measurements multiplied by 0.92. The normalization factors
are within the calibration uncertainties of the experiments, and were chosen by Hanany et
al (2000) to give the most consistent results between the two experiments.

shown in figure 7.3. The data from the two measurements appear consistent up to
calibration uncertainties, and for simplicity will be referred to here as ‘balloon
data’ and discussed as a single result. While a few experimenters and data
analysers were members of both experimental teams, the measurements and data
reductions were done essentially independently. Earlier data from the previous
year (Miller et al 1999) had clearly demonstrated the existence and angular
scale of the first peak in the power spectrum and produced the first maps of the
microwave background at angular scales below a degree. But the new results from
balloon experiments utilizing extremely sensitive bolometric detecters represent
a qualitative step forward. These experiments begin to exploit the potential of the
microwave background for ‘precision cosmology’; their power spectra put strong
constraints on several cosmological parameters simultaneously and rule out many
variants of cosmological models. In fact, what is most interesting is that, at face
value, these measurements put significant pressure on all of the standard models
outlined earlier.
The balloon data show two major features: first, a large peak in the power
spectrum centred around l = 200 with an amplitude of approximately l 2 Cl =

Cosmological models and constraints

249

36 000 µK2 , and second, a broad plateau between l = 400 and l = 700 with
an amplitude of approximately l 2 Cl = 10 000 µK2 . The first peak is clearly
delineated and provides good evidence that the universe is spatially flat, i.e.
 = 1. The issue of a second acoustic peak is much less clear. In most flat
universe models with acoustic oscillations, the second peak is expected to appear
at an angular scale of around l = 400. The angular resolution of the balloon
experiments is certainly good enough to see such a peak, but the power spectrum
data show no evidence for one. I argue that a flat line is an excellent fit to the
data past l = 300, and that any model which shows a peak in this region will be a
worse fit than a flat line. This does not necessarily mean that no peak is present;
the error bars are too large to rule out a peak, but the amplitude of such a peak is
fairly strongly constrained to be lower than expected given the first peak.
What does this mean for cosmological models? Within the model space
outlined in the previous section, there are three ways to suppress the second peak.
The first would be to have a power spectrum index n substantially less than one.
This solution would force abandonment of the assumption of power-law initial
fluctuations, in order to match the observed amplitude of large-scale structure at
smaller scales. While this is certainly possible, it represents a drastic weakening in
the predictive power of the microwave background: essentially, a certain feature
is reproduced by arbitrarily changing the primordial power spectrum. While
no physical principle requires power-law primordial perturbations, we should
wait for microwave background measurements on a much wider range of scales
combined with overlapping large-scale structure measurements before resorting
to departures from power-law initial conditions. If the universe really did possess
an initial power spectrum with a variety of features in it, most of the promise
of precision cosmology is lost. Recent power spectra extracted from the IRAS
Point Source Survey Redshift Catalogue (Hamilton and Tegmark 2000), which
show a remarkable power law behaviour spanning three orders of magnitude in
wavenumber, seem to argue against this possibility.
The second possibility is a drastic amount of reionization. It is not clear the
extent to which this might be compatible with the height of the first peak and still
suppress the second peak sufficiently. This possibility seems unlikely as well, but
would show clear signatures in the microwave background polarization.
The most commonly discussed possibility is that the very low second
peak amplitude reflects an unexpectedly large fraction of baryons relative to
DM in the universe. The baryon signature discussed in section 7.4.4 gives a
suppression of the second peak in this case. However, primordial nucleosynthesis
also constrains the baryon–photon ratio. Recent high-precision measurements
of deuterium absorption in high-redshift neutral hydrogen clouds (Tytler et al
2000) give a baryon–photon number ratio of η = 5.1 ± 0.5 × 1010, which
translates to b h 2 = 0.019 ± 0.002 assuming that the entropy (i.e. photon
number) per comoving volume remains constant between nucleosynthesis and
the present. Requiring b to satisfy this nucleosynthesis constraint leads to
microwave background power spectra which are not particularly good fits to the

250

The cosmic microwave background

data. An alternative is that the entropy per comoving volume has not remained
fixed between nucleosynthesis and recombination (see, e.g., Kaplinghat and
Turner 2000). This could be arranged by having a DM particle which decays
to photons, although such a process must evade limits from the lack of microwave
background spectral distortions (Hu and Silk 1993). Alternately, a large chemical
potential for the neutrino background could lead to larger inferred values for the
baryon–photon ratio from nucleosynthesis (Esposito et al 2000). Either way, if
both the microwave background measurements and the high-redshift deuterium
abundances hold up, the discrepancy points to new physics. Of course, a final
explanation for the discrepancies is simply that the balloon data have significant
systematic errors.
I digress for a brief editorial comment about data analysis. Straightforward
searches of the conventional cosmological model space described earlier for good
fits to the balloon data give models with very low DM densities, high baryon
fractions and very large cosmological constants (see model P1 in table 1 of Lange
et al 2000). Such models violate other observational constraints on age, which
must be at least 12 billion years (see, e.g., Peacock et al 1998), and quasar and
radio source strong lensing number counts, which limit a cosmological constant
to  ≤ 0.7 (Falco et al 1998). The response to this situation so far has been to
invoke Bayesian prior probability distributions on various quantities like b and
the age. This leads to a best-fit model with a nominally acceptable χ 2 (Lange et
al 2000, Tegmark et al 2000 and others). But be wary of this procedure when
the priors have a large effect on the best-fit model! The microwave background
will soon provide tighter constraints on most parameters than any other source
of prior information. Priors probabilities on a given parameter are useful and
justified when the microwave background data have little power to constrain that
parameter; in this case, the statistical quality of the model fit to the microwave
background data will not be greatly affected by imposing the prior. However,
something fishy is probably going on when a prior pulls a parameter multiple
sigma away from its best-fit value without the prior. This is what happens
presently with b when the nucleosynthesis prior is enforced. If your priors
make a big difference, it is likely either that some of the data are incorrect or
that the model space does not include the correct model. Both the microwave
background measurements and the high-redshift deuterium detections are taxing
observations dominated by systematic effects, so it is certainly possible that one or
both are wrong. However, MAXIMA and BOOMERanG are consistent with each
other while using different instruments, different parts of the sky, and different
analysis pipelines, and the deuterium measurements are consistent for several
different clouds. This suggests possible missing physics ingredients, like extreme
reionization or an entropy increase mentioned earlier, or perhaps significant
contributions from cosmic defects. It has even been suggested by otherwise
sober and reasonable people that the microwave background results, combined
with various difficulties related to dynamics of spiral galaxies, may point towards
a radical revision of the standard cosmology (Sellwood and Kosowsky 2000).

Model-independent cosmological constraints

251

We should not rest lightly until the cosmological model preferred by microwave
background measurements is comfortably consistent with all relevant priors
derived from other data sources of comparable precision.
The picture will come into sharper relief over the next two years. The
MAP satellite (http://map.gsfc.nasa.gov), launched by NASA on 30 June 2001,
will map the full microwave sky in five frequency channels with an angular
resolution of around 15 arc minutes and a temperature sensitivity per pixel of
a part in a million. Space missions offer unequalled sky coverage and control of
systematics and, if it works as advertized, MAP will be a benchmark experiment.
Prior to its launch, expect to see the first interferometric microwave data at
angular scales smaller than a half degree from the CBI interferometer experiment
(http://www.astro.caltech.edu/∼tjp/CBI/). In this same time frame, we also may
have the first detection of polarization. The most interesting power spectrum
feature to focus on will be the existence and amplitude of a third acoustic peak. If
a third peak appears with amplitude significantly higher than the putative second
peak, this almost certainly indicates conventional acoustic oscillations with a
high baryon fraction and possibly new physics to reconcile the result with the
deuterium measurements. If, however, the power spectrum remains flat or falls
further past the second peak region, then all bets are off. In a time frame of the
next 5 to 10 years, we can reasonably expect to have a cosmic-variance limited
temperature power spectrum down to scales of a few arcminutes (say, l = 4000),
along with significant polarization information (though probably not cosmicvariance limited power spectra). In particular, ESA’s Planck satellite mission
(http://astro.estec.esa.nl/SA-general/Projects/Planck/) will map the microwave
sky in nine frequency bands at significantly better resolution and sensitivity
than the MAP mission. For a comprehensive listing of past and planned
microwave background measurements, see Max Tegmark’s experiments web
page, http://www.hep.upenn.edu/∼max/cmb/experiments.html.

7.6 Model-independent cosmological constraints
Most analysis of microwave background data and predictions about its ability
to constrain cosmology have been based on the cosmological parameter space
described in section 7.5.1. This space is motivated by inflationary cosmological
scenarios, which generically predict power-law adiabatic perturbations evolving
only via gravitational instability. Considering that this space of models is broad
and appears to fit all current data far better than any other proposed models, such
an assumed model space is not very restrictive. In particular, proposed extensions
tend to be rather ad hoc, adding extra elements to the model without providing
any compelling underlying motivation for them. Examples which have been
discussed in the literature include multiple types of DM with various properties,
non-standard recombination, small admixtures of topological defects, production
of excess entropy, or arbitrary initial power spectra. None of these possibilities

252

The cosmic microwave background

are attractive from an aesthetic point of view: all add significant complexity and
freedom to the models without any corresponding restrictions on the original
parameter space. The principle of Occam’s Razor should cause us to be sceptical
about any such additions to the space of models.
However, it is possible that some element is missing from the model space,
or that the actual cosmological model is radically different in some respect. The
microwave background is the probe of cosmology most tightly connected to
the fundamental properties of the universe and least influenced by astrophysical
complications, and thus the most capable data source for deciding whether
the universe actually is well described by some model in the usual model
space. An interesting question is the extent to which the microwave background
can determine various properties of the universe independent from particular
models. While any cosmological interpretation of temperature fluctuations in
the microwave sky requires some kind of minimal assumptions, all of the
conclusions outlined later can be drawn without invoking a detailed model of
initial conditions or structure formation. These conclusions are in contrast to
precision determination of cosmological parameters, which does require the
assumption of a particular space of models and which can vary significantly
depending on the space.
7.6.1 Flatness
The Friedmann–Robertson–Walker spacetime describing homogeneous and
isotropic cosmology comes in three flavours of spatial curvature: positive,
negative and flat, corresponding to  > 1,  < 1 and  = 1 respectively. One of
the most fundamental questions of cosmology, dating to the original relativistic
cosmological models, is the curvature of the background spacetime. The fate
of the universe quite literally depends on the answer: in a cosmology with only
matter and radiation, a positively curved universe will eventually recollapse in a
fiery ‘Big Crunch’ while flat and negatively curved universes will expand forever,
meeting a frigid demise. Note these fates are at least 40 billion years in the future.
(A cosmological constant or other energy density component with an unusual
equation of state can alter these outcomes, causing a closed universe eventually
to enter an inflationary stage.)
The microwave background provides the cleanest and most powerful probe
of the geometry of the universe (Kamionkowski et al 1994). The surface of
last scattering is at a high enough redshift that photon geodesics between the
last scattering surface and the Earth are significantly curved if the geometry of
the universe is appreciably different than flat. In a positively curved space, two
geodesics will bend towards each other, subtending a larger angle at the observer
than in the flat case; likewise, in a negatively curved space two geodesics bend
away from each other, resulting in a smaller observed angle between the two.
The operative quantity is the angular diameter distance; Weinberg (2000) gives
a pedagogical discussion of its dependence on . In a flat universe, the horizon

Model-independent cosmological constraints

253

length at the time of last scattering subtends an angle on the sky of around two
degrees. For a low-density universe with  = 0.3, this angle becomes smaller by
half, roughly.
A change in angular scale of this magnitude will change the apparent scale
of all physical scales in the microwave background. A model-independent
determination of  thus requires a physical scale of known size to be imprinted on
the primordial plasma at last scattering; this physical scale can then be compared
with its apparent observed scale to obtain a measurement of . The microwave
background fluctuations actually depend on two basic physical scales. The first is
the sound horizon at last scattering, rs (cf equation (7.29)). If coherent acoustic
oscillations are visible, this scale sets their characteristic wavelengths. Even if
coherent acoustic oscillations are not present, the sound horizon represents the
largest scale on which any causal physical process can influence the primordial
plasma. Roughly, if primordial perturbations appear on all scales, the resulting
microwave background fluctuations appear as a featureless power law at large
scales, while the scale at which they begin to depart from this assumed primordial
behaviour corresponds to the sound horizon. This is precisely the behaviour
observed by current measurements, which show a prominent power spectrum
peak at an angular scale of a degree (l = 200), arguing strongly for a flat universe.
Of course, it is logically possible that the primordial power spectrum has power
on scales only significantly smaller than the horizon at last scattering. In this case,
the largest scale perturbations would appear at smaller angular scales for a given
geometry. But then the observed power-law perturbations at large angular scales
must be reproduced by the Integrated Sachs–Wolfe effect, and resulting models
are contrived. If the microwave background power spectrum exhibits acoustic
oscillations, then the spacing of the acoustic peaks depends only on the sound
horizon independent of the phase of the oscillations; this provides a more general
and precise probe of flatness than the first peak position.
The second physical scale provides another test: the Silk damping scale is
determined solely by the thickness of the surface of last scattering, which in turn
depends only on the baryon density b h 2 , the expansion rate of the universe and
standard thermodynamics. Observation of an exponential suppression of power at
small scales gives an estimate of the angular scale corresponding to the damping
scale. Note that the effects of reionization and gravitational lensing must both be
accounted for in the small-scale dependence of the fluctuations. If the reionization
redshift can be accurately estimated from microwave background polarization
(see later) and the baryon density is known from primordial nucleosynthesis
or from the alternating peak heights signature (section 7.4.4), only a radical
modification of the standard cosmology altering the time dependence of the
scale factor or modifying thermodynamic recombination can change the physical
damping scale. If the estimates of  based on the sound horizon and damping
scales are consistent, this is a strong indication that the inferred geometry of the
universe is correct.

254

The cosmic microwave background

7.6.2 Coherent acoustic oscillations
If a series of peaks equally spaced in l is observed in the microwave background
temperature power spectrum, it strongly suggests we are seeing the effects
of coherent acoustic oscillations at the time of last scattering. Microwave
background polarization provides a method for confirming this hypothesis.
As explained in section 7.3.2, polarization anisotropies couple primarily to
velocity perturbations, while temperature anisotropies couple primarily to density
perturbations. Now coherent acoustic oscillations produce temperature power
spectrum peaks at scales where a mode of that wavelength has either maximum
or minimum compression in potential wells at the time of last scattering. The
fluid velocity for the mode at these times will be zero, as the oscillation is
turning around from expansion to contraction (envision a mass on a spring.) At
scales intermediate between the peaks, the oscillating mode has zero density
contrast but a maximum velocity perturbation. Since the polarization power
spectrum is dominated by the velocity perturbations, its peaks will be at scales
interleaved with the temperature power spectrum peaks. This alternation of
temperature and polarization peaks as the angular scale changes is characteristic
of acoustic oscillations (see Kosowsky (1999) for a more detailed discussion).
Indeed, it is almost like seeing the oscillations directly: it is difficult to
imagine any other explanation for density and velocity extrema on alternating
scales. The temperature-polarization cross-correlation must also have peaks with
corresponding phases. This test will be very useful if a series of peaks is detected
in a temperature power spectrum which is not a good fit to the standard space of
cosmological models. If the peaks turn out to reflect coherent oscillations, we
must then modify some aspect of the underlying cosmology, while if the peaks
are not coherent oscillations, we must modify the process by which perturbations
evolve.
If coherent oscillations are detected, any cosmological model must include a
mechanism for enforcing coherence. Perturbations on all scales, in particular on
scales outside the horizon, provide the only natural mechanism: the phase of the
oscillations is determined by the time when the wavelength of the perturbation
becomes smaller than the horizon, and this will clearly be the same for all
perturbations of a given wavelength. For any source of perturbations inside the
horizon, the source itself must be coherent over a given scale to produce phasecoherent perturbations on that scale. This cannot occur without artificial finetuning.
7.6.3 Adiabatic primordial perturbations
If the microwave background temperature and polarization power spectra reveal
coherent acoustic oscillations and the geometry of the universe can also be
determined with some precision, then the phases of the acoustic oscillations
can be used to determine whether the primordial perturbations are adiabatic

Model-independent cosmological constraints

255

or isocurvature. Quite generally, equation (7.28) shows that adiabatic and
isocurvature power spectra must have peaks which are out of phase. While
current measurements of the microwave background and large-scale structure rule
out models based entirely on isocurvature perturbations, some relatively small
admixture of isocurvature modes with dominant adiabatic modes is possible. Such
mixtures arise naturally in inflationary models with more than one dynamical field
during inflation (see, e.g., Mukhanov and Steinhardt 1998).
7.6.4 Gaussian primordial perturbations
If the temperature perturbations are well approximated as a Gaussian random
field, as microwave background maps so far suggest, then the power spectrum Cl
contains all statistical information about the temperature distribution. Departures
from Gaussianity take myriad different forms; the business of providing general
but useful statistical descriptions is a complicated one (see, e.g., Ferreira et
al 1997). Tiny amounts of non-Gaussianity will arise inevitably from the
nonlinear evolution of fluctuations, and larger non-Gaussian contributions can be
a feature of the primordial perturbations or can be induced by ‘stiff’ stress–energy
perturbations such as topological defects. As explained later, defect theories of
structure formation seem to be ruled out by current microwave background and
large-scale structure measurements, so interest in non-gaussianity has waned.
But the extent to which the temperature fluctuations are actually Gaussian is
experimentally answerable and, as observations improve, this will become an
important test of inflationary cosmological models.
7.6.5 Tensor or vector perturbations
As described in section 7.3.3, the tensor field describing microwave background
polarization can be decomposed into two components corresponding to the
gradient-curl decomposition of a vector field. This decomposition has the same
physical meaning as that for a vector field. In particular, any gradient-type tensor
field, composed of the G-harmonics, has no curl, and thus may not have any
handedness associated with it (meaning the field is even under parity reversal),
while the curl-type tensor field, composed of the C-harmonics, does have a
handedness (odd under parity reversal).
This geometric interpretation leads to an important physical conclusion.
Consider a universe containing only scalar perturbations, and imagine a single
Fourier mode of the perturbations. The mode has only one direction associated
with it, defined by the Fourier vector k; since the perturbation is scalar, it must
be rotationally symmetric around this axis. (If it were not, the gradient of the
perturbation would define an independent physical direction, which would violate
the assumption of a scalar perturbation.) Such a mode can have no physical
handedness associated with it and, as a result, the polarization pattern it induces
in the microwave background couples only to the G harmonics. Another way of

256

The cosmic microwave background

Figure 7.4. Polarization power spectra from tensor perturbations: the full curve is ClG and
the broken curve is ClC . The amplitude gives a 10% contribution to the COBE temperature
power spectrum measurement at low l. Note that scalar perturbations give no contribution
to ClC .

stating this conclusion is that primordial density perturbations produce no C-type
polarization as long as the perturbations evolve linearly. However, primordial
tensor or vector perturbations produce both G-type and C-type polarization of
the microwave background (provided that the tensor or vector perturbations
themselves have no intrinsic net polarization associated with them).
Measurements of cosmological C-polarization in the microwave background
are free of contributions from the dominant scalar density perturbations and thus
can reveal the contribution of tensor modes in detail. For roughly scale-invariant
tensor perturbations, most of the contribution comes at angular scales larger than
2◦ (2 < l < 100). Figure 7.4 displays the C and G power spectra for scaleinvariant tensor perturbations contributing 10% of the COBE signal on large
scales. A microwave background map with forseeable sensitivity could measure
gravitational wave perturbations with amplitudes smaller than 10−3 times the
amplitude of density perturbations (Kamionkowski and Kosowsky 1998). The
C-polarization signal also appears to be the best hope for measuring the spectral
index n T of the tensor perturbations.

Model-independent cosmological constraints

257

7.6.6 Reionization redshift
Reionization produces a distinctive microwave background signature.
It
suppresses temperature fluctuations by increasing the effective damping scale,
while it also increases large-angle polarization due to additional Thomson
scattering at low redshifts when the radiation quadrupole fluctuations are much
larger. This enhanced polarization peak at large angles will be significant for
reionization prior to z = 10 (Zaldarriaga 1997). Reionization will also greatly
enhance the Ostriker–Vishniac effect, a second-order coupling between density
and velocity perturbations (Jaffe and Kamionkowski 1998). The non-uniform
reionization inevitable if the ionizing photons come from point sources, as seems
likely, may also create an additional feature at small angular scales (Hu and
Gruzinov 1998, Knox et al 1998). Taken together, these features are clear
indicators of the reionization redshift z r independent of any cosmological model.
7.6.7 Magnetic fields
Primordial magnetic fields would be clearly indicated if cosmological Faraday
rotation were detected in the microwave background polarization. A field with
comoving field strength of 10−9 Gauss would produce a signal with a few degrees
of rotation at 30 GHz, which is likely just detectable with future polarization
experiments (Kosowsky and Loeb 1996). Faraday rotation has the effect of
mixing G-type and C-type polarization, and would be another contributor to the
C-polarization signal, along with tensor perturbations. Depolarization will also
result from Faraday rotation in the case of significant rotation through the lastscattering surface (Harari et al 1996). Additionally, the tensor and vector metric
perturbations produced by magnetic fields result in further microwave background
fluctuations. A distinctive signature of such fields is that for a range of power
spectra, the polarization fluctuations from the metric perturbations is comparable
to, or larger than, the corresponding temperature fluctuations (Kahniashvili et al
2000). Since the microwave background power spectra vary as the fourth power
of the magnetic field amplitude, it is unlikely that we can detect magnetic fields
with comoving amplitudes significantly below 10−9 Gauss. However, if such
fields do exist, the microwave background provides several correlated signatures
which will clearly reveal them.
7.6.8 The topology of the universe
Finally, one other microwave background signature of a very different character
deserves mention. Most cosmological analyses make the implicit assumption that
the spatial extent of the universe is infinite or, in practical terms, at least much
larger than our current Hubble volume so that we have no way of detecting the
bounds of the universe. However, this need not be the case. The requirement that
the unperturbed universe be homogeneous and isotropic determines the spacetime
metric to be of the standard Friedmann–Robertson–Walker form, but this is only

258

The cosmic microwave background

a local condition on the spacetime. Its global structure is still unspecified.
It is possible to construct spacetimes which at every point have the usual
homogeneous and isotropic metric, but which are spatially compact (have finite
volumes). The most familiar example is the construction of a three-torus from a
cubical piece of the flat spacetime by identifying opposite sides. Classifying the
possible topological spaces which locally have the metric structure of the usual
cosmological spacetimes (i.e. have the Friedmann–Robertson–Walker spacetimes
as a topological covering space) has been studied extensively. The zero-curvature
and positive-curvature cases have only a handful of possible topological spaces
associated with them, while the negative curvature case has an infinite number
with a very rich classification. See Weeks (1998) for a review.
If the topology of the universe is non-trivial and the volume of the universe
is smaller than the volume contained by a sphere with radius equal to the distance
to the surface of last scattering, then it is possible to detect the topology. Cornish
et al (1998) pointed out that because the last scattering surface is always a
sphere in the covering space, any small topology will result in matched circles of
temperature on the microwave sky. The two circles represent photons originating
from the same physical location in the universe but propagating to us in two
different directions. Of course, the temperatures around the circles will not match
exactly, but only the contributions coming from the Sachs–Wolfe effect and the
intrinsic temperature fluctuations will be the same; the velocity and Integrated
Sachs–Wolfe contributions will differ and constitute a noise source. Estimates
show the circles can be found efficiently via a direct search of full-sky microwave
background maps. Once all matching pairs of circles have been discovered, their
number and relative locations on the sky strongly overdetermine the topology of
the universe in most cases. Remarkably, the microwave background essentially
allows us to determine the size of the universe if it is smaller than the current
horizon volume in any dimension.

7.7 Finale: testing inflationary cosmology
In summary, the CMB radiation is a remarkably interesting and powerful source
of information about cosmology. It provides an image of the universe at an early
time when the relevant physical processes were all very simple, so the dependence
of anisotropies on the cosmological model can be calculated with high precision.
At the same time, the universe at decoupling was an interesting enough place
that small differences in cosmology will produce measurable differences in the
anisotropies.
The microwave background has the ultimate potential to determine
fundamental cosmological parameters describing the universe with percent-level
precision. If this promise is realized, the standard model of cosmology would
compare with the standard model of particle physics in terms of physical scope,
explanatory power and detail of confirmation. But in order for such a situation

Finale: testing inflationary cosmology

259

to come about, we must first choose a model space which includes the correct
model for the universe. The accuracy with which cosmological parameters can
be determined is of course limited by the accuracy with which some model in the
model space represents the actual universe.
The space of models discussed in section 7.5.1 represents universes which
we would expect to arise from the mechanism of inflation. These models have
become the standard testing ground for comparisons with data because they are
simple, general and well motivated. So far, these types of models fit the data
well, much better than any competing theories. Future measurements may remain
perfectly consistent with inflationary models, may reveal inconsistencies which
can be remedied via minor extensions or modifications of the parameter space or
may require more serious departures from these types of models.
For the sake of a concluding discussion about the power of the microwave
background, assume that the universe actually is well described by inflationary
cosmology, and that it can be modelled by the parameters in section 7.5.1. For
an overview of inflation and the problems it solves, see Kolb and Turner (1990,
ch 8) or the chapter by A Linde in this volume. To what extent can we hope to
verify inflation, a process which likely would have occurred at an energy scale of
1016 GeV when the universe was 10−38 s old? Direct tests of physics at these
energy scales are unimaginable, leaving cosmology as the only likely way to
probe this physics.
Inflation is not a precise theory, but rather a mechanism for exponential
expansion of the universe which can be realized in a variety of specific
physical models. Cosmology in general and the cosmic microwave background,
in particular, can hope to test the following predictions of inflation (see
Kamionkowski and Kosowsky 1999 for a more complete discussion of inflation
and its observable microwave background properties):
•

•

The most basic prediction of inflation is a spatially flat universe. The flatness
problem was one of the fundamental motivations for considering inflation in
the first place. While it is possible to construct models of inflation which
result in a non-flat universe, they all must be finely tuned for inflation to
end at just the right time for a tiny but non-negligible amount of curvature
to remain. The geometry of the universe is one of the fundamental pieces
of physics which can be extracted from the microwave background power
spectra. Recent measurements make a strong case that the universe is indeed
flat.
Inflation generically predicts primordial perturbations which have a
Gaussian statistical distribution. The microwave background is the only
precision test of this prediction. Primordial Gaussian perturbations will
still be almost precisely Gaussian at recombination, whereas they will
have evolved significant non-Gaussianity by the time the local large-scale
structure forms, due to gravitational collapse. Other methods of probing
Gaussianity, like number densities of galaxies or other objects, inevitably

260
•

•

•

•

The cosmic microwave background
depend significantly on astrophysical modelling.
The simplest models of inflation, with a single dynamical scalar field, give
adiabatic primordial perturbations. The only real test of this prediction
comes from the microwave background power spectrum. More complex
models of inflation with multiple dynamical fields generically result
in dominant adiabatic fluctuations with some admixture of isocurvature
fluctuations. Limits on isocurvature fluctuations obtained from microwave
background measurements could be used to place constraints on the size of
couplings between different fields at inflationary energy scales.
Inflation generically predicts primordial perturbations on all scales,
including scales outside the horizon. Of course we can never test directly
whether perturbations on scales larger than the horizon exist, but the
microwave background can reveal perturbations at recombination on scales
comparable to the horizon scale. Zaldarriaga and Spergel (1997) have argued
that inflation generically gives a peak in the polarization power spectrum at
angular scales larger than 2◦ , and that no causal perturbations at the epoch of
last scattering can produce a feature at such large scales. Inflation further
predicts that the primordial power spectrum should be close to a scaleinvariant power law (e.g. Huterer and Turner 2000), although complicated
models can lead to power spectra with features or significant departures from
scale invariance. The microwave background can probe the primordial power
spectrum over three orders of magnitude.
Inflationary perturbations result in phase-coherent acoustic oscillations. The
coherence arises because on any given scale, the perturbations start in the
same state determined only by their character outside the horizon. For
a discussion in the language of squeezed quantum states, see Albrecht
(2000). It is extremely difficult to produce coherent oscillations by any
mechanism other than perturbations outside the horizon. The microwave
background temperature and polarization power spectra will together clearly
reveal coherent oscillations.
Inflation finally predicts potentially measurable relationships between the
amplitudes and power law indices of the primordial density and gravitational
wave perturbations (see Lidsey et al 1997 for a comprehensive overview),
and measuring a ClC power spectrum appears to be the only way to obtain
precise enough measurements of the tensor perturbations to test these
predictions, thanks to the fact that the density perturbations do not contribute
to ClC . Detection of inflationary tensor perturbations would reveal the
energy scale at which inflation occurred, while confirming the inflationary
relationships between scalar and tensor perturbations would provide a strong
consistency check on inflation.

The potential power of the microwave background is demonstrated by
the fact that inflation, a theoretical mechanism which likely would occur
at energy scales not too different from the Planck scale, would result in

References

261

several distinctive signatures in the microwave background radiation. Current
measurements beautifully confirm a flat universe and are fully consistent with
Gaussian perturbations; the rest of the tests will come into clearer view over
the coming years. If inflation actually occurred, we can expect to have very
strong circumstantial supporting evidence from the above signatures, along with
precision measurements of the cosmological parameters describing our universe.
However, if inflation did not occur, the universe will likely look different in some
respects from the space of models in section 7.5.1. In this case, we may not be able
to recover cosmological parameters as precisely, but the microwave background
will be equally important in discovering the correct model of our universe.

Acknowledgments
I thank the organizers for a stimulating and enjoyable Summer School. The
preparation of this chapter has been supported by a grant from NASA and through
the Cotrell Scholars Program of the Research Corporation.

References
Adams W S 1941 Astrophys. J. 93 11
Albrecht A 2000 Structure Formation in the Universe ed R Crittenden and N Turok
(Dordrecht: Kluwer) to appear (astro-ph/0007247)
Alpher R A and Herman R C 1949 Phys. Rev. 75 1089
Bardeen J M 1980. Phys. Rev. D 22 1882
Bennett C L et al 1996 Astrophys. J. 464 L1
Birkinshaw M 1999 Phys. Rep. 310 97
Bond J R, Efstathiou G and Tegmark M 1997 Mon. Not. R. Astron. Soc. 291 L33
Bouchet F R and Gispert R 1999 New Astron. 4 443
Bucher M, Moodley K, and Turok N 1999 Phys. Rev. D 62 083508
Caldwell R R, Dave R, and Steinhardt P J 1998 Phys. Rev. Lett. 80 1582
Challinor A and Lasenby A 1999 Astrophys. J. 513 1
Christensen N and Meyer R 2000 Preprint astro-ph/0006401
Cornish N J, Spergel D N and Starkman G D 1998 Phys. Rev. D 57 5982
de Bernardis P et al 2000 Nature 404 955
Denisse J F, Le Roux E and Steinberg J C 1957 C. R. Acad. Sci., Paris 244 3030 (in French)
Dicke R H, Peebles P J E, Roll P G and Wilkinson D T 1965 Astrophys. J. 142 414
Dodelson S, Gates E and Stebbins A 1996 Astrophys. J. 467 10
Doroshkevich A G and Novikov I D 1964 Sov. Phys. Dokl. 9 111
Ehlers J 1993 Gen. Rel. Grav. 25 1225
Ellis G F R and Bruni 1989 Phys. Rev. D 40 1804
Esposito S, Mangano G, Miele G, and Pisanti O 2000 J. High Energy Phys. 9 038
Falco E E, Kochanek C S,and Munoz J A 1998 Astrophys. J. 494 47
Ferreira P G, Magueijo J and Silk J 1997 Phys. Rev. D 56 4592
Gamow G 1956 Vistas Astron. 2 1726
Gebbie T, Dunsby P and Ellis G F R 2000 Ann. Phys. 282 321

262

The cosmic microwave background

Gnedin N Y and Jaffe A H 2001 Astrophys. J. 551 3
Hamilton A J and Tegmark M 2000 Preprint astro-ph/0008392
Hanany S et al 2000 Astrophys. J. 545 L5
Harari D D, Hayward J and Zaldarriaga M 1996 Phys. Rev. D 55 1841
Hu W 2000 Astrophys. J. 529 12
Hu W and Gruzinov A 1998 Astrophys. J. 508 435
Hu W and Silk J 1993 Phys. Rev. Lett. 70 2661
Hu W and Sugiyama N 1996 Astrophys. J. 471 542
Hu W and White M 1996 Astrophys. J. 471 30
——1997 Astron. Astrophys. 321 8
Hu W, Seljak U, White M and Zaldarriaga M 1998 57 3290
Huterer D and Turner M S 2000 Phys. Rev. D 62 063503
Jackson J D 1975 Classical Electrodynamics 2nd edn (New York: Wiley)
Jaffe A H and Kamionkowski M 1998 Phys. Rev. D 58 043001
Jaffe A H, Stebbins A and Frieman J A 1994 Astrophys. J. 420 9
Jungman G, Kamionkowski M, Kosowsky A and Spergel D N 1996 Phys. Rev. D 54 1332
Kahniashvili T, Mack A, Kosowsky A and Durrer R 2000 Cosmology and Particle Physics
2000 ed J Garcia-Bellido, R Durrer and M Shaposhnikov to appear
Kamionkowski M and Kosowsky A 1998 Phys. Rev. D 67 685
——1999 Annu. Rev. Nucl. Part. Sci. 49 77
Kamionkowski M, Kosowsky A and Stebbins A 1997a Phys. Rev. Lett. 78 2058
——1997b Phys. Rev. D 55 7368
Kamionkowski M, Spergel D N and Sugiyama N 1994 Astrophys. J. Lett. 426 L57
Kaplinghat M and Turner M S 2001 Phys. Rev. Lett. 86 385
Knox L 1995 Phys. Rev. D 52 4307
Knox L, Scoccimaro R and Dodelson S 1998 Phys. Rev. Lett. 81 2004
Kodama H and Sasaki M 1984 Prog. Theor. Phys. Suppl. 78 1
Kolb E W and Turner M S 1990 The Early Universe (Redwood City, CA: Addison-Wesley)
Kosowsky A 1996 Ann. Phys. 246 49
——1999 New Astron. Rev. 43 157
Kosowsky A and Loeb A 1996 Astrophys. J. 469 1
Kosowsky A and Turner M S 1995 Phys. Rev. D 52 1739
Kragh, H 1996 Cosmology and Controversy (Princeton, NJ: Princeton University Press)
Lange A et al 2001 Phys. Rev. D 63 042001
Lidsey J E et al 1997 Rev. Mod. Phys. 69 373
Ma C P and Bertschinger E 1995 Astrophys. J. 455 7
McKellar A 1940 Proc. Astron. Soc. Pac. 52 187
Miller A et al 1999 Astrophys. J. 524 L1
Mould J, Kennicut R C and Freedman W 2000 Rep. Prog. Phys. 63 763
Mukhanov V F, Feldman H A and Brandenberger R H 1992 Phys. Rep. 215 203
Mukhanov V F and Steinhardt P J 1998 Phys. Lett. B 422 52
Peacock J A et al 1998 Mon. Not. R. Astron. Soc. 296 1089
Penzias A A and Wilson R W 1965 Astrophys. J. 142 419
Perlmutter S et al 1999 Astrophys. J. 517 565
Polarski D and Starobinsky A A 1994 Phys. Rev. D 50 6123
Riess A G et al 1998 Astron. J. 116 1009
Sachs R K and Wolfe A M 1967 Astrophys. J. 147 73
Seager S, Sasselov D and Scott D 2000 Astrophys. J. Suppl. 128 407

References

263

Seljak U 1996 Astrophys. J. 463 1
Seljak U, Pen U and Turok N 1997 Phys. Rev. Lett. 79 1615
Seljak U and Zaldarriaga M 1996 Astrophys. J. 469 437
Sellwood J and Kosowsky A 2000 Gas and Galaxy Evolution ed J E Hibbard, M P Rupen
and J van Gorkom in press
Sharov A S and Novikov I D 1993 Edwin Hubble, the Discoverer of the Big Bang Universe
(Cambridge: Cambridge University Press)
Smoot G F et al 1990 Astrophys. J. 360 685
Tegmark M and Zaldarriaga M 2000 Astrophys. J. 544 30
Tegmark M, Zaldarriaga M and Hamilton A J S 2001 Phys. Rev. D 63 043007
Thorne K S 1980 Rev. Mod. Phys. 52 299
Tolman R C 1934 Relativity, Thermodynamics, and Cosmology (Oxford: Oxford
University Press)
Tytler D et al 2000 Phys. Scr. in press (astro-ph/0001318)
Weeks J R 1998 Class. Quantum Grav. 15 2599
Weinberg S 2000 Preprint astro-ph/0005265
White M, Scott D and Silk J 1994 Annu. Rev. Astron. Astrophys. 32 319
Zaldarriaga 1997 Phys. Rev. D 55 1822
Zaldarriaga M and Seljak U 1997 Phys. Rev. D 55 1830
Zaldarriaga M, Seljak U and Spergel D N 1997 Astrophys. J. 488 1
Zaldarriaga M and Spergel D N 1997 Phys. Rev. Lett. 79 2180

Chapter 8
Dark matter search with innovative
techniques
Andrea Giuliani
University of Insubria at Como, Italy

The evidence that most of the matter in the universe does not shine has firmly
established the concept of dark matter (DM). It is by now clear that there is room
in our galactic halo for DM in the form of exotic particles (WIMPs—Weakly
Interacting Massive Particles—or axions) [1,2], whose supposed properties make
their experimental observation within the reach of frontier detection methods.
This stimulates the creativity of experimental physicists, who are induced to push
the existing techniques to their extreme limits or to elaborate new ones in order to
attempt DM detection.
The scope of this chapter is to give a survey of the most innovative detection
techniques (sections 8.3 and 8.4), comparing their potential with existing results,
after a brief elementary introduction on the general concepts of CDM direct
detection (section 8.1). Since I consider the approach based on phonon-mediated
particle detection one of the most promising, an entire section (8.2) is devoted to
this subject.

8.1 CDM direct detection
8.1.1 Status of the DM problem
The abundance of the luminous matter in the universe, inferred by direct
observations, is in the range 0.002 < lum < 0.005, if a reduced Hubble constant
h = 0.65 is taken as a reference value. In contrast, primordial nucleosynthesis
suggests 0.015 < baryon < 0.025, while gravitational effects lead to matter >
0.3. This scenario [3] shows that there are two separate DM problems: the gap
between lum and baryon requires baryonic matter in some exotic form (like
MACHOs or hot intergalactic gas), while the gap between baryon and matter
264

CDM direct detection

265

can admit particle physics solutions. In particular, axions and neutralinos look
like plausible candidates and their detection is within the reach of the present
technologies.
Recent observational achievements, suggesting an accelerating universe
expansion and a flat universe, lead to a scenario which accommodates an
important contribution from the vacuum energy (  2/3), leaving some room
for baryonic and non-baryonic DM, since it is expected that   1/3. Which
features do we require for the particles which are supposed to form, at least in
part, the non-baryonic fraction of the matter that escapes our observation? They
should be
•
•
•
•
•

neutral,
massive,
weakly interacting,
steady, or at least long living with respect to the universe age, and
with a relic abundance   0.1–1.

DM is usually classified as cold dark matter (CDM) and hot dark matter (HDM),
consisting, respectively, of fast and slow moving particles (for a review see for
example [4]). Neutrinos with masses below 30 eV are an example of HDM,
since they were relativistic at the decoupling time. The mechanism of galaxy
formation requires, however, a substantial amount of CDM; therefore neutrinos
cannot represent a complete solution for the DM problem. Axions and neutralinos
are examples of CDM. Axions, although their mass is expected to lie in the range
10−6 –10−3 eV, are slow moving since they were never in thermal equilibrium
and were non-relativistic since their first appearance at 1 GeV temperature [5].
Techniques for axion detection [6] are beyond the scope of this chapter and will
not discussed here. Neutralinos will be briefly introduced in the next subsection.
8.1.2 Neutralinos
Neutralinos (χ) [2, 7–9] are supersymmetric Majorana fermions consisting of
four mass eigenstates, defined as the linear superposition of the two neutral
gauginos and higgsinos. The lowest mass eigenstate may play the role of the
lightest supersymmetric particle (LSP) and constitute a viable CDM candidate.
Supersymmetric models involve several free parameters, whose choice fixes the
neutralino properties, such as the χ–χ annihilation rates and interaction rates with
ordinary matter. It is therefore possible, once an assumption has been made about
the free parameters, to calculate the neutralino relic density χ and the cross
section with atomic nuclei. There are wide regions in the parameter space which
correspond to χ values relevant for the DM problem (χ  0.1–1) and to
measurable interaction rates with reasonable mass detectors. Typical neutralino
masses are in the range 30–300 GeV, where the lower limit is due to accelerator
constraints.

266

Dark matter search with innovative techniques

Neutralinos are supposed to interact with quarks within the nucleons [10,11].
This interaction can be described by a total χ–nucleon cross section σp . The
parameter experimentally accessible is of course the χ–nucleus cross section σ0 ,
that can, in a very general way, be expressed as
σ0 ∝

2
gχ2 gN

ME4

µ2 k,

where ME is the mass of a virtual particle exchanged between the neutralino
and the nucleus in a t-channel interaction, gχ and gN the coupling constants of
this particle with neutralino and nucleus respectively, µ the reduced mass of the
neutralino–nucleus system and k a dimensionless constant. Since gχ and gN are
weak interaction couplings and ME is in the Fermi scale (it is, for example, one
of the Higgs masses in the case of Higgs boson exchange), the total cross section
has a typical weak size: for this reason, neutralinos are sometimes referred to by
the more generic term ‘WIMPs’. Two types of couplings are usually discussed:
•

scalar spin-independent (SI) coupling, for which
k = A2 FN ,

•

where A is the nucleon number and FN [12] a nuclear form factor; the term
A2 describes an enhancement of the cross section determined by the coherent
interaction with the nucleons;
axial spin-dependent (SD) coupling, which requires odd A (non-zero nuclear
spin); in this case
k = (λCW )2 J (J + 1),
where λ and CW [12] are nuclear form factors and J the nuclear spin.

Due to the coherence effect, SI coupling is expected to lead to much higher cross
sections. Knowledge of the nuclear form factors allows us to express σ0 in terms
of the χ–nucleon cross section σp . This makes comparisons among experiments
with different nuclear targets possible.
8.1.3 The galactic halo
There is kinematic evidence that there is a halo of DM around spiral galaxies. The
evidence comes from the observation of the galactic rotation curves, in which the
velocity of the galactic objects is expressed as a function of the object distance
from the galactic centre. Since this function is flat sufficiently far way from
the centre, instead of the Keplerian decline expected from the distribution of the
luminous matter, it is inferred that an invisible mass M(R) is contained in a radius
R, with M(R) ∝ R.
Many uncertainties, however, affect the shape profile and the mass
distribution in the halo. Moreover, a substantial component could be of baryonic
origin (MACHOs). Standard assumptions [12] are the following:

CDM direct detection
•
•

267

ρl = 0.3 GeV cm−3 , where ρl is the local halo density (at the sun position);
and
ρχ = ξρl , with ξ < 1, where ξ is the neutralino fraction of the halo density.

The neutralino velocity distribution is unknown; it is usually taken is
Maxwellian:
  
v 2
2 −3/2
dn ∝ (πv0 )
d3 v.
exp −
v0
To be more exact, v 2 should be replaced by |v + vE |2 , where vE is the Earth
velocity with respect to the DM distribution. In addition, the Maxwellian should
be truncated at |v + vE | = vesc , vesc being the galactic escape velocity. The
usual assumptions for the Maxwellian parameters are v0 = 230 km s−1 and
vesc = 600 km s−1 . A complete discussion about the halo structure and the
possible choices for the Maxwellian parameters can be found in [12].
An important point for DM direct detection concerns the motion of the Earth
inside the DM distribution [12]. This motion is the composition of the Sun’s
motion in the galaxy and of the orbital terrestrial motion. The velocity of the sun
in the halo affects the WIMP flux as seen by a terrestrial detector (one speaks
about a ‘WIMP wind’); in addition, the terrestrial orbital velocity adds to the
Sun’s velocity in summer and subtracts from it in winter. This determines an
expected seasonal modulation (typically up to 7%) in the WIMP interaction rate in
terrestrial detectors, with a maximum on 2 June. As we shall see in section 8.1.4,
this modulation may be a signature for DM identification. The rotational motion
of the Earth can also be responsible for a diurnal modulation in the average impact
direction of the WIMPs. This effect, much more difficult to detect but also much
more pronounced (the modulation would be of the order of some 10%), can also
constitute a precious tool for DM detection [13, 31].
8.1.4 Strategies for WIMP direct detection
The interaction of the WIMPs supposed to compose part of the galactic halo
determines a nuclear recoil rate in a terrestrial detector. In the case of elastic
scattering, isotropic in the centre of mass, the differential energy spectrum of the
nuclear recoil dR/dE R can be easily evaluated [12]. It is exactly exponential in
case of stationary Earth:
 

ER
dR
R0
exp −
=
,
dE R
E 0r
E 0r

(8.1)

where E R is the recoil energy, R0 the total rate, r a kinematic factor given by
r=

4Mχ MN
(Mχ + MN )2

268

Dark matter search with innovative techniques

(with Mχ is the neutralino mass and MN the target nucleus mass) and E 0 a
characteristic WIMP velocity expressed by
E 0 = 12 Mχ v02 .
When the finite velocity of the Earth in the Galaxy is accounted for, equation (8.1)
no longer holds and must be replaced by a more complicate expression [12],
which preserves anyway an almost exponential shape. Therefore, the expected
energy spectrum is featureless and dangerously similar to any sort of radioactive
background, which can often be well represented by an exponential tail at low
energies. The typical energies over which the spectrum extends can be estimated
from the expected Mχ and from the nuclear target mass. It is easy to check
with equation (8.1) that most of the counts are expected below 20 keV in typical
situations, for example with Mχ = 40 GeV and A = 127 (iodine-based detector).
This means that the spectrum must be searched for in a region very close to the
physical threshold of most conventional nuclear detectors.
In the simplified assumptions that vE = 0 and vesc = ∞, the total recoil rate
is given by [12]




ρχ v0
Nav 1000
2
σ0 ,
(8.2)
R0 =
A
Mχ
π 1/2
where, after a numerical factor, we can identify the number of targets in
one kilogram (second factor), the neutralino flow (third factor) and the cross
section for each target (last factor). Equation (8.2) predicts rates so low as to
represent a formidable challenge for experimentalists. Since neutralinos relevant
for the solution of the DM problem are expected to have a nucleon cross
section lower than 10−41 cm2 , total rates lower than 1 event/(day kilogram) and
10−3 event/(day kilogram) are predicted for SI and SD couplings, respectively.
Now that we know the features of what we are looking for, it is possible
to conceive an ideal device for WIMP detection. We need a low-energy nuclear
detector with the following characteristics:
•

•

A very low-energy threshold for nuclear recoils (given the nearly exponential
shape of the spectrum, a gain in threshold corresponds to a relevant increase
in sensitivity). Thresholds of ∼10 keV are reachable with conventional
devices, while with phonon-mediated detectors (see section 8.2) thresholds
down to 300 eV have already been demonstrated.
Very low raw radioactive background at low energies. In general, it
requires hard work in terms of material selection and cleaning to reduce
raw background below 1 event/(day kilogram keV). Backgrounds lower
than 10−1 event/(day kilogram keV) have already been demonstrated.
Furthermore, an underground site is necessary to host high sensitivity
experiments, since cosmic rays produce a huge number of counts at low
energies.

CDM direct detection
•

•

269

Sensitivity to a recoil-specific observable. This allows the ordinary γ and
β background for which the energy deposition comes from a primary fast
electron to be rejected. When such an observable is available, the only
relevant background source left consists in fast neutrons.
Sensitivity to a WIMP-specific observable; it is necessary for an undisputable
signature and consists typically in the seasonal modulation of the rate.

A simple measurement of a background level performed with a low-energy
nuclear detector produces information on the neutralinos in the galactic halo.
Usually, this information is expressed in the form of an exclusion plot in a
(ξ σp , Mχ ) plane. The challenge is to test those regions in this plane which are
populated by points corresponding to neutralinos viable for DM composition, in
the sense explained in section 8.1.2. A simple background measurement cannot
prove the existence of neutralinos; it can only exclude neutralinos with given
features.
The parameters which affect the shape of the exclusion plot are the threshold,
the background spectrum and the target mass. The exclusion plot is constructed
by first fixing a neutralino mass: given the nuclear target mass, this allows the
recoil spectrum shape apart from a normalization factor to be determined using the
exact version of equation (8.1); the value of ξ σp which leads the recoil spectrum
to ‘touch’ the background spectrum at one point constitutes the upper limit to ξ σp
for that neutralino mass. (Higher values of ξ σp would produce a recoil spectrum
with more counts in one energy bin than those experimentally observed.) The
repetition of this procedure over the whole mass range provides the exclusion
plot.
The effect on the exclusion plot of the relevant detector parameters can be so
summarized: reducing the background improves the exclusion plot for any WIMP
mass; reducing the nuclear target mass, the exclusion plot improves at low WIMP
masses, but worsens at high WIMP masses; reducing the threshold improves the
exclusion plot mainly at low WIMP masses. It is useless nowadays to operate
detectors with low target masses (say A < 50), since in this case the region with
higher sensitivity is already excluded by accelerator constraints. It is important to
point out that the exclusion plot does not improve with longer exposition times or
with higher detector masses. Relevant results can therefore be achieved even with
small detectors and short measurements, provided the background level is low.
In order to get a DM signature, it is important to realize detectors sensitive
to a WIMP-specific observable, like the seasonal modulation. For a detailed
discussion of this subject, see [12, 14]. Here, we shall follow the simplified
discussion reported in [15]. In the presence of halo WIMP interactions, a
component of the background must present a seasonal modulation with very
specific features, hard to mimic with fake effects:
•
•
•

the modulation must be present only in a definite energy region;
the modulation must be ruled by a cosine function;
the proper period is T = 1 year;

270
•
•

Dark matter search with innovative techniques
the proper phase is 152.5th day in the year (2 June); and
the proper modulation amplitude is <7% in the maximum sensitivity region.

In order to have a signal at the 1σ level, we require:
Ssum + Bsum − (Swin + Bwin) > (Ssum + Bsum + Swin + Bwin)1/2 ,

(8.3)

where Ssum and Bsum are the signal and background counts in summer, while
Swin and Bwin represent the corresponding observables in winter. Equation (8.3)
ensures that the difference between the summer and winter number of counts is
statistically significant. If one assumes that
Bsum = Bwin
Ssum − Swin = a(dR/dE)Mdet T E
Ssum + Swin = 2(dR/dE)Mdet T E
Bsum + Bwin = 2B Mdet T E,
where a is the relative modulation amplitude, B a background coefficient that is
expressed in event/(day kilogram keV), (dR/dE) an average signal rate per unit
mass and energy, also expressed in event/(day kilogram keV), Mdet the detector
mass, T the experiment duration and E the energy range relevant for the signal
expressed in keV. Inserting these observables in (8.3), one has as a condition on
a:
1/2 
1/2

B
1
2
1+
.
(8.4)
a>
(dR/dE)E
(dR/dE)
(Mdet T )1/2
The second term in the inequality (8.4) represents the lower limit for the
modulation amplitude. The sensitivity of the experiment scales therefore as
(Mdet T )1/2 , since the signal, growing as (Mdet T ), is in competition with
background fluctuations growing as (Mdet T )1/2 .
Unlike experiments aiming at exclusion plot production, searches for a real
signal imply large detectors and long exposition time. Of course, the same set-up
can produce an exclusion plot both from a background measurement and from the
non-observation of a modulation amplitude. Increasing the detector mass and the
exposition time, the second method becomes more stringent than the first, since
in the first case the sensitivity is constant, while in the second one it grows with
(Mdet T )1/2 . If we take, for example, A = 127, an energy threshold 20 keV,
B  1.5 event/(day kilogram keV), a modulation analysis requires a detector
mass around 100 kg to get the same sensitivity as a background analysis, assuming
Mχ  40 GeV.
In sections 8.2 and 8.3, we shall focus attention on how detectors which are
sensitive to a recoil-specific observable can be realized, with total masses high
enough to ensure a significant sensitivity to a seasonal modulation.

Phonon-mediated particle detection

271

Table 8.1. Nuclear quenching factors.
Qn

Detector

Recoiling nucleus

0.25
0.30
0.30
0.09
0.80

Ge diode
Si diode
NaI(Tl) scint.
NaI(Tl) scint.
Liquid Xe scint.

Ge
Si
Na
I
Xe

8.2 Phonon-mediated particle detection
Conventional nuclear detectors [16] (like scintillators and semiconductor diodes)
are sensitive to the amount of ionization that an energetic particle produce in
them. Since a slow nuclear recoil (like those produced by WIMP interactions)
is a scarcely ionizing particle, the response of a conventional device to such an
event is much lower than the response to an electron depositing the same energy.
An important quantity characterizing a WIMP detector is, therefore, the nuclear
quenching factor Q n , defined by
Q n (E) =

Rn (E)
Re (E)

where Rn (E) and Re (E) are the responses of the detector (measured for example
in volts, since detectors have typically voltage outputs) to a nuclear recoil and
to an electron respectively, for a deposited energy E. In principle Q n depends on
energy, but it can be considered constant with an excellent approximation over the
energy range of interest for WIMPs. Q n can also depend on the type of recoiling
nucleus. Some experimentally important values are reported in table 8.1.
Since a detector is usually calibrated by means of β and γ sources, the
obtained energy scale must be divided by Q n in order to get the nuclear recoil
energy scale. The real threshold is therefore higher than that determined by
the calibration; as a trade-off, the background, if not due to fast neutrons, is
reduced by a factor Q n , since to an energy interval E in the electron scale there
corresponds an energy interval E/Q n in the nuclear recoil energy scale.
Phonon-mediated detectors have the unique feature [17] that their Q n is
very close to one [18]. Joined with the extraordinary energy sensitivity of
these devices, this property allows these detectors to reach impressively low
energy thresholds. On the other side, the raw β and γ background is a serious
problem. One possible solution consists of developing a detector which combines
a phonon-mediated with a conventional read-out. The remarkable advantages of
this approach are reported in section 8.3. In this section, as an introduction, we
shall present briefly the basic principle of a phonon-mediated detector (PMD).

272

Dark matter search with innovative techniques

Over the last few years, PMDs have provided better energy resolution, lower
energy thresholds and wider material choice than conventional detectors for many
applications.
8.2.1 Basic principles
PMDs were proposed initially as perfect calorimeters, i.e. as devices able to
thermalize thoroughly the energy released by the impinging particle [19, 20]. In
this approach, the energy deposited by a single quantum into an energy absorber
(weakly connected to a heat sink) determines an increase of its temperature T .
This temperature variation corresponds simply to the ratio between the energy
released by the impinging particle and the heat capacity C of the absorber. The
only requirements are therefore to work at low temperatures (usually <0.1 K and
sometimes <0.015 K) in order to make the heat capacity of the device low enough,
and to have a sensitive enough thermometer coupled to the energy absorber. The
thermometer is usually a high sensitivity thermistor consisting either in a properly
doped semiconductor thermistor (ST) or in a superconductive film kept at the
transition edge, usually called the transition edge sensor (TES).
8.2.2 The energy absorber
The energy-absorbing part of the detector is usually a diamagnetic dielectric
material in order to avoid dangerous contributions to the specific heat in addition
to the Debye term, proportional to T 3 at low temperatures. In such devices, the
energy resolution can be fantastically
high and close to the so (but not properly)
√
called ‘thermodynamic limit’ kT 2 C [20]. However, the constraint set by the
heat capacity limits the maximum mass for the energy absorber to about 1 kg.
In fact, the real situation is far more complicated. The interaction of an
elementary particle with a solid-detecting medium produces excitations of its
elastic field; in other terms, the energy spectrum of the target phonon system
is modified. Only when the time elapsed after the interaction is long enough to
allow the phonon system to relax on a new equilibrium energy distribution, does
the detector really work as a calorimeter. In contrast, if the sensor response is very
fast, excess non-equilibrium phonons are detected long before they thermalize.
(In this case, the sensing element should be defined a ‘phonon sensor’ rather than
a ‘thermometer’). In many experimental situations, it is difficult to distinguish
between these two extreme cases, and the nature of the detection mechanism is
still poorly known. Nevertheless, even when PMDs are not pure calorimeters,
their intrinsic energy resolution is better than for conventional detectors, since the
typical energy of the excitations produced (high-frequency phonons) is the order
of the Debye energy (∼10 meV), instead of 1 eV or more as in ordinary devices (in
conventional Ge diodes, for instance, the energy required to produce an electron–
hole pair is around 3 eV). Since the energy resolution is limited intrinsically by

Innovative techniques based on phonon-mediated devices

273

the fluctuations of the excitation number, its value scales as the square root of the
energy required on the average to produce a single excitation.
Detection of non-equilibrium phonons is very attractive because it can, in
principle, provide information about interaction position (space resolution has
already been proved with this method), discrimination about different types of
interacting radiation and the direction of the primary recoil in the target material.
The last two points remain to be proved.
8.2.3 Phonon sensors
As anticipated, the commonly used phonon sensors are STs and TESs. STs consist
usually of Ge or Si small crystals with a dopant concentration slightly below the
metal–insulator transition [21, 22]. This implies a steep dependence of the sensor
resistivity on temperature at low temperatures, where the variable range hopping
conduction mechanism dominates.
TESs are much more sensitive devices, since their resistivity changes rapidly
from a finite value to zero in a very narrow temperature interval. Normally,
the superconductive film is deposited on the absorber crystal, with a typical
thickness of few hundred nanometres, and the shape is defined after deposition by
photolithography and wet etching. With a rectangular shape the normal resistance
near the critical temperature is typically between several m and several , and
SQUID technology is required for the readout, but with meander shape resistances
of ∼10 k can be obtained, and a standard voltage-sensitive preamplifier can be
used. Films are usually made of a single superconductor. (The most interesting
results have been obtained with tungsten [23, 24].) In another approach, the film
consists of two layers (a normal metal in contact with a superconductor): this
structure allows the critical temperature to be tuned.

8.3 Innovative techniques based on phonon-mediated devices
8.3.1 Basic principles of double readout detectors
An important feature of PMDs is that a high response is expected for energies
deposited by slow (<100 keV) nuclear recoils, which are difficult to detect
with conventional devices because of their scarce ionizing power. In a perfect
calorimeter, a nuclear recoil produces the same signal of a fast electron of the
same energy, since it deposits the same amount of heat. In spite of the naiveness
of this approach, it has been proven with ad hoc measurements that the detecting
efficiency for recoiling nuclei and electrons is indeed the same within 2% in
dielectric ST-PMDs [18]. In other terms, as already anticipated, Q n  1 for
PMDs. As a consequence, impressively low thresholds can be achieved in large
amounts of low specific heat material (typically sapphire). If properly operated
in a low radioactive background environments, these low threshold PMDs can be
very sensitive DM detectors. The CRESST experiment has installed in the Gran

274

Dark matter search with innovative techniques

Sasso laboratory 4 × 250 g sapphire-TES detectors with a threshold of about
300 eV, which is well beyond the reach of any conventional scheme [23].
Perhaps, the best strategy for PMDs around WIMP search is the achievement
of an active rejection of background through the recognition of nuclear recoils,
expected from WIMP interactions. The basic idea consists of realizing a detector
with both a phonon-mediated and a conventional readout, which could be a charge
signal (in the case of semiconductor diodes) or a light signal (in the case of
scintillators). The charge signal is proportional to the number of electron–hole
pairs, while the light signal is proportional to the amount of scintillation produced
by the interacting particle. I will define the non-phonon signal Snp as the output
provided by the conventional (charge or light) readout, and the phonon signal Sph
the output given by the phonon sensor.
The basic point is that the same event produces, in general, both a phonon
and a non-phonon signal. If we consider the observable:
R=

Snp
Sph

(8.5)

the value of R depends on the type of primary interaction. In the case of slow
nuclear recoil R is significantly higher than for a fast electron of the same energy,
since the non-phonon component, connected to the amount of ionization, is much
less important. The parameter R defined in (8.5) therefore represents a powerful
recoil-specific observable in the sense exposed in section 8.1.4.
In practice, a nuclear detector which follows all the specifications of
section 8.1.4 could consist in one of the two following possibilities:
•

•

An array of large Ge or Si diodes operated as conventional semiconductor
devices with an additional phonon sensor. The total mass must be large
enough to make the detector competitive in terms of seasonal modulation
sensitivity (WIMP-specific observable). Therefore, the array must consist
of tens of individual elements. The double readout provides the recoilspecific observable R. The raw background and the energy threshold must
be conveniently low.
An array of large scintillators with an additional phonon sensor and with the
same features as in the previous point in terms of total mass, threshold and
background. A remarkable technical difficulty consists of the necessity to
operate a light detector at very low temperatures.

Three collaborations in the world are successfully developing detectors fulfilling
these two requirements. That is the topic of the next section.
8.3.2 CDMS, EDELWEISS and CRESST experiments
The American collaboration CDMS (‘Cold Dark Matter Search’) [24] is realizing
silicon and germanium detectors cooled to 20 mK and capable of measuring both

Innovative techniques based on phonon-mediated devices

275

the charge and the phonon component of any single energy release. The charge
is measured by means of conventional charge amplifier technology [16], whereas
the phonon measurement is performed with two different technologies. One is
based on eutecticly bonded Ge ST, and the other on W TES elements sensitive to
non-equilibrium phonons. In the second approach [25], non-equilibrium phonons
created by particle interactions break Cooper pairs in superconductive Al films
which cover a large fraction of the crystal surface. The created quasiparticles
are then trapped in a W film (with a critical temperature around 70 mK) which
is grown above the Al films. The W film is operated as a TES and, heated
by the trapped quasiparticles, provides the signal, proportional to the initially
deposited energy. The system of Al and W films presents a pattern which allows
reasonable space resolution (of the order of 1 mm) in the plane where the films lie
(the crystal surface) to be achieved. In the dimension orthogonal to this plane,
space resolution is also possible exploiting the risetime of the phonon signal.
This allows the events which occur close to the crystal surface to be recognized.
This detector capability helps substantially in background identification. The
point is that background events generated by β contamination in the surface can
mimic nuclear recoil events, since events at the surface suffer from incomplete
charge collection, while the phonon signal is, of course, unchanged. The space
resolution permits us to identify these close-to-surface events and to reject them.
Therefore, at the price of an acceptable loss of sensitive volume, the background
identification is much safer. In preliminary tests, a rejection capability better than
99% was achieved down to 20 keV. In figure 8.1, the points in the upper band
correspond to γ interactions, while the ones in the lower band to nuclear recoils.
In these tests, the nuclear recoils are induced by means of external sources of fast
neutrons.
The French collaboration EDELWEISS [26] adopts a scheme similar to
the first type of CDMS detector. The best results were obtained with a 70 g
high-purity Ge detector with a disk shape. The charge signal is provided by a
conventional readout, based on charge amplifier technology, while the phonon
signal comes from a Ge ST glued on the disk. In the range 15–70 keV a
raw background of about 40 event/(day kilogram keV) is reduced down to
0.6 event/(day kilogram keV). This collaboration aims at operating a large mass
experiment, realized by means of many independent detectors, in the Frejus
underground laboratory (France).
The German–English collaboration CRESST [27] is developing a detector
sensitive to phonons and scintillation light. A test device was realized, consisting
of a 1 cm3 CaWO4 crystal scintillator. A W film (with a critical temperature
around 11 mK) is deposited on the crystal and operated as a TES. The scintillation
photons which escape from the crystal are collected by auxiliary Al2 O3 PMDs
which surround the scintillator. Due to the very low threshold of the auxiliary
detectors, a few photons can be detected by them, allowing a safe threshold to be
set down to 15 keV (for nuclear recoils). The rejection capability at this energy is,
impressively enough, 99.7%. This result can be appreciated in figure 8.2, where

276

Dark matter search with innovative techniques

Figure 8.1. Discrimination capability of the CDMS experiment.

the parameter R in (8.5) is given by the slope of the bands. Even in this case, the
total detector mass can be increased only through the realization of a large array
of independent devices, which should be operated underground, for example in
the Gran Sasso Laboratory (Italy).
8.3.3 Discussion of the CDMS results
The CDMS collaboration was able to perform up to now the most sensitive
experiment in terms of exclusion plot [24] (see figure 8.4). This shows clearly the
potential of the double readout technique based on phonon-mediated detection.
The CDSM experiment, even if largely preliminary, is particularly important
since it allows us to probe, at least partially, the region in the (ξ σp , Mχ ) plane
corresponding to the modulation evidence claimed by the Italian collaboration
DAMA [28]. I shall just recall here that this modulation evidence can be
interpreted in terms of halo neutralino interactions with the most probable values
of Mχ = 44 GeV and ξ σp = 5.4 × 10−41 cm2 . The corresponding 3σ region is
reported in figure 8.4.
The CDMS detectors are operated at Stanford beneath only a 16 m water
equivalent overburden. A plastic scintillator veto is therefore necessary in order
to reject cosmic ray events. The results are based on two data-sets:
•

Two month exposure in 1998, providing 33 live days collected with one 100 g
Si detector operated with a W–Al film phonon readout (see previous section).

Innovative techniques based on phonon-mediated devices

277

Figure 8.2. Discrimination capability of the CRESST test detector.

•

In this exposition, the collected statistics correspond to Mdet T = 1.6 kg day.
After the background rejection, only four events survive as slow nuclear
recoils.
Twelve month exposure in 1999, providing 96 live days collected with four
165 g Ge detectors operated with a Ge ST phonon readout (see previous
section). In this exposition, the collected statistics correspond to Mdet T =
10.6 kg day. After the background rejection, only 17 events survive as slow
nuclear recoils. The four Ge detectors are tightly packed in order to increase
the neutron multiple scattering probability. Since four recoil events were cut
as in coincidence between two detectors (of course the probability of WIMP
double scattering is completely negligible), only 13 recoil events attributable
to WIMPs survive. (In figure 8.3 the nuclear recoil events are represented by
circled points.)

The 13-nuclear-recoil energy spectrum is compatible with the expected
WIMP-caused spectrum as deduced by the DAMA neutralino parameters.
However, the CDMS collaboration claims that there is clear evidence that these
13 single events are caused by background neutrons. In fact, the background

278

Dark matter search with innovative techniques

Figure 8.3. Gamma/beta background and nuclear recoils in CDMS results.

neutron spectrum can be estimated by the four Ge multiple events and the four Si
nuclear recoils. (Si events cannot be due to WIMPs, otherwise the WIMP rate in
Ge would be much higher than observed because of the A2 term (section 8.1.2)
in the cross section.) These safe neutron events, if analysed by means of a
Monte Carlo simulation of the neutron background, are fully compatible with
13 background neutron single events in the Ge experiment. In other terms, the Ge
multiple events and the Si single events fix the neutron background, that can be
subtracted by the Ge single event spectrum, leaving a WIMP signal compatible
with zero. Following this analysis, the CDMS collaboration claims to have
substantially falsified the DAMA interpretation of the seasonal modulation in
terms of the neutralino. In figure 8.4 the CDMS exclusion plot (black thick curve)
is compared with the shadow region which represents the DAMA 3σ evidence.
The CDMS sensitivity is also reported (as fixed by the a priori estimated neutron
background). The exclusion plot provided by a powerful conventional technique
(Ge semiconductor diodes, no double readout) is shown for comparison.
Anyway, the contrast between DAMA and CDMS looks far from being
clarified by the existing data. If it is true that the DAMA results have raised not
only excitement but also criticism in part of the DM community [29], it is also
clear that the CDMS results would require confirmation with higher statistics and
in an environment less affected by the cosmic neutron background. Information

Other innovative techniques

279

Figure 8.4. Exclusion plots (90% C.L.) and DAMA evidence.

to resolve the dilemma could come from the much larger underground set-up that
CDMS is going to operate in the near future and from the DAMA upgrading in
terms of total detector mass.

8.4 Other innovative techniques
There are many mid-term projects which are not based on a phonon readout
channel but which, however, point to a substantial increase in sensitivity to
neutralino interactions. I shall mention here, for lack of space, only three projects.
This selection was admittedly also made on the basis of personal taste, besides
scientific relevance. For a complete review, I suggest the reader refers to the
proceedings of a recent specific conference, for example [30].
The DRIFT experiment [31] represents the only attempt to detect the
direction of the nuclear recoil already at a test-phase. It consists of a low-pressure
TPC using a 20 Torr Xe–CS2 gas mixture. Such a device must be able to detect the
tiny tracks from nuclear recoils with less than 1 mm track resolution. In addition,
large detector masses are necessary. This requirement suggests that the magnetic
field should be abandoned, with a consequent deterioration in the space resolution
due to enhanced diffusion. In order to solve this problem, the new concept

280

Dark matter search with innovative techniques

consists of detecting not the drift electrons, but the negative CS2 ions, with a
considerably reduced diffusion because of the large ion mass. This experiment
points directly at the most decisive WIMP signature, the diurnal modulation of
the recoil average direction (section 8.1.4). By the end of 2001 a 20 m3 TPC
should be in operation.
The ZEPLIN programme [32] is based on double readout (scintillation and
charge) in liquid xenon. When an ionizing particle deposits energy in liquid xenon
and an electric field is applied, two scintillation pulses are developed. The primary
pulse (amplitude S1 ) is due to the excitations in Xe atoms produced directly by the
particle interaction. The secondary pulse (amplitude S2 ) is generated by the drift
of the charge created by the primary interaction. Therefore, a low ionizing particle
like a slow nuclear recoil will exhibit a secondary pulse depressed in amplitude
with respect to the first one, if compared with an electron of the same energy.
On a similar footing as in section 8.3.1, S1 /S2 plays the role of a recoil-specific
observable.
The project CUORE [33] (‘Cryogenic Underground Observatory for Rare
Events’) consists of the largest PMD set-up ever conceived. It is based on the
experience collected by the Milano group on large mass arrays of low-temperature
calorimeters for rare decays [34]. It should consist of a tightly packed array of
1020 TeO2 crystals for a total mass of 0.8 ton, to be cooled down to 10 mK.
Each element has a mass of about 800 g and uses a Ge ST (section 8.2.3) as
a thermometer. The relevant points of the project are the huge mass (which
provides sensitivity to seasonal modulation) and the low background, which can
be reduced significantly with respect to the ∼1 event/(day kilogram keV) already
demonstrated with similar devices. This reduction should be achieved by the
operation of the detectors in coincidence, particularly effective in this case due to
the minimal amount of inert material among them. A preliminary test of CUORE
is in preparation at the underground Gran Sasso Laboratory (Italy).

References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]

Turner M S and Tyson J A 1999 Rev. Mod. Phys. 71 S145
Masiero A and Pascoli S this volume
Dodelson S, Gates E I and Turner M S 1996 Science 274 69D
Sadoulet B 1999 Rev. Mod. Phys. 71
Abbott L and Sikivie P 1983 Phys. Lett. B 120 133
Sikivie P 1983 Phys. Rev. Lett. 51 141
Jungman G et al 1996 Phys. Rev. 267 195
Ellis J et al 1997 Phys. Lett. B 413 355
Edsjo J and Gondolo P Phys. Rev. D 56 1879
Goodman M W and Witten E 1985 Phys. Rev. D 31 3059
Primack J R et al 1988 Annu. Rev. Nucl. Part. Sci. 38 751
Lewin J D and Smith P F 1996 Astropart. Phys. 6 87
Spooner N J C and Kudryavtsev (eds) 1999 The Identification of Dark Matter
(Singapore: World Scientific)

References
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]

[25]
[26]
[27]
[28]
[29]
[30]

[31]

[32]

[33]
[34]

281

Freese K et al 1988 Phys. Rev. D 37 3388
Bernabei R 1995 Riv. Nuovo Cimento 18 5
Knoll G F 1989 Radiation Detection and Measurement (New York: Wiley)
Giuliani A 2000 Physica B 280 501
Alessandrello A et al 1997 Phys. Lett. B 408 465
Fiorini E and Niinikoski T O 1984 Nucl. Instrum. Methods 224 83
Moseley S H, Mather J C and McCammon D 1984 J. Appl. Phys. 56 1257
Mott N F 1969 Phil. Mag. 19 835
Giuliani A and Sanguinetti S 1993 Mater. Sci. Eng. R 11 1
Sisti M et al 2000 Nucl. Instrum. Methods 444 312
Gaitskell R et al 2001 Latest results from CDMS collaboration Sources and Detection
of Dark Matter in the Universe. Proc. 4th Int. Symp. Sources and Detection of
Dark Matter/Energy in the Universe, February 23–25, 2000, Marina del Rey, CA
ed D Cline (Berlin: Springer)
Cabrera B et al 2000 Nucl. Instrum. Methods 444 304
Chardin G et al 2000 Nucl. Instrum. Methods 444 319
Meunier P et al 1999 Appl. Phys. Lett. 75 1335
Bernabei R this volume
Gerbier G et al 1997 Preprint astro-ph/9710181
Gerbier G et al 1999 Preprint astro-ph/9902194
Cline D (ed) 2001 Sources and Detection of Dark Matter in the Universe. Proc. 4th
Int. Symp. Sources and Detection of Dark Matter/Energy in the Universe, February
23–25, 2000, Marina del Rey, CA (Berlin: Springer)
Martoff C J et al 2001 DRIFT Sources and Detection of Dark Matter in the Universe.
Proc. 4th Int. Symp. Sources and Detection of Dark Matter/Energy in the Universe,
February 23–25, 2000, Marina del Rey, CA ed D Cline (Berlin: Springer)
Wang H et al 2001 Design of the Zeplin II Detector Sources and Detection of
Dark Matter in the Universe. Proc. 4th Int. Symp. Sources and Detection of Dark
Matter/Energy in the Universe, February 23–25, 2000, Marina del Rey, CA ed
D Cline (Berlin: Springer)
Fiorini E 1998 Phys. Rep. 307 309
Alessandrello A et al 2000 Phys. Lett. B 486 13

Chapter 9
Signature for signals from the dark universe
The DAMA Collaboration
R Bernabei1 , M Amato2, P Belli1 , R Cerulli1 , C J Dai3 , H L He3 ,
G Ignesti2 , A Incicchitti2 , H H Kuang3 , J M Ma3 ,
F Montecchia1 , D Prosperi2
1 Department of Physics, University of Rome ‘Tor Vergata’ and
INFN, Rome, Italy
2 Department of Physics, University of Rome ‘La Sapienza’ and
INFN, Rome, Italy
3 IHEP, Chinese Academy, Beijing, China

The DAMA experiment is located at the Gran Sasso National Laboratories of the
INFN and is searching for dark matter (DM) particles using various scintillators as
target-detector systems. In particular the results, presented here, were obtained by
analysing, in terms of the WIMP annual modulation signature, the data collected
with the highly radiopure (∼100 kg NaI(Tl)) set-up during four annual cycles
(total statistics of 57 986 kg day).

9.1 Introduction
In the past few years, the many experimental and theoretical studies have changed
the main question on the DM problem from its existence to the nature of its
constituents. The stringent limit on the baryonic part (arising from a comparison
between the measured relative abundance of light elements with their expectations
in the nucleosynthesis scenario) and the results achieved in investigations of the
cosmic microwave background (which have ruled out the pure hot DM scenario)
support the view that—whatever the DM composition turns out to be (even if a
cosmological constant different from zero is definitively demonstrated)—a large
amount of CDM is necessary. This can be in the form of WIMPs or axions.
282

Introduction

283

In particular, the WIMPs should be neutral particles in thermal equilibrium
in the early stage of the universe, decoupling at the freeze-out temperature, with
a cross section for ordinary matter of the order of or lower than the weak one,
forming a dissipationless gas trapped in the gravitational field of the galaxy. To
be a suitable WIMP candidate a neutral particle should be stable or have a decay
time of the order of the age of the universe. The neutralino, which results in
stable MSSM and SUGRA models with R-parity conservation, is at present the
more studied candidate; it also remains a good candidate in the case of models
without R-parity conservation, if the decay time is of the order of the age of
the universe. Other candidates can also be considered; moreover, since this type
of search requires investigation beyond the SM of particle physics, the possible
nature of WIMPs is, in principle, fully open.
WIMPs can be searched for by direct and indirect techniques. However,
we have to remark that significant uncertainties exist in every model-dependent
analysis and—as can be easily understood—they are even larger in the indirect
approach.
In the following we will focus our attention on some of the main points
related to the WIMP direct searches by investigating elastic scattering on target
nuclei. As regards investigation of WIMP–nucleus inelastic scattering, we only
mention them here [1–3], stressing that much lower counting rates for the signal
are expected in this case.
The main strategy to search for these processes effectively is based on the use
of low radioactive experimental set-ups located deep underground. Significant
improvements in the overall radiopurity of the set-up have been reached over
several years of work, the ultimate limit remaining as the sea level activation
of the materials. This limitation would, however, be significantly overcome if
chemical/physical purifications of the used materials could occur just before their
storage deep underground and—even more—if all the operations for detector
construction were to be performed deep underground.
Another crucial point (as always in experiments which require a very low
energy threshold) is the possibility of identifying and effectively rejecting the
residual noise above the considered energy threshold. This problem has obviously
to be faced with every type of detector. For most of them the rejection is quite
uncertain (also affecting the quoted results), because the noise and the ‘physical’
pulses have indistinguishable features. In contrast, an almost unique effective
noise rejection is possible in scintillators:
(i) when the pulse decay time is relatively long with respect to the fast single
photoelectrons from the PMT noise;
(ii) when the number of photoelectrons/keV is really large;
(iii) when the noise contribution from the electronic chain is low; and
(iv) when a sensitive rejection procedure is used.
We note, in addition, that scintillators are unaffected by microphone noise in
contrast to ionizing and bolometer detectors.

284

Signature for signals from the dark universe

Although exclusion plots are widely used in practice, many uncertainties
arise in comparisons of the results arising from different experiments—even
more so when different techniques are used. Furthermore, direct comparison
is impossible when different target nuclei are used. To overcome this, it is
mandatory to realize experiments with a real signature for the possible signal.
If we discard the following possibilities:
(i) a possible comparison between results from different experiments (which
can, in principle, be considered since the rate is proportional to A2 for the
spin-independent interactions and to the spin factor for the spin-dependent
ones), because e.g. of the relevant role played by the different backgrounds;
(ii) the daily variation of the signal rate [4] (which can, in principle, be
considered since the Earth depth crossed by the WIMPs varies during the
day inducing a daily variation rate), because this effect is effective only in
the case of high cross sections; and
(iii) the correlation of the nuclear recoil track with the Earth’s galactic motion
(arising from the WIMP velocity distribution), because of the shortness of
the induced tracks.
Only the possibility of studying the annual modulation of the WIMP wind [5, 6]
remains. This so-called annual modulation signature is the annual modulation of
the WIMP rate induced by the Earth’s motion around the Sun [5–9].
In particular, the DAMA collaboration is performing this investigation
with the highly radiopure ∼100 kg NaI(Tl) set-up at the Gran Sasso National
Laboratory of INFN [7–15].
As has been clearly pointed out by DAMA [7–9, 12, 15], the annual
modulation signature is a well-distinguished one, requiring the presence not of a
‘generic’ rate variation but of a variation according to the following specifications:
(i)
(ii)
(iii)
(iv)

the presence of a correlation with the cosine function;
an appropriate proper period (1 year);
the proper phase (about 2 June);
only in a well-defined low-energy region (where WIMP-induced recoils
could be significantly present);
(v) for events where only one detector of many actually fires (single ‘hit’ events)
since the probability of a WIMP multi-scattering is negligible (in practice
each detector has all the others as a veto);
(vi) with modulated amplitude in the region of maximal sensitivity not exceeding
.7%.
That all these requirements have been realized by DAMA has been verified by the
following actions.
(i) The collection of the whole energy spectrum from single photoelectron to
the MeV range;
(ii) the continuous monitoring and control of several parameters; and

The highly radiopure ∼100 kg NaI(Tl) set-up

285

(iii) many consistency checks and statistical tests [7–9, 12, 13, 15].
Therefore, to mimic the WIMP annual modulation signature a systematic effect
should not only be quantitatively significant, but also able to satisfy the six
requirements for a WIMP-induced effect.
In the following, we will summarize only the more recently released results
on the WIMP search using the annual modulation signature using the 100 kg
NaI(Tl) DAMA set-up [12].
However, for the sake of completeness it is worth recalling that the DAMA
DM searches are based on the use of
(i) the ∼100 kg NaI(Tl) set-up;
(ii) the ∼2 l liquid xenon pure scintillator; and
(iii) the CaF2 (Eu) prototypes.
Recent references are, for example, [2, 3, 7–10, 12, 13, 16–21]. Moreover, several
results on different topics have also been achieved [11, 14, 17, 19, 22–28].

9.2 The highly radiopure ∼100 kg NaI(Tl) set-up
A detailed description of the DAMA set-up and of its performances is given
in [12], while the stability control of the various parameters, the noise rejection,
the efficiency, the calibrations, the higher energy stability, the total hardware rate,
etc have been discussed in [8, 9, 12, 13, 15]. Nine 9.70 kg NaI(Tl) detectors
have been especially built for the experiment on the WIMP annual modulation
signature by means of a joint effort with Crismatec company. The materials
used for these detectors have been selected—as well as those for the PMTs—
by measuring sample radiopurities with low background germanium detectors
deep underground in the low background facility of the Gran Sasso National
Laboratory [12]. As regards the samples of powders, their U/Th content was
measured in Ispra with a mass spectrometer, while their K content was determined
in the chemical department of the University of Rome ‘La Sapienza’ with an
atomic absorption spectrometer. A single growth has been used for all the crystals.
The crystals are enclosed in a low radioactive copper box inside a low radioactive
shield made from 10 cm of copper and 15 cm of lead; the lead is surrounded
by 1.5 mm Cd foils and about 10 cm of polyethylene/paraffin. The copper
box is maintained in a nitrogen atmosphere by continuously flushing high-purity
nitrogen gas. Each detector is viewed through 10 cm long light guides by two low
background EMI9265B53/FL—3 in diameter—PMTs working in coincidences;
the hardware threshold for each PMT is at single photoelectron level. The 9.70 kg
detectors have tetrasil-B light guides directly coupled to the bare crystals (also
acting as windows). Four other crystals of 7.05 kg—originally developed for
other purposes—are used as a cut-off for the other detectors and for special
triggers; they have tetrasil-B windows and are coupled to the PMTs in one case
by tetrasil-B and in the others by noUV-plexiglass light guides. All the crystals

286

Signature for signals from the dark universe
Table 9.1. Released data-sets; the number 1 to 4 refer to different annual cycles.
Period

Statistics (kg day)

Reference

DAMA/NAI-1
DAMA/NaI-2
DAMA/NaI-3
DAMA/NaI-4
Total statistics

4 549
14 962
22 455
16 020
57 986

[7]
[8]
[9]
[9]
[9]

+ DAMA/NaI-0

Limits on recoils fraction by PSD

[10]

have surfaces polished with the same procedure and enveloped in a TETRATEC-4
(Teflon) diffuser such as the light guides.
On the top of the shield a glove-box, maintained in the same nitrogen
atmosphere as the Cu box containing the detectors, is directly connected to it
through four Cu thimbles in which source holders can be inserted to calibrate all
the detectors at the same time without allowing them to enter in direct contact
with environmental air. The glove-box is equipped with a compensation chamber.
When the source holders are not inserted, the Cu bars completely fill the thimbles.
Since this set-up has been realized with the main purpose of studying the annual
modulation signature of WIMPs, several parameters are monitored and acquired
by CAMAC. A monitoring and alarm system operates continuously by a selfcontrolled computer processes.
Finally, we recall that the measured low-energy counting rate has been
published in various energy intervals [8, 9, 14, 15, 20], while in [26] higher energy
regions are shown.

9.3 Investigation of the WIMP annual modulation signature
The present result concerns four years of data-taking for the annual modulation
studies, namely DAMA/NaI-1,2,3 and 4 [7–9] for total statistics of 57 986 kg day,
the largest statistics ever collected in the field of WIMP search. Moreover, in the
final global analysis the constraint, arising from the upper limits on the recoil rate
measured in [10] (DAMA/NaI-0), has also been properly included (see table 9.1).
9.3.1 Results of the model-independent approach
In figure 9.1 we show the model-independent residual rate for the cumulative
2–6 keV energy interval as a function of the time [9], which offers immediate
evidence for the presence of modulation in the lowest energy region of the
experimental data.

Investigation of the WIMP annual modulation signature

287

Figure 9.1. Model-independent residual rate in the 2–6 keV cumulative energy interval
as a function of the time elapsed since 1 January of the first year of data-taking. The
expected behaviour of a WIMP signal is a cosine function with a minimum around the
broken vertical lines and with a maximum around the dotted ones.

The χ 2 test of the data in figure 9.1 is not favourable towards the hypothesis
of unmodulated behaviour giving a probability of 4×10−4 . However, fitting these
residuals with the function A cos ω(t − t0 ) (obviously integrated in each of the
considered time bins), one gets for the period T = 2π/ω = (1.00 ± 0.01) years,
when fixing t0 at 152.5 days and for the phase t0 = (144±13) days, when fixing T
at 1 year (similar results, but with slightly larger errors, are found when both these
parameters are kept free). The modulation amplitude as a free parameter gives
A = (0.022 ±0.005) cpd kg−1 keV−1 and A = (0.023 ±0.005) cpd kg−1 keV−1 ,
respectively. As is evident, the period and the phase fully agree with the ones
expected for a WIMP-induced effect.
As we will further comment, this model-independent analysis provides
evidence for the possible presence of a WIMP signal independently of the nature
of the WIMP and its interaction with ordinary matter. In the following we will
briefly summarize the investigation of possible systematics able to mimic such a
signature, that is not only quantitatively significant, but also able to satisfy the six
requirements given earlier; none has been found. A detailed discussion can be
found, for example, in [15].
9.3.2 Main points on the investigation of possible systematics in the new
DAMA/NaI-3 and 4 running periods
We have already presented elsewhere the results of the investigations of all
the possible known sources of systematics [7–9, 12, 13, 15]; however, in the
following we will briefly discuss, in particular, the data from the DAMA/NaI-

288

Signature for signals from the dark universe

3 and DAMA/NaI-4 running periods, which have been recently released [9]; a
devoted discussion can be found—as previously mentioned—in [15]. Similar
arguments for the DAMA/NaI-1 and DAMA/NaI-2 data have already been
discussed elsewhere [7, 8, 13] and at many conferences and seminars.
In our set-up the detectors have been continuously isolated from
environmental air for several years; different levels of closures are sealed and
maintained in a high-purity nitrogen atmosphere. However, the environmental
radon level in the installation is continuously monitored and acquired with the
production data; the results of the measurements are at the level of the sensitivity
of the used radonmeter. For the sake of completeness, we have examined the
behaviour of the environmental radon level with time. When fitting the radon
data with a WIMP-like modulation, the amplitudes (0.14 ± 0.25) Bq m−3 and
(0.12 ± 0.20) Bq m−3 are found in the two periods respectively, both consistent
with zero. Further arguments are given in [15]. Moreover, we remark that
a modulation induced by radon—in every case—would fail some of the six
requirements of the annual modulation signature and, therefore, a radon effect
can be excluded.
The installation, where the ∼100 kg NaI(Tl) set-up operates, is airconditioned. The operating temperature of the detectors in the Cu box is read by a
probe and stored with the production data [12]. In particular, sizeable temperature
variations could only induce a light variation in the output, which is negligible
considering:
(i) that around our operating temperature, the average slope of the light output
is . − 0.2%/◦ C;
(ii) the energy resolution of these detectors in the keV range; and
(iii) the role of the intrinsic and routine calibrations [12]; see [15].
In addition, every possible effect induced by temperature variations would fail
at least some of the six requirements needed to mimic the annual modulation
signature; therefore, a temperature effect can be excluded.
In long-term running conditions, knowledge of the energy scale is ensured
by periodical calibration with an 241Am source and by continuously monitoring
within the same production data (grouping the data approximately into 7 day
batches) the position and resolution of the 210 Pb peak (46.5 keV) [7–9, 12, 15].
The distribution of the relative variations of the calibration factor (proportionality
factor between the area of the recorded pulse and the energy), tdcal—without
applying any correction—estimated from the position of the 210 Pb peak for all
the nine detectors during both the DAMA/NaI-3 and the DAMA/NaI-4 running
periods, has been investigated. From the measured variation of tdcal an upper
limit of <1% of the modulation amplitude measured at very low energy in [7–9]
has been obtained [15].
The only data treatment which is performed on the raw data is to eliminate
obvious noise events (which sharply decrease when increasing the number of
available photelectrons) present below approximately 10 keV [12]. The noise

Investigation of the WIMP annual modulation signature

289

in our experiment is given by PMT fast single photoelectrons with decay
times of the order of tens of nanoseconds, while the scintillation pulses have
decay times of the order of hundreds of nanoseconds. The large difference in
decay times and the relatively large number of available photoelectrons response
(5.5–7.5 photoelectron/keV depending on the detector) ensures effective noise
rejection; see, e.g., [12] for details. To investigate quantitatively the possible role
of a noise tail in the data after noise rejection on the annual modulation result,
the hardware rate, R H j , of each detector above a single photoelectron, can be
considered. The distribution of & j (R H j − R H j ) shows a Gaussian behaviour
with σ = 0.6% and 0.4% for DAMA/NaI-3 and DAMA/NaI-4, respectively,
values well in agreement with those expected on the basis of simple statistical
arguments. Moreover, by fitting its time behaviour in both data periods including
a WIMP-like modulated term a modulation amplitude compatible with zero
(0.04 ± 0.12) × 10−2 Hz, is obtained. From this value, considering also the
typical noise contribution to the hardware rate of the nine detectors, the upper
limit on the noise relative modulation amplitude has been derived to be [15] less
than
1.6 × 10−3 Hz
 1.8 × 10−3
(90% C.L.).
9 × 0.10 Hz
This shows that even in the worst hypothetical case of a 10% contamination of the
residual noise—after rejection—in the counting rate, the noise contribution to the
modulation amplitude in the lowest energy bins would be less than 1.8 × 10−4 of
the total counting rate, that is a possible noise modulation could account only for
less than 1% of the annual modulation amplitude observed in [9]. In conclusion,
there is no evidence that a hypothetical tail of residual noise after rejection plays
any role in the results.
The behaviour of the efficiencies during the whole data-taking periods
has also been investigated; their possible time variation depends essentially on
the stability of the cut efficiencies, which are regularly measured by dedicated
calibrations [9, 15]. In this way, the unlikely idea of a possible role played by the
efficiency values in the observed effect in [7–9] has also been ruled out [9, 15].
In order to verify the absence of any significant background modulation, the
measured energy distribution in energy regions not of interest for the WIMP–
nucleus elastic scattering has been investigated [7–9, 13]. For this purpose, we
have considered the rate integrated above 90 keV, R90 , as a function of time. The
distributions of the percentage variations of R90 with respect to their mean values
for all the crystals during the whole DAMA/NaI-3 and DAMA/NaI-4 running
periods show cumulative Gaussian behaviour with σ  1%, well accounted for
by the statistical spread expected from the used sampling time [9, 15]. This result
excludes any significant background variation. Moreover, including a WIMP-like
modulation in the analysis of the time behaviour of R90 , an amplitude compatible
with zero is found in both the running periods: −(0.11 ± 0.33) cpd kg−1 and
−(0.35±0.32) cpd kg−1 . This excludes the presence of a background modulation
in the whole energy spectrum at a level much lower than the effect found in the

290

Signature for signals from the dark universe

lowest energy region in [7–9]; in fact, if it were otherwise—considering the R90
mean values—the modulated term should be of the order of tens of cpd kg−1 , that
is ∼100σ far away from the measured value. This also accounts for the neutron
environmental background; see for further arguments [15]. A similar analysis
performed in other energy regions, such as the one just above the first pole of the
iodine form factor, leads to the same conclusion.
As regards possible side reactions, the only process which has been found as
a hypothetical possibility is the muon flux modulation reported by the MACRO
experiment [29]. In fact, MACRO has observed that the muon flux shows a
nearly sinusoidal time behaviour with a 1 year period and with a maximum in the
summer with amplitude of ∼2%; this muon flux modulation is correlated with the
temperature of the atmosphere. This effect would give, in our set-up, modulation
amplitudes much less than 10−4 cpd kg−1 keV−1 , that is much smaller than we
observe. Moreover, it will also fail some of the six requirements necessary to
mimic the signature. Thus, it can be safely ignored [15]. The search for other
possible side reactions able to mimic the signature has so far not offered any
other candidate.
For the sake of completeness, we recall that—using pulse shape
discrimination—no evidence for the anomalous events with a decay time shorter
than the recoils has ever been found in our data [10, 15].
As a result of the model-independent approach and a full investigation of
known systematic effects, the presence of an annual modulation compatible with
WIMPs in the galactic halo indocates that WIMPs are possible candidates to
account for the data, independently of their nature and coupling with ordinary
matter.
In the next section a particle candidate will be investigated; for that a model
is needed as well as an effective energy and time correlation analysis. We take this
occasion to remark that a large scenario exists in the model-dependent analyses
not only because various candidates with different couplings can be considered
but also because of the large uncertainties affecting several parameters involved
in the calculation which are generally neglected, although they should generally
play a significant role.
9.3.3 Results of a model-dependent analysis
Properly considering the time occurrence and the energy of each event, a time
correlation analysis of the data collected between 2 and 20 keV has been
performed, according to the method described in [7–9]. This allows us to test
effectively the possible presence in the rate of a contribution having the typical
features of a WIMP candidate. In particular we have considered a particle with
a dominant spin-independent scalar interaction (which is also possible for the
neutralino [30]). A detailed discussion is available in [9]; here the main result
is outlined. In the minimization procedure by the standard maximum likelihood
method [7–9] the WIMP mass has been varied from 30 GeV up to 10 TeV; the

Investigation of the WIMP annual modulation signature

291

lower bound accounted for results achieved in accelerators. The calculations
have been performed according to the same astrophysical, nuclear and particle
physics considerations given in [7–9] and to the 90% C.L. recoil limit of [10]
(DAMA/NaI-0). Alternative analytical approaches, such as the one based on the
χtest variable described in [8] and the Feldman and Cousins method [31], offer
substantially the same results.
Since the analysis of each data cycle independently [7–9,13] gave consistent
results, a global analysis has been made properly including both the known
uncertainties on astrophysical local velocity, v0 [21] and the constraint arising
from the upper limit on recoils measured in [10] (DAMA/NaI-0). According
to [21], the minimization procedure has been repeated by varying v0 from 170 to
270 km s−1 to account for its present uncertainty; moreover, the case of possible
bulk halo rotation has also been analysed. The positions of the minima for
the log-likelihood function consequently vary [21]; for example, in this model
framework for v0 = 170 km s−1 the minimum is at MW = (72+18
−15 ) GeV
−6
−1
it is at
and ξ σp = (5.7 ± 1.1) × 10 pb, while for v0 = 220 km s
−6 pb. The results obtained
)
GeV
and
ξ
σ
=
(5.4
±
1.0)
×
10
MW = (43+12
p
−9
in this model framework are summarized in figure 9.2, where the regions allowed
at 3σ C.L. are shown:
(i) when v0 = 220 km s−1 (dotted contour);
(ii) when the uncertainty on v0 is taken into account (continuous contour); and
(iii) when possible bulk halo rotation is considered (broken contour).

The latter two calculations have been performed according to [21]. The
confidence levels quoted here have also been verified by suitable Monte Carlo
calculations; in particular, we note that the Feldman and Cousins analysis [31]
of the data gives quite similar results. These regions are well embedded in the
Minimal Supersymmetric Standard Model (MSSM) estimates for the neutralino
[32]. A quantitative comparison between the results from the model-independent
and model-dependent analyses has been discussed in [9].
Finally, many assumptions on the nuclear and particle physics used in these
calculation (as well as in those of exclusion plots) are affected by uncertainties,
which—when taken into account—would enlarge the regions of figure 9.2 and, as
mentioned, consequently vary the positions of the minima for the log-likelihood
function. For example, as in [9] we mention the case of the iodine form factor,
which depends on the nuclear radius and on the thickness parameter of the nuclear
surface; it has been verified that, varying their values with respect to those used in
the analysis in [9] by 20%, the locations of the minima will move toward slightly
larger MW and toward lower ξ σp values, while the calculated 2–6 keV Sm values
will increase by about 15%.

292

Signature for signals from the dark universe

ρWIMP
Figure 9.2. Regions allowed at 3σ C.L. in the plane ξ σp (ξ =
and
0.3 GeV cm−3
σp = WIMP scalar cross section on proton) versus MW (WIMP mass) by the global
analysis: (i) for v0 = 220 km s−1 (dotted contour); (ii) when accounting for v0 uncertainty
(170 km s−1 ≤ v0 ≤ 270 km s−1 ; continuous contour); and (iii) when considering
also a possible bulk halo rotation as in [21] (broken contour). The constraint arising
from the measured upper limit on recoils measured in [10] has been properly taken into
account. We note that the inclusion of present uncertainties on some nuclear and particle
physics parameters would enlarge these regions since the positions of the minima for the
log-likelihood function would consequently vary; full estimates are in progress.

9.4 DAMA annual modulation result versus CDMS exclusion
plot
As is well known, intrinsic uncertainties exist in the comparison of results
achieved by different experiments and, even more, when different techniques
are used as in the case of DAMA [7–9] and of CDMS [33]. In fact, DAMA
is searching for a distinctive signature by using a large mass NaI(Tl) setup deep underground, while CDMS is exploiting a widely unknown hybrid
bolometer/ionizing technique at a depth of 10 m to reject a huge background.
Moreover, always when different target nuclei are used (as is also the case in
DAMA and CDMS), no absolute comparison can be pursued at all; only modeldependent comparisons can be considered with further intrinsic uncertainties.
In table 9.2 a few numbers are given to offer an immediate view on the two
experiments.
The techniques used by CDMS would require several technical papers to
be credited at the necessary level (quenching factor values, sensitive volumes,
windows for rejection, efficiencies, energy calibrations, etc; the stability of

DAMA annual modulation result versus CDMS exclusion plot

293

Table 9.2. Several numbers on the DAMA and CDMS experiments as in [9, 33].
DAMA

CDMS

Exposure

57 986.0 kg day

10.6 kg day

Depth

1400 m

10 m

Number of events
in the observed effect

Total modulated
amplitude
∼2000 events

13 evt in Ge, 4 evts in Si
4 multiple evts in Ge
+ Monte Carlo on neutron flux

these quantities during the running period; justification of the performed data
selection; quantitative control of systematic uncertainties in the various hardware
and software handlings), which have not been made available. Every small
deviation from the assumptions used by CDMS in [33] can significantly change
their conclusion.
The exclusion plot quoted by CDMS [33] arises from the joint analyses
of two different experiments with two different target nuclei (Si and Ge) and,
practically, by a neutron Monte Carlo subtraction.
In the Si experiment (used exposure was ∼1.5 kg day of the ∼3.3 kg day
available) a large number of events survived the ionizing/heat discrimination in
the whole energy region allowed for recoil candidates. Thereafter, by the so-called
athermal pulse shape discrimination, four events remained and were classified as
‘mostly neutrons’, while all the others as ‘surface electrons’. The amount and
the Y (ratio between ionizing and heat charges) and energy distributions of the
latter ones give a hint that the four ‘mostly neutrons’ events could indeed be—all
or partially—ascribed to the tail of the huge population of ‘surface electrons’
surviving the ionizing/heat discrimination. Obviously this possibility would
significantly change the conclusions in [33].
In the Ge experiment (used exposure was ∼10.6 kg day of the ∼48 kg day
available for three Ge detectors, having already excluded a fourth detector), 13
recoil candidates survive the ionization/heat discrimination. This number of
events is largely compatible with the DAMA allowed region estimated in [9] in the
framework of a model for a spin-independent candidate with mass above 30 GeV.
The interpretation on the real nature of these 13 candidates strongly depends on
the Monte Carlo estimates of the neutron background, which is constrained by
the hypothesized nature of the four Si candidates and of four multi-hit events.
A similar procedure is strongly uncertain since it is based on the previously
mentioned assumptions and on the neutron transport code; the latter—as is widely
known—is affected by huge uncertainties due to the assumptions on the original
neutron energy spectrum and to the transport calculations in all the involved
materials. This can be verified by considering that the result of such a calculation

294

Signature for signals from the dark universe

gives in [33] about 30 expected neutrons to be compared with the 13 quoted
recoil candidates; this, in particular, can suggest an overestimate of the neutron
background and, therefore, of the given exclusion plot.
Summarizing we can state that the CDMS result can be expressed by the
combination of two quantities: the real number of recoil candidates (when
accounting for realistic values of the physical parameters) and the expected
number of neutron background. Varying these quantities several different
conclusions can be obtained. In every case, a CDMS representative has stated
that analysing these data to determine their compatibility with DAMA, the result
gives an upper limit for presence of WIMPs in CDMS Ge data of eight events at
90% C.L. [34], evidently compatible with the DAMA allowed region in the model
considered in [9]. Moreover, simple calculations assuming again ideal values for
the CDMS physical parameters and the values measured for the related quantities
in our experiment [7–10, 12] show that in the framework of the model of [9],
CDMS should measure from ∼15 events down to less than 1, that is compatibility
is still substantially present.
Moreover, we note that the comparison through a model requires, for
each considered target nucleus, fixing not only the coupling and the scaling
laws, but also several specific different nuclear and particle physics parameters,
which are affected instead by uncertainties. The same is for the choice of the
astrophysical model, such as the WIMP velocity distribution and the various
related parameters. For example if the real WIMP velocity distribution should
be such as to enhance, to a certain extent, the modulated part of the signal
with respect to the unmodulated one, a comparison in the framework of usual
assumptions would fail. The same would hold if the candidate were to have a
partial (or total) spin-dependent interaction component (as is also possible for
the neutralino) and one of the two experiments is insensitive to spin-dependent
interactions (such as practically those using natural Ge). Several other scenarios
could also be considered.
For the sake of completeness, we note that in [33] the complete DAMA result
has not been considered.
Briefly, many experimental and theoretical reasons do not support the
conclusion of [33] to the necessary extent.

9.5 Conclusion
In conclusion, a WIMP contribution to the measured rate is a candidate by the
model-independent approach and by the absence of any known systematics able
to mimic the signature [7–9, 13, 15] independently of the nature and coupling
of the possible particle. The complete global correlation analysis in terms of a
spin-independent candidate with a mass greater than 30 GeV favours modulation
at approximately 4σ C.L. in the given framework [9]. Moreover, neutralino
configurations in the allowed region appear to be of cosmological interest [32].

References

295

In [35] a possible heavy neutrino of the fourth family has been considered instead.
Further studies on model frameworks are in progress.
The data for a fifth annual cycle are now at hand, while new electronics
and data acquisition systems were installed in August 2000. Moreover, after
new dedicated R&D for the radiopurification of NaI(Tl) detectors, efforts to
increase the experimental sensitivity are in progress; the target mass will become
approximately 250 kg.

References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]
[26]
[27]
[28]
[29]
[30]
[31]
[32]
[33]
[34]
[35]

Fushimi K et al 1994 Nucl. Phys. B (Proc. Suppl.) 35 400
Belli P et al 1996 Phys. Lett. B 387 222
Bernabei R et al 2000 New J. Phys. 2 15.1–15.7
Collar J I et al 1992 Phys. Lett. B 275 181
Drukier K A et al 1986 Phys. Rev. D 33 3495
Freese K et al 1988 Phys. Rev. D 37 3388
Bernabei R et al 1998 Phys. Lett. B 424 195
Bernabei R et al 1999 Phys. Lett. B 450 448
Bernabei R et al 2000 Phys. Lett. B 480 23
Bernabei R et al 1996 Phys. Lett. B 389 757
Bernabei R et al 1997 Phys. Lett. B 408 439
Bernabei R et al 1999 Nuovo Cimento A 112 545
Belli P et al 1999 3K-Cosmology (New York: AIP) p 65
Bernabei R et al 1999 Phys. Lett. B 460 236
Bernabei R et al 2000 Preprint ROM2F/2000-26
Belli P et al 1996 Nuovo Cimento C 19 537
Bernabei R et al 1997 Astropart. Phys. 7 73
Bernabei R et al 1998 Phys. Lett. B 436 379
Belli P et al 1999 Nucl. Phys. B 563 97
Bernabei R et al 1999 Nuovo Cimento A 112 1541
Belli P et al 2000 Phys. Rev. D 61 023512
Belli P et al 1996 Astropart. Phys. 5 217
Belli P et al 1999 Astropart. Phys. 10 115
Belli P et al 1999 Phys. Lett. B 465 315
Bernabei R et al 1999 Phys. Rev. Lett. 83 4918
Belli P et al 1999 Phys. Rev. C 60 065501
Belli P et al 2000 Phys. Rev. D 61 117301
Bernabei R et al 2000 Phys. Lett. B 490 16
Ambrosio M et al 1997 Astropart. Phys. 7 109
Bottino A et al 1997 Phys. Lett. B 402 113
Feldman G J and Cousins R D 1998 Phys. Rev. D 57 387
Bottino A et al 2000 Phys. Rev. D 62 056006
Abusaidi R et al 2000 Phys. Rev. Lett. 84 5699
Shutt T 2000 Seminar Given at LNGS (March)
Fargion D et al 1998 Pis. Zh. Eksp. Teor. Fiz. 68 (JETP Lett. 68 685)

Chapter 10
Neutrino oscillations: a phenomenological
overview
GianLuigi Fogli
Dipartimento di Fisica e INFN, Sezione di Bari, Via Amendola
173, 70126 Bari, Italy

The evidence for solar and atmospheric neutrino oscillations is analysed in a
three-flavour oscillation framework, including the most recent Super-Kamiokande
data, as well as the constraints on νe mixing coming from the CHOOZ reactor
experiment. The regions of the mass-mixing parameter space compatible with
the data are determined and their features discussed. In particular, it is shown
that bimaximal mixing (or nearly bimaximal mixing) of atmospheric and solar
neutrinos is also possible within the MSW solution to the solar neutrino problem.

10.1 Introduction
The recent atmospheric neutrino data from the Super-Kamiokande (SK)
experiment [1] are in excellent agreement with the hypothesis of flavour
oscillations in the νµ ↔ ντ channel [2]. Such a hypothesis is consistent with
all the SK data, including sub-GeV e-like and µ-like events (SGe, µ), multiGeV e-like and µ-like events (MGe, µ), and upward-going muon events (UPµ),
and is also corroborated by independent atmospheric neutrino results from the
MACRO [3] and Soudan-2 [4] experiments. Oscillations in the νµ ↔ ντ channel
are also compatible with the negative results of the reactor experiment CHOOZ
in the νe ↔ νe channel [5, 6].
However, dominant νµ ↔ ντ transitions plus subdominant νµ ↔ νe
transitions are also consistent with SK+CHOOZ data, and lead to a much richer
three-flavour oscillation phenomenology for atmospheric νs [7]. A three-flavour
framework is also needed in order to accommodate, in addition, the evidence for
solar νe disappearance [8].
296

Three-neutrino mixing and oscillations

297

In this chapter we analyse atmospheric and solar data in a common 3ν
oscillation framework. Concerning atmospheric νs, we include 30 data points
from the SK experiment (52 kTy) [1], namely the zenith distributions of sub-GeV
events (SG e-like and µ-like, 5 + 5 bins), multi-GeV events (MGe, µ 5 + 5 bins)
and upward-going muons (UPµ, 10 bins). We also include, when appropriate,
the rate of events in the CHOOZ reactor experiment (one bin). Concerning
solar neutrinos, we use the total rate information from the Homestake (chlorine),
GALLEX+SAGE (gallium), Kamiokande and Super-Kamiokande experiments,
as well as the day–night asymmetry and the 18-bin energy spectrum from
Super-Kamiokande (825 days) [1], with emphasis on the Mikheyev–Smirnov–
Wolfenstein solutions.

10.2 Three-neutrino mixing and oscillations
The combined sources of evidence for neutrino flavour transitions coming from
the solar ν problem and from the atmospheric ν anomaly demand an approach
in terms of three-flavour oscillations among massive neutrinos (ν1 , ν2 , ν3 ) [7–9].
The three-flavour ν parameter space is then spanned by six variables:
δm 2 = m 22 − m 21 ,
2

m 23

− m 22 ,

(10.1)

m =
ω = θ12 ∈ [0, π/2],

(10.2)
(10.3)

φ = θ13 ∈ [0, π/2],
ψ = θ23 ∈ [0, π/2],

(10.4)
(10.5)

δ = C P violation phase,

(10.6)

where the θi j rotations are conventionally ordered as for the quark mixing matrix
[10].
In the phenomenologically interesting limit |δm 2 |  |m 2 |, the two
eigenstates closest in mass (ν1 , ν2 ) are expected to drive solar ν oscillations,
while the ‘lone’ eigenstate ν3 drives atmospheric ν oscillations. In such a limit
(see [7–10] and references therein) the following occur:
(i) the phase δ becomes unobservable;
(ii) the atmospheric parameter space is spanned by (m 2 , ψ, φ); and
(iii) the solar ν parameter space is spanned by (δm 2 , ω, φ).
In other words, in the previous limit it can be shown that solar neutrinos probe the
composition of νe in terms of mass eigenstates
νe = Ue1 ν1 + Ue2 ν2 + Ue3 ν3
= cφ (cω ν1 + sω ν2 ) + sφ ν3

(10.7)
(10.8)

2
2
2
, Ue2
, Ue3
),
(δm 2 , ω, φ) ≡ (δm 2 , Ue1

(10.9)

in the parameter space

298

Neutrino oscillations: a phenomenological overview

Figure 10.1. Parameter spaces of solar and atmospheric neutrinos in the limit
2 = s2 .
|δm 2 |  |m 2 |, for assigned δm 2 and m 2 . The only common parameter is Ue3
φ
2 + U 2 + U 2 = 1 for unitarity, whereas atmospheric (more generally,
where Ue1
e2
e3
‘terrestrial’) neutrinos probe the flavour composition of ν3 ,

ν3 = Ue3 νe + Uµ3 νµ + Uτ 3 ντ
= sφ νe + cφ (sψ νµ + cψ ντ )

(10.10)
(10.11)

2
2
(m 2 , ψ, φ) ≡ (m 2 , Ue3
, Uµ3
, Uτ23 ),

(10.12)

in the parameter space

2 + U 2 + U 2 = 1 for unitarity. The two unitarity constraints can be
where Ue3
µ3
τ3
conveniently embedded [9] in two triangle plots (see figure 10.1), which describe
the mixing parameter spaces for given δm 2 and m 2 for solar and atmospheric
neutrinos, respectively. The only parameter common to the two triangles is
2 = s 2 †.
Ue3
φ

10.3 Analysis of the atmospheric data
In this section we report an updated analysis of the Super-Kamiokande data,
and combine them with the limits coming from the CHOOZ reactor experiment,
by assuming the ‘standard’ three-neutrino framework discussed in the previous
section. Details about our calculations can be found in [7]. Constraints on the
mass-mixing parameters are obtained through a χ 2 statistics, and are plotted in
the atmospheric ν triangle described in figure 10.1.
Figure 10.2 shows the regions favoured at 90% and 99% C.L. in the triangle
plots, for representative values of m 2 . The CHOOZ data, which exclude a large
horizontal strip in the triangle, appear to be crucial in constraining three-flavour
mixing. Pure νµ ↔ νe oscillations (right-hand side of the triangles) are excluded
† In the special case φ = 0, the atmospheric and solar parameter spaces are decoupled into the twofamily oscillation spaces (δm 2 , ω) and (m 2 , ψ).

Analysis of the atmospheric data

299

Figure 10.2. Three-flavour analysis in the triangle plot, for five representative values of
m 2 . Left-hand and middle column: separate analyses of Super-Kamiokande (52 kTy) and
CHOOZ data, respectively. Right-hand column: combined SK+CHOOZ allowed regions.
Although the SK+CHOOZ solutions are close to pure νµ ↔ ντ oscillations, the allowed
2 are not completely negligible, especially in the lower range of m 2 .
values of Ue3

300

Neutrino oscillations: a phenomenological overview

Figure 10.3. 90% C.L. bounds on the mass parameter m 2 from atmospheric data, without
and with reactor data. Upper part: pre-SK and pre-CHOOZ bounds. Intermediate part:
SK bounds at 32 kTy (+CHOOZ). Lower part: present bounds from SK data at 52 kTy
(+CHOOZ).

by SK and CHOOZ independently. The centre of the lower side, corresponding to
pure νµ ↔ ντ oscillations with maximal mixing, is allowed in each triangle both
by SK and SK+CHOOZ data. However, deviations from maximal (νµ ↔ ντ )
mixing, as well as subdominant mixing with νe , are also allowed to some extent.
Such deviations from maximal 2ν mixing are now more constrained than in the
previous analysis of the 33 kTy SK data [7], also as a result of tighter constraints
from the finalized CHOOZ data [5].
Figure 10.3 shows the progressively tighter constraints on the mass
parameter m 2 for unconstrained three-flavour mixing, for pre-SK [11] and postSK [7] analyses, with and without reactor constraints. The current best-fit value
(lower part of figure 10.3) is reached at m 2 ∼ 3 × 10−3 eV2 , and is only slightly
influenced by the inclusion of CHOOZ data. However, the upper bound on m 2
is significantly improved by including CHOOZ. Note that there is consistency
between pre-and post-SK information.
Figures 10.2 and 10.3 clearly show the tremendous impact of the SK
experiment in constraining the neutrino oscillation parameter space. Prior to SK,
the data could not significantly favour νµ ↔ ντ over νµ ↔ νe oscillations, and

Analysis of the atmospheric data

301

2 as a function of m 2 from SK data (52 kTy), with and without
Figure 10.4. Bounds on Ue3
the finalized CHOOZ data.

could only put relatively weak bounds on m 2 (see [11]).
2 is
The impact of CHOOZ in constraining the mixing matrix element Ue3
clearer in figure 10.4, where the 90% and 99% C.L. bounds are shown as a
function of m 2 , for unconstrained values of the angle ψ. It can be seen that,
2 cannot be larger than a few
when CHOOZ data are included, the element Ue3
percent.
Figure 10.5 shows the best-fit zenith distributions of SGe, µ, MGe, µ and
UPµ events, normalized to the no-oscillation rates in each bin, with and without
2 at the best-fit point (SK data
the CHOOZ constraint. The non-zero value of Ue3
only) leads to a slight expected excess in the MGe sample for cos θ → −1. A
significant reduction in the errors is needed to probe such possible distortions,
which would be unmistakable signals of subdominant νµ → νe oscillations.
Figure 10.5 also shows that, when the results of CHOOZ are included, pure
νµ → ντ oscillations represent the best fit to the SK data. In this context, it
is useful to show that the pieces of information coming from the shape of the
zenith distributions (figure 10.5) and from the total event rates are consistent with

302

Neutrino oscillations: a phenomenological overview

Figure 10.5. SK zenith distributions of leptons at best fit (broken lines), also including
CHOOZ (full lines), as compared with the 52 kTy experimental data (dots with error bars).
The 3ν mass-mixing values at best fit are indicated in the upper right-hand corner.

each other, contrary to recent claims [12].
To this purpose, figure 10.6 shows the curve of theoretical predictions for
2 = U 2 and U 2 = 0) and variable m 2 , in the plane of
maximal 2ν mixing (Uµ3
τ3
e3
the double ratio of µ-to-e events for SG and MG events, together with the SK
data (cross of error bars). The SK data on the double ratio, within one standard
deviation, are perfectly consistent with the νµ → ντ oscillation hypothesis at
m 2 ∼ 3 × 10−3 eV2 .

10.4 Analysis of the solar data
10.4.1 Total rates and expectations
In this section we present an updated phenomenological analysis of the solar
neutrino data, assuming oscillations between two and three neutrino families, with
emphasis on the MSW [13] solutions.
As far as expectations are concerned, we use the so-called BP98 standard
solar model [14] for the electron density in the Sun and for the input neutrino
parameters (νe fluxes, spectra, and production regions), and compare the
predictions to the experimental data for the following observables: total neutrino
event rates, SK energy spectrum and SK day–night asymmetry.
The total neutrino event rates are those measured at Homestake [15],
Kamiokande [16], SAGE [17], GALLEX [18], and Super-Kamiokande (825 live
days) [1]. Since the SAGE and GALLEX detectors measure the same quantity

Analysis of the solar data

303

Figure 10.6. Double ratio of µ/e events (data/theory) for SG and MG events in SK: full
curve, predictions for maximal νµ → ντ mixing; cross, SK data (±1σ ).

their results are combined in a single (Ga) rate. The Kamiokande and SK data,
however, are treated separately (rather than combined in a single datum), since the
two experiments, although based on the same ν–e scattering detection technique,
have rather different energy thresholds and resolution functions.
The SK electron recoil energy spectrum and its uncertainties (825 lifetime
days, E e > 5.5 MeV) are graphically reduced from the 18-bin histograms
shown by SK members in recent Summer ’99 conferences [1]. Our theoretical
calculation of the binned spectrum properly takes into account energy threshold
and resolution effects. Standard 8 B [19] and hep [14] neutrino spectra and fluxes
are used, unless otherwise noted. Concerning the SK day–night asymmetry of
the event rates, we use the latest measurement [1]: 2(N − D)/(N + D) =
0.065 ± 0.031 ± 0.013.
In the presence of 2ν or 3ν oscillations, the MSW effect in the Sun is
computed as in [9]. The additional Earth matter effects are treated as in [20].
The χ 2 analysis basically follows the approach developed in [21, 22], with the
necessary updates to take into account the BP98 SSM predictions and the energy
spectrum information. Further details can be found in [23].

304

Neutrino oscillations: a phenomenological overview

Figure 10.7. The solar neutrino deficit, shown as a discrepancy between data and
expectations in the gallium (Ga), chlorine (Cl), and Super-Kamiokande total event rates.
In each plane, the error ellipses represent 99% C.L. contours for two degrees of freedom
(i.e. χ 2 = 9.21). The projection of an ellipse onto one of the axis gives approximately
the ±3σ range for the corresponding rate.

We start our analysis by comparing the standard (no oscillation) predictions
with the experimental data for the Cl, Ga, and SK total rates. Figure 10.7 shows
the 99% C.L. error ellipses for data and expectations in the planes charted by the
(Cl, Ga), (SK, Ga) and (SK, Cl) total rates. The distance between observations
and standard predictions makes the solar ν problem(s) evident. At present, such
information is the main evidence for solar neutrino physics beyond the standard
electroweak model; however, since the theoretical errors are dominant—as far
as total rates are concerned—no substantial improvements can be expected by a
reduction in the experimental errors. Conversely, decisive information is expected
from the SK spectrum and day–night asymmetry, but no convincing deviation has
emerged from such data yet. Therefore, it is not surprising that, in oscillation
fits, the total rates mainly determine allowed regions, while the SK spectrum and
day–night asymmetry determine excluded regions.

Analysis of the solar data

305

Figure 10.8. Two-generation vacuum solutions to the solar neutrino problem (all data
included). Upper solutions fit the SK spectrum better than total rates. Conversely, the
solution at lowest δm 2 fits the total rates (Cl + Ga + K + SK) better.

10.4.2 Two-flavour oscillations in vacuum
In figure 10.8 we report our 2ν vacuum oscillation analysis of the solar neutrino
data coming from total rates end SK electron energy spectrum. We can see several
distinct solutions, allowed at the 90% C.L., with the peculiar behaviour that in
general a certain disagreement can be found by a comparison of the total rates
and energy spectrum constraints. There are solutions which are preferred by the
total rates analysis, but disfavoured by the energy spectrum, and solutions that,
conversely, are mainly indicated by the energy spectrum but not by total rates.
This behaviour has also been noted in [24]. An interesting feature is that, if one of
the vacuum solutions is selected by future data, then we will be able to determine
the mass difference δm 2 in a very accurate way.
10.4.3 Two-flavour oscillations in matter
Figure 10.9 shows the results of our 2ν MSW analysis of the solar ν data, shown
as C.L. contours in the (δm 2 , sin2 2ω/ cos 2ω) plane. The choice of the variable
sin2 2ω/ cos 2ω, rather than the usual sin2 2ω, allows an expanded view of the
large mixing region.
In each of the six panels, we determine the absolute minimum of the χ 2
and then plot the iso-χ 2 contours corresponding to 90%, 95% and 99% C.L. for

306

Neutrino oscillations: a phenomenological overview

Figure 10.9. Two-generation MSW solutions to the solar neutrino problem.
upper four panels correspond to the following separate fits to data subsets:
rates (Cl + Ga + K + SK); Super-Kamiokande night–day asymmetry N − D/N
Super-Kamiokande electron energy spectrum with standard hep neutrino
Super-Kamiokande spectrum with enhanced (20×) hep neutrino flux. The two
panels show the results of the global fits to all data.

The
total
+ D;
flux;
lower

Analysis of the solar data

307

two degrees of freedom (the oscillation parameters). In fits including the total
rates, there is a global χ 2 minimum and two local minima; such minima, and the
surrounding favoured regions, are usually indicated as MSW solutions at small
mixing angle (SMA), large mixing angle (LMA), and low δm 2 (LOW).
The first panel of figure 10.9 refers to the fit to the total rates only. The three
χ 2 minima are indicated by dots. The absolute minimum is reached within the
2 = 1.08): it represents a very good fit to the data. The LMA
SMA solution (χmin
solution is also acceptable, while the LOW solution gives a marginal fit.
The SK data on the day–night asymmetry (second panel) and energy
spectrum (third panel) exclude large regions in the mass-mixing parameter space;
but are unable to (dis)prove any of the three solutions, which in fact are also
present in the global fit to all data (fifth panel).
The spectrum information is sensitive to the (uncertain) value of the hep
neutrino flux; for instance, an enhancement by a factor 20 helps to fit the highenergy part of the SK spectrum [25], and thus it produces a reduction in the
excluded regions in the mass-mixing plane (fourth panel in figure 10.9), and a
corresponding slight enlargement of the globally allowed regions (sixth panel).
From a careful analysis [23], the following situation emerges for the three
MSW solutions SMA, LMA, and LOW. None of them can be excluded at 99%
C.L. by the present experimental data. Different pieces of the data give indications
that are not as consistent as would be desirable: the total rate information favours
the SMA solution, the spectral data favour the LMA and LOW solutions, and the
day–night data favour the LMA solution. In a global fit, the three solutions have
comparable likelihoods. Although such solutions are subject to change shape
and likelihood as more accurate experimental data become available, no dramatic
improvement can be really expected in their selection, unless
(1) the theoretical uncertainties on the total rates are reduced to the size of the
corresponding experimental uncertainties;
(2) the total errors associated with the SK spectrum and day–night measurement
are significantly reduced (by, say, a factor ∼2); or
(3) decisive results are found in new generation solar neutrino experiments. Any
of these conditions require a time scale of a few years at least; the same time
scale should then be expected in order to (patiently) single out one of the
three MSW solutions (SMA, LMA, or LOW).
Another aspect of the LMA and LOW solutions emerging from figure 10.9
is their extension to large values of the mixing angle (sin2 2ω → 1), which are
often assumed to be realized only through the vacuum oscillation solutions. Since
the possibility of nearly maximal (ν1 , ν2 ) mixing for solar neutrinos has gained
momentum after the SK evidence for maximal (νµ , ντ ) mixing (sin2 2ψ ∼ 1),
it is interesting to study it in detail by dropping the usual ‘2ω’ variable and by
exploring the full range ω ∈ [0, π/2], as was done earlier in [9]. The subcase
ω = π/4 will receive special attention in the next section.

308

Neutrino oscillations: a phenomenological overview

10.4.4 Three-flavour oscillations in matter
As stated in section 10.2, for large values of m 2 ( 10−4 eV2 ) the parameter
space relevant for 3ν solar neutrino oscillations is spanned by the variables
(δm 2 , ω, φ). As far as ω is taken in its full range [0, π/2], one can assume
δm 2 > 0, since the MSW physics is invariant under the substitution (δm 2 , ω) →
(−δm 2 , π/2 − ω) at any φ.
For graphical representations, we prefer to use the mixing variables (tan2 ω,
2
tan φ) introduced in [9], which properly chart both small and large mixing. The
case tan2 φ = 0 corresponds to the familiar 2ν scenario, except that now we also
consider the usually neglected case ω > π/4 (tan2 ω > 1). For each set of observables (rates, spectrum, day-night difference, and combined data) we compute
the corresponding MSW predictions and their uncertainties, identify the absolute
2
minimum of the χ 2 function, and determine the surfaces at χ 2 − χmin
= 6.25,
2
7.82 and 11.36, which define the volumes constraining the (δm , tan2 ω, tan2 φ)
parameter space at 90%, 95% and 99% C.L. Such volumes are graphically presented in (δm 2 , tan2 ω) slices for representative values of tan2 φ.
Figure 10.10 shows the combined fit to all data. The minimum χ 2 is reached
within the SMA solution and shows a very weak preference for non-zero values
of φ (tan2 φ  0.1). It can be seen that the SK spectrum excludes a significant
fraction of the solutions at δm 2 ∼ 10−4 eV2 , including the upper part of the
LMA solution at small φ, and the merging with the SMA solution at large φ.
In particular, at tan2 φ = 0.1 the 95% C.L. upper limit on δm 2 drops from
2 × 10−4 eV2 (rates only) to 8 × 10−5 eV2 (all data). This indication tends to
disfavour neutrino searches of CP violation effects, since such effects decrease
with δm 2 /m 2 at φ = 0.
The 95% C.L. upper bound on φ coming from solar neutrino data alone
(φ < 55◦–59◦ ) is consistent with the one coming from atmospheric neutrino
data alone (φ < 45◦ ), as well as with the upper limit coming from the
combination of CHOOZ and atmospheric data (φ < 15◦ ) (see figure 10.4). This
indication supports the possibility that solar, atmospheric and CHOOZ data can
be interpreted in a single three-flavour oscillation framework [7, 23]. In this case,
the CHOOZ constraints on φ exclude a large part of the 3ν MSW parameter space
(basically all but the first two panels in figure 10.9).
However, even small values of φ can be interesting for solar ν
phenomenology. Figure 10.11 shows the section of the volume allowed in
the 3ν MSW parameter space, for ω = π/4 (maximal mixing), in the massmixing plane (δm 2 , sin2 φ). All data are included. It can be seen that both the
LMA and LOW solutions are consistent with maximal mixing (at 99% C.L.) for
2 = 0. Moreover, the consistency of the LOW solution with maximal
sin2 φ ≡ Ue3
2  0.1, while the opposite happens for the
mixing improves significantly for Ue3
LMA solution. This gives the possibility of obtaining nearly bimaximal mixing
(ω = ψ = π/4 with φ small) within the LOW solution to the solar neutrino
problem—an interesting possibility for models predicting large mixing angles.

Conclusions

309

Figure 10.10. Results of the global three-flavour MSW fit to all data. Note that, in the first
two panels, the 99% C.L. contours are compatible with maximal mixing (tan2 ω = 1) for
both the LOW and the LMA solutions. Note that, when the CHOOZ constraints on φ are
included, only the first two panels are permissible (see figure 10.4).

10.5 Conclusions
We have analysed the most recent experimental evidence for solar and
atmospheric ν oscillations in a common theoretical framework including three-

310

Neutrino oscillations: a phenomenological overview

Figure 10.11. Allowed regions in the plane (δm 2 , sin2 φ), assuming maximal (ν1 , ν2 )
mixing (ω = π/4). For sin2 φ = 0, both the LMA and LOW solutions are compatible
with maximal mixing at 99% C.L. For small values of sin2 φ, the maximal mixing case
favours the LOW solution.

flavour transitions. We have investigated the regions of the mass-mixing
parameter space compatible with the data, with and without the CHOOZ
constraints. Such regions are of interest both for model-building and as a guidance
for future experimental tests. It turns out that both atmospheric and solar ν data
2 even without the inclusion of reactor
prefer low values of the matrix element Ue3
constraints, which represents a non-trivial consistency check.
2 < few %.
The addition of CHOOZ data implies the further restriction Ue3
Even within such limits, a novel feature emerges from the 3ν MSW analysis
of solar neutrinos [23]: bimaximal mixing of atmospheric and solar νs, usually
studied in terms of vacuum solar ν solutions, is possible also within the LMA and
LOW MSW solutions.

Acknowledgments
The author would like to thank the organizers of the School in ‘Contemporary
Relativity and Gravitational Physics’ for their kind hospitality. This work is
co-financed by the Italian Ministero dell’Università e della Ricerca Scientifica
e Tecnologica (MURST) within the ‘Astroparticle Physics’ project.

References

References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]

Kajita T 2000 Nucl. Phys. B (Proc. Suppl.) 85 44
Super-Kamiokande Collaboration 1998 Phys. Rev. Lett. 81 1562
Macro Collaboration 1998 Phys. Lett. B 434 451
Soudan 2 Collaboration 2000 Nucl. Phys. Proc. Suppl. 91 134
CHOOZ Collaboration 1999 Phys. Lett. B 466 415
Mikaelyan L 2000 Nucl. Phys. B (Proc. Suppl.) 87 284
Fogli G L, Lisi E Marrone A and Scioscia G 1999 Phys. Rev. D 59 033001
Fogli G L, Lisi E and Montanino D 1994 Phys. Rev. D 49 3626
Fogli G L, Lisi E and Montanino D 1996 Phys. Rev. D 54 2048
Kuo T K and Pantaleone J 1989 Rev. Mod. Phys. 61 937
Fogli G L, E Lisi, D Montanino and G Scioscia 1997 Phys. Rev. D 55 4385
LoSecco J M Preprint hep-ph/9807359
J M LoSecco Preprint hep-ph/9807432
Wolfenstein L 1978 Phys. Rev. D 17 2369
Mikheyev S P and Smirnov A Yu 1986 Nuovo Cimento C 9 17
Bahcall J N, Basu S and Pinsonneault M 1998 Phys. Lett. B 433 1
See also J N Bahcall’s homepage, www.sns.ias.edu/∼jnb
Homestake Collaboration 1998 Astrophys. J. 496 505
Kamiokande Collaboration 1996 Phys. Rev. Lett. 77 1683
SAGE Collaboration 1999 Phys. Rev. C 60 055801
GALLEX Collaboration 1999 Phys. Lett. B 447 127
Bahcall J N et al 1996 Phys. Rev. C 54 411
Lisi E and Montanino D 1997 Phys. Rev. D 56 1792
Fogli G L and Lisi E 1994 Astropart. Phys. 2 91
Gonzalez Garcia M C 2000 Nucl. Phys. B 573 3
Fogli G L, Lisi E, Montanino D, and Palazzo A 2000 Phys. Rev. D 62 013002
Bahcall J N, Krastev P I and Smirnov A Yu 2000 Phys. Lett. B 477 401
Bahcall J N and Krastev P I 1998 Phys. Lett. B 436 243

311

Chapter 11
Highlights in modern observational
cosmology
Piero Rosati
European Southern Observatory, Garching b. München,
Germany

11.1 Synopsis
In this chapter, we focus on the fundamental methods of observational cosmology
and summarize some of the recent observational results which have deepened
our understanding of the structure and evolution of the universe. The chapter is
divided into three parts. In the first section, we briefly describe the Friedmann
world models, which constitute the theoretical framework, we define the main
observables and we illustrate some common applications. In the second section,
we describe how galaxy surveys (primarily in the optical band) are utilized to map
the structure and evolution of the universe over a large fraction of its age, focusing
on observational methodologies and some recent results. In the third section, we
describe how surveys of galaxy clusters can be used to constrain cosmological
models, and measure the fundamental cosmological parameters. Throughout the
chapter, we touch only on a few recent highlights in observational cosmology. We
refer the reader to fundamental textbooks, such as Longair (1998), Peebles (1993)
and Peacock (1999), for a complete overview of the theoretical and observational
framework.

11.2 The cosmological framework
This section gives a very brief summary of the basics of Friedmann–Robertson–
Walker (FRW) models; only the essentials formulae which are used throughout
the chapter and the definition of observable quantities which are often used in
cosmology are included.
312

The cosmological framework

313

11.2.1 Friedmann cosmological background
What is generally referred to as the standard cosmological framework is the result
of the solution of the Einstein equations in the hypothesis that the universe is,
on very large scales, homogeneous and isotropic. There are several pieces of
observational evidence which support this cosmological principle, such as the
distribution of galaxies and clusters of galaxies on large scales and the remarkable
isotropy of the cosmic microwave background (CMB).
The FRW models provide the background on which the formation and
evolution of the large-scale structure in the universe can be studied as the
evolution of small perturbations to an otherwise uniform FRW model. The
application of the cosmological principle leads to the following FRW spacetime
line element (see Landau and Lifshitz (1971) for an elegant and simple
derivation):


2
dr
1
+ r12 (dθ 2 + sin2 θ dφ 2 )
(11.1)
ds 2 = c2 dt 2 − R 2 (t)
1 − kr12
= c2 dt 2 − R 2 (t)[dr 2 + Sk2 (r )(dθ 2 + sin2 θ dφ 2 )]

(11.2)

where two possible definitions of the comoving coordinate, r , have been used.
This is the coordinate measured by observers at rest with respect to the local
matter distribution. The first expression is commoonly used in the literature. In
the second form, following the notation by Peacock (1999), we have defined:
 sin(r ) k = 1 (close)
Sk (r ) =

r
sinh(r )

k = 0 (flat)
k = −1 (open).

(11.3)

The cases k = −1, 0, 1 represent, respectively, an open universe (infinite,
hyperbolic space), a flat universe (infinite, flat space) and a closed universe (finite,
spherical space).
The solution of the Einstein field equations (with cosmological constant )
leads to the following equation for the evolution of the scale factor, R(t):
 2
8π G
1
kc2
Ṙ
ρM + c2 − 2 .
=
R
3
3
R

(11.4)

This shows three competing terms driving the universal expansion: a matter
term, a cosmological constant term and a curvature term. We are neglecting
here a radiation term, as appropriate when the universe is dominated by nonrelativistic matter (‘dust’) with density ρM , i.e. the directly observable universe.
The respective fractional contributions to the energy density in the universe at the
present epoch are commonly defined as
m ≡

8π G
3H02

ρ M0 ,

 ≡

c2
3H02

,

k ≡ −

kc2
H02 R02

(11.5)

314

Highlights in modern observational cosmology

with
m +  + k = 1,

tot = m +  = 1 − k

(11.6)

where H0 ≡ ( Ṙ/R)t =0 = 100 km s−1 Mpc−1 h = h(9.78 × 109 )−1 years,
is the present value of the Hubble constant. The matter density parameter,
m (sometimes denoted as 0 ), can also be written as m = ρ0 /ρcr , where
ρcr = 3H02/(8π G) = 1.9 × 10−29h 2 g cm−2 is the critical density, which splits
open and close models in a matter-dominated universe.
The deceleration parameter is also often used:
q ≡ − R̈ R/ Ṙ 2 = m /2 −  .
With these definitions, the equation (11.4) can be written:


 3
 2
R0
R0
2
2
+ k
+  .
H = H0  m
R
R

(11.7)

(11.8)

11.2.2 Observables in cosmology
Suppose we are at r = 0 and observe an object at radial coordinate r1 , when
the expansion factor was R1 = R(t1 ) < R0 , at some lookback time t1 < t0 .
Quantities like r1 , t1 , R1 are not accessible to measurement. However, there are
directly measurable quantities which can be used to test the validity of the FRW
metric and to derive its parameters.
First of all, the redshift. From the spectrum of a distant source we can easily
recognize, say, an emission line whose rest-frame (emitted) wavelength is λe . In
general, we will measure a redshifted emission line at wavelength λ0 , so that the
redshift z is defined as
λ0
1+z = .
(11.9)
λe
If the expansion factor of the universe was R at redshift z, the following simple
relation holds:
R0
.
(11.10)
1+z =
R
Using this relation, we can now immediately write the lookback time, τ (z),
by integrating equation (11.8) after a change of variable, from R to z:
 z
τ (z) = H0−1
(1 + z )−1 [k (1 + z )2 + m (1 + z )3 +  ]−1/2 dz . (11.11)
0

τ (z) is plotted in figure 11.1 for three different values of (m ,  ). The age of
the universe is obtained for z → ∞.
We now examine the other measurable quantities.

The cosmological framework

315

11.2.2.1 Angular diameters
Photons from our distant object at radial distance r follow radial, null geodesics
(ds 2 = 0). Using the FRW metric (11.2), we can then link the angular size (θ ) of
an object to its proper length d, perpendicular to the radial coordinate at redshift
z:
d = RSk (r )θ = R0 Sk (r )θ/(1 + z)
d
d(1 + z)
=
θ =
dM
DA

(11.12)

where we have defined the distance measure, dM ≡ R0 Sk (r ), and the angular
diameter distance DA = dM /(1 + z).
The distance measure out to redshift z, dM (z), can be derived integrating
the equation of motion for a photon, R dr = c dt = c dR/(R H ), and using the
equations (11.8) and (11.10):

!
 z
cH0−1
1/2
2
3
−1/2
S
|
|
[
(1
+
z
)
+

(1
+
z
)
+

]
dz
k
k
m

|k |1/2
0
!
 z
−1 
cH0
1/2
2
−1/2
=
S |k |
[(1 + z ) (1 + m z ) − z (2 + z ) ]
dz
|k |1/2
0
(11.13)

dM (z) =

where the multiple function S is defined in (11.3); in the flat case of k = 0 only
the integral remains. Such an integral can easily be evaluated numerically.
For  = 0, an analytical solution exists (Mattig 1957):
dM =

2cH0−1
20 (1 + z)

{0 z + (0 − 2)[(0 z + 1)1/2 − 1]}.

(11.14)

Equation (11.12) shows that if a ‘standard rod’ existed, e.g. a class of objects
associated with a fixed physical size with negligible evolutionary effects, then it
would be possible to infer cosmological parameters (particularly q0 ) by plotting
the angular size as a function of redshift (e.g. Kellerman 1993).
11.2.2.2 Apparent intensities
If L is the rest-frame luminosity of an object at redshift z (in a given band), then
its flux (measured in erg cm−2 s−1 in cgs units) is
S=

L
2 (1 +
4πdM

z)2

=

L
4π DL2

(11.15)

where DL = dM (z)(1 + z) is the so called luminosity distance of the source,
which is defined so that the flux assumes the familiar expression in Euclidean

Highlights in modern observational cosmology

316

geometry (inverse square law). Observations (i.e. fluxes, luminosities) in a given
band [ν1 , ν2 ] can be related to the rest-frame band through the computation of
the K-correction, K z , which is essentially the ratio of fluxes in the rest-frame to
the observed (redshifted) band [(1 + z)ν1 , ν2 (1 + z)]. In optical astronomy the
magnitude system is used (m ∼ −2.5 log(S)) so that (11.15) can be written as a
relation between the apparent (m) and absolute magnitude (M) of the object:


DL
m = M + 5 log
(11.16)
+ Kz .
10 pc
If the flux spectra density is a power law, i.e. f ν ∼ ν −α (like most of the galaxies),
then one easily obtains K z = 2.5(α − 1) log(1 + z). Such a term can add up to
several magnitudes for early type (i.e. red) galaxies at z ∼ 1.
A low redshift expansion of (11.16) leads to the simple formula (e.g. Sandage
1995):
m = 5 log z + 1.086(1 − q0 )z + 5 log cH0−1 + M + 25.

(11.17)

This shows that if we can recognize a class of astrophysical sources as
‘standard candles’, by measuring the dimming of these sources over a wide
range of redshifts we can measure the deceleration parameter, q0 , and eventually
separate m and  . The application of this fundamental test to high redshifts
Type Ia supernovae has lead to spectacular results in recent years (e.g. Perlmutter
et al 1999, Schmidt et al 1998).
11.2.2.3 Number densities
One of the main goal of redshift surveys is to quantify the comoving volume
density of objects as a function of redshift. A frequently used quantity is therefore
the comoving volume element in the redshift interval, z to z +dz, in the solid angle
d, which follows directly from the FRW metric (11.1), (11.2):
dV =

2
dM
2 )1/2
(1 + k c−2 H02dM

d(dM ) d.

(11.18)

Using equation (11.13), and defining the functions E(z) and A(z) as
 z
 z
E(z) =
[k (1 + y)2 + m (1 + y)3 +  ]−1/2 dy ≡
A(y) dy,
0

0

we have:
dV
= (cH0−1)3 A(z)|k |−1 S 2 {|k |1/2 E(z)}
d dz
2
= cH0−1 A(z)dM
≡ Q(z, m ,  ),

(11.19)

The cosmological framework

317

where, as usual, we defined S 2 as sinh2 if k > 0 (open universe) and sin2 if
k < 0 (close universe). In the flat case, S 2 → E 2 (z). Remember that k is not
an independent parameter but rather given by 1 − m −  .
For  = 0, one finds:
(cH0−1)3 {q0 z + (q0 − 1)[(2q0 z + 1)1/2 − 1]}2
dV
=
d dz
(1 + z)3
q04 (1 + 2q0 z)1/2

(11.20)

cH0−1  3000h −1 Mpc is the Hubble length.
The volume element (11.19) is plotted in figure 11.1 for three reference
models. We will see later that the flat case (m ,  ) = (0.3, 0.7) is currently
favoured by measurements. This plot shows that if we peer into a patch of
the sky with deep observations, at z = 2–3 we have a good chance to explore
a large comoving volume (which is ultimately determined by the observational
technique).
11.2.2.4 Surface brightnesses
The observed surface brightness &obs of an extended object is defined as the
flux per unit emitting area. This is the observable that ultimately drives the
detection of faint galaxies (rather than its flux), and has the remarkable property
of being independent on cosmological parameters. For a FRW model, using
equations (11.15), (11.12), it is:
&obs

2
dM
Sobs (ν1 , ν2 )
L obs (ν1 , ν2 )K z
1
=
=
=
2
2
2
4
π
π'
πd (1 + z)
4πdM



L obs
4πd 2



Kz
.
(1 + z)4

This is also known as the Tolman law, and can be used as a direct test of the
expansion of the universe (e.g. Sandage 1995). L obs /4πd 2 is the intrinsic surface
brightness of the source with physical size d (in units of, e.g., erg s−1 kpc−2 ).
Besides the K-correction, this relation shows that the surface brightness of
extended objects drops very rapidly with redshifts, making the detection of high-z
extended objects difficult.
11.2.3 Applications
One of the most common application of the expressions derived in the previous
section is the computation of observed distributions, such as source number
counts, or the redshift-dependent volume density of a class of objects, based on
known local (z  0) distributions. By comparing these observed distributions,
at different redshifts, with those predicted on the basis of observations in the
local universe or models of structure formation, one can set constraints on
the evolutionary history of a given class of objects, and, in principle, on the
cosmological model itself (i.e. on m ,  ).

318

Highlights in modern observational cosmology

(a)

(b)
Figure 11.1. (a) Lookback time as a function of the redshift for three reference FRW
models (Einstein–de Sitter, open, flat). At z = 20 the lookback time is approximately 99%
of the age of the universe in all models. (b) Derivative of the comoving volume element,
per unit solid angle, as a function of redshift for the same models.

The cosmological framework

319

11.2.3.1 Number counts
By number counts we mean the surface density on the sky of a given class of
sources as a function of the limiting flux of the observations (e.g. magnitude,
radio flux). This is the simplest observational tool which can be used to study the
evolution of a sample of objects, and, to some extent, to test cosmological models.
It does not require redshift measurements but only a knowledge of the selection
function (indeed, a major challenge in any survey in comology!).
The space density of sources of different intrinsic luminosities, L, is
described by the luminosity function (LF), φ(L), so that dN = φ(L) dL is the
number of sources per unit volume with luminosity in the range L to L + dL. The
most common functional form to describe observational data is the one proposed
by Schechter (1976):
φ∗
φ(L) =
L∗



L
L∗

−α

e−L/L ∗ .

(11.21)

L ∗ is the characteristic luminosity of the population,
 ∞ the normalization φ∗
determines the volume density of sources, as n 0 = 0 φ(L) dL = φ∗ (1 − α),
where  is the gamma-function. The product φ∗ L ∗ is an estimate of the integrated
luminosity of all sources in a given volume, since the the luminosity density is
∞
defined as  L = 0 Lφ(L) dL = φ∗ L ∗ (2 − α).
The determination of the local LF of galaxies is not completely
straighforward since one has to take into account the morphological mix of
galaxies (i.e. the existence of a variety of morphological types, from ellipticals
to spirals and irregulars) and clustering effects which bias the measurement of the
space density. Most of the observations in the nearby universe (e.g. Loveday et al
1992) find best-fit parameters:
L ∗  1010h −2 L
(corresponding to a B band absolute magnitude MB  20 + 5 log h);
φ∗  (1.2–1.5) × 10−2 h 3 Mpc−3 ,

α  1.

Let us consider, for simplicity, the local or nearby Euclidean universe
uniformly filled with sources with LF φ(L). If S is the limiting flux, sources with
luminosity L can be observed out to r = (L/4π S)1/2 . The number of sources
over the solid angle , observable down to the flux S are:



 3
−3/2
r φ(L) dL =
S
L 3/2 φ(L) dL.
N(> S) =
3
3(4π)3/2
Once the integral over all luminosities is evaluated, the surface density of sources
down to the flux S is always N(> S) ∝ S −3/2 . If we use magnitude instead
of luminosities, then log N(> m) ∝ 0.6 m. Therefore, number counts in

320

Highlights in modern observational cosmology

the nearby universe, where curvature terms can be neglected, are characterized
by a Euclidean slope of −1.5 (or 0.6 mag). In general, at large distances,
curvature effects (cf equations (11.13) and (11.18)) cause number counts to have
slopes always shallower than the Euclidean one. However, as we will see in
section 11.3.3, evolutionary effects (φ = φ(L, z)) can counteract such a natural
behaviour and produce counts steeper than 1.5.
11.2.3.2 Redshift distribution and number counts (general case)
We now have all the ingredients to compute the expected redshift distribution,
n(z), and number counts, n(> S), for an evolving population of sources with
LF φ(L, z). Typically, on the basis of the known local LF, φ(L, 0), one wants
to compare the observed redshift distribution of sources with the one expected
on an empirical evolutionary scenario, or the one predicted by some theory of
structure formation. In general, there will be some degree of degeneracy between
evolutionary parameters and cosmological paramaters (m ,  ) when matching
theoretical models with observational data.
With Q(z, m ,  ) given by equation (11.19) (or (11.20)), the number of
sources per unit solid angle and redshift, in the luminosity range L to L + d L, is:
d2 N
φ∗
φ(L) dL = Q(z, m ,  )
ddz
L∗



L
L∗

−α

e−L/L ∗ dL.

(11.22)

We now change variable, y = L/L ∗ , and call L 1 and L 2 the minimum and
maximum luminosity of the source population (for example, a magnitude range
within which we want to compute the redshift distribution). Thus, the surface
density of sources, per unit redshift, observed down to the flux S can be written
as:
 y2
dN(> S, z)
= φ∗ Q(z, m ,  )
y −α e−y dy
(11.23)
d dz
y1 (z)
= φ∗ (1 − α)Q(z, m ,  )[P(1 − α, y2 ) − P(1 − α, y1 )],
where P is the generalized -function, y2 = L 2 /L ∗ , and


L 1 L min (S, z)
,
L min (S, z) = S4π D 2L (z)K z .
,
y1 (z) = max
L∗
L∗

(11.24)

L min is the rest-frame miminum luminosity detectable at redshift z, at the limiting
flux S (equation (11.15)).
The numerical integration of equations (11.23) and (11.24) can also include
an evolving LF, e.g. φ∗ = φ∗ (z), L ∗ = L ∗ (z). The result can be directly compared
with the observed redshift distribution of sources, i.e. the number of sources per
deg2, in each redshift bin. The number counts n(> S) are obtained by integrating
(11.23) over all redshifts.

Galaxy surveys

321

11.3 Galaxy surveys
11.3.1 Overview
Over the last ten years, significant progress has been made in both observational
and theoretical studies aimed at understanding the evolutionary history of
galaxies, the physical processes driving their evolution and leading to the Hubble
sequence of types (ellipticals, spirals, irregulars) that we observe today.
Deep galaxy surveys have had a central role in cosmology back to the
pioneering work of Hubble. In the 1960s (see Sandage 1995) several studies
used galaxy counts as a tool to test cosmological models; however, it was soon
realized that it was difficult to disentangle the effects of evolution from those due
to the universal geometry, as well as the effects of object selection, which, if not
properly understood, can easily alter the slope of the number counts (see later).
The modern era of observational cosmology began with the advent of CCD
detectors in the 1980s and soon after with multi-object spectrographs. Scientific
progress has obviously been driven by a series of technological breakthroughs
with telescopes and instrumentation, that we can summarize as follows:
•
•

•

•
•

Mid 1980s: First deep CCD surveys (Tyson 1988) revealed a large number
of faint, blue galaxies in nearly confusion limited images.
Early 1990s: (a) the development of multi-object spectrographs allows the
first spectroscopic surveys of distant galaxies (e.g. Ellis et al 1996, Lilly et
al 1995); and (b) central role of Hubble Space Telescope (HST) (resolved
images of distant galaxies, morphological information).
Mid 1990s: (a) spectroscopy with the Keck telescope (10 m collecting area)
pushed the limit to two magnitudes fainter; (b) significant improvement in
near-IR imaging (sensitivity and detector area); and (c) deep imaging in the
millimetre wavelength with the SCUBA instrument.
Late-1990s: wide-field optical imaging; (b) high-multiplexing spectroscopy
(several hundreds of spectra at once); and (c) 8 m class telescopes with active
optics (VLT) (delivering angular resolution of 0.5 or better).
On-going/upcoming: (a) next generation of spectrographs + near-IR
spectroscopy on 8–10 m class telescopes; (b) Integral-field spectrographs
(x, y, λ information); (c) adaptive optics delivering diffraction-limited
images (∼0.05 resolution); and (d) Advance Camera for Survey on HST
(2001).

This rapid technological development has allowed a number of major
surveys to be carried out. We can classify those which have had a major impact
on the way we understand the structure and evolution of the universe today as
follows.

322

Highlights in modern observational cosmology

Large area surveys
•
•
•
•

APM (Automatic Plate-measuring Machine, e.g. Maddox et al 1990)—
imaging photographic plates;
CfA survey (Center for Astrophysics, e.g. Huchra et al 1990);
LCRS (Las Campanas Redshift Survey, e.g. Shectman et al 1996)—∼104
galaxy redshifts, over 700 deg2 out to z  0.2;
2dF survey (2 degree field, e.g. Colless 1999)—∼105 redshifts covering
1700 deg2; and
SDSS survey (Sloan Digital Sky Survey: http://www.sdss.org)—∼106
redshifts + multicolour imaging (104 deg2, m lim  22).

The first three surveys have provided the power spectrum of the large-scale
structure, by measuring the correlation function over a wide range of scales (see
L Guzzo, this volume), and the luminosity functions of different galaxy types in
the local universe. The on-going 2dF and SDSS surveys will soon bring these
measurements to an unprecedented level of precision.
Deep, small area surveys
•
•
•
•

The LDSS autofib survey (Ellis et al 1996)—B-band selected redshift survey
down to B  24 (z . 0.7).
The CFRS survey (Lilly et al 1995)—I-band selected redshift survey down
to I  22 (∼ 600 galaxies at z . 1).
The Keck Survey (Cowie et al 1996)—150 galaxy redshifts out to z  1.5
(22.5 < B < 24).
The CNOC2 surveys (Yee et al 2000)—6000 galaxy redshifts over 1.5 deg2
area (z . 0.6).

These surveys have established a clear evolutionary pattern for different galaxy
types out to z ∼ 1 (see section 11.3.3).
Ultra-deep, tiny area surveys
•

Hubble Deep Field North and South (e.g. Williams et al 1996, Ferguson et
al 2000)—5 arcmin2, m lim  29 (see later).

11.3.2 Survey strategies and selection methods
When planning an imaging survey (not necessarily in optical or near-IR
wavelengths which are the primary subject here), the balance between the depth
and the solid angle, as well as the selection of the observed band play a central
role. These decisions are driven by the nature of the sources under study, as
well as their typical volume density and luminosity, i.e. φ∗ and L ∗ (see (11.21)).
Rare objects, such as quasars or galaxy clusters, require large-area surveys to be
found in sizeable numbers. Large surveys also probe the bright end of the LF

Galaxy surveys

323

Figure 11.2. Several optical and near-IR surveys (carried out over the last ten
years) in the depth–solid angle plane. The AB magnitude system is defined as
m(AB) = −2.5 log f ν (nJy) + 31.4.

of any source population, as opposed to small-area surveys which mostly probe
the faint end of the LF (L . L ∗ ). In general, the deeper the survey is the more
distant are the L ∗ objects which can be detected. The combination depth–solid
angle will determine the sampled volume at different redshifts, for a given object
selection method. Obviously, the product (limiting flux × survey area) is kept
approximately constant by observational time constraints. In figure 11.2, we
plot several cosmological surveys which have been carried out over the last ten
years with the aim of mapping the structure in the universe and understanding
its evolution. The Sloan Digital Sky Survey (SDSS) and the Hubble Deep Field
(HDF) represent the two complementary extremes, i.e., a shallow survey covering
a significant fraction of the sky and a very deep pencil beam survey.
For a given depth and survey area, the probed volume is ultimately
determined by the selection function, i.e. the set of criteria which lead to the object
detection. There are basically three different selection methods:
(1) Flux-limited selection. All the sources with a flux greater than a given
threshold, Slim , are included in the sample. The simplicity of this method
leads to a straightfoward computation of the probed volume (however, see
caveats later). If AS is the survey area, the maximum redshift, z max , at which
a source of rest frame luminosity L can be detected, is given implicitily by
L = Slim 4π DL2 (z max ) (11.15). Thus, using (11.19), the survey volume is:
 zmax
Q(z, m ,  ) dz.
(11.25)
Vmax (z, L) = AS
0

Note that the K -correction is also involved in this calculation when

324

Highlights in modern observational cosmology

converting from observed to rest-frame luminosities. By counting sources
in different luminosity–redshift bins one can thus estimate the LF φ(z, L).
(2) Colour selection. Sources are selected on the basis of their flux and colour.
A relevant case is described in section 11.3.4. The advantage of this method
is that it is extremely efficient at isolating objects in a given redshift range,
for example a distant volume in the universe. However, the selection function
(i.e. the survey volume) critically depends on the knowledge of the spectral
energy distribution (SED) of the sources under study.
(3) Narrow-band filter selection: This technique consists of selecting sources
which have a flux excess when observed through a narrow-band filter, as
compared to their broad-band flux. Emission line objects (e.g. starbursts,
AGN) are the targets of these surveys. Sources are detected at redshift
1 + z = λfilter /λem.line , within a z given by the width of the filter, which
needs to be narrow enough (.100 Å) to boost the contrast of the emitting
line object against the background sky. The equivalent width of the emission
line ultimately determines the selection function. Several searches for very
high redshifts objects have been conducted using the Lyα (1216 Å) as a
tracer. Such surveys have had some success (Hu et al 1999), but have also
underscored the difficulties of this method. First, a very narrow redshift slice
is probed, and therefore samples are small and prone to cosmic variance and
large-scale structure effects. Second, only a limited portion of the galaxy
population (e.g., galaxies with large equivalent width) is selected. These
limitations make it difficult to draw statistical conclusions on the volume
density, or luminosity density of distant galaxies.
11.3.2.1 Caveats
There are several caveats inherent in the aforementioned selection methods, which
if not properly addressed, can lead to a biased view of the evolution of the
structure in the universe and underlying cosmological models.
First of all, the flux-limit approach is an idealization of our detection process.
Sources are never detected on the basis of their flux, but rather on the basis of
their surface brightness (a detection consists of an excess of flux within a given
aperture, above a given threshold, which is usually a few times the rms value
of the surrounding background). A major concern of any survey is to establish
whether the sample is, to a good approximation, flux-limited rather than surfacebrightness-limited. As a result, the flux limit (Slim ) should be chosen high enough
so to cover the whole range of surface brightness of our sources. Low surface
brightness sources will be the first to drop out of the sample if this process is ill
defined.
Second, the computation of the K-correction requires a knowledge or
assumptions on the SED of sources at different redshifts.
Third, the effect of reddening especially due to dust enshrouding distant
objects (and, to a lesser extent, to intervening neutral hydrogen) can have a

Galaxy surveys

325

Figure 11.3. A compilation of number counts in the U, B, I, K bands from different
surveys (Ferguson et al 2000 and references therein). Full symbols are from the
HDF North and South, open symbols from several ground-based surveys. Full lines
are no-evolution models obtained integrating the observed local luminosity function for
(m ,  ) = (0.3, 0.7).

significant impact on the selection function and completeness of the sample by
absorbing the UV part of the continuum and selectively suppressing different
emission lines.
11.3.3 Galaxy counts and evolution
We show in figure 11.3 a compilation of number counts from ground-based and
HST surveys over a 13 magnitude range, as observed in the U, B, I and K
passbands (see Ferguson et al 2000). Each set is displaced by a factor of 10 for
clarity. Full curves represent the theoretical expectations obtained by integrating
the local luminosity function assuming no evolution and (m ,  ) = (0.3, 0.7),
as described in section 11.3.3. These no-evolution (NE) models make reasonable
assumptions on the morphological mix of the local galaxy population (relative
fraction of irregulars, spirals, ellipticals), their LFs and their SEDs (required to

326

Highlights in modern observational cosmology

compute the K-corrections). Such assumptions reflect observations of the nearby
universe but are still affected by some uncertainty, therefore it is not uncommon
to find in the literature NE models which differ by ∼50%. This uncertainty will
be drastically reduced when the 2dF and SDSS surveys are completed.
A clear trend is apparent in figure 11.3. At blue wavelengths the observed
counts exceeds the NE predictions by as much as a factor three, a problem
which was recognized in the first deep surveys and which has become known
as the faint blue galaxy excess. Such an excess progressively disappear at longer
wavelengths. Observations in blue filters are sensitive to late type, star-forming
galaxies with young stellar populations. Therefore, it had already become evident
in the early 1990s (e.g. Ellis et al 1996) that this is the galaxy population which
has undergone most of the evolution (in luminosity and/or number density) out
to z ∼ 1, i.e. the last 50% of the life of the universe. The first deep redshift
surveys (Lilly et al 1995) confirmed this scenario directly measuring a significant
evolution of the LF for the ‘blue population’ out to z  0.7, while revealing no
significnt evolution for the ‘red population’ consisting of galaxy types earlier than
an Sbc (see figure 11.4). Red wavelength observations, particularly in the K-band
(λ0 = 2 µm), collect rest-frame optical light out to z ∼ 3, thus probing old,
long-lasting stellar populations in distant galaxies (i.e. earlier types). All these
observations (see also Cowie et al 1996) have shown a remarkable increase in the
space and/or luminosty density of star-forming galaxies with redshift. However,
interpreting these results, and understanding the physical processes responsible
for this evolutionary pattern, has remained a difficult task.
In this respect, HST observations have driven us a big step forward by
allowing intrinsic sizes and morphologies of distant galaxies to be measured.
The combination of angular resolution (0.05 ) and depth has also pushed these
studies well beyond z = 1. As an example, in figure 11.5 we show number
counts for different morphological types as directly determined by the HDFN images (Driver et al 1998). Along with NE model predictions (full lines),
passive evolution models are also shown. The latter are constructed using spectral
synthesis models (e.g. Bruzual and Charlot 1993), assuming a formation redshift
(generally varying by type), and a star formation history (with a given initial mass
function, IMF). As an example, in figure 11.6 we show the evolution of the SED
of a 3 Gyr burst stellar population over approximately a Hubble time. This model
well reproduces the evolution of an early type galaxy. The UV luminosity declines
rapidly after the end of the burst of star formation, as hot O and B stars burn off
the main sequence and the population is more and more dominated by red giants.
In general, passive evolution models are characterized by luminosity
evolution, which is the result of letting the stellar populations evolve with a
pre-defined star formation history, without including any merging. Figure 11.5
confirms that morphologically selected early types show little (simple passive)
evolution to faint magnitudes, and hence to relatively high redshifts. Counts of
intermediate types (i.e. spiral-like galaxies) are broadly consistent with passive,
luminosity evolution models, whereas later types and irregulars are not fitted by

Galaxy surveys

327

Figure 11.4. Measurement of the LF at different redshifts from the CFRS survey (Lilly
et al 1995). The redshift bin and number of objects for each LF are given in the label
in each panel. The dividing line between ‘red’ and ‘blue’ samples corresponds to the
rest-frame colour of an Sbc galaxy. A clear evolution is visible in the blue sample, whereas
no significant evolution is observed in the red sample out to z  0.7.

any of these models. It is believed that most of the morphological evolution
of these irregular and peculiar galaxies occurs at 1 . z . 2, as a result
of interactions or merging, to lead to the assembling of the familiar Hubble
sequence. In general, fairly complex luminosity evolution models, which also
include a prescription for dust obscuration, fail to predict number counts at the
faintest magnitudes or the number density of galaxies at z & 2. This is a clear
indication that a much deeper physical understanding of the galaxy formation
processes is needed (‘active’ versus simple passive evolution). Central, unsolved
key questions are how the star formation activity is modulated by merging and

328

Highlights in modern observational cosmology

Figure 11.5. Number counts for different morphological types as derived from the HDF-N
survey (Driver et al 1998). The full and broken curves are predictions from no-evolution
and passive evolution models respectively (for m = 1,  = 0).

how the stellar mass is assembled over time in a hierarchical structure formation
scenario.
11.3.4 Colour selection techniques
The measurement of the redshift of distant (say z > 1), faint galaxies is a timeconsuming task and becomes impossible at magnitudes fainter than ∼25, even
with 8–10 m class telescopes equipped with modern spectrographs. As outlined
in section 11.3.3, statistical studies of the nature and evolution of galaxies require
an estimate of their SED and their redshift at magnitude selections well beyond
the spectroscopic limit. This has stimulated intensive activity over the last few
years, aimed at exploiting colour selection techniques to isolate and study galaxy
populations at different redshifts. The basic idea has been to use multi-colour
imaging, in as many passbands as possible, to constrain the SED of galaxies by
detecting spectral features and measuring the continuum slope, thus estimating
the redshift.
The most successful colour selection method in recent years, which has
become known as Lyman break technique, was devised to detect the ubiquitous
Lyman limit discontinuity at 912 Å, which is redshifted into the HST bandpasses
at z & 2 (or at z & 2.5 for redder ground-based filters) (e.g. Steidel et al 1996).
This technique is illustrated in figure 11.7 (see the review by Dickinson 1998). A
galaxy with an unreddened UV continuum (i.e. a star-forming galaxy or an AGN)
has a nearly flat spectrum in fν , and a sharp break due to photolectric absorption
of intervening neutral hygrogen (in the galaxy itself and in the intergalctic space
along the line of sight) shortward of 912 Å (lyman limit). The integrated effect

Galaxy surveys

329

Figure 11.6. Evolution of the spectral energy distribution (SED) of a stellar population
modelled as a star formation burst of 3 Gyr, over the lifetime of the universe. From top to
bottom, the SEDs are shown at ages: 0.2, 3.2, 3.4, 4, 5, 10, 18 Gyr. The latest Bruzual and
Charlot spectral synthesis models have been used.

of neutral hydrogen clouds along the sightline (Lyα forest) produces a further
depression blueward of the Lyα, which becomes stronger at higher redshifts. As
a result, a star-forming galaxy at z  3 is seen disappearing in the transition from
the B to the U band (‘U drop-out’). In general, by measuring colours, such as
U–B and B–V, one can select a large sample of galaxies around z ∼ 3, since
these sources will stand out in a colour–colour diagram, having very red U–B
colours (lyman limit passing through the two filters) and nearly zero B–V colours
(flat spectrum). Such a technique was first successfully applied to ground-based
imaging data (e.g. Steidel et al 1996), which have the advantage of covering much
larger solid angles than the HDF, although they cannot match the photometric
accuracy of HST, which is critical to measuring colours accurately.

330

Highlights in modern observational cosmology

Figure 11.7. Illustration of the Lyman break (‘drop-out’) technique in the HDF-N from
Dickinson (1998). Top panel: model spectrum of a star-forming galaxy at z = 3.0.
Its flat UV continuum (in f ν units) is truncated by the 912 Å Lyman limit, which is
redshifted between the U and B filters of the WFPC2 camera aboard the HST. Intervening
neutral hydrogen along the light of sight further suppresses the continuum blueward of Lyα
(1216 Å). Bottom: HDF-N galaxy, spectroscopically confirmed at z = 2.8, as observed in
the four WFPC2 bandpasses. Its flux is constant in V and I, it dims in B and completely
vanishes in the U-band image.

Follow-up spectroscopy with the Keck telescope has confirmed that objects
selected in this fashion were indeed star-forming galaxies at 2 . z . 3.5
(Steidel et al 1996). The same technique can be applied to search for higher
redshifts galaxies/AGN, for example, objects at z & 4, the so-called ‘B drop-outs’
(Steidel et al 1999), although it becomes much harder as they become fainter
(R > 24) and more rare. To date, approximately 900 galaxies have measured
with a spectroscopic redshift at z  3 ± 0.5 and approximately 50 at 4 . z . 5.
By exploring relatively large volumes at z ∼ 3, these studies have taught us much
about the star formation density (see section 11.3.5) and large-scale structure (e.g.

Galaxy surveys

331

Giavalisco et al 1998) in the universe back to epochs which represent only 20%
of the cosmic time (e.g. Steidel et al 1998, 1999).
The Lyman-break technique is just a particular case of a more general
method known as photometric redshifts. Photometric information from a multicolour survey can be used as a very low resolution spectrograph to constrain
the galaxy SED and thus to estimate the redshift. A good example is shown in
figure 11.8 (Giallongo et al 1998). A set of SED templates, generally generated
with spectral synthesis models (i.e. Bruzual and Charlot models, including UV
absorption by the intergalactic medium and dust reddening), is compared with
broad photometry data. The best-fit template yields the redshift and the nature of
the galaxy.
The photometric redshift technique has been extensively tested in the HDFN data, since approximately 150 spectroscopic redshifts are available in this
field out to z  4.5 and high photometric accuracy can be achieved with the
angular resolution and depth of HST images. For example, Benitez (2000)
has shown that an accuracy of z ≤ 0.08(1 + z spec ) can be reached using a
Bayesian estimation method (see figure 11.9). With such an accuracy, one can
use photometric redshifts to study the evolution of global statistical properties of
galaxy populations, such as clustering at z . 1 and the star formation history out
to z  4 (see later).
11.3.5 Star formation history in the universe
The UV continuum of a star-forming galaxy probes the emission from young
stars and therefore it directly reflects the ongoing star formation rate (SFR). The
optimal wavelength range is ∼1250–2500 Å, longward of the Lyα forest but at
wavelengths short enough that the contribution from older stellar populations
can be neglected. In order to establish the relationship between SFR and UV
luminosity, evolutionary synthesis models are used. This is a multiparameter
exercise though. Basic ingredients include: the metallicity of the stars, the star
formation history, the IMF, as well as stellar tracks and atmospheres. A series of
these constant SF models, with a range input parameters, is shown in figure 11.10
(lower curves). After ∼1 Gyr, the UV luminosity settles around a well defined
value which can be used to convert UV luminosities into SFRs. Madau et al
(1998) used the following relation:
SFR(M yr−1 ) = 1.4 × 10−28 L UV (erg s−1 Hz−1 ).

(11.26)

For models with a short burst of star formation (upper curves) such a simple
relation does not exist, although, statistically speaking, (11.26) is still a reasonable
approximation, if a sample of galaxies is caught during their first Gyr of life.
Equation (11.26) applies in the wavelength range 1500–2800 Å since the
spectrum f ν of a star-forming galaxy is nearly flat in that region. At z & 1
optical observations probe this UV rest frame portion of spectrum, therefore the
observed luminosity function, or luminosity density, can be directly converted to

332

Highlights in modern observational cosmology

Figure 11.8. Illustration of the photometric redshift technique on a variety of intermediate
and high redshift galaxies (Giallongo et al 1998). The data points are broad-band
photometric meaurements in BVRIK filters used to constrain the spectral energy
distribution of galaxies, thus estimating their redshift.

SFR density. By using photometric redshifts (possibly supported by a subset of
spectroscopic measurements), one can thus trace the star formation history in the
distant universe. Madau et al (1998) exploited this method to measure the global
SFR at 0.5 . z . 4 using HDF and ground-based surveys. This measurement has
been repeated by many others in recent years (e.g. Steidel et al 1999), and most of
the debate has focused on the critical role of dust which is surely present in highz galaxies and is very effective in absorbing UV radiation. To some extent all
UV-based SFR measurements are biased low due to dust extinction (e.g. Steidel

Galaxy surveys

333

Figure 11.9. Comparison between the spectroscopic redshift (z spec ) and the photometric
redshift (z B ) in the HDF-N (Benitez 2000).

et al 1999). The standard procedure is to apply statistical corrections, which use
empirical correlations of the UV slope β with the extinction derived from the
Balmer decrement in nearby starburst galaxies (Calzetti et al 1994).
A collection of (mostly dust corrected) estimates of the SFR density over
a broad range of redshifts is shown in figure 11.11, which illustrates the great
progress made in recent years. This picture seems to suggest that a large fraction
of the stars had already been formed by z ∼ 3. However, global average SFR
densities over large cosmic volumes, even in the hypothesis that we can correct
for dust extinction, tell us very little about the processes which modulate the star
formation (e.g. merging events) and lead to build galaxy masses over time. Future
space-based far-infrared (5–30 µm) observations, by providing rest-frame nearIR radiation (which is well correlated with the stellar and dynamical mass) and by
measuring the thermally reradiated dust emission in distant galaxy, hold the best
promise to shed new light on these issues.

334

Highlights in modern observational cosmology

Figure 11.10. Linking the star formation rate (SFR) to the UV luminosity (L 1500 ) using
population synthesis models (from Schaerer 1999). Lower curves give the temporal
evolution of L 1500 for models with a constant SFR of 1M yr−1 . Upper curves are models
with a burst of SF with duration 5, 20, 100 Myr, forming the same total mass (109 M ).

11.4 Cluster surveys
11.4.1 Clusters as cosmological probes
The distribution and masses of galaxy clusters are important testing tools
for models describing the formation and evolution of cosmic structures. In
standard scenarios, clusters form in correspondence with the high peaks (i.e.
rare fluctuations) of the primordial density field (e.g. Kaiser 1984). Therefore,
both the statistics of their large-scale distribution and their abundance are highly
sensitive to the nature of the underlying dark matter density field. Furthermore,
their typical scale, ∼10h −1 Mpc relates to fluctuation modes which are just
approaching the nonlinear stage of gravitational evolution. Thus, although
their internal gravitational and gas dynamics are rather complex, a statistical
description of global cluster properties can be obtained by resorting to linear
theory or perturbative approaches. By following the redshift evolution of clusters,
we have a valuable method to trace the global dynamics of the universe and,
therefore, to determine its geometry.

Cluster surveys

335

Figure 11.11. History of the star formation rate (SFR) in the universe (∼80% of the
cosmic time): SFR density versuss. redshift as derived by the UV luminosity density of
different distant galaxy samples (see Ferguson et al (2000) for a review). Loopback time
and distances are computed using m ,  , h = 0.3, 0.7, 0.65.

In this context, the cluster abundance at a given mass has long been
recognized as a stringent test for cosmological models. Typical rich clusters have
masses of about 5 × 1014h −1 M , i.e. similar to the average mass within a sphere
of ∼8h −1 Mpc radius in the unperturbed universe. Therefore, the local abundance
of clusters is expected to place a constraint on σ8 , the rms mass fluctuation on the
8h −1 Mpc scale. Analytical arguments based on the approach devised by Press
and Schechter (1974) show that the cluster abundance is highly sensitive to σ8
for a given value of the density parameter m . Once a model is tuned so as to
predict the correct abundance of local (z . 0.1) clusters, its evolution will mainly
depend on m (e.g. Eke et al 1996). Therefore, by following the evolution of the
cluster abundance with redshift one can constrain the value of the matter density
parameter and the fluctuation amplitude level at the cluster scale.
The evolution of cosmic structures, building up in a process of hierarchical
clustering, is well illustrated in the VIRGO simulations (Jenkins et al 1998)
of figure 11.12 (see also the chapter by Anatoly Klypin in this volume). The

336

Highlights in modern observational cosmology

Figure 11.12.
Evolution of the cosmic structure (projected mass distribution)
from z = 3 to the present, as obtained with large N-body simulations by
the VIRGO Colloboration (Jenkins et al 1998). The three models are -CDM,
S(tandard)-CDM and O(pen)-CDM with, respectively, the following parameters
(m ,  , , h) = (0.3, 0.7, 0.21, 0.7), (1, 0, 0.5, 0.5), (0.3, 0, 0.21, 0.7).  is the shape
parameter of the power spectrum. Each box is 240h −1 Mpc across.

projected mass distribution is shown in three snapshots (z = 3, 1, 0), for three
different cold dark matter (CDM) models. Model parameters have been chosen
to reproduce approximately the same abundance of clusters at z = 0 (using a
different normalization σ8 ). These simulations clearly show that the growth rate
of perturbations depends mainly on m and, to a lesser extent, on  . In low
density models, fluctuations start growing in the early universe and stop growing
at 1 + z ∼ −1
m . In SCDM (m = 1) large structure form much later, and
end up evolving rapidly at z < 1. The effect of the cosmological constant is
to lengthen cosmic time (figure 11.1) and to ‘counteract’ the effect of gravity,
so that perturbations cease to grow at slighly later epochs (a close inspection of

Cluster surveys

337

figure 11.12 shows indeed less structure at z = 3 in the CDM model when
compared with OCDM).
One of the fundamental quantities that a CDM model predicts is the cluster
mass function, N(M, z), i.e. the number of virialized clusters per unit volume
and mass, at different epochs. This can be derived by applying cluster-finding
algorithms directly on simulations, as in figure 11.12. A very simple and
powerful method proposed by Press and Schechter (1974) is, however, often
used to compute N(M, z). This analytical approach is found to be in remarkable
agreement with N-body simulations, although slight refinements have recently
been proposed (Sheth and Tormen 1999). We refer the reader to the original
papers or the aforementioned textbooks for a derivation of the Press–Schechter
method.
11.4.2 Cluster search methods
The cluster mass is not a direct observable, although several methods exist to
estimate the total gravitational mass of clusters. In order to derive the cluster
mass function at varying redshifts, one needs three essential tools:
(1) an efficient method to find clusters at least out to z  1;
(2) an estimator (observable), M̂, of the cluster mass; and
(3) a simple method to compute the selection function, i.e. the comoving volume
within which clusters are found.
We can summarize the methods of finding distant clusters as follows:
•

•

•

•

Galaxy overdensities in optical/IR images: this is the traditional way
which was successfully used by Abell to compile his milestone cluster
catalogue. At high redshifts, chance superpositions of unvirialized systems
and strong K -corrections for cluster galaxies make optical searches very
inefficient. Near-IR searches, supported by some colour information,
improve substantially the effectivness of this method. In general, however,
the estimate of the survey volume is ill defined and model dependent. In
addition, the optical luminosity is poorly correlated with the cluster mass.
X-ray selected searches: arguably, the most efficient method used so far to
construct distant cluster samples and to estimate the mass function. The xray luminosty is well correlated with the mass and the selection function
is straightforward, since it is the one of a (x-ray) flux-limited sample.
Possible biases, similar to galaxy searches, are connected to possible surface
brightness limits.
Search for galaxy overdensities around high-z radio galaxies or AGN:
searches are conducted in near-IR or narrow-band filters. This method has
provided so far the only examples of possibly virialized systems at z > 1.5
(e.g. Pentericci et al 2000).
Sunyaev–Zeldovich (SZ) effect: distortion of the CMB spectrum due to
the cluster hot intra-cluster medium. Being a detection in absorption,

338

•

Highlights in modern observational cosmology
sensitivity does not depend on redshift. This will possibly be one of the most
powerful methods to find distant clusters in the years to come. At present,
serendipitous surveys with interferometric techniques (e.g. Carlstrom 1999)
cannot cover large areas (i.e. more than ∼1 deg2) and their sensitivity is
limited to the most x-ray luminous clusters.
Clustering of absorption line systems: this method has lead to a few
detections of ‘proto-clusters’ at z & 2 (e.g. Francis et al 1996). The most
serious limitation of this technique is that it is limited to explore small
volumes.

To date, the most common procedure used to estimate the cluster mass
function has been to exploit x-ray selected samples, for which the survey
volume can be computed. Follow-up observations are then used to estimate
the cluster mass of a statistical subsample. Most common mass estimators
are the temperature of the x-ray emitting gas (directly measured with x-ray
spectroscopy), and the galaxy velocity dispersion (virial analysis of galaxy
dynamics). We will see later that the x-ray luminosity is also a valid estimator.
Gravitational lensing (either in the strong or weak regime) is also a powerful
tool to estimate the cluster mass; however, this method is difficult to apply to
distant clusters and has some inherent limitations (e.g. mass-sheet degeneracy).
For a review of gravitational lensing methods of mass reconstruction, the reader
is referred to the chapter by Philippe Jetzer in this volume.
A robust method to quantify the volume density of clusters at different
redshifts is to use the x-ray luminosity function (XLF), i.e. the number of clusters
per unit volume and per unit x-ray luminosity. By comparing the XLF of an xray flux-limited samples of clusters at different redshifts, one can characterize the
evolution in luminosity and/or number density. This tool is the exact counterpart
of the optical LF used in galaxy surveys (section 11.3.3). Perhaps surprisingly,
this standard method applied to cluster surveys has several advantages over galaxy
surveys. First, the local XLF is very well determined and no ambiguity exists
as from different ‘types’. Clusters are basically a single parameter family, the
gas temperature, which is also well correlated with the x-ray luminosity. For
this reason, K -corrections are also easy to handle as opposed to galaxies in the
optical–near-IR. The only point of major concern, as previously discussed, has to
do with biases due to surface brightness limits.
In figure 11.13 we show the best determination to date of the XLF from z  0
out to z  1.2, coming from different surveys (Rosati et al 1999 and references
therein). The most striking result is perhaps the lack of any significant evolution
out to z  1, for L X . L ∗X  5 × 1044 erg s−1 (i.e. approximately the Coma
cluster). This range of luminosities includes the bulk of the cluster population in
the universe. However, there is evidence of evolution of the space density of the
most luminous, presumably most massive clusters. Using the observed L X − T
relation for clusters and the virial theorem, which links the temperature to the
mass, one can show that the XLF can be used as a robust estimator of the cluster

Cluster surveys

339

Figure 11.13. The best determination to date of the cluster x-ray luminosity function
(i.e. the cluster space density) out to z  1.2.
Data points at z < 0.85
are derived from a complete RDCS sample of 103 clusters over 47 deg2 , with
FXlim = 3 × 10−14 erg s−1 cm−2 (Rosati et al 1999). The triangles represent a lower
limit (due to incomplete optical identification). to the cluster space density obtained from
a fainter and more distant subsample. Long dash curves are Schechter best fits to the XLF
φ(L X , z), plotted at z = 0.4 and z = 0.6.

mass function, i.e. N(L X , z) → N(T, z) → N(M, z) (e.g. Borgani et al 1999).
Such a method can be used to set significant constraints on m (figure 11.14). The
fact that a large fraction of relatively massive clusters is already in place at z  1,
indicates that the dynamical evolution of structure has proceeded at a relatively
slow pace since z  1, a scenario which fits naturally in a low density universe
(figure 11.14, see Borgani et al 2001, Eke et al 1996).
11.4.3 Determining m and 
Besides the method of the evolution of cluster abundance (which we can call
‘universal dynamics’), galaxy clusters, as the largest collapsed objects in the
universe, also offer two other independent means to estimate the mean density
of matter that participates to gravitational clustering (i.e. m ):

340

Highlights in modern observational cosmology

Figure 11.14. Constraints in the plane of the cosmological parameters m − σ8 derived
from the observed evolution of the cluster abundance in the RDCS sample (Borgani et
al 2001). Contours are 1σ , 2σ and 3σ C.L. The three parameters (A, α, β) describe
the uncertainties in converting cluster masses into temperatures (T ∼ M 2/3 /β), and
temperatures into x-ray luminosities (L X ∼ T α (1 + z) A ). The two values for each
parameter bracket the range which is allowed from current x-ray observations of distant
clusters.

(1) b − f gas method,
(2) Oort method (M/L) and
(3) universal dynamics.
11.4.3.1 b − f gas method (White et al 1993)
A reasonable assumption is that clusters are large enough that they should host a
‘fair sample’ of the matter in the universe (e.g. there is no special segregation of
baryons over the dark matter). In addition, x-ray observations clearly show that
most of the baryons in clusters reside in the hot intracluster gas. The gas-to-totalmass ratio, f gas , can be measured using x-ray or SZ observations. The fraction of
baryons, b = ρB /ρcr , is well constrained by the primordial nucleosynthesis
theory and the measurement of deuterium abundance from high-z absorption
systems. If we know f gas and b , then we simply have: m = b / f gas .
Deuterium measurements in recent years have settled on the value (Burles
and Tytler 1998) b h 2 = 0.02 ± 0.002. Ettori and Fabian (1999) have used 36
x-ray clusters to estimate a mean value  fgas  = 0.059h −3/2 with a 90% range of
f gas = (0.036–0.087)h −3/2 . Hence,
m = B / f gas  0.34h −1/2  0.4 ± 0.2

(for H0 = 65),

(11.27)

where the error represents an approximate range reflecting the scatter in f gas .
11.4.3.2 Oort method (M/L)
The mean density of the universe is equal to the mass of a large galaxy cluster
divided by the equivalent comoving volume in the field from which that mass

Cluster surveys

341

Figure 11.15. Constraints to m and  from CMB anisotropies (Boomerang: De
Bernardis et al 2000; Maxima: Hanany et al 2000), distant Type Ia supernovae (Perlmutter
et al 1999; Schmidt et al 1998) and several methods based on galaxy clusters.

originated. Such a volume can be evaluated from the ratio of the luminosity of
the cluster galaxies, L, with the field luminosity density, jf . Thus,
ρ0 = Mcl /Vcl = (M/L)cl × jf ,

and

m = (M/L)cl /(M/L)cr (11.28)

where (M/L)cr = ρcr /jf.
Important effects which could bias this measurement are luminosity
segregation of the cluster versus the field, and differential evolution of the cluster
galaxies compared to the field. With enough spectrophotometric data, one can
reasonably control these issues. The CNOC survey (e.g. Carlberg et al 1996)
is the best study to date of cluster dynamics of an x-ray selected sample of 16
clusters at z . 0.5. This study lead to a measurement of average mass-to-light
ratio (M/L) = 295 ± 54 h M L −1 , as well as of the luminosity density jf in the
field. Thus, Carlberg et al obtain: m = 0.24 ± 0.05 ± 0.09 (the second error
is the sytematic one).
Using the constraint on m derived in the previous section from the
application of the third method (universal dynamics), we note a remarkable
agreement from completeley independent techniques based on galaxy clusters,
i.e. m  0.2–0.5.
These bounds on the matter density parameter are shown in figure 11.15
together with measurements of (m ,  ) from high redshift supernovae used
as standard candles (Perlmutter et al 1999, Schmidt et al 1998), and from
the recent landmark experiments—Boomerang (De Bernardis et al 2000) and
Maxima (Hanany et al 2000) which have measured CMB anisotropies on small

342

Highlights in modern observational cosmology

scales (see the chapter by Arthur Kosowsky in this volume). The power of these
three independent means of measuring (m ,  ) is that they have degeneracies
which lie almost orthogonally to each other. The directions of degeneracy in the
(m ,  ) plane can be written as
SN: 43 m −   constant

CMB: m +   constant

clusters: m  constant.
These three measurements of the cosmological parameters are well in
agreement with each other and define a relatively small allowed region, a
circumstance which is sometimes referred to as ‘cosmic concordance’ (Bahcall
et al 1999). This explains why by ‘standard cosmology’ these days one adopts
the values (m ,  ) = (0.3, 0.7). Interestingly, the age of the universe for this
model is TU = 0.965H0−1.

References
Bahcall N, Ostriker J P, Perlmutter S and Steinhardt P J 1999 Science 284 1481
Benitez M 2000 Astrophys. J. 536 571
Borgani S, Rosati P, Tozzi P and Norman C 1999 Astrophys. J. 517 40
Borgani S et al 2001 Astrophys. J. 559 L71
Bruzual A G and Charlot S 1993 Astrophys. J. 405 538
Burles S and Tytler D 1998 Astrophys. J. 507 732
Calzetti D, Kinney A L and Storchi-Bregmann T 1994 Astrophys. J. 429 582
Carlberg et al 1996 Astrophys. J. 462 32
Carlstrom J S 1999 Phys. Scr. ed L Bergstrom, P Carlson and C Fransson
Colless M M 1999 Proc. ‘Looking Deep in the Southern Sky’ (ESO Astrophysics Symposia)
ed R Morganti and W J Couch (Berlin: Springer) p 9
Cowie L L, Sonfalia A, Hu E M and Cohen J D 1996 Astron. J. 112 839
Dickinson M 1998 Proc. STScI May 1997 Symposium ‘The Hubble Deep Field’ ed M Livio,
S M Fall and P Madau Preprint astro-ph/9802064
de Bernardis P et al 2000 Nature 404 955
Driver S P, Fernandez-Soto A, Couch W J, Odewahn S C, Windhorst R A, Phillipps S,
Lanzetta K and Yahil A 1998 Astrophys. J. 496 L93
Eke R et al 1996 Mon. Not. R. Astron. Soc. 282 263
Ellis R G, Colless M, Broadhurst T J, Heyl J S and Glazebrook K 1996 Mon. Not. R.
Astron. Soc. 280 235
Ellis R G 1997 Annu. Rev. Astron. Astrophys. 35 389
Ettori S and Fabian A C 1999 Mon. Not. R. Astron. Soc. 305 834
Ferguson H C, Dickinson M and Williams R 2000 Annu. Rev. Astron. Astrophys. 38 667
Giallongo E, D’Odorico S, Fontana A, Cristiani S, Egami E, Hu E and McMahon R G
1998 Astron. J. 115 2169
Giavalisco M, Steidel C C, Adelberger K L, Dickinson M, Pettini M and Kellogg M 1998
Astrophys. J. 503 543
Hanany S et al 2000 Astrophys. J. 545 L5
Hu E M, McMahon R G and Cowie L L 1999 Astrophys. J. 522 L9

References

343

Hubble E 1926 Astrophys. J. 64 321
Huchra J P, Geller M J, de Lapparant V and Corwin H G 1990 Astrophys. J. Suppl. 42 433
Jenkins et al 1998 Astrophys. J. 499 20
Kaiser N 1994 Astrophys. J. 284 L9
Kellermann K I 1993 Nature 361 134
Landau L D and Lifshitz E M 1971 The Classical Theory of Fields (Oxford: Pergamon)
Longair M S 1998 Galaxy Formation (Berlin: Springer)
Lilly S J, Tresse L, Hammer F, Crampton D and LeFevre O 1995 Astrophys. J. 455 108
Loveday J, Peterson B A, Efstathiou G and Maddox S J 1992 Astrophys. J. 390 338
Madau P, Pozzetti L and Dickinson M 1998 Astrophys. J. 498 106
Maddox S J, Efstathiou G, Sutherland W J and Loveday J 1990 Mon. Not. R. Astron. Soc.
247 1
Pentericci L et al 2000 Astron. Astrophys. 361 L25
Peacock J A 1999 Cosmological Physics (Cambridge: Cambridge University Press)
Peebles P J E Principles of Physical Cosmology (Princeton, NJ: Princeton University Press)
Perlmutter S et al 1999 Astrophys. J. 517 565
Press W H and Schechter P 1974 Astrophys. J. 187 425
Rosati P, Della Ceca R, Burg R, Norman C and Giacconi R 1998 Astrophys. J. 492 L21
Rosati et al 1999 Proc. ‘Large Scale Structure in the X-ray Universe’ ed M Plioniz and
I Georgantopoulos (Greece: Santorini) (astro-ph/0001119)
Sandage A 1995 The Deep Universe (Saas-Fee Advanced Course 23) (Berlin: Springer)
Schaerer D 1999 Proc. XIXth Moriond Astrophysics Meeting ‘Building the Galaxies: from
the Primordial Universe to the Present’ ed Hammer et al (Paris: Editions Frontières)
(astro-ph/9906014)
Schechter P 1976 Astrophys. J. 203 297
Shectman S A et al 1996 Astrophys. J. 470 172
Sheth R K and Tormen G 1999 Mon. Not. R. Astron. Soc. 308 119
Schmidt B P et al 1998 Astrophys. J. 507 46
Steidel C C, Giavalisco M, Dickinson M and Adelberger K L 1996 Astrophys. J. 462 L17
Steidel C C, Adelberger K L, Dickinson M, Giavalisco M, Pettini M and Kellogg M 1998
Astrophys. J. 492 428
Steidel C C, Adelberger K L, Giavalisco M, Dickinson M and Pettini M 1999 Astrophys. J.
519 1
Tyson J A 1988 Astron. J. 96 1
White S D M, Navarro J F, Evrard A E and Frenk C S 1993 Nature 366 429
Yee H K C et al 2000 Astrophys. J. Suppl. 129 475 (astro-ph/0004026)

Chapter 12
Clustering in the universe: from highly
nonlinear structures to homogeneity
Luigi Guzzo
Osservatorio Astronomico di Brera, Italy

12.1 Introduction
This chapter concentrates on a few specific topics concerning the distribution of
galaxies on scales from 0.1 to nearly 1000h −1 MPc. The main aim is to provide
the reader with the information and tools to familiarize him/her with a few basic
questions:
(1) What are the scaling laws followed by the clustering of luminous objects
over almost four decades of scales?
(2) How do galaxy motions distort the observed maps in redshift space, and how
we can correct and use them to our benefit?
(3) Is the observed clustering of galaxies suggestive of a fractal universe? and
consequently,
(4) Is our faith in the cosmological principle still well placed? i.e. do we see
evidence for a homogeneous distribution of matter on the largest explorable
scales, in terms of the correlation function and power spectrum of the
distribution of luminous objects?
For some of these questions we have a well-defined answer, but for some others
the idea is to indicate the path along which there is still a good deal of exciting
work to be done.

12.2 The clustering of galaxies
I believe most of the students reading this book will be familiar with the beautiful
cone diagrams showing the distribution of galaxies in what have often been called
344

The clustering of galaxies

345

Figure 12.1. The distribution of the nearly 140 000 galaxies observed so far (September
2000) in the 2dF survey (from [3]): compare this picture to that in [2] to see how rapidly
this survey is progressing towards its goal of 250 000 redshifts measured (note that this is
a projection over a variable depth in declination, due to the survey being still incomplete).

slices of the universe. This has been made possible by the tremendous progress in
the efficiency of redshift surveys, i.e. observational campaigns aimed at measuring
the distance of large samples of galaxies through the cosmological redshift
observed in their spectra. This is one of the very simple, yet fundamental pillars
of observational cosmology: reconstructing the three-dimensional positions of
galaxies in space to be able to study and characterize statistically their distribution.
Figure 12.1 shows the current status of the ongoing 2dF survey and gives an
idea of the state of the art, with ∼130 000 redshifts measured and a planned final
number of 250 000 [1]. From this plot, the main features of the galaxy distribution
can be appreciated. One can easily recognize clusters, superclusters and voids,
and get the feeling of how the galaxy distribution is extremely inhomogeneous to
at least 50h −1 MPc (see [2] for a more comprehensive review).
The inhomogeneity we clearly see in the galaxy distribution can be quantified
at the simplest level by asking what is the excess probability over random to find
a galaxy at a separation r from another one. This is one way by which one can
define the two-point correlation function, certainly the most perused statistical
estimator in cosmology (see [5] for a more detailed introduction). When we have
a catalogue with only galaxy positions on the sky (and usually their magnitudes),
however, the first quantity we can compute is the angular correlation function

346

Clustering in the universe

Figure 12.2. The two-point correlation function of galaxies, as measured from a few
representative optically-selected surveys (from [2]). The plot shows results from the ESP
[9], LCRS [10], APM-Stromlo, [11] and Durham–UKST [12] surveys, plus the real space
ξ(r ) de-projected from the angular correlation function w(θ) of the APM survey [13].

w(θ ). This is a projection of the spatial correlation function ξ(r ) along the
redshift path covered by the sample. The relation between the angular and spatial
functions is expressed for small angles by the Limber equation (see [4] and [5]
for definitions and details)
 ∞
 ∞


(12.1)
dv v 4 φ 2 (v)
du ξ
u 2 + v2 θ 2
w(θ ) =
0

−∞

where φ(v) is the radial selection function of the two-dimensional catalogue,
that in this version gives the comoving density of objects at a given distance v
(which depends, for example, on the magnitude limit of the catalogue and the
specific luminosity function of the type of galaxies one is studying). For optically
selected galaxies [6, 7] w(θ ) is well described by a power-law shape ∝θ −0.8 ,
corresponding to a spatial correlation function (r/r0 )γ , with r0  5h −1 Mpc and
γ  −1.8, and a break with a rapid decline to zero around scales corresponding
to r ∼ 30h −1 Mpc.
The advantage of angular catalogues remains the large number of galaxies
they include, up to a few millions [6]. Since the beginning of the 1980s (e.g. [8]),
redshift surveys have allowed us to compute ξ(r ) directly in three-dimensional
space, and the most recent samples have pushed these estimates to separations of
∼100h −1 Mpc (e.g. [9]). Figure 12.2 shows the two-point correlation function
in redshift space,† indicated as ξ(s), for a representative set of published redshift
surveys [9–12]. In addition, the dotted lines show the real-space ξ(r ) obtained
† This means that distances are computed from the redshift in the galaxy spectrum, neglecting the
Doppler contribution by its peculiar velocity which adds to the Hubble flow (section 12.3).

Our distorted view of the galaxy distribution

347

through de-projection of the angular w(θ ) from the APM galaxy catalogue [13].
The two different lines correspond to two different assumptions about galaxy
clustering evolution, which has to be taken into account in the de-projection, given
the depth of the APM survey. This illustrates some of the uncertainties inherent in
the use of the angular function. As can be seen from figure 12.2, the shape of ξ(s)
below 5–10h −1 Mpc is reasonably well described by a power law, but for the four
redshift samples the slope is shallower than the canonical ∼ − 1.8 nicely followed
by the APM ξ(r ). This is due to the redshift-space smearing of structures that
suppresses the true clustering power on small scales, as we shall discuss in the
following section. Note how ξ(s) maintains a low-amplitude, positive value out
to separations of more than 50h −1 Mpc, showing explicitly why large-size galaxy
surveys are important: we need large volumes and good statistics to be able to
extract such a weak clustering signal from the noise. Finally, the careful reader
might have noticed a small but significant positive change in the slope of the APM
ξ(r ) (the only one for which we can see the undistorted real-space clustering at
small separations), around r ∼ 3–4h −1 Mpc. On scales larger than this, all data
show a ‘shoulder’ before breaking down. This inflection point appears around
the scales where ξ ∼ 1, thus suggesting a relationship with the transition from the
linear regime (where each mode of the power spectrum grows by the same amount
and the shape is preserved), to fully nonlinear clustering on smaller scales [14].
We shall come back to this in section 12.4.

12.3 Our distorted view of the galaxy distribution
We have just seen an explicit example of how unveiling the true scaling laws
describing galaxy clustering from redshift surveys is complicated by the effects
of galaxy-peculiar velocities. Separations between galaxies—indicated as s to
emphasize this very point—are not measured in real 3D space, but in redshift
space: what we actually measure when we take the redshift of a galaxy is the
quantity cz = cz true +vpec// , where vpec// is the component of the galaxy-peculiar
velocity along the line of sight. This quantity, while being typically ∼100 km s−1
for ‘field’ galaxies, can rise above 1000 km s−1 in rich clusters of galaxies.
As explicitly visible in figure 12.2, the resulting ξ(s) is flatter than its realspace counterpart. This is the result of two concurrent effects: on small scales,
clustering is suppressed by high velocities in clusters of galaxies, that spread close
pairs along the line of sight producing in redshift maps what are sometimes called
‘fingers of God’. Many of these are recognizable in figure 12.1 as thin radial
structures, particularly in the denser part of the upper cone. The net effect on ξ(s)
is, in fact, to suppress its amplitude below ∼1–2h −1 Mpc. However, on larger
scales where motions are still coherent, streaming flows towards higher-density
structures enhance their apparent contrast when they appear to lie perpendicularly
to the line of sight. This, in contrast, amplifies ξ(s) above 10–20h −1 Mpc. Both
effects can be better appreciated with the help of a computer N-body simulation,

348

Clustering in the universe

for which we have the leisure to see both a real-and a redshift-space snapshot, as
in Figure 12.3.
How can we recover the correlation function of the undistorted spatial
pattern, i.e. ξ(r )? This can be accomplished by computing the two-dimensional
correlation function ξ(rp , π), where the radial separation s of a galaxy pair is split
into two components, π, parallel to the line of sight, and rp , perpendicular to it,
defined as follows [15]. If d1 and d2 are the distances to the two objects (properly
computed) and we define the line of sight vector l ≡ (d1 + d2 )/2 and the redshift
difference vector s ≡ d1 − d2, then one defines
π≡

s·l
|l|

rp2 ≡ s · s − π 2 .

(12.2)

The resulting correlation function is a bidimensional map, whose contours at
constant correlation look as in the example of figure 12.4. By projecting ξ(rp , π)
along the π direction, we obtain a function that is independent of the distortion,
 ∞
 ∞
dπ ξ(rp , π) = 2
dy ξR [(rp2 + y 2 )1/2]
(12.3)
wp (rp ) ≡ 2
0

0

and is directly related to the real-space correlation function (here indicated with
ξR (r ) for clarity), as shown. Modelling ξR (r ) as a power law, ξR (r ) = (r/r0 )−γ
we can carry out the integral analytically, yielding

wp (rp ) = rp

r0
rp

γ

( 12 )( γ −1
2 )
γ
( 2 )

(12.4)

where  is the gamma function. Such a form can then be fitted to the observed
wp (rp ) to recover the parameters describing ξ(r ) (e.g. [16]). Alternatively, one
can perform a formal Abel inversion of wp (rp ) [17].
So far, we have treated redshift-space distortions merely as an annoying
feature that prevents the true distribution of galaxies from being seen directly.
In fact, being a dynamical effect they carry precious direct information on the
distribution of mass, independently from the distribution of luminous matter.
This information can be extracted, in particular by measuring the value of the
pairwise velocity dispersion σ12 (r ). This, in practice, is a measure of the smallscale ‘temperature’ of the galaxy soup, i.e. the amount of kinetic energy produced
by the differences in the potential energy created by density fluctuations. Thus,
finally, a measure of the mass variance on small scales.
ξ(rp , π) can be modelled as the convolution of the real-space correlation
function with the distribution function of pairwise velocities along the line of
sight [8, 18], Let F(w, r ) be the distribution function of the vectorial velocity
differences w = u2 − u1 for pairs of galaxies separated by a distance r (so it
is a function of four variables, w1 , w2 , w3 , r ). Let w3 be the component of w
along the direction of the line of sight (that defined by l); we can then consider

Our distorted view of the galaxy distribution

349

Figure 12.3. Particle distribution from a one-degree thick mock survey through a large-size
Open-CDM N-body simulation in real (top) and redshift space (bottom). The appearance
of the two diagrams gives a clear visual impression of the effect of redshift-space
distortions (note that here, unlike in the real survey of figure 12.1, no apparent luminosity
selection is applied, i.e. the sample is volume limited).

350

Clustering in the universe

Figure 12.4. The typical appearance of the bidimensional correlation function ξ(rp , π),
in this specific case computed for the ESP survey [9]. Note the elongation of the contours
along the π direction for small values of rp , produced by high-velocity pairs in clusters.
The broken circles show contours of equal correlation in the absence of distortions.

the corresponding distribution function of w3 ,

f (w3 , r ) = dw1 dw2 F(w, r ).

(12.5)

It is this distribution function that is convolved with ξ(r ) to produce the observed
ξ(rp , π). If we now call y the component of the separation r along the line of
sight, with our convention we have that w3 = H0(π − y) and the convolution
1 + ξ(rp , π) = [1 + ξ(r )] ⊗ f (w3 , r ),

(12.6)

can be expressed as

1 + ξ(rp , π) = H0

+∞
−∞

1

dy {1 + ξ [(rp2 + y 2 ) 2 ]} f [H0(π − y)].

(12.7)

Note that this expression gives essentially a model description of the effect
produced by peculiar motions on the observed correlations, but does not take
into account the intimate relation between the mass density distribution and the
velocity field which is, in fact, a product of mass correlations (see [19] and [20]
and references therein). Within this model, therefore, we have no specific physical

Our distorted view of the galaxy distribution

351

reason for choosing one or another form for the distribution function f . Peebles
[21] first showed that an exponential distribution best fits the observed data, a
result subsequently confirmed by N-body models [22]. According to this choice,
f can then be parametrized as



√  w3 (r ) − w3 (r ) 
1

(12.8)
exp − 2 
f (w3 , r ) = √

σ12 (r )
2σ12 (r )
where w3 (r ) and σ12 (r ) are, respectively, the first and second moment of f .
The projected mean streaming w3 (r ) is usually explicitly expressed in terms
of v12 (r ), the first moment of the distribution F defined earlier, i.e. the mean
relative velocity of galaxy pairs with separation r , w3 (r ) = yv12 (r )/r . The
final expression for f becomes therefore



 π − y 1 + v12 (r) 

√

H0 r 
1

f (w3 , r ) = √
(12.9)
exp − 2H0 


σ12 (r )
2σ12 (r )


(see e.g. [18] and [16] for more details).
The practical estimate of σ12 (r ) is typically performed on the data by fitting
the model of equation (12.7) to a cut at fixed rp of the observed ξ(rp , π). To do
this, one has first to estimate ξ(r ) from the projected function wp (rp ) and choose a
model for the mean streaming v12 (r ), as e.g. that based on the similarity solution
of the BBGKY equations [8]:
v12 (r ) = −H0r

F
.
1 + (r/r0 )2

(12.10)

The traditional approach considers two extreme cases, corresponding to the
somewhat idealized situations of stable clustering (F = 1, a mean infall
streaming that compensates exactly the Hubble flow, such that clusters are stable
in physical coordinates) and free expansion with the Hubble flow (F = 0, no
mean peculiar streaming). It is instructive to see explicitly what happens to the
contours of ξ(rp , π) in these two limiting cases. In figure 12.5, I have used
equations (12.7), (12.9) and (12.10) to plot the model for ξ(rp , π), keeping σ12 (r )
fixed and varying the amplitude F of the mean streaming. Here the two competing
dynamical effects (small-scale stretching and large-scale compression) are clearly
evident. The observational results yield values of σ12 at small separations around
300–400 km s−1 , with a mild dependence on scale [16, 18, 23]. This value has
been shown to be rather sensitive to the survey volume, because of the strong
weight the technique puts on galaxy pairs in clusters [23], and the fluctuations
in the number of clusters due to their clustering. A different method has been
proposed more recently by Landy and collaborators [24] to alleviate this problem.
The method is very elegant, and reduces the weight of high-velocity pairs in
clusters by working in the Fourier domain where, in addition, the convolution
of the two functions becomes a simple product of their transforms. A direct

352

Clustering in the universe

Figure 12.5. The relative effect of the mean streaming v12 (r ) and pairwise velocity
dispersion σ12 (r ) on the shape of the contours of ξ(rp , π), seen through the model of
equation (12.7). While a high pairwise dispersion, σ12 = 700 km s−1 independent of scale
is assumed (a reasonable approximation), the two cases of zero mean streaming (F = 0)
and stable clustering (F = 1) are considered in the infall model of Davis and Peebles [8].
Here the effect of the coherent motions is more evident than in the data plot of figure 12.4:
the contours of ξ(rp , π) are clearly compressed along the π direction. This compression is
0.6 /b.
a measure of m

Is the universe fractal?

353

application to data and N-body simulations under particularly severe survey
conditions seems, however, to give results which are not significantly dissimilar
to the standard method [25].
Rather than assuming a model for the mean streaming v12 (r ), one could
measure it directly from the compression of the contours of ξ(rp , π), i.e. doing
a simultaneous fit to the first and second moment. This quantity also carries
important cosmological information, being directly proportional to the parameter
0.6 /b, where  is the matter density parameter and b is the bias parameter
β = m
m
of the class of galaxies one is using (see Peacock, this volume). This has been
done, e.g. on the IRAS 1.2 Jy survey [18], but the uncertainty on β is very large
due to the weak signal and the need to simultaneously fit both the first and second
moments. The situation in this respect will soon improve dramatically thanks to
the ongoing 2dF [1] and Sloan (SDSS) surveys [26], that will provide 250 000 and
1000 000 redshifts respectively.

12.4 Is the universe fractal?
The observation of a power-law shape for the two-point correlation function
together with the self-similar aspect of galaxy maps as that of figure 12.1,
suggested several years ago a possible description of the large-scale structure of
the universe in terms of fractal objects [27]. A fractal universe without a crossover to a homogeneous distribution would imply abandoning the cosmological
principle. Also, under such conditions most of our standard statistical descriptions
of large-scale structure would be inappropriate [28]: no mean density could be
defined and, as a consequence, the whole concept of density fluctuations (with
respect to a mean density) would make little sense.
It is therefore of significant interest: (1) to compare the scaling properties
of galaxy clustering to those expected for a fractal distribution (keeping in mind
that on different scales there are different effects at work, as we have seen in
the previous section); and (2) to put under serious scrutiny the observational
evidences for a convergence of statistical measures to a homogeneus distribution
within the boundaries of current samples. Attempts to address these questions
using redshift survey data during the last ten years or so have come to different
conclusions, mostly because of disagreement on which data can be used and how
they should be treated and analysed [29–31]. It is because of the relevance of
the issues raised that this subject has been the focus of an intense debate, as also
demonstrated by the discussions in this book (see also Montuori, this volume).
12.4.1 Scaling laws
Let us review the arguments for and against the fractal interpretation of the
clustering data, by first recalling the basic relations involved.
A fractal set is characterized by a specific scaling relation, essentially
describing the way the set fills the ambient space. This scaling law can be by itself

354

Clustering in the universe

taken as an heuristic definition of fractal (although it is not strictly equivalent to
the formal definition in terms of Hausdorff dimensions, see e.g. [32]): the number
of objects counted in spheres of radius r around a randomly chosen object in the
set must scale as
(12.11)
N(r ) ∝ r D
where D is the fractal dimension (or, more correctly, the fractal correlation
dimension). Analogously, the density within the same sphere will scale as
n(r ) ∝ r D−3 .

(12.12)

Similarly, the expectation value of the density measured within shells of width dr
at separation r from an object in the set, the conditional density (r ) [28], will
scale in the same way,
(12.13)
(r ) = A · r D−3
with A being constant for a given fractal set. (r ) can be directly connected to
the standard two-point correlation function ξ(r ): suppose for a moment that we
can define a mean density n for this sample (we shall see in a moment what this
implies), then it is easy to show that
1 + ξ(r ) =

(r )
∝ r D−3 .
n

(12.14)

Therefore, if galaxies are distributed as a fractal, a plot of 1 + ξ(r ) will have
a power-law shape, and in the strong clustering regime (where ξ(r )  1) this
will also be true for the correlation function itself. This demonstrates the classic
argument (see e.g. [5]), that a power-law galaxy correlation function as observed
ξ(r ) = (r/r0 )−γ , is consistent with a scale-free, fractal clustering with dimension
D = 3 −γ (although it does not necessarily imply it: fractals are not the only way
to produce power-law correlation functions, see [31]). Note, however, that when
ξ(r ) ∼ 1 or smaller, only a plot of (r ) or 1 + ξ(r ), and not ξ(r ), could properly
detect a fractal scaling, if present.
When this happens over a range of scales which is significant with respect
to the sample size, the mean density n becomes an ill-defined quantity which
depends on the sample size itself. Considering a spherical sample with radius Rs
and the case of a pure fractal for simplicity, the mean density is the integral of
equation (12.13)
3A
· RsD−3 ,
n =
(12.15)
D
and is therefore a function of the sample radius Rs . Under the same conditions,
the two-point correlation function becomes
D
(r )
−1=
·
ξ(r ) =
n
3



r
Rs

D−3
− 1,

(12.16)

Is the universe fractal?
with a correlation length


r0 =

6
D



1
D−3

· Rs ,

355

(12.17)

which also depends on the sample size. Therefore, if the galaxy distribution has a
fractal character, with a well-defined dimension D one should observe:
(1) that the number of objects within volumes of increasing radius N(R) grows
as R D ;
(2) that analogously, the function (r ) or, equivalently, 1 + ξ(r ), is a power law
with slope D − 3; and
(3) that the correlation length r0 is a linear function of the sample size.
If the fractal distribution extends only up to a certain scale, the transition to
homogeneity would show up first as a flattening of 1+ξ(r ) and (less rapidly, given
that they depend on an integral over r ) as a growth N(r ) ∝ r 3 and a convergency
of r0 to a stable value.
12.4.2 Observational evidences
Pietronero [28] originally made the very important point that the use of ξ(r ) was
not fully justified, given the size (with respect to the clustering scales involved)
of the samples available at the time, and the consequent uncertainty on the value
of the mean density. In reality, this warning was already clear in the original
prescription [5]: one should be confident to have a fair sample of the universe
before drawing far-reaching conclusions from the correlation function. As often
happens, due to the scarcity of data the recommendation was not followed too
strictly (see [31] for more discussion on this point).
Although the data available today have increased by an order of magnitude
at least, the debate on the scaling properties and homogeneity of the universe is
still lively. Given the subject of this book and the extensive use we have made so
far of correlation functions, I shall concentrate here on the evidence concerning
points 2 and 3 in the previous summary list. In figure 12.6, I have plotted the
function 1 + ξ(s) for the same surveys of figure 12.2. Taken at face value, the
figure shows that the redshift-survey data can be reasonably fitted by a single
power law only out to ∼5h −1 Mpc. However, as soon as we compare these to the
real space 1 + ξ(r ) from the APM survey, we realize that what we are seeing here
is dominated by the redshift-space distortions. In other words, a fractal dimension
on small scales can only be measured from angular or projected correlations, and
if the data are interpreted in this way, it is in fact close to D  1.2. Above
∼5h −1 Mpc, a second range follows where D varies between two and three, when
moving out to scales approaching 100h −1 Mpc. The range between 5h −1 and
∼30h −1 Mpc can, in principle, be described fairly well by a fractal dimension
D  2, as originally found in [14], a dimension that could perhaps be topological
rather than fractal, reflecting a possible sheet-like organization of structures in

356

Clustering in the universe

Figure 12.6. The function 1 + ξ(s) for the same surveys of figure 12.2. A stable power-law
scaling would indicate a fractal range. It is clear how peculiar motions that affect all
data plotted but the APM ξ(r ) which is computed in projection do significantly distort the
overall shape. What would seem to be an almost regular scaling range with D ∼ 2 from
0.3 to 30h −1 Mpc, hides in reality a more complex structure, with a clear inflection around
3h −1 Mpc, which is revealed only when redshift-space effects are eliminated.

this range [33]. Above 100h −1 Mpc the function 1 + ξ(r ) seems to be fairly
flat, indicating a possible convergence to homogeneity. However, once this is
established, this kind of plot does not allow one to deduce evidence of clustering
signals of the order of 1%, which can only be seen when the contrast with respect
to the mean is plotted, i.e. ξ(s). For a similar analysis and more details, see the
pedagogical paper by Martı̀nez [34].
Another way of reading the same statistics and on which I would like to
give an update with respect to [31] is the scaling of the correlation length r0
with the sample size. It is known that for samples which are too small there is
indeed a growth of r0 with the sample size (see e.g. early results in [35]). This is
naturally expected: galaxies are indeed clustered with a power-law correlation
function, and inevitably samples which are too small will tend statistically to
overestimate the mean density, when measuring it in a local volume. When
we consider modern samples, however, and we pay attention not to compare
apples with pears (galaxies with different morphology and/or different luminosity
have different correlation properties, [31]), then the situation is more reassuring:
table 12.1 represents an update of that presented in [31], and reports the general
properties of the four redshift surveys I have used so far as examples. As the

Is the universe fractal?

357

Table 12.1. The behaviour of the correlation length r0 for the surveys discussed in previous
figures, compared to predictions of a D = 2 model. All estimates of r0 are in real space.
d is the effective depth of the surveys, while the ‘sample radius’ Rs has been computed as
in [31]. All measures of distance are expressed in h −1 Mpc.
Survey

d

Rs

r0 (predicted)

r0 (observed)

ESP
Durham/UKST
LCRS
Stromlo/APM

∼600
∼200
∼400
∼200

5
30
32
83

1.7
10
11
28

4.50+0.22
−0.25
4.6 ± 0.2
5.0 ± 0.1
5.1 ± 0.2

survey volumes are not spherical, here the ‘sample radius’ is defined as that of the
maximum sphere contained within the survey boundaries (see [31]). All these are
estimates of r0 in real space. The observed correlation lengths are significantly
different from the values predicted by the simple D = 2 fractal model. The result
would be even worse using D = 1.2. The bare evidence from table 12.1 is that
the measured values of r0 are remarkably stable, despite significant changes in the
survey volumes and shapes.
The counter-arguments in favour of a fractal interpretation of the available
data are instead summarized in the chapter by M Montuori. As the readers
can check, the main points of disagreement are related to (a) the use of some
samples whose incompleteness is very difficult to assess (as e.g. heterogeneous
compilations of data from the literature); and (b) the estimators used for
computing the correlation function and the way they take the survey shapes into
account. Also on these issues, the 2dF and SDSS surveys will provide data-sets
to fully clarify the scene. In fact, preliminary estimates of the correlation function
from the 2dF survey provide a result in good agreement with the analyses shown
here [1].
12.4.3 Scaling in Fourier space
It is of interest to spend a few words on the complementary, very important view
of clustering in Fourier space. The Fourier transform of the correlation function
is the power spectrum P(k):
 ∞
sin(kr ) 2
r dr,
ξ(r )
(12.18)
P(k) = 4π
kr
0
which describes the distribution of power among different wavevectors or modes
k = 2π/λ once we decompose the fluctuation field δ = δρ/ρ over the Fourier
basis [4]. The amount of information contained in P(k) is thus formally the same
as that yielded by the correlation function, although their estimates are affected

358

Clustering in the universe

Figure 12.7. The power spectrum of galaxy clustering estimated from the same surveys
as in figure 12.2 (also from [2], power spectrum estimates from [36–39]). Also in
Fourier space the differences between real- and redshift-space clustering are evident above
k  0.2h Mpc−1 .

differently by the uncertainties in the data (e.g. [4, 36]). One practical benefit of
the description of clustering in Fourier space through P(k) is that for fluctuations
of very long spatial wavelength (λ > 100h −1 Mpc), where ξ(r ) is dangerously
close to zero and errors easily make the measured values fluctuate around it (see
figure 12.2), P(k) is, in contrast, very large. Around these scales, most models
predict a maximum for the power spectrum, the fingerprint of the size of the
horizon at the epoch of matter–radiation equivalence. More technical details on
power spectra can be found in the chapter by J Peacock in this book.
In figure 12.7, I have plotted the estimates of P(k) for the same surveys of
figure 12.2. Here again the projected estimate from the APM survey allows us
to disentangle the distortions due to peculiar velocities, which have to be taken
properly into account in the comparisons to cosmological models. Here scales
are reversed with respect to ξ(r ), and the effect manifests itself in the different
slopes above ∼0.3h Mpc−1 : an increased slope in real space (broken line)
corresponds to a stronger damping by peculiar velocities, diluting the apparent
clustering observed in redshift space (all points). Below these strongly nonlinear
scales, there is good agreement between the slopes of the different samples (with
the exception of the LCRS, see [36] for discussion), with a well-defined k −2
power-law range between ∼0.08 and ∼0.3h Mpc−1 . The APM data show a
slope ∼k −1.2 , corresponding to the γ  −1.8 range of ξ(r ), while at smaller
ks (larger scales) they steepen to ∼k −2 , in agreement with the redshift-space
points. It is this change in slope that produces the shoulder observed in ξ(s) (cf.

Do we really see homogeneity?

359

section 12.2). Peacock [40] showed that such a spectrum is consistent with a steep
linear P(k) (∼k −2.2 ), the same value originally suggested to explain the shoulder
when first observed in earlier redshift surveys [14]. A dynamical interpretation
of this transition scale has been recently confirmed by a re-analysis of the APM
data [41].
At even smaller ks all spectra seem to show an indication for a turnover.
However, when errors are checked in detail, they are at most consistent with
a flattening, with the Durham–UKST survey providing possibly the cleanest
evidence for a maximum around k ∼ 0.03h Mpc−1 or smaller. A flattening or
a turnover to a positive slope would be an indication for a scale over which finally
the variance is close to or smaller than that of a random (Poisson) process. But
we learn by looking at older data that a turnover can also be an artifact produced
when wavelengths comparable to the size of the samples are considered, and here
we are close to that case.

12.5 Do we really see homogeneity?
Variance on ∼1000h−1 Mpc scales
Wu and collaborators [42] and Lahav [43] nicely reviewed the evidence for a
convergence to homogeneity on large scales using several observational tests. On
scales corresponding to spatial wavelengths λ ∼ 1000h −1 Mpc, the constraints on
the mean-square density fluctuations are provided essentially by the smoothness
in the x-ray and microwave backgrounds. Measuring directly the clustering of
luminous objects over such enormous volumes, is only now becoming feasible.
The 2dF survey will get close to these scales. The SDSS [26] will do even
better through a sub-sample of early type galaxies selected as to reach a redshift
z ∼ 0.5. If the goal of a redshift survey is mapping density fluctuations on the
larges possible scales a viable alternative to using single galaxies is represented
by clusters of galaxies. Here I would like to discuss the properties of the largest of
such surveys, that is in fact currently producing remarkable results on the amount
of inhomogeneity on scales nearing 1000h −1 Mpc.
12.5.1 The REFLEX cluster survey
With mean separations >10h −1 Mpc, clusters of galaxies are ideal objects
for sampling efficiently long-wavelength fluctuations over large volumes of the
universe. Furthermore, fluctuations in the cluster distribution are amplified with
respect to those in galaxies, i.e. they are biased tracers of large-scale structure:
rich clusters form at the peaks of the large-scale density field, and their variance is
amplified by a factor that depends on their mass, as was first shown by Kaiser [44].
X-ray selected clusters have a further major advantage over galaxies or other
luminous objects when used to trace and quantify clustering in the universe: their
x-ray emission, produced through thermal bremsstrahlung by the thin hot plasma

360

Clustering in the universe

permeating their potential well, is a good measure of their mass and this allows
us to directly compare observations to the predictions of cosmological models
(see [45] for a review and [46] for a direct application).
The REFLEX (ROSAT-ESO Flux Limited X-ray) cluster survey is the result
of the most intensive effort for a homogeneous identification of clusters of
galaxies in the ROSAT All Sky Survey (RASS). It combines a thorough analysis
of the x-ray data , and extensive optical follow-up with ESO telescopes, to
construct a complete flux-limited sample of about 700 clusters with measured
redshifts and x-ray luminosities [47, 48]. The survey covers most of the southern
celestial hemisphere (δ < 2.5◦ ), at galactic latitude |bII | > 20◦ to avoid high
absorption and stellar crowding. The present, fully identified version of the
REFLEX survey contains 452 clusters and is more than 90% complete to a
nominal flux limit of 3 × 10−12 erg s−1 cm−2 (in the ROSAT band, 0.1–2.4 keV).
Mean redshifts for virtually all these have been measured during a long observing
campaign with ESO telescopes. Details on the identification procedure and the
survey properties can be found in [49], while earlier results are reported in [50,51].
Figure 12.8 shows the spatial distribution of REFLEX clusters, giving
evidence for a number of superstructures with sizes ∼100h −1 Mpc. One of the
main motivations for this survey was to compute the power spectrum on extremely
large scales, benefiting from the efficiency of cluster samples to cover very large
volumes of the universe. Figure 12.9 shows the estimates of P(k) from three
subsamples of the survey (from [46]).
One of the strong advantages of working with x-ray selected clusters of
galaxies is that connection to model predictions is far less ambiguous than with
optically selected clusters (e.g. [45, 53]). We have therefore used the specific
REFLEX selection function (converted essentially to a selection in mass), to
determine that a low-M model (open or -dominated), best matches both the
shape and amplitude (i.e. bias value) of the observed power spectrum [46] (broken
curve in the figure). In fact, the samples shown here do not reach the maximum
spatial wavelengths we can possibly sample with the current data, as the Fourier
box could be made to be as large as 1000h −1 Mpc (the survey reaches z = 0.3
with the most luminous objects). In such a case, however, our control over
systematic effects becomes poorer, and work is currently undergoing to pin errors
down and understand how trustable are our results on ∼1 Gpc scale, where we
do see extra power coming up. At the very least, REFLEX is definitely showing
more clustering power on very large scales than any galaxy redshift survey to date.
Similar hints for large-scale inhomogeneities seem to be suggested by the most
recent analysis of Abell-ACO samples [54].
For k > 0.05h Mpc−1 , however, a comparison of REFLEX to galaxy power
spectra shows a rather similar shape. This is probably better appreciated by
looking at the two-point correlation function ξ(s) [52], compared in figure 12.10
to that of the ESP galaxy redshift survey. The agreement in shape between
galaxies and clusters is remarkable on all scales, with a break to zero around 60–
70h −1 Mpc for both classes of objects. This is, in general, expected in a simple

Do we really see homogeneity?

361

Figure 12.8. The spatial distribution of x-ray clusters in the REFLEX survey, out to
600h −1 Mpc. Note how, despite the coarser mapping of large-scale structure, filamentary
superclusters (‘chains’ of clusters) are clearly visible.

biasing scenario where clusters represent the high, rare peaks of the mass density
distribution. This result strongly corroborates the simpler, reassuring view that at
least above ∼5h −1 Mpc the galaxy and mass distributions are linked by a simple
constant bias.
12.5.2 ‘Peaks and valleys’ in the power spectrum
Most of the discussion so far has concentrated on the beauty of finding ‘smooth’
simple shapes for ξ(r ) or P(k), as symptoms of an underlying order of Nature.
Rather than being a demonstration of Nature’s inclination for elegance, however,
this smoothness and simplicity might simply indicate our ignorance and lack of
data. In fact, while smooth power spectra are predicted in models dominated by

362

Clustering in the universe

Figure 12.9. Estimates of the power spectrum of x-ray clusters from flux-limited
subsamples of the the REFLEX survey, framed within Fourier boxes of 300 (open squares),
400 (filled hexagons), and 500 (open hexagons)h −1 Mpc side, containing 133, 188 and
248 clusters, respectively. The two curves correspond to the best-fitting parameters using a
phenomenological shape with two power laws (full), or a CDM model, with M = 0.3
and  = 0.7 (broken) (from [46]).

non-interacting dark matter particles, as cold dark matter, a very different situation
is expected in cases where ordinary (baryonic) matter plays a more significant
role, with wiggles appearing in P(k) that would be difficult to detect with the size
and ‘Fourier resolution’ of our current data-sets.
The possibility that the power spectrum shows a sharp peak (or more peaks)
around its maximum has been suggested a few times during the last few years.
For example, Einasto and collaborators [55] found evidence for a sharp peak
around k  0.05h Mpc−1 in the power spectrum of an earlier sample of Abell
clusters, a feature later confirmed with lower significance by a more conservative
analysis of the same data [56]. The position of this feature is remarkably close
to the ∼130h −1 Mpc ‘periodicity’ revealed by Broadhurst and collaborators in
a ‘pencil-beam’ survey towards the galactic poles [57] and, more recently, in an
analysis of the redshift distribution of Lyman-break selected galaxies [58]. Other
evidence has been claimed from two-dimensional analyses of redshift ‘slices’ [59]
or QSO superstructures [60].
These observations have stimulated some interesting work on models with
high baryonic content. In this case, the power spectrum can exhibit a detectable
inprint from ‘acoustic’ oscillations within the last scattering surface at z ∼
1000, the same features observed in the Cosmic Microwave Background (CMB)

Conclusions

363

Figure 12.10. The two-point correlation function of the whole flux-limited REFLEX
cluster catalogue (filled circles, [52]), compared to that of ESP galaxies (open circles, [9]).
The broken curves show the Fourier transform of a phenomenological fit to P(k) which
tries to include the large-scale power seen from the largest subsamples (top line). The
bottom curve is that obtained after scaling down by an arbitrary bias factor (bc2 = (3.3)2 in
this specific case).

radiation [61]. While the most recent estimates of the REFLEX power spectrum
do not show clear features around the scales of interest to justify ‘extreme’ highbaryon models (contrary to early indications [62], which shows the importance of
the careful assessment of errors), the extra power below k ∼ 0.02 could still be an
indication of an higher-than-conventional baryon fraction [61,63], along the lines
that seem to be suggested by the Boomerang CMB results [64].

12.6 Conclusions
At the end of this chapter, a student is possibly more confused than he/she was
in the beginning, at least after a first read. I hope, however, that once the dust
settles, a few important points emerge. First, that the processes which shaped
the large-scale distribution of luminous objects we observe today are different
at different scales. At small scales, we observe essentially the outcome of fully
nonlinear gravitational evolution that re-shaped the linear power spectrum into
a collection of virialized or nearly so structures. Therefore, one cannot naively
take the redshift survey data and look for specific patterns or statistical properties
without taking into account galaxy peculiar motions. For this reason, one should
be careful in over-interpreting things like a single power-law scaling from scales
of a tenth of a megaparsecs to hundred megaparsecs, because, again, different
phenomena are being compared. However, one can use these distortions to really
‘see’ how the true mass distribution is, and I have spent a considerable part of

364

Clustering in the universe

this chapter describing some of the techniques in use. Moving to larger and
larger scales, we enter a regime where we are lucky enough that we can still
see something related to the original scaling law of fluctuations. This is what
was originally produced by some generator in the early universe (inflation?) and
processed through a matter (dark plus baryons) controlled amplifier. On even
larger scales, we hope we are finally entering a regime where the variance in the
mass is consistent with a homogeneous distribution, although we have seen that
even the largest galaxy and cluster samples are barely sufficient to see hints of that,
perhaps suggesting even more inhomogeneity than we expect. Does this mean that
we are living in a pure fractal universe? The scaling behaviour of galaxies and the
stability of the correlation length seem to imply that this cannot be the case. On
top of everything, the smoothness of the cosmic microwave background (treated
elsewhere in this book) is probably the most reassuring observation in this respect.
What we seem to understand is that our samples still have difficulty in sampling
the very largest fluctuations of the density field, properly on scales where this is
not fully Poissonian (or sub-Poissonian) yet.
Finally, I hope readers get the message that despite the tremendous progress
of the last 25 years which transformed cosmology into a real science, we still
have a number of fascinating questions to answer and still feel far away from
convincing ourselves that we have understood the universe.

Acknowledgments
Most of the results I have shown here rely upon the work of a number of
collaborators in different projects. I would like to thank in particular my
colleagues in the REFLEX collaboration, especially C Collins and P Schuecker
for the work on correlations and power spectra shown here. Thanks are due to F
Governato for providing me with the simulation used for producing Figure 12.3,
and to Alberto Fernandez-Soto and Davide Rizzo for a careful reading of the
manuscript. Finally, thanks are due to the organizers of the Como School, for
their patience in waiting for this chapter and for allowing me extra page space.

References
[1] Colless M 1999 Proc. II Coral Sea Workshop
http://www.mso.anu.edu.au/DunkIsland/Proceedings/
[2] Guzzo L 2000 Proc. XIX Texas Symposium on Relativistic Astrophysics (Nucl. Phys.
Proc. Suppl. 80) ed E Aubourg et al
[3] http://www.mso.anu.edu.au/2dFGRS/
[4] Peacock J A 1999 Cosmological Physics (Cambridge: Cambridge University Press)
[5] Peebles P J E 1980 The Large-Scale Structure of the Universe (Princeton, NJ:
Princeton University Press)
[6] Maddox S J, Efstathiou G, Sutherland W J and Loveday J 1990 Mon. Not. R. Astron.
Soc. 242 43p

References

365

[7] Heydon-Dumbleton N H, Collins C A and MacGillivray H T 1989 Mon. Not. R.
Astron. Soc. 238 379
[8] Davis M and Peebles P J E 1983 Astrophys. J. 267 465
[9] Guzzo L et al (ESP Team) 2000 Astron. Astrophys. 355 1
[10] Tucker D L et al 1997 Mon. Not. R. Astron. Soc. 285 L5
[11] Loveday J, Peterson B A, Efstathiou G and Maddox S J 1992b Astrophys. J. 390
338
[12] Ratcliffe A, Shanks T and Broadbent A et al 1996 Mon. Not. R. Astron. Soc. 281
L47
[13] Baugh C M 1996 Mon. Not. R. Astron. Soc. 280 267
[14] Guzzo L et al 1991 Astrophys. J. 382 L5
[15] Fisher K B et al 1994a Mon. Not. R. Astron. Soc. 266 50
[16] Guzzo L et al 1997 Astrophys. J. 489 37
[17] Ratcliffe A, Shanks T, Parker Q A and Fong D 1998 Mon. Not. R. Astron. Soc. 296
173
[18] Fisher K B et al 1994b 267 927
[19] Fisher K B 1995 Astrophys. J. 448 494
[20] Sheth R K, Hui L, Diaferio A and Scoccimarro R 2000 Mon. Not. R. Astron. Soc.
326 463
[21] Peebles P J E 1976 Astrophys. Space Sci. 45 3
[22] Zurek W, Quinn P J, Warren M S and Salmon J K 1994 Astrophys. J.431 559
[23] Marzke R O, Geller M J, da Costa L N and Huchra J P 1995 Astron. J. 110 477
[24] Landy S D, Szalay A S and Broadhurst T J 1998 Astrophys. J. 494 L133
[25] Quarello S and Guzzo L 2000 Clustering at High Redshift (ASP Conf. Series 200)
ed A Mazure, O Le Fèvre and V Le Brun (San Francisco, CA: ASP) p 446
[26] Margon B 1998 Phil. Trans. R. Soc. A (astro-ph/9805314)
[27] Mandelbrot B B 1982 The Fractal Geometry of Nature (San Francisco, CA:
Freeman)
[28] Pietronero L 1987 Physica A 144 257
[29] Davis L Critical Dialogues in Cosmology ed N Turok (Singapore: World Scientific)
p 13
[30] Critical Dialogues in Cosmology ed N Turok (Singapore: World Scientific) p 24
[31] Guzzo L 1997 New Astronomy 2 517
[32] Provenzale A 1991 Applying fractals in Astronomy ed A Heck and J Perdang
(Berlin: Springer)
[33] Provenzale A, Guzzo L and Murante G 1994 Mon. Not. R. Astron. Soc. 266 555
[34] Martı̀nez V 1999 Science 284 445
[35] Einasto J, Klypin A and Saar E 1986 Mon. Not. R. Astron. Soc. 219 457
[36] Carretti E et al 2001 Mon. Not. R. Astron. Soc. 324 1029
[37] Lin H et al 1996 Astrophys. J. 471 617
[38] Tadros H and Efstathiou G P 1996 Mon. Not. R. Astron. Soc. 282 138
[39] Hoyle F, Baugh C M, Ratcliffe A and Shanks T 1999 Mon. Not. R. Astron. Soc. 309
659
[40] Peacock J A 1997 Mon. Not. R. Astron. Soc. 284 885
[41] Gaztañaga E and Juszkiewicz R 2000 Mon. Not. R. Astron. Soc. submitted (astroph/0007087)
[42] Wu K K S, Lahav O and Rees M J 1999 Nature 225 230

366

Clustering in the universe

[43] Lahav O 2000 Proc. NATO-ASI Cambridge July 1999 ed R Critenden and N Turok
(Dordrecht: Kluwer) in press (astro-ph/0001061)
[44] Kaiser N 1984 Astrophys. J. 284 L9
[45] Borgani S and Guzzo L 2001 Nature 409 39
[46] Schuecker P et al (REFLEX Team) Astron. Astrophys. submitted
[47] Böhringer H et al (REFLEX Team) 1998 The Messenger 94 21 (astro-ph/9809382)
[48] Guzzo L et al (REFLEX Team) 1999 The Messenger 95 27
[49] Böhringer H et al (REFLEX Team) 2000 Astron. Astrophys. submitted
[50] De Grandi S et al (REFLEX Team) 1999 Astrophys. J. 513 L17
[51] De Grandi S et al (REFLEX Team) 1999b Astrophys. J. 514 148
[52] Collins C A et al (REFLEX Team) 2000 Mon. Not. R. Astron. Soc. 319 939
[53] Moscardini L, Matarrese S, Lucchin F and Rosati P 2000 Mon. Not. R. Astron. Soc.
316 283
[54] Miller C J and Batuski D J 2001 Astrophys. J. 551 635
[55] Einasto J et al 1997 Nature 385 139
[56] Retzlaff J et al 1998 New Astronomy 3 631
[57] Broadhurst T J, Ellis R S, Koo D C and Szalay A S 1990 Nature 343 726
[58] Broadhurst T J and Jaffe A H 1999 Astrophys. J. submitted (astro-ph/9904348)
[59] Landy D S et al 1996 Astrophys. J. 456 L1
[60] Roukema B F and Mamon G 2001 Astron. Astrophys. 366 1
[61] Eisenstein D J, Hu W, Silk J and Szalay A S 1998 Astrophys. J. 494 L1
[62] Guzzo L 1999 Proc. II Coral Sea Workshop
http://www.mso. anu.edu.au/DunkIsland/Proceedings/
[63] Guzzo L et al (REFLEX Team) 2001 in preparation
[64] De Bernardis P et al 2000 Nature 404 955

Chapter 13
The debate on galaxy space distribution: an
overview
Marco Montuori and Luciano Pietronero
Deptartment of Physics, University of Rome—‘La Sapienza’ and
INFM, Rome, Italy

13.1 Introduction
A critical assumption of the hot big bang model of the universe is that matter is
homogeneously distributed in space over a certain scale. It is usually assumed that
under this condition the Friedmann–Robertson–Walker (FRW) metric correctly
describes the dynamics of the universe. Investigating this assumption is then
of fundamental importance in cosmology and much current research is devoted
to this issue. In this chapter, we will review the current debate on the spatial
properties of galaxy distribution.

13.2 The standard approach of clustering correlation
The usual way to investigate the properties of the spatial distribution of glaxies
is to measure the two-point autocorrelation function ξ(r ) [1]. This is the spatial
average of the fluctuations in the galaxy number density at distance r , with respect
to a homogeneous distribution of the same number of galaxies. Let n(ri ) the
density of galaxies in a small volume δV at the position ri . The relative fluctuation
in δV is
n(ri ) − n
δn(ri )
=
(13.1)
n
n
where n = N/V is the density of the sample.
It is clear that the fluctuations are defined with respect to the density of
the sample n. The two-point correlation function ξ(r ) at scale r is the spatial
367

368

The debate on galaxy space distribution: an overview

average of the product of the relative fluctuations in two volumes centred on data
points at distance r :


δ(ri + r ) δ(ri )
n(ri )n(ri + r)i
ξ(r ) =
=
− 1,
(13.2)
n
n i
n2
where the index i means that the average is performed over the all the galaxies in
the samples. A set of points is correlated on scale r if ξ(r ) > 0; it is uncorrelated if
ξ(r ) = 0. In the latter case the points are evenly distributed at scale r or, in another
words, they have a homogeneous distribution at scale r . In the definition of ξ(r ),
the use of the sample density n as a reference value for the fluctuations of
galaxies is the conceptual assumption that the galaxy distribution is homogeneous
at the scale of the sample.
In such a framework, a relevant scale r0 for the correlation properties is
usually defined by the condition ξ(r0 ) = 1. The scale r0 is called the correlation
length of the distribution.

13.3 Criticisms of the standard approach
Let us summarize the conclusions of the previous section:
•
•

The ξ(r ) analysis assumes homogeneity at the sample size; and
a characteristic scale for the correlation is defined by the amplitude of ξ(r ),
i.e. the scale at which ξ(r ) is equal to one [1].

These two points raise two main criticisms:
•

•

As the ξ(r ) analysis assumes homogeneity, it is not reliable for testing
homogeneity. In order to use ξ(r ) analysis, the density of galaxies in the
sample must be a good estimation of the density of the whole distribution of
the galaxies. This may either be true or not; in any case, it should be checked
before ξ(r ) analysis is applied [2].
The correlation length r0 does not concern the scale of fluctuations. In this
sense, it is not correct to refer to it as a measure of the characteristic size of
correlations and call it the correlation length. According to the definition of
ξ(r ), r0 simply separates a regime of large fluctuations δn/n  1 from a
regime of small fluctuations δn/n  1 [3, 4].

Again the argument is valid if the average density n of the sample is the average
density of the distribution or, in other words, if the distribution is homogeneous on
the sample size. In statistical mechanics, the correlation length of the distribution
is defined by how fast the correlations vanish as a function of the scale, i.e. by the
functional form of ξ(r ) and not by its amplitude.
In this respect, the first step in a spatial correlation analysis of a data-set
should be a study of the density behaviour versus the scale. This should be done
without any a priori assumptions about the features of the underlying distribution
[2].

Mass–length relation and conditional density

369

13.4 Mass–length relation and conditional density
The mass–length relation links the average number of points at distance r from
any other point of the structure to the scale r . Starting from an i th point occupied
by an object of the distribution, we count how many objects N(< r )i (‘mass’) are
present within a volume of linear size r (‘length’) [5]. The average over all the
points of the structure is:
(13.3)
N(< r )i  = B · r D .
The exponent D is called the fractal dimension and characterizes in a quantitative
way how the system fills the space, while the prefactor B depends on the lower
cut-off point of the distribution.
The conditional density (r ) is the average number of points in a shell of
width dr at distance r from any point of the distribution.
According to equation (13.3), (r ) is:
(r ) =

1
dN(< r )i 
B D D−3
=
·r
2
4πr dr
dr
4π

(13.4)

(see [2, 6] for details of the derivation).

13.5 Homogeneous and fractal structure
If the distribution crosses over to a homogeneity distribution at scale r , (r )
shows a flattening toward a constant value at such a scale. In this case, the fractal
dimension in equations (13.3) and (13.4) has the same value as the dimension of
the embedding space d, D = d (in three-dimensional space D = 3) [2, 5, 6].
If this does not happen, the density of the sample will not correspond to the
density of the distribution and it will show correlations up to the sample size.
The simplest distribution with such properties is a fractal structure [5]. A fractal
consists of a system in which more and more structures appear at smaller and
smaller scales and the structures at small scales are similar to those at large scales.
The distribution is then self-similar. It has a value of D that is smaller than d,
D < d. In three-dimensional space d = 3, a fractal has D < 3 and (r ) is a
power law. The value of N(< r )i largely fluctuates by changing both the starting
i th point and the scale r . This is due to the scale-invariant feature of a fractal
structure, which does not have a characteristic length [5, 7].

13.6 ξ(r) for a fractal structure
Equation (13.4) shows that (r ) is a well-defined statistical tool for the generic
distribution of points, since it depends only on the intrinsic quantities (B and D).
The same is not true for ξ(r ) statistics.

370

The debate on galaxy space distribution: an overview

Assuming for simplicity a spherical sample volume with radius Rs (V (Rs ) =
(4/3)π Rs3 ), containing N(Rs ) galaxies. The average density of the sample will
be
3
N(Rs )
=
B Rs−(3−D).
n =
(13.5)
V (Rs )
4π
For a fractal, D < 3 and its average density is a decreasing function of the sample
size: n → 0 for Rs → ∞. Then the average density depends explicitly on the
sample size Rs and it is not a meaningful quantity.
From equation (13.2), the expression for ξ(r ) for a fractal distribution is [2]:
ξ(r ) = ((3 − γ )/3)(r/Rs )−γ − 1.

(13.6)

From equation (13.6) it follows that, for the fractal sample the so-called
correlation length r0 (defined as ξ(r0 ) = 1) is a linear function of the sample
size Rs :
r0 = ((3 − γ )/6)1/γ Rs .
(13.7)
It is then a quantity without any statistical significance, one simply related to the
sample size [2].
Neither is ξ(r ) a power law. For r ≤ r0 ,
((3 − γ )/3)(r/Rs )−γ  1

(13.8)

and ξ(r ) is well approximated by a power law [6].
For larger distances there is clear deviation from power-law behaviour due
to the definition of ξ(r ). This deviation, however, is just due to the size of the
observational sample and does not correspond to any real change in the correlation
properties. It is clear that if one estimates the exponent of ξ(r ) at distances
r ≈ r0 , one systematically obtains a higher value of the correlation exponent
due to the break of ξ(r ) in the log–log plot. Only if the sample set has a crossover
to homogeneity inside the sample side, is ξ(r ) correct. However, this information
is given only by the (r ) analysis which, for this reason, should always come
before the ξ(r ) investigation.

13.7 Galaxy surveys
Galaxy catalogues are angular catalogues (three-dimensional), which can be
computed in real or in redshift space. The latter defines the galaxy positions
by the redshift distance s, which is derived by the galaxy redshift z, according to
Hubble’s law. s is not the real distance, but contains an additional term called the
redshift distortion, which is small on scales s > 5h −1 Mpc [8].
We will report the statistical properties of redshift surveys, which contain the
large majority of avalaible three-dimensional data.

Galaxy surveys

371

13.7.1 Angular samples
ξ(r ) can be obtained from two-dimensional data, by means of the angular twopoint function w(θ ). ξ(r ) is reconstructed using the luminosity function, which
is derived assuming homogeneneity in the sample [1]. No independent check
is usually performed on this assumption. The procedure is currently considered
one of the best estimates of three-dimensional clustering properties of galaxies,
at least on a small scale (≤20h −1 Mpc) [9, 10]. Such a claim is considered to
be justified by the great quantity of available data in angular catalogues with
respect to three-dimensional surveys and by the absence of redshift distortions
in the two-dimensional data. The main conclusion obtained by this approach is
that the galaxy correlation (more precisely for optical selected galaxies) ξgg (r ) is
quite close to a power law in the range 10h −1 kpc–(10–20h −1) Mpc and more
precisely [9, 10]:
 r −1.77
0
ξgg (r ) =
(13.9)
r
with a correlation length r0 ≈ 4.5 ± 0.5h −1 Mpc.
This is considered to give the ‘canonical shape and parameter values’ of ξ(r )
and is a well-established result in cosmology [1, 10–14].
13.7.2 Redshift samples
13.7.2.1 ML samples
An ML sample is simply the whole redshift catalogue. By construction, any
ML sample is incomplete in the distribution of galaxies. At larger distances, it
contains fewer and fewer galaxies, as more and more galaxies fall beyond the
threshold of detectability. To account for such an effect, the galaxies in the sample
are weighted, according the luminosity function [1].
The value of s0 in different ML catalogues is found to span from 4.5–
8h −1 Mpc [10, 13].
ξ(s) does not appear to be a power law. According to Guzzo [15], the shape
of ξ(s) at very small scales (<3h −1 Mpc) is well fitted by a power law with
exponent γ = −1.
13.7.2.2 VL samples
It is possible to extract subsamples from the ML catalogues, which are unaffected
by the aforementioned incompleteness. Such samples are called VL samples [16].
The main result of ξ(s) analysis is that different VL samples have different values
for the correlation length s0 . The general trend is that deeper and brighter samples
show larger s0 (figure 13.1) [6, 15, 19–23]. Again, ξ(s) is not a power law in
the whole observed range of scale (≈1–50h −1 Mpc). This has been recognized
by several authors, who have performed the fit with the power law in a limited
range of scales. The value of the exponent γ (see equation (13.9)) is in the range

372

The debate on galaxy space distribution: an overview

Figure 13.1. ξ(r ) measure in various VL galaxy samples. The general trend is an increase
of the ξ(r ) amplitude for brighter and deeper samples. In the insert panel we show
the dependence of correlation length r0 on sample size Rs for all samples. The linear
behaviour is a consequence of the fractal nature of galaxy distribution in these samples.

1.17–2.1, which demonstrates the deviation of ξ(s) from the canonical power-law
shape [6, 15, 21, 22].

13.8 (r) analysis
According to our criticism of the standard analysis, we have measured the
galaxy conditional average density (r ) in all the three-dimensional catalogues

(r ) analysis

373

Figure 13.2. Full correlation for the same data of figure 13.1 in the range of distances
0.5–100h −1 Mpc. A reference line with a slope −1 is also shown (i.e. fractal dimension
D = 2.

avalaible. Our analysis was carried out for VL samples; the results are collected
in figure 13.2 [6].
We can derive the following conclusions:
•

(s) measured in different catalogues is a power law as a function of the
scale s, extending from approximately 1 to 40–50h −1 Mpc, without any
tendency towards homogenization (flattening) [6]. This implies that all the
optical catalogues show well-defined fractal correlations up to their limits,
with the fractal dimension D  2 [6].

374
•

•
•

The debate on galaxy space distribution: an overview
Only in a single case, the LEDA database [17, 18], is it possible to reach
larger scales of ∼100h −1 Mpc. This data sample has been largely criticized
but, to our knowledge, never in a quantitative way. The statistical tests we
performed show clearly that up to 50h −1 Mpc the results are completely
consistent with all other data [6]. This agreement also appears to extend to
the range 50–100h −1 Mpc, with the same overall statistical properties found
at smaller scales [6].
We do not detect any difference between the various optical catalogues, as
expected if they are simply different parts of the same distribution.
Such results imply that the ξ(s) analysis is inappropriate as it describes
correlations as deviations from an assumed underlaying homogeneity.
According to the (s) results, the value of s0 (derived from the ξ(s)
approach) has to scale with the sample size Rs . The behaviour observed
corresponds to a fractal structure with dimension D  2.

13.9 Interpretation of standard results
Here we attempt a comparison between the different interpretations.
In the standard interpretation, the rough constancy of s0 for the different
ML samples (s0  4.5–8h −1 Mpc) and within the angular data is considered
evidence for the validity of this approach. Moreover, since the samples have
different volumes, these results should discount a fractal interpretation, which
predicts an increase in s0 with sample volume [13, 22].
In contrast, in the fractal approach, in our opinion, the analysis of the angular
and ML samples is heavily biased by the use of the luminosity function and the
corresponding homogeneity assumption. To measure the correlation function
of such samples, one has to estimate the number of missing galaxies and their
positions in the space. This is done by assuming the existence of a homogeneity
scale. As an aside, we stress that the three-dimensional correlation in a fractal
structure cannot be reconstructed in such a way from its angular features [6].
Regarding the shape of the ξ(s), the difference from a power law is
attributed:
(1) in the standard model, to the presence of redshift distortions [15]; and
(2) in the fractal model, to the fact that ξ(s) is not in itself a power law [6].
If (s) is a power law, ξ(s) is expressed by equation (13.6). In particular,
it should be close to a power law only on very small scales and with an
exponent γ ∼ 1, as in the data reported by Guzzo [15].
With regards to the VL results, the increase in s0 could be due to either of the
following cases.
(1) Luminosity segregation (standard model). The increase in ξ(s) corresponds
to a real change in clustering properties for galaxy distribution, called
luminosity segregation. This is considered just one aspect of the general

Interpretation of standard results

375

Figure 13.3. Correlation length s0 versus sample luminosity Mlim , for several VL
samples. VL samples, with the same luminosity Mlim , have different volumes and very
different s0 . This is in contrast to luminosity segregation and in agreement with a fractal
distribution inside the sample Volume.

expected dependence of the clustering features on the internal properties of
galaxies, such morphology, colour, surface brightness and internal dynamics
[15, 19–23].
(2) In the fractal model, the increase in ξ(s) is just a geometrical effect and it
is not related to any variation of the clustering of the corresponding dataset, as shown in equation (13.7). The effect is simply a byproduct of the
inappropriate use of a statistical tool for the distribution under analysis [2,6].
The two interpretations seem to be equivalent; this is the reason why
the same data-set is considered to confirm both fractality (and no luminosity
segregation) and homogeneity (with luminosity segregation).
In our opinion there is a difference between the two interpretations: for the
fractal case we have a quantitative prediction of an increase in s0 within the sample
size, while the theoretical expectation for luminosity segregation does not have a
general consensus [24].
In principle it is possible to disentangle the two effects. A possible test is
presented in figure 13.3. Here, we have reported the value of s0 for the collection
of VL samples versus the luminosity Mlim of the corresponding samples. Samples
with the same Mlim have different volumes.
For each value of luminosity Mlim there is a range of values of s0 . These
appear in contradiction to the luminosity segregation effect, according to which
we should find only a single value for s0 for samples with the same luminosity

376

The debate on galaxy space distribution: an overview

Mlim . Experimental uncertainities in the determination of s0 do not explain such
a spread. Conversely, the behaviour seems to be in agreement with a fractal
distribution of galaxies within the sample size. The spread in the s0 values for
a single Mlim is due to the difference in the volume between different samples (in
agreement with fractal prediction).

Acknowledgments
We warmly thank F Sylos Labini with whom we carried out the large part of
this work. We are also grateful to A Gabrielli and M Joyce for many fruitful
discussions. We would like to thank the organizers of the Graduate School
in Contemporary Relativity and Gravitational Relativistic Cosmology for the
invitation to the School. This work has been supported by INFM Forum Project:
‘Clustering’.

References
[1] Peebles P J E 1993 Principles of Physical Cosmology (Princeton, NJ: Princeton
University Press)
[2] Coleman P H and Pietronero L 1992 Phys. Rep. 213 311
[3] Gaite J, Domnguez A and Pérez-Mercader J 1999 Astrophys. J. Lett. 522 L5
[4] Gabrielli A, Sylos Labini F and Durrer R 2000 Astrophys. J. Lett. 531 L1
[5] Mandelbrot B 1983 The Fractal Geometry of Nature (San Francisco, CA: Freeman)
[6] Sylos Labini F, Montuori M and Pietronero L 1998 Phys. Rep. 293 66
[7] Montuori et al 1997 Europhys. Lett. 39 103
[8] Hamilton A J S 1998 The Evolving Universe ed D Hamilton (Dordrecht: Kluwer
Academic) p 185
[9] Peebles J P E 1998 Les Rencontres de Physique de la Vallee d’Aosta ed M Greco
(Gif-sur-Yvette: Frontières)
[10] Strauss M A and Willick J A 1995 Phys. Rep. 261 271
[11] Kolb E W and Turner M S 1989 The Early Universe (Frontiers in Physics) (Boston,
MA: Addison-Wesley)
[12] Davis M 1997 Critical Dialogues in Cosmology ed N Turok (Singapore: World
Scientific) p 13
[13] Guzzo L 1997 New Astronomy 2 517
[14] Wu K K S, Lahav O and Rees M 1999 Nature 397 225
[15] Guzzo L 1998 Abstracts of the 19th Texas Symposium on Relativistic Astrophysics
and Cosmology ed J Paul and T Montmerle (Paris)
[16] Davis M and Peebles P J E 1983 Astrophys. J. 267 465
[17] Paturel G, Bottinelli L and Gouguenheim L 1994 Astron. Astrophys. 286 768
[18] Di Nella H et al 1996 Astron. Astrophys. Lett. 308 L33
[19] Davis M et al 1988 Astrophys. J. Lett. 333 L9
[20] Park C, Vogeley M S, Geller M and Huchra J 1994 Astrophys. J. 431 569

References
[21]
[22]
[23]
[24]

Benoist C et al 1999 Astrophys. J. 514 563
Willmer C N A, da Costa L N and Pellegrini P S 1998 Astrophys. J. 115 869
Cappi A et al 1998 Astron. Astrophys. 335 779
Colbert J M et al 2000 Mon. Not. R. Astron. Soc. 319 209

377

Chapter 14
Gravitational lensing
Philippe Jetzer
Laboratory for Astrophysics of PSI and Institute of Theoretical
Physics University of Zürich, Switzerland

14.1 Introduction
Gravitational lensing—i.e. light deflection by gravity—has become, in the last
few years, one of the most important fields in present-day astronomy. The
enormous activity in this area has mainly been driven by the considerable
improvements in observational capabilities. Due to the new wide-field cameras
and telescopes which are already in place or will become operational in the
near future the rate and quality of the lensing data will increase dramatically.
As gravitational lensing is independent of the nature and physical state of the
deflecting mass, it is perfectly suited to study dark matter at all scales.
Indeed, the determination of the amount and nature of the matter present in
the universe is an important problem for contemporary astrophysics and cosmology. This knowledge is directly related to the question of the fate of the universe: Will it expand forever or, after a phase of expansion, will it collapse again?
There are several astrophysical observations which indicate that most of the matter present in the universe is actually dark and, therefore, cannot be detected using
telescopes or radiotelescopes. The most recent studies seem to suggest that the total matter density is only about 30% of the ‘closure density’ of the universe—the
amount of mass that would make the universe balance between expanding forever
and collapsing. Measurements based on high-redshift supernovae suggest that
there is also a non-vanishing cosmological constant, such that the sum of matter
density and cosmological constant implies a flat universe [1].
Important evidence for the existence of large quantities of dark matter comes
from the measured rotation curves of several hundreds of spiral galaxies [2],
which imply the presence of a huge dark halo in which these galaxies are
embedded. Typically, a galaxy including its halo contains ∼10 times more dark
378

Introduction

379

than luminous matter, the latter being in the form of stars and gas. There are also
clear indications for the presence of important quantities of dark matter on larger
scales, in particular in clusters of galaxies. This was first pointed out in 1933 by
Zwicky [3]. Since then, much effort has been put into the search for dark matter,
the nature of which is still largely unknown.
The field of gravitational lensing is growing very rapidly and almost daily
there are new results. It will not therefore be possible to give here a complete and
exhaustive review of the field and of all the results achieved so far. The present
chapter is intended more as a way of rapidly acquiring the main ideas and tools
of lensing, which will then enable readers to approach the original literature. For
more details see the book by Schneider et al [4] as well as some reviews [5–7]
and the references therein.
Before starting the theory of lensing let us briefly give some historical
remarks on the development of the field.
14.1.1 Historical remarks
Nowadays we know that light propagation in a gravitational field has to be
described using the theory of general relativity formulated by Einstein in 1915.
However, long before then it was argued that gravity might influence the
behaviour of light (for a historical account, see, for instance, the book by
Schneider et al [4]). Indeed, Newton in the first edition of his book on optics
which appeared in 1704 discussed the possibility that celestial bodies could
deflect the light trajectory. In 1804 the astronomer Soldner published a paper in
which he computed the error induced by the light deflection on the determination
of the position of stars. To that purpose he used the Newtonian theory of gravity
assuming that the light is made of particles. He also estimated that a light ray
which just grazes the surface of the sun would be deflected by a value of only
0.85 arcseconds. Within general relativity this value is about twice as much, more
precisely 1.75 arcseconds. The first measurement of this effect has been made
during the solar eclipse of 29 May 1919 and confirmed the value predicted by
general relativity [8].
In 1936 Einstein published a short paper in Science in which he computed the
deflection of light coming from a distant star by the gravitational field of another
star [9]. He mentioned that if the source and the lens are perfectly aligned the
image would be a ring. If instead the alignment is not perfect one would see two
images with, however, a very small separation angle. Einstein also wrote: ‘Of
course, there is no hope of observing this phenomenon’. In fact, it has recently
been found that Einstein had already made most of the calculations presented
in that paper by 1912 as can be seen on some pages of his notebook [10]. The
recent developments in microlensing show that Einstein’s conclusion, although
understandable at that time, was too pessimistic. Indeed, the formulae developed
by Einstein in his 1936 paper are still the basis for the description of gravitational
lensing.

380

Gravitational lensing

Figure 14.1. Giant arc in Cl2244-02 (image from CFHT). The lensing cluster is at
z = 0.329 and the source of the arc is a very distant field galaxy at z = 2.238. (Courtesy
of G Soucail, Observatoire Midi-Pyrénées, ESO Messenger 69, September 1992.)

In the following year (1937) the swiss astronomer Zwicky wrote two short
articles in Physical Review suggesting that galaxies should be as sources and
lenses rather than stars as mentioned by Einstein [11]. He came to the conclusion
that such a configuration would have a much higher chance of being seen, since
the typical mass of a galaxy is several billion times higher than the mass of a single
star. He argued that such configurations must almost certainly be seen. Moreover,
he also gave a list of possible applications which included the possibility of
determining the total mass of galaxies, including their dark matter content better.
The first gravitational lens was discovered in 1979, when spectra of two
point-like quasars which lie only about 6 arcseconds away were obtained. The
spectra showed that both objects have the same redshift and are thus at the same
distance. Later on a galaxy acting as a lens was also found, making it clear that
the two objects are the images of the same quasar, which is lensed. Since then
many other examples have been found, and in 1986 the first lensing case with

Lens equation

381

a galaxy acting as a source was discovered. The galaxy then appears distorted
as one or more arcs. Many such systems have since then been discovered, with
some thanks to the Hubble space telescope. In 1979, Chang and Refsdal [12],
and in 1981, Gott [13] noted that even though a point mass in a halo of a distant
galaxy would create an unresolvable double image of a background quasar, the
time variation of the combined brightness of the two images could be observed.
In this way, the effect of non-luminous matter in the form of compact objects
could be observed. The term microlensing was proposed by Paczyński [14] to
describe gravitational lensing which can be detected by measuring the intensity
variation of a macro-image made up of any number of unresolved micro-images.
In 1993 the first galactic microlensing events were observed, in which the
source is a star in the Large Magellanic Cloud and the galactic bulge. In the
former case the lens is a compact object probably located in the galactic halo,
whereas in the latter case the lens is a low mass star in the galactic disk or in the
bulge itself.

14.2 Lens equation
14.2.1 Point-like lenses
The propagation of light in a curved spacetime is, in general, a complicated
problem. However, for almost all relevant applications of gravitational lensing
one can assume that the geometry of the universe is described in good
approximation by the Friedmann–Lemaître–Robertson–Walker metric. The
inhomogeneities in the metric can be considered as local perturbations. Thus
the trajectory of the light coming from a distant source can be divided into three
distinct parts. In the first, the light coming from a distant source propagates
in a flat unperturbed spacetime, near the lens the trajectory is modified due
to the gravitational potential of the lens and, afterwards, in the third part the
light again travels in an unperturbed spacetime until it reaches to the observer.
The region around the lens can be described by a flat Minkowskian spacetime
with small perturbations induced by the gravitational potential of the lens. This
approximation is valid as long as the Newtonian potential  is small, which means
||  c2 (c being the velocity of light), and if the peculiar velocity v of the lens
is negligible compared to c. These conditions are almost always fulfilled in all
cases of interests for the astrophysical applications. An exception, for instance, is
when the light rays get close to a black hole. We will not discuss such cases in
the following.
With these simplifying assumptions we can describe the light propagation
nearby the lens in a flat spacetime with a perturbation due to the gravitational
potential of the lens described in a first-order post-Newtonian approximation. The
effect of the spacetime curvature on the light trajectory can be described as an

382

Gravitational lensing

effective refraction index, given by
2
2
 = 1 + 2 ||.
(14.1)
2
c
c
The Newtonian potential is negative and vanishes asymptotically. As in
geometrical optics a refraction index n > 1 means that the light travels with a
speed which is lower compared with its speed in the vacuum. Thus the effective
speed of light in a gravitational field is given by
n =1−

2
c
 c − ||.
(14.2)
n
c
Since the effective speed of light is less in a gravitational field, the travel time
becomes longer compared to the propagation in empty space. The total time delay
t is obtained by integrating along the light trajectory from the source until the
observer, as follows
 observer
2
t =
|| dl.
(14.3)
3
c
source
This is also called the Shapiro delay.
The deflection angle for the light rays which pass through a gravitational
field is given by the integration of the gradient component of n perpendicular to
the trajectory itself:


2
(14.4)
α = − ∇⊥ n dl = 2 ∇⊥  dl.
c
v=

For all astrophysical applications of interest the deflection angle is always
extremely small, so that the computation can be substantially simplified by
integrating ∇⊥ n along an unperturbed path, rather than the effective perturbed
path. The so induced error is of higher order and thus negligible.
As an example let us consider the deflection angle of a point-like lens of
mass M. Its Newtonian potential is given by
(b, z) = −

GM
,
(b2 + z 2 )1/2

(14.5)

where b is the impact parameter of the unperturbed light ray and z denotes
the position along the unperturbed path as measured from the point of minimal
distance from the lens. This way we obtain
∇⊥ (b, z) =

(b2

GMb
,
+ z 2 )3/2

(14.6)

where b is orthogonal to the unperturbed light trajectory and is directed towards
the point-like lens. Inserting equation (14.6) in equation (14.4) we find, for the
the deflection angle,

2
4G M b
.
(14.7)
α = 2 ∇⊥  dz = 2
c
c b b

Lens equation

383

The Schwarzschild radius for a body of mass M is given by
RS =

2G M
,
c2

(14.8)

thus the absolute value of the deflection angle can also be written as α = 2RS /b.
For the Sun the Schwarzschild radius is 2.95 km, whereas its physical radius
is 6.96 × 105 km. Therefore, a light ray which just grazes the solar surface is
deflected by an angle corresponding to 1.7 .
14.2.2 Thin lens approximation
From these considerations one sees that the main contribution to the light
deflection comes from the region z ∼ ±b around the lens. Typically, z is
much smaller than the distance between the observer and the lens and the lens
and the source, respectively. The lens can thus be assumed to be thin compared
to the full length of the light trajectory. Thus one considers the mass of the lens,
for instance a galaxy cluster, projected onto a plane perpendicular to the line of
sight (between the observer and the lens) and going through the centre of the lens.
This plane is usually referred to as the lens plane and, similarly, one can define
the source plane. The projection of the lens mass on the lens plane is obtained by
integrating the mass density ρ along the direction perpendicular to the lens plane:

&(ξ ) = ρ(ξ , z) dz,
(14.9)
where ξ is a two-dimensional vector in the lens plane and z is the distance from
the plane. The deflection angle at the point ξ is then given by summing over the
deflection due to all mass elements in the plane as follows.

4G
(ξ − ξ )&(ξ ) 2
α= 2
d ξ.
(14.10)
c
|ξ − ξ |2
In the general case the deflection angle is described by a two-dimensional vector.
However, in the special case that the lens has circular symmetry one can reduce
the problem to a one-dimensional situation. Then the deflection angle is a vector
directed towards the centre of the symmetry with absolute value given by
α=

4G M(ξ )
,
c2 ξ

(14.11)

where ξ is the distance from the centre of the lens and M(ξ ) is the total mass
inside a radius ξ from the centre, defined as


ξ

M(ξ ) = 2π
0

&(ξ )ξ dξ .

(14.12)

384

Gravitational lensing

Figure 14.2. Notation for the lens geometry.

14.2.3 Lens equation
The geometry for a typical gravitational lens is given in figure 14.2 A light
ray from a source S (in η) is deflected by the lens by an angle α (with impact
parameter |ξ |) and reaches the observer located in O.
The angle between the optical axis (arbitrarily defined) and the true source
position is given by β, whereas the angle between the optical axis and the image
position is θ . The distances between the observer and the lens, the lens and the
source, and the observer and the source are, respectively, Dd , Dds and Ds . From
figure 14.2 one can easily derive (assuming small angles) that θ Ds = β Ds +α Dds .
Thus the positions of the source and the image are related by the following
equation:
Dds
,
(14.13)
β = θ − α(θ )
Ds
which is called the lens equation. It is a nonlinear equation so that it is possible
to have several images θ corresponding to a single source position β.
The lens equation (14.13) can also be derived using the Fermat principle,
which is identical to the classical one in geometrical optics but with the refraction
index defined as in equation (14.1). The light trajectory is then given by the
variational principle

δ

n dl = 0.

(14.14)

It expresses the fact that the light trajectory will be such that the travelling time
will be extremal. Let us consider a light ray emitted from the source S at time

Lens equation

385

t = 0. It will then proceed straight until it reaches the lens, located at the point I,
and where it will be deflected and then proceed again straight to the observer in
O. We thus have


 
2
1
l
2φ
t=
(14.15)
1 − 2 dl = − 3 φ dl,
c
c c
c
where l is the distance SIO (Euclidean distance). The term containing φ has to be
integrated along the light trajectory. From figure 2.1 we see that
l=

2 +
(ξ − η)2 + Dds

 Dds + Dd +

ξ 2 + Dd2

1
1 2
(ξ − η)2 +
ξ ,
2Dds
2Dd

(14.16)

where η is a two-dimensional vector in the source plane If we take φ = −G M/|x|
(corresponding to a point-like lens of mass M) we get



 I
|ξ |
(η − ξ )2
2φ
2G M
ξ · (η − ξ )
ln
(14.17)
dl =
+
+O
3
2Dds
|ξ |Dds
Dds
c3
S c
O
and similarly for I 2φ/c3 dl.
Only the logarithmic term is relevant for lensing, since the other ones are of
higher order. Moreover, instead of a point-like lens we consider a surface mass
density &(ξ ) (as defined in equation (14.9)) and so we obtain, for the integral
containing the potential term (neglecting higher-order contributions)


|ξ − ξ |
4G
2
,
(14.18)
φ
dl
=
d2 ξ &(ξ ) ln
3
3
ξ0
c
c
where ξ0 is a characteristic length in the lens plane and the right-hand side term
is defined up to a constant.
The difference in the arrival time between the situation which takes into
account the light deflection due to the lens and without the lens, is obtained
by summing equation (14.16)–(14.18) and by subtracting the travel time without
deflection from S to O. This way one obtains
ct = φ̂(ξ , η) + constant,

(14.19)

where φ̂ is the Fermat potential defined as
φ̂(ξ , η) =
and

Dd Ds
2Dds

4G
ψ̂(ξ ) = 2
c





ξ
η
−
Dd
Ds

2



− ψ̂(ξ )

|ξ − ξ |
d ξ &(ξ ) ln
ξ0
2

(14.20)


(14.21)

386

Gravitational lensing

is the deflection potential, which does not depend on η. The Fermat principle can
thus be written as dt/dξ = 0, and inserting equation (14.19) one once again
obtains the lens equation
Ds
ξ − Dds α(ξ ),
Dd

η=

(14.22)

where α is defined in equation (14.10). (If we define β = η/Ds and θ = ξ /Dd
we obtain equation (14.13). One can also write equation (14.22) as follows.
ˆ , η) = 0,
∇ξ (ξ

(14.23)

which is an equivalent formulation of the Fermat principle.
The arrival time delay of light rays coming from two different images (due
to the same source in η) located in ξ (1) and ξ (2) is given by
ˆ (1) , η) − (ξ
ˆ (2), η).
c(t1 − t2 ) = (ξ

(14.24)

14.2.4 Remarks on the lens equation
It is often convenient to write (14.22) in a dimensionless form. Let ξ0 be a length
parameter in the lens plane (whose choice will depend on the specific problem)
and let η0 = (Ds /Dd )ξ0 be the corresponding length in the source plane. We set
x = ξ /ξ0 , y = η/η0 and
κ(x) =

&(ξ0 x)
,
&cr

α(x) =

Dd Dds
α̂(ξ0 x),
ξ0 Ds

(14.25)

where we have defined a critical surface mass density
Ds
c2
= 0.35 g cm−2
&cr =
4π G Dd Dds
with D ≡

Dd Dds
Ds



1 Gpc
D

(14.26)

(1 Gpc = 109 pc). Then equation (14.22) reads as follows
y = x − α(x),

with



1
α(x) =
π


R2

x−x
κ(x ) d2 x .
|x − x |2

(14.27)

(14.28)

In the following we will mainly use the previous notation rather than that in
equation (14.28).
An interesting case is a lens with a constant surface mass density &. With
equation (14.11) one then finds, for the deflection angle,
α(θ ) =

4π G&
4G
Dd θ,
&πξ 2 =
2
c ξ
c2

(14.29)

Lens equation

387

using ξ = Dd θ . In this case the lens equation (14.13) is linear, which means that
β is proportional to θ :
β =θ −β =θ −

4π G Dds Dd
&
&θ = θ −
θ.
2
c
Ds
&cr

(14.30)

From equation (14.30) we immediately see that for a lens with a critical surface
mass density we get for all values of θ : β = 0. Such a lens would perfectly focus,
with a well-defined focal length. Typical gravitational lenses behave, however,
quite differently. A lens which has & > &cr somewhere in it is defined as
supercritical, and has, in general, multiple images.
Defining k(θ ) := &(θ Dd )/&cr we can write the lens equation as
β = θ − α̃(θ ),
with
α̃(θ ) =

1
π


R2

d2 θ k(θ )

(14.31)
θ −θ
.
|θ − θ |2

(14.32)

Moreover,
α̃(θ ) = ∇θ +(θ )
where
+(θ ) =

1
π

(14.33)


R2

d2 θ k(θ ) ln |θ − θ |.

(14.34)

The Fermat potential is given by
(θ , β) = 12 (θ − β)2 − +(θ )

(14.35)

and we then obtain the lens equation from
∇θ (θ , β) = 0.

(14.36)

+ = 2k ≥ 0

(14.37)

Note that
(using  ln |θ | = 2πδ 2 (θ )), since k as a surface mass density is always positive
(or vanishes).
The flux of a source, located in β, in the solid angle d(β) is given by
S(β) = Iν d(β).

(14.38)

Iν is the intensity of the source in the frequency ν. S(β) is the flux one would see
if there were no lensing. However, the observed flux from the image located in θ
is
S(θ ) = Iν d(θ ).
(14.39)

388

Gravitational lensing

Iν does not change, since the total number of photons stays constant as well as
their frequency. The amplification factor µ is thus given by the ratio
µ=
with
A(θ ) =

dβ
dθ

d(θ )
1
=
,
d(β)
det A(θ )

Ai j =


dβi
= δi j − +,i j ,
dθ j

(14.40)

(14.41)

(where +,i j = ∂i ∂ j +) which is the Jacobi matrix of the corresponding lens
mapping given by equation (14.31). Notice that the amplification factor µ can be
positive or negative. The corresponding image will then have positive or negative
parity, respectively.
For some values of θ , det A(θ ) = 0 and thus µ → ∞. The points (or the
curve) θ in the lens plane for which det A(θ ) = 0 are defined as critical points
(or critical curve). At these points the geometrical optics approximation used so
far breaks down. The corresponding points (or curve) of the critical points in the
source plane are the so called caustics.
The matrix Ai j is often parametrized as follows.


−γ2
1 − k − γ1
(14.42)
Ai j =
−γ2
1 − k + γ1
with γ1 =
therefore
and γ =

1
2 (+,11

− +,22 ), γ2 = +,12 = +,21 and γ = (γ1 , γ2 ). We have

γ12 + γ22 ,

det Ai j = (1 − k)2 − γ 2

(14.43)

tr Ai j = 2(1 − k).

(14.44)

The eigenvalues of Ai j are a1,2 = 1 − k ± γ .
In the next paragraphs, we study how small circles in the source plane are
deformed. Consider a small circular source with radius R at y, bounded by a
curve described by


R cos t
c(t) = y +
(0 ≤ t ≤ 2π).
(14.45)
R sin t
The corresponding boundary curve of the image is


R cos t
.
d(t) = x + A−1
R sin t

(14.46)

Inserting the parametrization (14.42) one finds that the image is an ellipse centred
on x with semi-axes parallel to the main axes of A, with magnitudes
R
,
|1 − κ ± γ |

(14.47)

Lens equation
and the position angles ϕ± for the axes are

 2
γ1
γ1
tan ϕ± =
∓
+1
or
γ2
γ2

tan 2ϕ± = −

γ2
.
γ1

389

(14.48)

The ellipticity of the image is defined as follows.
 = 1 + i2 =

1 − r 2iϕ
e ,
1+r

r≡

b
,
a

(14.49)

where ϕ is the position angle of the ellipse and a and b are the major and minor
semi-axes, respectively. a and b are given by the inverse of the eigenvalues
of the matrix Ai j defined in equation (14.42), thus a = (1 − k − γ )−1 and
b = (1 − k + γ )−1 .  describes the orientation and the shape of the ellipse
and is thus observable. Let us denote g = || with


γ
γ
g=
g=
,
(14.50)
1−κ
1−κ
which is called the reduced shear. One often uses a complex notation with
γ = γ1 + iγ2 and then accordingly one defines a complex reduced shear.
14.2.4.1 Classification ordinary images
If we consider a fixed value for β, then (θ , β) defines a (two-dimensional)
surface for the arrival time of the light. Ordinary images, for which det A(θ ) = 0,
are formed at the points θ , where ∇θ (θ , β) = 0. Thus the images are localized
at extremal or saddle points of the surface (θ , β) and are classified as follows.
•
•
•

Images of type I: These correspond to minima of , with det A > 0, tr A > 0
1
(and thus γ < 1 − k ≤ 1, ai > 0, µ ≥ 1−γ
2 ≥ 1).
Images of type II: These correspond to saddle points of , with det A < 0
(then (1 − k)2 < γ 2 , a2 > 0 > a1 ).
Images of type III: These correspond to maxima of , with det A > 0,
tr A < 0 (with (1 − k)2 > γ 2 , k > 1, ai < 0).

Consider a thin lens with a smooth surface mass density k(θ ), which
decreases faster than |θ |−2 for |θ | → ∞. For such a lens the total mass is
finite and the deflection angle α(θ ) is continuous and tends to zero for |θ | → ∞,
therefore α is bounded: |α| ≤ α0 . Moreover, let us denote by n I the number of
images of type I for a source located in β, similarly for n II and n III and define
n tot = n I + n II + n III . If these conditions are fulfilled then the following theorems
hold.
Theorem 14.1. If the previous conditions hold and β is not situated on a caustic,
the following conditions apply:

390

Gravitational lensing
(a) n I ≥ 1
(b) n tot < ∞
(c) n I + n III = 1 + n II
(d) for |β| sufficiently large n tot = n I = 1.

It thus follows from (c) that the total number of images n tot = 1 + 2n II is odd.
The number of images with positive parity (n I + n III ) exceeds by one those with
negative parity (n II ); n II ≥ n III and n tot > 1 if and only if n II ≥ 1. The number of
images is odd; however, in practice some images may be very faint or be covered
by the lens itself and are thus not observable.
Theorem 14.2. The image of the source which will appear first to the observer
is of type I and it is at least as bright as the unlensed source would appear
(µ(θ1) ≥ 1).
For a proof of the two theorems we refer to [4]. The second theorem is a
consequence of the fact that the surface mass density k is a positive quantity.

14.3 Simple lens models
14.3.1 Axially symmetric lenses
Let us consider a lens with an axially symmetric surface mass density, that is
&(ξ ) = &(|ξ |), in which case the lens equation reduces to a one-dimensional
equation. By symmetry we can restrict the impact vector θ to be on the positive
θ1 -axis, thus we have θ = (θ, 0) with θ > 0. We can then use polar coordinates:
θ = θ (cos φ, sin φ) (thus d2 θ = θ dθ dφ). With k(θ ) = k(θ ) we get for
equation (14.32)

1 ∞
θ dθ k(θ
π 0

1 ∞
α2 (θ ) =
θ dθ k(θ
π 0

α1 (θ ) =



θ − θ cos φ
,
2 + θ 2 − 2θ θ cos φ
θ
0
 2π
−θ sin φ
)
dφ 2
.
2 − 2θ θ cos φ
θ
+
θ
0
)

2π

dφ

(14.51)
(14.52)

Due to symmetry, α is parallel to θ and with equation (14.52) we get α2 (θ ) = 0.
Only the mass inside the disc of radius θ around the centre of the lens contributes
to the light deflection, therefore from equation (14.51) one finds
2
α(θ ) ≡ α1 (θ ) =
θ



θ

θ dθ k(θ ) ≡

0

m(θ )
.
θ

(14.53)

This way we can write the lens equation as
β = θ − α(θ ) = θ −

m(θ )
θ

(14.54)

Simple lens models

391

for θ ≥ 0. Due to the axial symmetry it is enough to consider β ≥ 0. Since
m(θ ) ≥ 0 it follows that θ ≥ β (for θ ≥ 0). Instead of equation (14.34) we get
 
 θ
θ
+(θ ) = 2
,
(14.55)
θ dθ k(θ ) ln
θ
0
whereas the Fermat potential can be written as
(θ, β) = 12 (θ − β)2 − +(θ ).

(14.56)

This way we get the lens equation (14.54) from
∂(θ, β)
= 0.
∂θ

(14.57)

To get the Jacobi matrix we write:
α(θ ) =

m(θ )
θ
θ2

and thus



m(θ ) θ22 − θ12
1 0
A=
− 4
0 1
−2θ1θ2
θ

(with θ = (θ1 , θ2 ) and θ = |θ |)

 2
2k(θ )
−2θ1 θ2
θ1
−
θ12 − θ22
θ1 θ2
θ2

θ1 θ2
θ22


, (14.58)

where we made use of m (θ ) = 2θ k(θ ). The determinant of the Jacobi matrix is
given by
 



m 
m
d m 
m
= 1 − 2 1 + 2 − 2k . (14.59)
det A = 1 − 2
1−
dθ θ
θ
θ
θ
14.3.1.1 Tangential and radial critical curves
The critical curves (the points for which det A(θ ) = 0) are then circles of radius
θ . From equation (14.59) we see that there are two possible cases:
(1)
(2)

m
= 1 : defined as tangential critical curve;
θ2
d m
dθ ( θ ) = 1: defined as radial critical curve.

and

For case (1) one gets m/θ = θ and thus from the lens equation (14.54) we
see that β = 0 is the corresponding caustic, which reduces to a point. If the axial
symmetric gets only slightly perturbed this degeneracy is lifted.
We can look at the critical points on the θ1 -axis with θ = (θ, 0), θ > 0. Then




m 1 0
m(θ ) −1 0
(14.60)
−
A =1− 2
0 +1
θ 0 0
θ
and this matrix must have an eigenvector X with eigenvalue zero. For symmetry
reasons, the vector must be either tangential, X = (0, 1), or normal, X = (1, 0),

392

Gravitational lensing

to the critical curve (which must be a circle). We see readily that the first case
occurs for a tangential critical curve, and the second for a radial critical curve.
The image of a circle (in the source plane) which lies close to a tangential critical
curve will be deformed to an ellipse with major axis tangential to the critical
curve. However, if the image of a circle gets close to a radial critical curve it will
be deformed to an ellipse with major axis radial to the critical curve.
For a tangential critical curve (|θ | = θt ) we get
 θt
2θ κ(θ ) dθ = θt2 .
(14.61)
m(θt ) =
0

With the definition of κ this translates to
 ξt
2ξ &(ξ ) dξ = ξt2 &cr .

(14.62)

0

The total mass M(ξt ) inside the critical curve is thus
M(ξt ) = πξt2 &cr .

(14.63)

This shows that the average density &t inside the tangential critical curve is
equal to the critical density &cr . This can be used to estimate the mass of a
deflector if the lens is sufficiently strong and the geometry is such that almost
complete Einstein rings are formed.
14.3.1.2 Einstein radius
For a lens with axial symmetry we get, with (14.11), the following equation:
β(θ ) = θ −

Dds 4G M(θ )
,
Ds Dd c 2 θ

(14.64)

from which we see that the image of a source, which is perfectly aligned (that
means β = 0), is a ring if the lens is supercritical. By setting β = 0 in
equation (14.64) we get the radius of the ring

θE =

4G M(θE ) Dds
c2
Dd Ds

1/2
,

(14.65)

which is called Einstein radius. The Einstein radius depends not only on the
characteristics of the lens but also on the various distances.
The Einstein radius sets a natural scale for the angles entering the description
of the lens. Indeed, for multiple images the typical angular separation between
the different images turns out to be of order 2θE . Moreover, sources with angular
distances smaller than θE from the optical axis of the system are magnified quite
substantially whereas sources which are at a distance much greater than θE are
only weakly magnified.

Simple lens models

393

In several lens models the Einstein radius delimits the region within which
multiple images occur, whereas outside this region there is a single image. By
comparing equation (14.26) with equation (14.65) we see that the surface mass
density inside the Einstein radius precisely corresponds to the critical density. For
a point-like lens with mass M the Einstein radius is given by


4G M Dds 1/2
θE =
,
(14.66)
c 2 Dd Ds
or instead of an angle one often also uses


4G M Dds Dd 1/2
R E = θ E Dd =
.
c2
Ds

(14.67)

To get some typical values we can consider the following two cases: a lens of
mass M located in the galactic halo at a distance of Dd ∼ 10 kpc and a source in
the Magellanic Cloud, in which case

 
−1/2
M 1/2
D
−3
θE = (0.9 × 10 )
(14.68)
M
10 kpc
and a lens with the mass of galaxy (including its halo) M ∼ 1012 M located at a
distance of Dd ∼ 1 Gpc

1/2 

M
D −1/2
θE = 0.9
,
(14.69)
1012 M
Gpc
where D = Dd Ds /Dds .
14.3.2 Schwarzschild lens
A particular case of a lens with axial symmetry is the Schwarzschild lens, for
which &(ξ ) = Mδ 2 (ξ ) and thus m(θ ) = θE2 . The source is also considered as
point-like, this way we get, for lens equation (14.13), the following expression
β=θ−

θE2
,
θ

(14.70)

where θE is given by equation (14.66). This equation has two solutions:


θ± = 12 β ± β 2 + 4θE2 .
(14.71)
Therefore, there will be two images of the source located one inside the Einstein
radius and the other outside. For a lens with axial symmetry the amplification is
given by
θ dθ
.
(14.72)
µ=
β dβ

394

Gravitational lensing

For the Schwarzschild lens, which is a limiting case of an axial symmetric one,
we can substitute β using equation (14.71) and obtain this way the amplification
for the two images

µ± = 1 −



θE
θ±

4 −1
=

1
u2 + 2
± .
√
2u u 2 + 4 2

(14.73)

u = r/RE is the ratio between the impact parameter r , that is the distance between
the lens and the line of sight connecting the observer and the source and the
Einstein radius RE defined in equation (14.67). u can also be expressed as β/θE .
Since θ− < θE we have that µ− < 0. The negative sign for the amplification
indicates that the parity of the image is inverted with respect to the source. The
total amplification is given by the sum of the absolute values of the amplifications
for each image
u2 + 2
µ = |µ+ | + |µ− | = √
.
(14.74)
u u2 + 4
If r = RE then we get u = 1 and µ = 1.34, which corresponds to an increase
of the apparent magnitude of the source of m = −2.5 log µ = −0.32. For
lenses with a mass of the order of a solar mass and which are located in the halo
of our galaxy the angular separation between the two images is far too small to be
observable. Instead, one observes a time-dependent change in the brightness of
the the source star. This situation is also referred to as microlensing.
Much research activity is devoted to studying microlensing in the context of
quasar lensing. Today, several cases of quasars which are lensed by foreground
galaxies, producing multiple observable images are known. The stars contained
in the lensing galaxy can act as microlenses on the quasar and, as a result, induce
time-dependent changes in the quasar brightness, but in a rather complicated way,
since here the magnification is a coherent effect of many stars at the same time.
This is an interesting field of research, which will lead to important results on
the problem of the dark matter in galaxies [15]. However, we will not discuss
extragalactic microlensing in detail (see, for instance, [16]), whereas we will
report in some depth on galactic microlensing (see section 14.4).
The time delay between the two images of a Schwarzschild lens is given by


√
2+4+u
u
4G M 1
u u 2 + 4 + ln √
.
(14.75)
ct =
2
c2
u2 + 4 − u
The two images have a comparable luminosity only if u ≤ 1 (otherwise the
difference is such that one image is no longer observable since it gets too
faint). For u = 1 one obtains t ∼ 4RS /c (typically for a galaxy with mass
M = 1012 M one finds t ∼ 1.3 years). Such measurements are important
since they allow to determine the value H0 of the Hubble constant (see section
14.5.1).

Simple lens models

395

14.3.3 Singular isothermal sphere
A simple model for describing the matter distribution in a galaxy is to assume that
the stars forming the galaxy behave like the particles in an ideal gas, confined by
the total gravitational potential, which we assume to have spherical shape. The
equation of state of the ‘particles’ (stars) has the form
p=

ρkB T
,
m

(14.76)

where ρ and m are the matter density and the mass of a star, respectively. In the
equilibrium case the temperature T is defined via the one-dimensional dispersion
velocity σv of the stars as obtained from
mσv2 = kB T.

(14.77)

In principle the temperature could depend on the radius; however, in the simplest
model, of the isothermal spherical model, one assumes that the temperature is
constant and hence also σv . The equation for hydrostatic equilibrium is given by
p
G M(r )
=−
,
ρ
r2

(14.78)

M (r ) = 4πr 2 ρ,

(14.79)

with
where M(r ) is the mass inside the sphere of radius r . A solution of the previous
equations is
σ2 1
ρ(r ) = v 2 .
(14.80)
2π G r
This mass distribution is called singular isothermal sphere (it is indeed singular
for r → 0). Since ρ(r ) ∼ r 2 , M(r ) ∼ r , the velocity of the stars in the
gravitational field of an isothermal sphere is given by
2
(r ) =
vrot

G M(r )
= 2σv2 ,
r

(14.81)

which is constant. Such a mass distribution can (at least in a qualitative way)
describe the flat rotation curves of the galaxies, as measured beyond a certain
galactic radius. Thus the dark matter in the halo can, in a first approximation, be
described by a singular isothermal sphere model.
The projected mass density on the lens plane perpendicular to the line of
sight is:
σ2 1
(14.82)
&(ξ ) = v ,
2G ξ

396

Gravitational lensing

where ξ is the distance (in the lens plane) from the the centre of mass. For the
light deflection angle we get
σ2
α̂ = 4π v2 = 1.4
c



σv

2

220 km s−1

(14.83)

independent of the position ξ (220 km s−1 is a typical value for the rotation
velocity in spiral galaxies).
The Einstein radius RE is given by
RE = 4π

σv2 Dds Dd
Dds Dd
= α̂
= α Dd .
Ds
c 2 Ds

(14.84)

Multiple images occur only if the source is located within the Einstein radius. Let
be ξ0 = RE , then &(ξ ) = &(xξ0 ) where x = ξ/ξ0 . This way the lens equation
becomes
x
.
(14.85)
y=x−
|x|
For 0 < y < 1 we have two solutions: x = y + 1 and x = y − 1. For y > 1 (the
source is located outside the Einstein radius) there is only one image: x = y + 1.
The images with x > 0 are of type I, whereas the ones with x < 0 are of type
II. If the singularity in ξ = 0 is removed then there will be a third image in the
centre.
The amplification of an image in x is given by
µ=

|x|
|x| − 1

(14.86)

(the circle |x| = 1 corresponds to a tangential critical curve). For y → 1 the
second image (corresponding to the solution x = y − 1) becomes very faint.
The potential is given by ψ(x) = |x| and the time delay between the images
is
   2
σv 2 Dd Dds
ct = 4π
2y.
(14.87)
c
Ds
14.3.4 Generalization of the singular isothermal sphere
The singular isothermal sphere model can, for instance, be generalized by
adopting for the projected mass density & the following expression
&(ξ ) = &0

1 + p(ξ/ξc )2
,
(1 + (ξ/ξc )2 )2− p

(14.88)

with 0 ≤ p ≤ 1/2 and &0 is the central density. ξc is a typical distance of the
order of the scale on which the matter decreases, often one can take it as the core

Simple lens models

397

radius of a galaxy. p = 0 corresponds to the Plummer distribution, whereas for
p = 1/2 we get the isothermal sphere for large values of ξ .
Defining x = ξ/ξ0 and k0 = &0 /&cr we can write equation (14.88) as
1 + px 2
.
(1 + x 2 )2− p

(14.89)

k0
[(1 + x 2 ) p − 1],
2p

(14.90)

k(x) = k0
The deflection potential is given by
+(x) =

which is valid for p = 0, whereas for p = 0 we get
+(x) =

k0
ln(1 + x 2 ).
2

(14.91)

Thus the lens equation is
y = x − α(x) = x −

k0 x
.
(1 + x 2 )1− p

(14.92)

If k0 > 1 there is one tangential critical curve for x = x t , where x t =
1/1− p

k0
− 1, and a radial critical curve for x = x r , which is defined by the
equation
1 − k0 (1 + x r2 ) p−2 [1 + (2 p − 1)x r2] = 0.
(14.93)
The corresponding caustics are given by yt ≡ y(x t ) = 0, whereas
yr ≡ |y(x r )| =

2(1 − p)x r3
.
1 − (1 − 2 p)x r2

(14.94)

Sources with |y| < yr lead to the formation of three images, whereas for |y| > yr
there is only one image. The three images are at: x > x t (image of type I),
−x t < x < rr (image of type II) and −x r < x < 0 (image of type III).
14.3.5 Extended source
The magnification for an extended source with surface brightness profile I ( y) is
given by

I ( y)µp ( y) d2 y

µe =
,
(14.95)
I ( y) d2 y
where µp ( y) is the magnification of a point source at position y. As an example
let us consider a disk-like source with radius R centred in y with a brightness

398

Gravitational lensing

profile I (r/R), where r is the distance of a source point from the centre of the
source. Adopting polar coordinates centred on the circular source, we obtain
 
µe (y) = 2π

×

∞

I (r/R)r dr

0
2π

−1 

∞

I (r/R)r dr

0

dφ
0

µp (y)y
y2

+ r 2 + 2r y cos φ

.

(14.96)

For a uniform brightness profile the maximum of µe is at y = 0 (with µe (0) =
2/R if µp is the magnification of a point source, since then µp (y)y → 0 for
y → 0). Indeed, for a Schwarzschild lens with µp = (y 2 + 2)/(y y 2 + 4) one
finds
√
4 + R2
max
.
(14.97)
µe =
R
14.3.6 Two point-mass lens
A natural generalization of the Schwarzschild lens is to consider a lens with two
point masses. This case is also of relevance for the applications, since many binary
microlensing events have been observed. For several point masses Mi located at
transversal positions ξi the general formula equation (14.10) for the deflection
angle gives
N

4G Mi ξ − ξi
.
(14.98)
α(ξ ) =
c2 |ξ − ξ |2
i=1

N
i

Let M =
Mi be the total mass and Mi = ηi M. For the typical length scale ξ0
we choose the Einstein radius equation (14.67) for the total mass. Then the lens
map becomes
N

ηi
(x − xi ),
(14.99)
y=x−
|x − xi |2
1=1

where xi = ξi /ξ0 . For a detailed discussion see [4].

14.4 Galactic microlensing
14.4.1 Introduction
There are cases in which the deflection angles are tiny, of the order of milliarcseconds or smaller, such that the multiple images are not observable. However,
lensing magnifies the affected source, and since the lens and the source are
moving relative to each other, this can be detected as a time-variable brightness.
This behaviour is referred to as gravitational microlensing, a powerful method
to search for dark matter in the halo of our own galaxy, if it consists of massive

Galactic microlensing

399

astrophysical compact halo objects (MACHOs), and to study the content of lowmass stars in the galactic disk.
The idea to use gravitational light deflection to detect MACHOs in the halo
of our galaxy by monitoring the light variability of millions of stars in the Large
Magellanic Cloud (LMC) was first proposed by Paczyński in 1986 [17] and then
further developed—from a theoretical point of view—in a series of papers by
De Rújula et al [18, 19], Griest [20] and Nemiroff [21]. Following these first
studies, the field has grown very rapidly, especially since the discovery of the
first microlensing events at the end of 1993 and many new applications have been
suggested, including the detection of Earth-like planets around stars in our galaxy.
(For reviews on microlensing see, for instance, [22–25].)
Since the discovery of the first microlensing events in September 1993 by
monitoring millions of stars in the Large Magellanic Cloud (LMC) and in the
direction of the galactic centre, several hundreds of events have been found. The
still few observed events towards the LMC indicate that the halo dark matter
fraction in the form of MACHOs is of the order of 20%, assuming a standard
spherical halo model.
The best evidence for dark matter in galaxies comes from the observed
rotation curves in spiral galaxies. Measurements of the rotation velocity vrot of
stars up to the visible edge of the spiral galaxies (of about 10 kpc) and of atomic
hydrogen gas in the disk beyond the optical radius (by measuring the Doppler shift
in the characteristic 21-cm radio line emitted by neutral hydrogen gas) imply that
vrot remains constant out to very large distances, rather than showing a Keplerian
fall-off, as expected if there is no more matter beyond the visible edge.
There are also measurements of the rotation velocity for our own galaxy.
However, these observations turn out to be rather difficult, and the rotation curve
has been measured accurately only up to a distance of about 20 kpc. Without any
doubt, our own galaxy has a typical flat rotation curve and thus it is possible to
search directly for dark matter characteristic of spiral galaxies in the Milky Way.
The question which naturally arises is the nature of dark matter in galactic
halos. A possibility is that the dark matter is comprised of baryons, which have
been processed into compact objects (MACHOs), such as stellar remnants (for
a detailed discussion see [26]). If their mass is below ∼0.08M , they are too
light to ignite hydrogen-burning reactions. Otherwise, MACHOs might be either
low-mass (∼0.1–0.3M ) hydrogen burning stars (also called M-dwarfs) or white
dwarfs. As a matter of fact, a deeper analysis makes the M-dwarf option look
problematic. The null result of several searches for low-mass stars both in the disk
and in the halo of our galaxy suggests that the halo cannot be mainly in the form
of hydrogen-burning main-sequence M-dwarfs. Optical imaging of high-latitude
fields taken with the Wide Field Planetary Camera of the Hubble Space Telescope
indicates that less than ∼6% of the halo can be in this form [27]. However,
this result is derived under the assumption of a smooth spatial distribution of Mdwarfs, and the problem becomes considerably less severe in the case of a clumpy
distribution [28]. Recent observations of four nearby spiral galaxies carried out

400

Gravitational lensing

with the satellite Infrared Space Observatory (ISO) also seem to exclude Mdwarfs as significantly contributing to halo dark matter [29].
A scenario with white dwarfs as a major constituent of the galactic halo dark
matter has been explored [30]. However, it requires a rather ad hoc initial mass
function sharply peaked around 2–6M . Future Hubble Deep Field exposures
could either find the white dwarfs or put constraints on their fraction in the halo
[31]. A substantial component of neutron stars and black holes with masses higher
than ∼1M is also excluded, for otherwise they would lead to an overproduction
of heavy elements relative to the observed abundances.
A further possibility is that the hydrogen gas is in molecular form, clumped
into very cold clouds, as we proposed some years ago [32, 33]. Indeed, the
observation of such clouds is very difficult and, therefore, at present there are
no stringent limits on their contribution to the halo dark matter [34].
14.4.1.1 Microlensing probability
When a MACHO of mass M is sufficiently close to the line of sight between
us and a more distant star, the light from the source star suffers a gravitational
deflection and we see two images of the source (figure 14.3). For most
applications we can consider the lens and the source as point-like and thus use
the Schwarzschild lens approximation previously discussed. RE is then defined in
equation (14.67).
For a cosmological situation, where the lens is a galaxy or even a cluster
of galaxies and the source is a very distant quasar, one indeed sees two or more
images which are typically separated by an angle of some arcseconds. However,
in the situation being considered here, namely of a MACHO of typically ∼0.1M
and a source star located in the LMC at about 50 kpc from us, the separation
angle turns out to be of the order of some milli-arcseconds. Thus, the images
cannot be seen separately. However, the measured brightness of the source star
varies with time. It increases until the MACHO reaches the shortest distance from
the line of sight between the observer on Earth and the source star. Afterwards,
the brightness decreases and eventually returns to its usual unlensed value. The
magnification of the original star brightness turns out to be typically of the order
of 30% or even more, corresponding to an increase of at least 0.3 magnitudes of
the source star (see figures 14.4 and 14.5). Such an increase is easily observable.
An important quantity is the optical depth τ due to gravitational
microlensing, which is the probability that a source is found within a circle of
radius r ≤ RE around a MACHO. It is defined as follows

τ=

1

dx
0

4π G
ρ(x)Ds2 x(1 − x)
c2

(14.100)

with ρ(x) being the mass density along the line of sight at distance s = x Ds from
the observer.

Galactic microlensing

401

Figure 14.3. The set-up of a gravitational lens situation: The lens L located between
source S and observer O produces two images S1 and S2 of the background source. Dd
is the distance between the observer and the lens, Ds between the observer and the source
and Dds between the lens and the source.

Figure 14.4. Einstein ring (broken curve) and some possible relative orbits of a background
star with projected minimal distances p = r/RE = 0.1, 0.3, . . . , 1.1 from a MACHO M
(from [22]).

402

Gravitational lensing

Figure 14.5. Light curves for the different cases of figure 4.2. The maximal magnification
is m = 0.32 mag, if the star just touches the Einstein radius ( p = 1.0). For smaller
values of p the maximum magnification gets larger. t is the time in units of t0 (from [22]).

We can easily compute τ assuming that the mass distribution in the galactic
halo is of the following form
ρ(r) =

2 )
ρ0 (a 2 + RGC
,
2
2
a +r

(14.101)

which is consistent with a flat rotation curve. |r| is the distance from Earth, a is
the core radius, ρ0 the local density nearby the solar system of dark matter and
RGC the distance to the galactic centre. Standard values for these parameters are:
ρ0 = 0.3 GeV cm−3 = 7.9 × 10−3 M pc−3 , a = 5.6 kpc and RGC = 8.5 kpc.
Assuming a spherical halo made entirely of MACHOs, one finds an optical
depth towards the LMC of τ = 5 × 10−7 . This means that at any one moment
out of 2 million stars, one is being lensed. From this number it can be seen that
in order to obtain a reasonable number of microlensing events, an experiment has
to monitor several million stars in the LMC or in other targets such as the galactic
centre region (also referred to as the galactic bulge).
The magnification of the brightness of a star by a MACHO is a timedependent effect, since the MACHO, which acts as a lens, changes its location
relative to the line of sight to the source as it moves along its orbit around the
galaxy. Typically, the velocity transverse to the line of sight for a MACHO

Galactic microlensing

403

Table 14.1. The expected number of events Nev is obtained for a halo made entirely of
MACHOs of a given mass.
MACHO mass (M )

Mean RE (km)

Mean microlensing duration

Nev

10−1
10−2
10−4
10−6

0.3 × 109
108
107
106

1 month
9 days
1 day
2h

4.5
15
165
1662

in the galactic halo is vT ≈ 200 km s−1 , which can be inferred from the
measured rotation curve of our galaxy. Clearly, the duration of the microlensing
phenomenon and thus of the brightness increase of the source star depends on the
MACHO mass, its distance and transverse velocity (see table 14.1).
Since the light deflection does not depend on the frequency of the light, the
change in luminosity of the source star will be achromatic. For this reason, the
observations are done in different wavelengths in order to check that. Moreover,
the light curve will be symmetric with respect to the maximum value, since
the transverse velocity of the MACHO is in excellent approximation constant
during the period in which the lensing occurs. The probability that a given
star is lensed twice is practically zero. Therefore, the achromaticity, symmetry
and uniqueness of the signal are distinctive features that allow a microlensing
event to be discriminated from background events such as variable stars (some of
which are periodic, others show chromaticity and most often the light curve is not
symmetric).
14.4.1.2 Microlensing towards the LMC
Another important quantity is the microlensing rate, which depends on the mass
and velocity distributions of MACHOs. To determine this one has to model the
galaxy and its halo. For simplicity one usually assumes a spherically symmetric
shape for the halo with matter density decreasing as 1/r 2 with distance as
in equation (14.101), to obtain naturally a flat rotation curve. The velocity
distribution is assumed to be Maxwellian. The least known quantity is the mass
distribution of the MACHOs. For that, one makes the simplifying assumption that
all MACHOs have the same mass. The number Nev of microlensing events (such
that the increase in magnitude is at least 30%) can then be computed. Table 14.1
shows some values for Nev assuming monitoring of a million stars for 1 year in
the LMC.
Microlensing allows the detection of MACHOs located in the galactic halo
in the mass range 10−7 < M/M < 1 [19], as well as MACHOs in the disk or
bulge of our galaxy [35, 36].

404

Gravitational lensing

Figure 14.6. Microlensing event observed by the MACHO collaboration in their first year
of data towards the LMC. The event lasted about 33 days. The data are shown for blue
light, red light and the ratio red light to blue light, which for perfect achromaticity should
be equal to one (from [38]).

In September 1993, the French collaboration EROS (Expérience de Recherche d’Objets Sombres) [37] announced the discovery of two microlensing
candidates, and the American–Australian collaboration MACHO (for the
collaboration they use the same acronym as for the compact objects) of one
candidate [38] by monitoring several millions of stars in the LMC (figure 14.6).
The MACHO team went on to report the observation of 13 to 17 events (one
being a binary lensing event; see figure 4.5) by analysing their 5.7 year of LMC
−7 with an additional
data [39]. The inferred optical depth is τ = 1.2+0.4
−0.3 × 10
20% to 30% of systematic error. Correspondingly, this implies that about 20% of
the halo dark matter is in the form of MACHOs with a most likely mass in the
range 0.15–0.9M depending on the halo model. Moreover, it might well be that
not all the MACHOs are in the halo: some could be stars in the LMC itself or

Galactic microlensing

405

Figure 14.7. Binary microlensing event towards the LMC by the MACHO collaboration
(taken from the web page http://darkstar.astro.washington.edu). The two light curves
correspond to observations in different colours taken in order to test achromaticity.

located in an extended disk of our galaxy, in which case an average mass value
including all events would produce an incorrect value. These considerations show
that, at present, the values for the average mass as well as the fraction of halo dark
matter in the form of MACHOs have to be treated with care.
As mentioned, one of the events discovered was due to a lens made from two
objects, namely a binary system. Such events are more rare, but their observation
is not surprising; since almost 50% of the stars are double systems, it is quite
plausible that MACHOs also form binary systems. The light curve is then more
complicated than for a single MACHO.
EROS has also searched for very-low-mass MACHOs by looking for
microlensing events with time scales ranging from 30 min to 7 days [40]. The
lack of candidates in this range places significant constraints on any model for the
halo that relies on objects in the range 5 × 10−8 < M/M < 2 × 10−2 . Indeed,
such objects may make up at most 20% of the halo dark matter (in the range
between 5 × 10−7 < M/M < 2 × 10−3 at most 10%). Similar conclusions have
also been reached by the MACHO group [39].
A few events have also been discovered towards the Small Magellanic Cloud
[41, 42].
14.4.1.3 Microlensing towards other targets
To date, the MACHO [43] and OGLE collaborations have found several hundred
microlensing events towards the galactic bulge, most of which are listed among

406

Gravitational lensing

the alert events, which are constantly updated†. During their first season, the
MACHO team found 45 events towards the bulge, which led to an estimated
−6
optical depth of τ  2.43+0.54
−0.45 × 10 , which is roughly in agreement with
the OGLE result [44], and also implies the presence of a bar in the galactic
centre. They also found three events by monitoring the spiral arms of our galaxy
in the region of Gamma Scutum. Meanwhile, the EROS II collaboration also
found some events towards the spiral arm regions. These results are important for
studying the structure of our galaxy [45].
Microlensing searches towards the Andromeda galaxy (M31) have also been
proposed [46–48]. In this case, however, one has to use the so-called ‘pixellensing’ method. Since the source stars are, in general, no longer resolvable, one
has to consider the luminosity variation of a whole group of stars, which are,
for instance, registered on a single pixel element of a CCD camera. This makes
the subsequent analysis more difficult; however, if successful it allows M31 and
other objects to be used as targets, which would otherwise not be possible to
use. For information on the shape of the dark halo, which is presently unknown,
it is important to observe microlensing in different directions. Two groups
have started to perform searches: the French AGAPE (Andromeda Gravitational
Amplification Pixel Experiment) [49, 50] and the American VATT/COLUMBIA
[51] [52] which uses the 1.8-m VATT-telescope (Vatican Advanced Technology
Telescope). Both teams showed that the pixel-lensing method works; however,
the small number of observations so far does not allow firm conclusions to be
drawn. Both the AGAPE and VATT/COLUMBIA teams found some candidate
events which are consistent with microlensing; however, additional observations
are needed to confirm them.
There are also networks involving different observatories with the aim of
performing accurate photometry on alert microlensing events and in particular
with the goal to find planets [53–55].
Although a rather young observational technique, microlensing has already
enabled us to make substantial progress and the prospects for further contributions
to solve important astrophysical problems look very bright.

14.5 The lens equation in cosmology
Until now, we have considered only almost static, weak localized perturbations
of Minkowski spacetime. In cosmology the unperturbed spacetime background
is given by a Robertson–Walker metric and this induces various changes in the
previous discussions. It turns out that the final result for the lens map and the
time delay looks practically unchanged, essentially we only have to insert some
obvious redshift factors and interpret all distances as angular diameter distances.
† Current information on the MACHO collaboration’s alert events is maintained at the WWW site
http://darkstar.astro.washington.edu.

The lens equation in cosmology

407

We recall that the expression for the time delay in an almost Newtonian
situation is given by equation (14.19) with equations (14.20), (14.21):
ct =
Note that

Dd Ds
2Dds



ξ
η
−
Dd
Ds



2

ξ
η
−
Dd
Ds

− ψ̂(ξ ) + constant.

(14.102)


= (θ − β).

If the distances involved are cosmological, we must multiply the whole expression
by (1 + z d ), where z d is the redshift of the lens. In addition all distances must be
interpreted as angular diameter distances. (For a detailed derivation we refer to
the book by Schneider et al [4] or [56]). With these modifications we obtain for
the time delay,


Dd Ds
2
(θ − β) − +̂(ξ ) + constant,
(14.103)
ct = (1 + z d )
2Dds
where the prefactor of the first term is proportional to 1/H0 (H0 is the present
Hubble parameter).
For cosmological applications, it is convenient to rewrite the potential term
using the length scale ξ0 = Dd as defined in equation (14.18) and θ = ξ /Dd .
This way we get

ψ̂(ξ ) = 4G d2 θ Dd2 &(Dd θ ) ln |θ − θ | = 2RS ψ̃(θ ),
(14.104)
where RS = 2G M is the Schwarzschild radius of the total mass M of the lens,
and

˜ ) ln |θ − θ |,
ψ̃(θ ) = d2 θ &(θ
(14.105)
with

&(Dd θ ) 2
Dd .
(14.106)
M
This quantity gives the fraction of the total mass M per unit solid angle as seen
by the observer. We can now write equation (14.103) in the form
˜ ) :=
&(θ

ct = φ̂(θ , β) + constant,

(14.107)

where φ̂ is the cosmological Fermat potential:
φ̂(θ , β) =

Dd Ds
1
(1 + z d )
(θ − β)2 − 2RS (1 + z d )ψ̃(θ ).
2
Dds

(14.108)

For a Friedmann–Lemaitre model with density parameter 0 and vanishing
cosmological constant , the angular diameter distance D(z 1 , z 2 ) between two

408

Gravitational lensing

events at redshifts z 1 and z 2 (z 1 < z 2 ), is given by
√
√
1 + 0 z 1 (2 − 0 + 0 z 2 ) − 1 + 0 z 2 (2 − 0 + 0 z 1 )
.
H0 20 (1 + z 2 )2 (1 + z 1 )
(14.109)
Equations (14.107)–(14.108) provide the basis for the determination of the
Hubble parameter with gravitational lensing. One should also take into account
that the universe might have a clumpy structure, which then affects the light
propagation (for details on this problem see [57, 58]).
From equation (14.108) we obtain the cosmological lens mapping using
Fermat’s principle, which implies that ∇θ φ̂(θ , β) = 0 and gives an equation
identical to equation (14.22), but, with the present meaning of the symbols, it
holds for arbitrary redshifts.
Consider two images at the (observed) positions θ1 , θ2 , with separation
θ12 ≡ θ1 − θ2 and time delay t12 . Using the lens equation we obtain


 
∂ ψ̃ 
Dds
∂ ψ̃ 
.
(14.110)
−
θ12 = 2RS
Dd Ds ∂θ θ1
∂θ θ2
D(z 1 , z 2 ) = 2c

The time delay t12 = φ̂(θ1 , β) − φ̂(θ2 , β) contains the unobservable angle β,
but this can be eliminated with the lens equation and equation (14.110):
 
(

 


1 ∂ ψ̃ 
∂ ψ̃ 
t12 = 2RS (1 + z d )
· θ12 − ψ̃(θ1 ) − ψ̃(θ2 ) .
+
2 ∂θ 
∂θ 
θ1

θ2

(14.111)
˜ )), then equations (14.110) and (14.111) give a
Given a lens model (i.e. &(θ
relation between the observables θ12, t12 and H0, provided that 0 , z d , z s are
also known. Fortunately, the dependence on 0 is, in practice, not strong.
Consider as an example a point source lensed by a point mass (Schwarzschild
lens). Then ψ̃(θ ) = ln |θ | and equation (14.110) gives


1
Dds
1
,
(14.112)
θ12 = 2RS
−
Dd Ds θ 1 θ 2
However, equation (14.111) becomes
 (
 θ2 
θ22 − θ12
+ ln   .
= 2RS (1 + z d )
2|θ1 θ2 |
θ1


t12

(14.113)

We write this in terms of the ratio ν of the magnifications. Using equation (14.74)
one finds ν = ln(θ2 /θ1 )2 and thus
t12 = Rs (1 + z d ){ν 1/2 − ν −1/2 + ln ν}.

(14.114)

Galaxy clusters as lenses

409

14.5.1 Hubble constant from time delays
As first noted by Refsdal in 1964 [59], time delay measurements can yield, in
principle, the Hubble parameter. Unfortunately, the use of this method requires
a reliable lens model. This introduces systematic uncertainties. Moreover, the
cosmological Fermat potential involves the density parameter 0 and  (set equal
to zero in equation (14.109)). The dependence on 0 and  is, however, not
strong, at least in some redshift domains (z s ≤ 2, z d ≤ 0.5).
Measuring the time delay is not an easy task as the history of the
famous double QSO0957+561 demonstrates. Fortunately, the time delay for
QSO0957+561 is now well known: t = 417 ± 3 days [60]. Modellings lead to
a best estimate of H0  61 km s−1 Mpc−1 . For this example there are constraints
for modelling the lens; nevertheless, it is difficult to assess an error for the value
of H0.
Another example is the Einstein ring system B0218+357. A single galaxy is
responsible for the small image splitting of 0.3 . The time delay was reported to
be 12 ± 3 days and the value H0 ∼ 70 km s−1 Mpc−1 was deduced. The ongoing
surveys will hopefully find new lenses that possess the desirable characteristics
for a reliable determination of H0.
Besides having the above mentioned problems, the determination of H0
through gravitational lensing offers also some advantages compared to the other
methods. It can be directly used for large redshifts (∼0.5) and it is independent
of any other method. Moreover, it is based on fundamental physics, while other
methods rely on models for variable stars (Cepheids), or supernova explosions
(type II) or empirical calibrations of standard candles (Tully–Fisher distances,
type I supernovae).
Finally, we note that lensing can also lead to bounds on the cosmological
constant. The volume per unit redshift of the universe at high redshifts increases
for a large . This implies that the relative number of lensed sources for a given
comoving number density of galaxies increases rapidly with . This can be used
to constrain  by making use of the observed probability of lensing. Various
authors have used this method and came up with a limit  ≤ 0.6 for a universe
with 0 +  = 1. It remains to be seen whether such bounds, based on lensing
statistics, can be improved.

14.6 Galaxy clusters as lenses
Galaxy clusters similarly to galaxies can act as gravitational lenses for more
distant galaxies. One classifies the observed lensing effects due to clusters into
two types:
(1) rich centrally condensed clusters produce sometimes giant arc when a
background galaxy turns out to be almost aligned with one of the cluster
caustics (strong lensing) (see, for instance, figure 14.1); and

410

Gravitational lensing

Figure 14.8. Light curves of the two images of the gravitationally lensed quasar
Q0957+561. Note the sudden decrease in image A at the beginning in the 1995 season
(taken from T Kundić et al 1997 [60]).

(2) every cluster produces weakly distorted images of a large number of
background galaxies (weak lensing) (A nice example is in figure 14.11).
Both of these cases have been observed and have provided important
information on the distribution of the matter in galaxy clusters. For the analysis
of giant arcs, we have to use parametrized lens models which are fitted to the
observational data. The situation is much better for weak lensing, because
there now exist several parameter-free reconstruction methods of projected mass
distributions from weak lensing data now exist.

Galaxy clusters as lenses

411

Figure 14.9. The light curve of image A of figure 14.3 is advanced by the optimal value
of the time delay, 417 days (taken from T Kundić et al 1997 [60]).

Strong lensing requires that the mass density per surface & has to be in some
parts of the lens bigger than the critical mass density given by
& ≥ &cr =

c 2 Ds
.
4π G Dd Dds

(14.115)

Indeed, if this condition is satisfied there will be one or more caustics. The
observation of an arc in a cluster of galaxies allows the projected cluster mass
which lies inside a circle traced by the arc, even if no ring-shaped image is
produced to be easily estimated. For an axisymmetric lens, the average surface
mass density within the tangential critical curve is given by &cr . Tangentially
oriented large arcs occur close to the tangential critical curves, and thus the radius
θarc of the circle traced by the arc gives an estimate of the Einstein radius θE .

412

Gravitational lensing

Figure 14.10. Wavefronts in the presence of a cluster perturbation.

Inside the so defined circle the surface mass is &cr , and this way, knowing the
redshifts of the lens and the source, one finds the total mass enclosed by θ = θarc

M(< θ ) = &cr π(Dd θ )  1.1 × 10 M
2

14

θ
30

2 


Dd
,
1 Gpc

(14.116)

A mass estimate with this procedure is useful and often quite accurate.
If we assume that the cluster can, at least as a first approximation, be
described as a singular isothermal sphere, then using equation (14.84) we obtain
for the dispersion velocity in the cluster
σv  103 km s−1



θ
28

1/2 

Ds
Dds

1/2
.

(14.117)

A limitation of strong lensing is that it is model-dependent and, moreover,
one can only determine the mass inside a cylinder of the inner part of a lensing
cluster. The fact that the observed giant arcs never have a counter-arc of
comparable brightness and even small counter-arcs are rare, implies that the
lensing geometry has to be non-spherical.

Galaxy clusters as lenses

413

Figure 14.11. Hubble Space Telescope image of the cluster Abell 2218. Beside arcs
around the two centres of the cluster, many arclets can be seen (NASA HST Archive).

A remarkable phenomenon is the occurrence of so-called radial arcs in
galaxy clusters. These are radially rather than tangentially elongated, as most
luminous arcs are. They are much less numerous (examples: MS 2137, Abell
370). Their position has been interpreted in terms of the turnover of the mass
profile and a core radius ∼20h −1 kpc has been deduced, quite independent of
any details of the lens model. There are, however, other mass profiles which
can produce radial arcs, and have no flat core; even singular density profiles can
explain radial arcs [61]. Such singular profiles of the dark matter are consistent
with the large core radii inferred from x-ray emission.
14.6.1 Weak lensing
There is a population of distant blue galaxies in the universe whose spatial density
reaches 50–100 galaxies per square arc minute at faint magnitudes. The images
of these distant galaxies are coherently distorted by any forground cluster of
galaxies. Since they cover the sky so densely, the distortions can be determined
statistically (individual weak distortions cannot be determined, since galaxies are
not intrinsically round). Typical separations between arclets are ∼(5–10) and
this is much smaller than the scale over which the gravitational cluster potential
changes appreciably.
Starting with a paper by Kaiser and Squires [62], a considerable amount of
theoretical work on various parameter-free reconstruction methods has recently
been carried out. The main problem consists in making an optimal use of limited
noisy data, without modelling the lens. For reviews see [63, 64]. The derivation
of most of the relevant equations becomes much easier when using a complex

414

Gravitational lensing

formulation of lensing theory (see, for instance, [65]). In the following we will,
however, not use it.
The reduced shear g is, in principle, observable over a large region. What
we are really interested in, however, is the mean curvature κ, which is related to
the surface mass density. Since
g=

γ
1−κ

(14.118)

we first look for relations between the shear γ = (γ1 , γ2 ) and κ.
From equation (14.37) we get that
+ = 2k

(14.119)

or if, instead, we use the notation θ = (θ1 , θ2 ) for the image position
equation (14.119) can be explicitly written as


1 ∂ 2 +(θ ) ∂ 2 +(θ )
.
(14.120)
+
k(θ ) =
2
∂θ12
∂θ22
Using the definition for γi as given in equation (14.42) we find


1 ∂ 2 +(θ ) ∂ 2 +(θ )
≡ D1 +
−
γ1 (θ ) =
2
∂θ12
∂θ22
and
γ2 (θ ) =
where
D1 :=

∂ 2 +(θ )
≡ D2 +.
∂θ1 ∂θ2


1 2
∂1 − ∂22 ,
2

D2 := ∂1 ∂2 .

(14.121)

(14.122)

(14.123)

Note the identity
D12 + D22 = 14 2 .
Hence
κ = 2



Di γ i .

(14.124)
(14.125)

i=1,2

Here, we can substitute the reduced shear, given by equation (14.118), on the
right-hand side for γi . This gives the important equation

κ = 2
Di [gi (1 − κ)].
(14.126)
i

For a given (measured) g this equation does not determine uniquely κ, indeed
equation (14.126) remains invariant under the substitution
κ → λκ + (1 − λ)

(14.127)

Galaxy clusters as lenses

415

where λ is a real constant. This is the so-called mass-sheet degeneracy (a
homogeneous mass sheet does not produce any shear).
Equation (14.126) can be turned into an integral equation, by making use of
the fundamental solution
(14.128)
G = 2π1 ln |θ |

for which G = δ 2 (δ 2 is the two-dimensional delta function). Then we get


d2 θ G (θ − θ )
(Di γi )(θ ) + k0 .
(14.129)
k(θ ) = 2
R2

i=1,2

After some manipulations we can bring equation (14.129) into the following form

1 
d2 θ [D̃i (θ − θ )γi (θ )] + k0 ,
(14.130)
k(θ ) =
π
R2
i=1,2

or, in terms of the reduced shear,

1 
d2 θ [D̃i (θ − θ )(gi (1 − k))(θ )],
k(θ ) = k0 +
π
R2

(14.131)

i=1,2

where
D1 ln |θ | =

θ22 − θ12
≡ D̃1 ,
|θ |4

D2 ln |θ | = −

2θ1 θ2
≡ D̃2 .
|θ |4

(14.132)

The crucial fact is that γ (θ ) is an observable quantity and thus using
equation (14.130) one can infer the matter distribution of the considered galaxy
cluster. This result is, however, fixed up to an overall constant k0 (problem of the
mass-sheet degeneracy).
As discussed in section 14.2.4 we can define the ellipticity  of an image of
a galaxy as
1 − r 2iϕ
b
e ,
(14.133)
 = 1 + i2 =
r≡
1+r
a
where ϕ is the position angle of the ellipse and a and b are the major and minor
semi axis, respectively. a and b are given by the inverse of the eigenvalues of the
matrix defined in equation (14.42). If we take the average on the ellipticity due to
lensing and make use of equation (14.133) as well as of the expressions for a and
b we find the relation


γ
.
(14.134)
 =
1−k
The angle bracket means average over a finite sky area. In the weak lensing
limit k  1 and |γ |  1 the mean ellipticity directly relates to the shear:
γ1 (θ )  1 (θ ) and γ2 (θ )  2 (θ ). Thus a measurement of the average

416

Gravitational lensing

ellipticity allows γ , to be determined and, making use of equation (14.130) one
can get the surface mass density k of the lens. Recently, several groups have
reported the detection of cosmic shear, which clearly demonstrates the technical
feasibility of using weak lensing surweys to measure dark matter clustering and
the potential for cosmological measurements, in particular with the upcoming
wide-field CCD cameras [67, 68].
14.6.2 Comparison with results from x-ray observations
Beside the lensing technique, there are two other methods for determining mass
distributions of clusters:
(1) the observed velocity dispersion, combined with the Jeans–equation from
stellar dynamics gives the total mass distribution, if it is assumed that light
traces mass; and
(2) x-ray observations of the intracluster gas, combined with the condition of
hydrostatic equilibrium and spherical symmetry also lead to the total mass
distribution as well as to the baryonic distribution.
If the hydrostatic equilibrium equation for the hot gas
dPg
G Mt (r )
= −ρg
dr
r2

(14.135)

is combined with the ideal equation of state Pg = (kB Tg /µm H )ρg and assuming
spherical symmetry, one easily finds for the total mass profile


d ln Tg
kB Tg
d ln ρg
+
Mt (r ) = −
r.
(14.136)
Gµm H d ln r
d ln r
The right-hand side can be determined from the intensity distribution and some
spectral information. (At present, the latter is not yet good enough, because of
relatively poor resolution which, however, will change with the XMM survey.)
Weak lensing, together with an analysis of x-ray observations, offers a
unique possibility for probing the relative distributions of the gas and the dark
matter, and for studying the dynamical relationship between the two. As an
example consider the cluster of galaxies A2163 (z=0.201) which is one of the
two most massive clusters known so far.
ROSAT measurements reach out to 2.3h −1 Mpc (∼15 core radii)(h being
the Hubble constant in units of 100). The total mass is 2.6 times greater than
that of COMA, but the gas mass fraction, ∼0.1h −3/2 is typical for rich clusters.
The data together suggest that there was a recent merger of two large clusters.
The optical observations of the distorted images of background galaxies were
made with the CFHT telescope. The resulting lensing and x-ray mass profiles
are compared in figure 14.12. The data-sets only overlap out to a radius of
200  500h −1 kpc to which the lensing studies were limited. It is evident

References

417

Figure 14.12. The radial mass profiles determined from the x-ray and lensing analysis for
Abell 2163. The triangles display the total mass profile determined from the x-ray data.
The filled squares are the weak lensing estimates ‘corrected’ for the mean surface density
in the control annulus determined from the x-ray data. The conversion from angular to
physical units is 60 = 0.127h −1 Mpc (taken from Squires et al 1997 [66]).

that the lensing mass estimates are systematically lower by a factor of ∼2 than
the x-ray results, but generally the results are consistent with each other, given
the substantial uncertainties. There are reasons that the lensing estimate may
be biased downward. Correcting for this gives the results displayed by open
squares. The agreement between the lensing and x-ray results then becomes quite
impressive. The rate and quality of such data will increase dramatically during the
coming years. With weak lensing one can also test the dynamical state of clusters.
By selecting the relaxed ones one can then determine, with some confidence, the
relative distributions of gas and dark matter.
In addition, it will become possible to extend the investigations to
supercluster scales, with the aim of determining the power spectrum and obtain
information on the cosmological parameters [63, 64].

References
[1] Perlmutter S et al 1999 Astrophys. J. 517 565
[2] Trimble V 1987 Annu. Rev. Astron. Astrophys. 25 425
[3] Zwicky F 1933 Helv. Phys. Acta 6 110

418

Gravitational lensing

[4] Schneider P, Ehlers J and Falco E E 1992 Gravitational Lensing (Berlin: Springer)
[5] Refsdal S and Surdej J 1994 Gravitational lenses Rep. Prog. Phys. 56 117
[6] Narayan R and Bartelmann M 1999 Lectures on gravitational lensing Formation of
Structure in the Universe ed A Dekel and J P Ostriker (Cambridge: Cambridge
University Press)
[7] Straumann N, Jetzer Ph and Kaplan J 1998 Topics on Gravitational Lensing (Napoli
Series on Physics and Astrophysics) (Naples: Bibliopolis)
[8] Dyson F W, Eddington A S and Davidson C R 1920 Mem. R. Astron. Soc. 62 291
[9] Einstein A 1936 Science 84 506
[10] Renn J, Sauer T and Stachel J 1997 Science 275 184
[11] Zwicky F 1937 Phys. Rev. 51 290
Zwicky F 1937 Phys. Rev. 51 679
[12] Chang K and Refsdal S 1979 Nature 282 561
[13] Gott R J 1981 Astrophys. J. 243 140
[14] Paczyński B 1986 Astrophys. J. 301 503
[15] Schmidt R and Wambsganss J 1998 Astron. Astrophys. 335 379
[16] Wambsganss J 2001 Microlensing 2000: A New Era of Microlensing Astrophysics ed
J W Menzies (San Francisco, CA: ASP)
[17] Paczyński B 1986 Astrophys. J. 304 1
[18] De Rújula A, Jetzer Ph and Massó 1991 Mon. Not. R. Astron. Soc. 250 348
[19] De Rújula A, Jetzer Ph and E Massó 1992 Astron. Astrophys. 254 99
[20] Griest K 1991 Astrophys. J. 366 412
[21] Nemiroff R J 1991 Astron. Astrophys. 247 73
[22] Paczyński B 1996 Annu. Rev. Astron. Astrophys. 34 419
[23] Roulet E and Mollerach S 1997 Phys. Rep. 279 67
[24] Zakharov A F and Sazhin M V 1998 Phys. Usp. 41 945
[25] Jetzer Ph 1999 Naturwissenschaften 86 201
[26] Carr B 1994 Annu. Rev. Astron. Astrophys. 32 531
[27] Bahcall J, Flynn C, Gould A and Kirhakos S 1994 Astrophys. J. 435 L51
[28] Kerins E J 1997 Astron. Astrophys. 322 709
[29] Gilmore G and Unavane M 1998 Mon. Not. R. Astron. Soc. 301 813
[30] Tamanaha C M, Silk J, Wood M A and Winget D E 1990 Astrophys. J. 358 164
[31] S D Kawaler 1996 Astrophys. J. 467 L61
[32] Paolis F De, Ingrosso G, Jetzer Ph and Roncadelli M 1995 Phys. Rev. Lett. 74 14
[33] Paolis F De, Ingrosso G, Jetzer Ph and Roncadelli M 1995 Astron. Astrophys. 295
567
[34] Paolis F De, Ingrosso G, Jetzer Ph and Roncadelli M 1999 Astrophys. J. 510 L103
[35] Paczyński B 1991 Astrophys. J. 371 L63
[36] Griest K et al 1991 Astrophys. J. 372 L79
[37] Aubourg E et al 1993 Nature 365 623
[38] Alcock C et al 1993 Nature 365 621
Alcock C et al 1995 Astrophys. J. 445 133
[39] Alcock C et al 2000 Astrophys. J. 542 281
[40] Renault C et al 1997 Astron. Astrophys. 324 L69
[41] Alcock C et al 1997 Astrophys. J. 491 L11
[42] Palanque-Delabrouille N et al 1999 Astron. Astrophys. 332 1
[43] Alcock C et al 1997 Astrophys. J. 479 119
[44] Udalski A et al 1994 Acta Astron. 44 165

References
[45]
[46]
[47]
[48]
[49]
[50]
[51]
[52]
[53]
[54]
[55]
[56]
[57]
[58]
[59]
[60]
[61]
[62]
[63]
[64]
[65]
[66]
[67]
[68]

419

Grenacher L, Jetzer Ph, Strässle M and De Paolis F 1999 Astron. Astrophys. 351 775
Crotts A P 1992 Astrophys. J. 399 L43
Baillon P, Bouquet A, Giraud-Héraud Y and Kaplan J 1993 Astron. Astrophys. 277 1
Jetzer Ph 1994 Astron. Astrophys. 286 426
Ansari R et al 1997 Astron. Astrophys. 324 843
Ansari R et al 1999 Astron. Astrophys. 344 L49
Crotts A P S and Tomaney A B 1996 Astrophys. J. 473 L87
Crotts A and Uglesich R 2001 Microlensing 2000: A New Era of Microlensing
Astrophysics ed J W Menzies (San Francisco, CA: ASP)
Mao S and Paczyński B 1991 Astrophys. J. 374 L37
Gould A and Loeb A 1992 Astrophys. J. 396 104
Bennett D and Rhie S H 1996 Astrophys. J. 472 660
Straumann N 1999 Lectures on Gravitational Lensing Troisième Cycle de la
Physique en Suisse Romande
Sachs R K 1961 Proc. R. Soc. A 264 309
Dyer C C and Roeder R C 1973 Astrophys. J. 180 L31
Refsdal S 1966 Mon. Not. R. Astron. Soc. 134 315
Kundić T et al 1997 Astrophys. J. 482 648
Bartelmann M 1996 Astron. Astrophys. 313 697
Kaiser N and Squires G 1993 Astrophys. J. 404 441
Mellier Y 1999 Annu. Rev. Astron. Astrophys. 37 127
Bartelmann M and Schneider P 1999 Weak gravitational lensing Preprint astroph/9912508
Straumann N 1997 Helv. Phys. Acta 70 894
Squires G et al 1997 Astrophys. J. 482 648
Van Waerbeke L et al 2000 Astron. Astrophys. 358 30
Wittman D M et al 2000 Nature 405 143

Chapter 15
Numerical simulations in cosmology
Anatoly Klypin
Astronomy Department, New Mexico State University, Las
Cruces, USA

15.1 Synopsis
In section 15.2 we give a short description of different methods used in
cosmology. The focus is on the major features of N-body simulations: equations,
main numerical techniques, the effects of resolution and methods of halo
identification.
In section 15.3 we give a summary of recent results on spatial and velocity
biases in cosmological models. Progress in numerical techniques made it possible
to simulate halos in large volumes with such an accuracy that halos survive in
dense environments of groups and clusters of galaxies. Halos in simulations look
like real galaxies, and, thus, can be used to study the biases—differences between
galaxies and the dark matter. The biases depend on scale, redshift and circular
velocities of selected halos. Two processes seem to define the evolution of the
spatial bias: (1) statistical bias and (2) merger bias (merging of galaxies, which
happens preferentially in groups, reduces the number of galaxies, but does not
affect the clustering of the dark matter). There are two kinds of velocity bias. The
pair-wise velocity bias is b12 = 0.6–0.8 at r < 5h −1 Mpc, z = 0. This bias
mostly reflects the spatial bias and provides almost no information on the relative
velocities of the galaxies and the dark matter. One-point velocity bias is a better
measure of the velocities. Inside clusters the galaxies should move slightly faster
(bv = 1.1–1.3) than the dark matter. Qualitatively this result can be understood
using the Jeans equations of stellar dynamics. For the standard LCDM model
we find that the correlation function and the power spectrum of galaxy-size halos
at z = 0 are antibiased on scales r < 5h −1 Mpc and k ≈ (0.15–30)h Mpc−1 .
In section 15.4 we give a review of the different properties of dark matter halos.
Taken from different publications, we present results on (1) the mass and velocity
420

Methods

421

functions, (2) density and velocity profiles and (3) concentration of halos. The
results are not sensitive to the parameters of cosmological models, but formally
most of them were derived for popular flat CDM model. In the range of radii
r = (0.005–1)rvir the density profile for a quiet isolated halo is very accurately
approximated by a fit suggested by Moore et al (1997): ρ ∝ 1/x 1.5 (1 + x 1.5 ),
where x = r/rs and rs is a characteristic radius. The fit suggested by Navarro
et al (1995), ρ ∝ 1/x(1 + x)2 , also gives a very satisfactory approximation with
relative errors of about 10% for radii not smaller than 1% of the virial radius. The
mass function of z = 0 halos with mass below ≈ 1013h −1 M is approximated
by a power law with slope α = −1.85. The slope increases with the redshift. The
velocity function of halos with Vmax < 500 km s−1 is also a power law with the
slope β = −3.8–4. The power law extends to halos at least down to 10 km s−1 .
It is also valid for halos inside larger virialized halos. The concentration of halos
depends on mass (more massive halos are less concentrated) and environment,
with isolated halos being less concentrated than halos of the same mass inside
clusters. Halos have intrinsic scatter of concentration: at 1σ level halos with
the same mass have (log cvir ) = 0.18 or, equivalently, Vmax /Vmax = 0.12.
Velocity anisotropy for both sub-halos and the dark matter is approximated by
β(r ) = 0.15 + 2x/[x 2 + 4], where x is the radius in units of the virial radius.

15.2 Methods
15.2.1 Introduction
Numerical simulations in cosmology have a long history and numerous important
applications. The different aspects of the simulations including the history of the
subject were reviewed recently by Bertschinger (1998); see also Sellwood (1987)
for an older review. More detailed aspects of simulations were discussed by Gelb
(1992), Gross (1997) and Kravtsov (1999). Numerical simulations play a very
significant role in cosmology. It all started in the 1960s (Aarseth 1963) and 1970s
(Peebles 1970, Press and Schechter 1974) with simple N-body problems solved
using N-body codes with a few hundred particles. Later the Particle–Particle code
(direct summation of all two-body forces) was polished and brought to the state
of art (Aarseth 1985). Already those early efforts brought some very valuable
fruits. Peebles (1970) studied the collapse of a cloud of particles as a model of
cluster formation. The model had 300 points initially distributed within a sphere
with no initial velocities. After the collapse and virialization the system looked
like a cluster of galaxies. Those early simulations of cluster formation, though
producing cluster-like objects, signalled the first problem—a simple model of an
initially isolated cloud (top-hat model) results in a density profile for the cluster
which is way too steep (power-law slope −4) as compared with real clusters (slope
−3). The problem was addressed by Gunn and Gott (1972), who introduced the
notion of secondary infall in an effort to solve the problem. Another keystone
work of those times is the paper by White (1976), who studied the collapse of 700

422

Numerical simulations in cosmology

particles with different masses. It was shown that if one distributes the mass of a
cluster to individual galaxies, two-body scattering will result in mass segregation
not compatible with observed clusters. This was another manifestation of the dark
matter in clusters. This time it was shown that inside a cluster the dark matter
cannot reside inside individual galaxies.
The survival of substructures in galaxy clusters was another problem
addressed in that paper. It was found that halos of dark matter, which in real
life may represent galaxies, do not survive in the dense environment of galaxy
clusters. White and Rees (1978) argued that the real galaxies survive inside
clusters because of energy dissipation by the baryonic component. That point of
view was accepted for almost 20 years. Only recently was it shown that the energy
dissipation probably does not play a dominant role in the survival of galaxies
and the dark matter halos are not destroyed by tidal stripping and galaxy–galaxy
collisions inside clusters (Klypin et al 1999a (KGKK), Ghigna et al 2000). The
reason why early simulations came to a wrong result was purely numerical: they
did not have enough resolution. But 20 years ago it was impossible to make a
simulation with sufficient resolution. Even if at that time we had present-day
codes, it would have taken about 600 years to make one run.
The generation of initial conditions with a given amplitude and spectrum of
fluctuations was a problem for some time. The only correctly simulated spectrum
was the flat spectrum which was generated by randomly distributing particles. In
order to generate fluctuations with a power spectrum, say P(k) ∝ k −1 , Aarseth et
al (1979) placed particles along rods. Formally, it generates the spectrum, but the
distribution has nothing to do with cosmological fluctuations, which have random
phases. Doroshkevich et al (1980) and Klypin and Shandarin (1983) were the
first to use the Zeldovich (1970) approximation to set the initial conditions. Since
then this method has been used to generate initial conditions for arbitrary initial
spectrum of perturbations.
Starting in the mid-1980s the field of numerical simulations has blossomed:
new numerical techniques have been invented, old ones perfected. The number of
publications based on numerical modelling has skyrocketed. To a large extent, this
has changed our way of doing cosmology. Instead of questionable assumptions
and waving-hands arguments, we have tools for testing our hypotheses and
models. As an example, I mention two analytical approximations which were
validated by numerical simulations. The importance of both approximations is
difficult to overestimate. The first is the Zeldovich approximation, which paved
the way for understanding the large-scale structure of the galaxy distribution. The
second is the Press and Schechter (1974) approximation, which gives the number
of objects formed at different scales at different epochs. Both approximations
cannot be formally proved. The Zeldovich approximation is not formally
applicable for hierarchical clustering. It must start with smooth perturbations (a
truncated spectrum). Nevertheless, numerical simulations have shown that even
for the hierarchical clustering the approximation can be used with appropriate
filtering of the initial spectrum (see Sahni and Coles (1995) and references

Methods

423

therein). The Press–Schechter approximation is also difficult to justify without
numerical simulations. It operates with an initial spectrum and a linear theory,
but then (a very long jump) it predicts the number of objects at very nonlinear
stage. Because it is not based on any realistic theory of nonlinear evolution, it
was an ingenious but wild guess. If anything, the approximation is based on a
simple spherical top-hat model. But simulations show that objects do not form
in this way—-they are formed in a complicated fashion through multiple mergers
and accretion along filaments. Still this very simple and very useful prescription
gives quite accurate predictions.
This chapter is organized in the following way. Section 15.2 gives the
equations which we solve to follow the evolution of initially small fluctuations.
Initial conditions are discussed in section 15.3. A brief discussion of different
methods is given in section 15.4. The effects of the resolution and some other
technical details are also discussed in section 15.5. Identification of halos
(‘galaxies’) is discussed in section 15.6.
15.2.2 Equations of evolution of fluctuations in an expanding universe
Usually the problem of the formation and dynamics of cosmological objects is
formulated as an N-body problem: for N point-like objects with given initial
positions and velocities, find their positions and velocities at any later moment.
It should be remembered that this is just a short-cut in our formulation—to make
things simple. While it is still mathematically correct in many cases, it does
not give a correct explanation for what we do. If we are literally to take this
approach, we should follow the motion of zillions of axions, baryons, neutrinos
and whatever else our universe is made of. So, what has it to do with the motion of
those few millions of particles in our simulations? The correct approach is to start
with the Vlasov equation coupled with the Poisson equation and with appropriate
initial and boundary conditions. If we neglect the baryonic component, which
of course is very interesting, but would complicate our situation even more, the
system is described by distribution functions f i (x, ẋ, t) which should include all
different clustered components i . For a simple CDM model we have only one
component (axions or whatever it is). For more complicated Cold plus Hot Dark
Matter (CHDM) with several different types of neutrinos the system includes one
distribution function for the cold component and one distribution function for
each type of neutrino (Klypin et al 1993). In the comoving coordinates x, the
equations for the evolution of fi are:
∂ fi
∂ fi
∂ fi
+ ẋ
− ∇φ
= 0,
∂t
∂x
∂p

p = a 2 ẋ,

∇ 2 φ = 4π Ga 2 (ρdm (x, t) − ρdm (t)) = 4π Ga 2 dm δdm ρcr ,
δdm (x, t) = (ρdm − ρdm )/ρdm ),
 
−3
ρdm (x, t) = a
m i d3 p f i (x, ẋ, t).
i

(15.1)
(15.2)
(15.3)
(15.4)

424

Numerical simulations in cosmology

Here a = (1 + z)−1 is the expansion parameter, p = a 2 ẋ is the momentum, dm
is the contribution of the clustered dark matter to the mean density of the universe,
m i is the mass of a particle of the i th component of the dark matter. The solution
of the Vlasov equation can be written in terms of equations for the characteristics,
which look like equations of particle motion:
∇φ
dv
ȧ
∇φ
dp
=−
,
+2 v =− 3 ,
da
ȧ
dt
a
a
dx
dx
p
= 2,
= v,
da
dt
ȧa
φ = aφ,
∇ 2 φ = 4π G0 δdm ρcr,0 /a,



1
− 1 +  (a 2 − 1).
ȧ = H0 1 + 0
a

(15.5)
(15.6)
(15.7)
(15.8)

In these equations ρcr,0 is the critical density at z = 0; 0 , and ,0 , are the
density of the matter and of the cosmological constant in units of the critical
density at z = 0.
The distribution function f i is constant along each characteristic. This
property should be preserved by numerical simulations. The complete set of
characteristics coming through every point in the phase space is equivalent to the
Vlasov equation. We cannot have the complete (infinite) set, but we can follow
the evolution of the system (with some accuracy), if we select a representative
sample of characteristics. One way of doing this would be to split the initial phase
space into small domains, to take only one characteristic as being representative
of each volume element, and to follow the evolution of the system of ‘particles’
in a self-consistent way. In models with one ‘cold’ component of clustering dark
matter (like the CDM or CDM models) the initial velocity is a unique function
of the coordinates (only the ‘Zeldovich’ part is present, no thermal velocities).
This means that we need only to split the coordinate space, not the velocity space.
For complicated models with a significant thermal component, the distribution
in the full phase space should be taken into account. Depending on what we are
interested in, we might split the initial space into equal-size boxes (a typical set-up
for PM or P3 M simulations) or we could divide some area of interest (say, where
a cluster will form) into smaller boxes, and use much bigger boxes outside the
area (to mimic the gravitational forces of the outside material). In any case, the
mass assigned to a ‘particle’ is equal to the mass of the domain it represents. Now
we can think of the ‘particle’ either as a small box, which moves with the flow but
does not change its original shape, or as a point-like particle. Both presentations
are used in simulations. None is superior to another.
There are different forms of final equations. Mathematically they are all
equivalent but computationally there are very significant differences. There are
considerations, which may affect the choice of a particular form of the equations.
Any numerical method gives more accurate results for a variable, which changes
slowly with time. For example, for the gravitational potential we can choose either

Methods

425

φ or φ . At early stages of evolution perturbations still grow almost linearly. In
this case we expect that δdm ∝ a, φ ≈ constant and φ ≈ a. Thus, φ can
be a better choice because it does not change. This is especially helpful, if
the code uses the gravitational potential from a previous moment of time as an
initial ‘guess’ for the current moment, as happens in the case of the ART code.
In any case, it is better to have a variable which does not change much. For
equations of motion we can choose, for example, either the first equations in
(15.5)–(15.6) or the second equations. If we choose the ‘momentum’ p = a 2 ẋ
as the effective velocity and take the expansion parameter a as the time variable,
then for linear growth we expect the change of coordinates per each step to be
constant: x ∝ a. Numerical integration schemes should not have a problem
with this type of growth. For the t and v variables, the rate of change is more
complicated: x ∝ a −1/2t, which may produce some errors at small expansion
parameters. The choice of variables may affect the accuracy of the solution even
at a very nonlinear stage of the evolution as was argued by Quinn et al (1997).
15.2.3 Initial conditions
15.2.3.1 The Zeldovich approximation
The Zeldovich approximation is commonly used to set initial conditions. The
approximation is valid in mildly nonlinear regimes and is much superior
to the linear approximation. We slightly rewrite the original version of
the approximation to incorporate cases (like CHDM) when the growth rates
g(t) depends on the wavelength of the perturbation |k|. In the Zeldovich
approximation the comoving and Lagrangian coordinates are related in the
following way:




ġ|k|
2
g|k|(t)S|k| (q),
p = −αa
g|k|(t)
S|k| (q), (15.9)
x = q −α
g|k|
k

k

where the displacement vector S is related to the velocity potential  and the
power spectrum of fluctuations P(|k|):
S|k| (q) = ∇q |k| (q),

|k| =



ak cos(kq) + b k sin(kq),

(15.10)

k

where a and b are Gaussian random numbers with mean zero and dispersion
σ 2 = P(k)/k 4 :
ak =

P(|k|)

Gauss(0, 1)
,
|k|2

bk =

P(|k|)

Gauss(0, 1)
.
|k|2

(15.11)

The parameter α, together with the power spectrum P(k), define the
normalization of the fluctuations.

426

Numerical simulations in cosmology

In order to set the initial conditions, we choose the size of the computational
box L and the number of particles N 3 . The phase space is divided into small equal
cubes of size 2π/L. Each cube is centred on a harmonic k = 2π/L × {i, j, k},
where {i, j, k} are integer numbers with limits from zero to N/2. We realize the
spectrum of perturbations ak and bk , and find the displacement and the momenta
of particles with q = L/N × {i, j, k} using equation (15.9). Here i, j, k = 1, N.
15.2.3.2 Power spectrum
There are approximations of the power spectrum P(k) for a wide range of
cosmological models. The publicly available COSMICS code (Bertschinger
1996) gives accurate approximations for the power spectrum. Here we follow
Klypin and Holtzman (1997) who give the following fitting formula:
P(k) =

kn
.
(1 + P2 k 1/2 + P3 k + P4 k 3/2 + P5 k 2 )2 P6

(15.12)

The coefficients Pi are presented by Klypin and Holtzman (1997) for a variety
of models. A comparison of some of the power spectra with the results from
COSMICS (Bertschinger 1996) indicate that the errors of the fits are smaller than
5%. Table 15.1 gives the parameters of the fits for some popular models. The
power spectrum of cosmological models is often approximated using a fitting
formula given by Bardeen et al (1986, BBKS):
P(k) = k n T 2 (k),
ln(1 + 2.34q)
T (k) =
[1 + 3.89q + (16.1q)2 + (5.4q)3 + (6.71q)4]−1/4 ,
2.34q
(15.13)
where q = k/(0 h 2 Mpc−1 ). Unfortunately, the accuracy of this approximation
is not great and it should not be used for accurate simulations. We find that the
following approximation, which is a combination of a slightly modified BBKS fit
and the Hu and Sugiyama (1996) scaling with the amount of baryons, provides
errors in the power spectrum which are less than 5% for the range of wavenumbers
k = (10−4 –40)h Mpc−1 and for b /0 < 0.1:
P(k) = k n T 2 (k),
ln(1 + 2.34q)
T (k) =
[1 + 13q + (10.5q)2 + (10.4q)3 + (6.51q)4]−1/4 ,
2.34q
k(TCMB /2.7 K)2
− /  −( /  )3
q=
,
α = a1 b 0 a2 b 0 ,
2
1/2
0.60
0 h α (1 − b /0 )
a1 = (46.90h 2 )0.670 [1 + (32.10h 2 )−0.532 ],
a2 = (120h 2 )0.424[1 + (450 h 2 )−0.582 ].

(15.14)

Methods

427

Table 15.1. Approximations of the power spectra.
0

bar

h

P2

P3

P4

P5

P6

0.3
0.3
0.3
1.0
1.0

0.035
0.030
0.026
0.050
0.100

0.60
0.65
0.70
0.50
0.50

−1.7550E+00
−1.6481E+00
−1.5598E+00
−1.1420E+00
−1.3275E+00

6.0379E+01
5.3669E+01
4.7986E+01
2.9507E+01
3.0152E+01

2.2603E+02
1.6171E+02
1.1777E+02
4.1674E+01
5.5515E+01

5.6423E+02
4.1616E+02
3.2192E+02
1.1704E+02
1.2193E+02

9.3801E-01
9.3493E-01
9.3030E-01
9.2110E-01
9.2847E-01

15.2.3.3 Multiple masses: high resolution for a small region
In many cases we would like to set initial conditions in such a way that inside
some specific region(s) there are more particles and the spectrum is better
resolved. A rigorous but complicated approach for the problem is described by
Bertschinger (2001). Here I give a simplified prescription. The procedure has
two steps. First, we run a low-resolution simulation which has a sufficiently large
volume to include the effects of the environment. For this run all the particles have
the same mass. A halo is picked for rerunning with high resolution. Second, using
particles of the halo, we identify a region in the Lagrangian (initial) space, where
the resolution should be increased. We add high-frequency harmonics, which are
not present in the low-resolution run. We then add the contributions from all
the harmonics and get initial displacements and momenta (equation (15.9)). Let
us be more specific. In order to add the new harmonics, we must specify (1)
how we divide the phase space and place the harmonics and (2) how we sum the
contributions of the harmonics.
The simplest way is to divide the phase space into many small boxes of
size 2π/L, where L is the box size. This is the same division, which we use to
set the low-resolution run. But now we extend it to very high frequencies up to
2π/L × N/2, where N is the new effective number of particles. For example,
we used N = 64 for the low-resolution run. For a high-resolution run we may
choose N = 1024. Simply replace the value and run the code again. Of course,
we really cannot do it because it would generate too many particles. Instead,
in some regions, where the resolution should not be high, we combine particles
together (by taking average coordinates and average velocities) and replace many
small-mass particles with fewer larger ones. The top panel in figure 15.1 gives an
example of mass refinement. Note that we try to avoid jumps that are too large in
the mass resolution by creating layers of particles of increasing mass.
This approach is correct and relatively simple. It may seem that it takes
too much CPU time to obtain the initial conditions. In practice, CPU time is
not much of an issue because initial conditions are generated only once and it
takes only a few CPU hours even for a 10243 mesh. For most applications 10243
particles is more than enough. The problem arises when we want to have more

428

Numerical simulations in cosmology

Figure 15.1. An example of the construction of mass refinement in real space (top)
and in phase space (bottom). In real space (top panel) three central blocks of particles
were marked for highest mass resolution. Each block produces 162 of smallest particles.
Adjacent blocks get one step lower resolution and produce 82 particles each. The procedure
is repeated recursively. In phase space (bottom panel) small points in the left-hand
bottom corner represent the harmonics used for the low-resolution simulation. For the
high-resolution run with box ratios 1:1/8:1/16 the phase space is sampled more coarsely,
but high frequencies are included. Each harmonic (different markers) represents a small
cube of the phase space indicated by squares. In this case the matching of the harmonics is
not perfect: there are overlapping blocks and gaps. In any case, the waves inside domains
A and B are missed in the simulation.

Methods

429

then 10243 particles. We simply do not have enough computer memory to store
the information for all the harmonics. In this case we must decrease the resolution
in the phase space. It is a bit easier to understand the procedure, if we consider
phase-space diagrams like the one presented in figure 15.3. The low-resolution
run in this case was done for 323 particles with harmonics up to 16 × 2π/L (small
points). For the high-resolution run we choose a region of size 1/8 of the original
large box. Inside the small box we place another box, which is twice as small.
Thus, we will have three levels of mass refinement. For each level we have the
corresponding size of the phase-space block. The size is defined by the size of
real-space box and is equal to 2π/L × K , K = 1, 8, 16. Harmonics from different
refinements should not overlap: if a region in the phase space is represented on a
lower level of resolution, it should not appear in the higher resolution level. This
is why the rows of the highest resolution harmonics (circles) with K x = 16 and
K y = 16 are absent in figure 15.3: they have already been covered by the lower
resolution blocks marked by stars. Figure 15.3 clearly illustrates that matching
harmonics is a complicated process: we failed to do the match because there are
partially overlapping blocks and there are gaps. We can get much better results,
if we assume different ratios of the sizes of the boxes. For example, if instead
of box ratios 1:1/8:1/16, we choose ratios 1:3/32:5/96, the coverage of the phase
space is almost perfect as shown in figure 15.2.
15.2.4 Codes
There are many different numerical techniques to follow the evolution of a
system of many particles. For earlier reviews see Hockney and Eastwood (1981),
Sellwood (1987) and Bertschinger (1998). Most of the methods for cosmological
applications take some ideas from three techniques: the Particle–Mesh (PM)
code, direct summation or the Particle–Particle code and the TREE code. For
example, the Adaptive Particle–Particle/Particle–Mesh (AP3 M) code (Couchman
1991) is a combination of the PM code and the Particle–Particle code. The
Adaptive-Refinement-Tree code (ART) (Kravtsov et al 1997, Kravtsov 1999) is
an extension of the PM code with the organization of meshes in the form of a tree.
All methods have their advantages and disadvantages.
15.2.4.1 The PM code
This uses a mesh to produce the density and potential. As a result, its resolution
is limited by the size of the mesh. There are two advantages of the method: (i)
it is fast (the smallest number of operations per particle per time step of all the
other methods); and (ii) it typically uses a very large number of particles. The
latter can be crucial for some applications. There are several modifications of the
code. ‘Plain-vanilla’ PM was described by Hockney and Eastwood (1981). It
includes a cloud-in-cell density assignment and a seven-point discrete analogue
of the Laplacian operator. Higher-order approximations improve the accuracy on

430

Numerical simulations in cosmology

Figure 15.2. Another example of construction of mass refinement in phase space. For
the high-resolution run with box ratios 1:3/3:5/96 the phase space is sampled without
overlapping blocks or gaps.

large distances but degrades the resolution (e.g. Gelb 1992). The PM code is
available (Klypin and Holtzman 1997).
15.2.4.2 The P3 M code
The P3 M code is described in detail in Hockney and Eastwood (1981) and
Efstathiou et al (1985). It has two parts: the PM part, which takes care of the
large-scale forces; and the PP part, which adds the small-scale particle–particle
contribution. Because of strong clustering at late stages in the evolution, the PP
part becomes prohibitively expensive once large objects start to form in large
numbers. A significant speed is achieved in a modified version of the code,
which introduces sub-grids (the next levels of PM) in areas with high density

Methods

431

Figure 15.3. Distribution of particles of different masses in a thin slice going through
the centre of halo A1 at redshift 10 (top panel) and at redshift zero (bottom panel). To
avoid crowding of points the thickness of the slice is made smaller in the centre (about
30h −1 kpc) and larger (1h −1 Mpc) in the outer parts of the forming halo. Particles of
different mass are shown with different symbols.

432

Numerical simulations in cosmology

(Couchman 1991). With modification the code is as fast as the TREE code even
for heavily clustered configurations. The code expresses the inter-particle force
as a sum of a short-range force (computed by a direct particle–particle pair force
summation) and the smoothly varying part (approximated by the particle–mesh
force calculation). One of the major problems for these codes is the correct
splitting of the force into a short-range and a long-range part. The grid method
(PM) is only able to produce reliable inter-particle forces down to a minimum of at
least two grid cells. For smaller separations the force can no longer be represented
on the grid and therefore one must introduce a cut-off radius re (larger than two
grid cells), where for r < re the force should smoothly go to zero. The parameter
re defines the chaining-mesh and for distances smaller than this cut-off radius re
a contribution from the direct particle–particle (PP) summation needs to be added
to the total force acting on each particle. Again this PP force should smoothly
go to zero for very small distances in order to avoid unphysical particle–particle
scattering. This cut-off of the PP force determines the overall force resolution of
a P3 M code.
The most widely used version of this algorithm is currently the adaptive P3 M
3
(AP M) code of Couchman (1991), which is available publicly. The smoothing
of the force in this code is connected to an S2 sphere, as described in Hockney
and Eastwood (1981).
15.2.4.3 The TREE code
The TREE code is the most flexible code in the sense of the choice of boundary
conditions (Appel 1985, Barnes and Hut 1986, Hernquist 1987). It is also more
expensive than PM: it takes 10–50 times more operations. Bouchet and Hernquist
(1988) and Hernquist et al (1991) extended the code for periodical boundary
conditions, which is important for simulating large-scale fluctuations. Some
variants of TREE are publicly available. A very useful example is the GADGET
code available at http://www.mpa-garching.mpg.de/gadget/right.html. There are
variants of the code modified for massively parallel computers and there are
variants with variable time stepping, which is vital for extremely high-resolution
simulations.
15.2.4.4 The ART code
Multi-grid methods were introduced long ago, but only recently have they started
to produce important results. Examples of adaptive multi-grid codes are the
Adaptive Refinement Tree code (ART; Kravtsov et al 1997), the AMR code
written by Bryan and Norman and MLAPM (Knebe et al 2001). The ART
code reaches high-force resolution by refining all high-density regions with an
automated refinement algorithm. The refinements are recursive: the refined
regions can also be refined, each subsequent refinement having half of the
previous level’s cell size. This creates a hierarchy of refinement meshes with

Methods

433

different resolutions covering the regions of interest. The refinement is done
cell-by-cell (individual cells can be refined or de-refined) and meshes are not
constrained to have a rectangular (or any other) shape. This allows the code to
refine the required regions in an efficient manner. The criterion for refinement
is the local overdensity of particles: the code refines an individual cell only if
the density of particles (smoothed with the cloud-in-cell scheme; Hockney and
Eastwood 1981) is higher than n TH particles, with typical values n TH = 2–5.
The Poisson equation on the hierarchy of meshes is solved first on the base grid
using FFT techniques and then on the subsequent refinement levels. On each
refinement level the code obtains the potential by solving the Dirichlet boundary
problem with boundary conditions provided by the already existing solution at the
previous level or from the previous moment of time.
Figure 15.4 (courtesy of A Kravtsov) gives an example of mesh refinement
for the hydro-dynamical version of the ART code. The code produced this
refinement mesh for a spherical strong explosion (Sedov solution).
The refinement of the time integration mimics the spatial refinement and the
time step for each subsequent refinement level is twice as small as the step on the
previous level. Note, however, that particles on the same refinement level move
with the same step. When a particle moves from one level to another, the time
step changes and its position and velocity are interpolated to appropriate time
moments. This interpolation is first-order accurate in time, whereas the rest of the
integration is done with the second-order accurate-time centred leap-frog scheme.
All equations are integrated with the expansion factor a as a time variable and the
global time step hierarchy is thus set by the step a0 at the zeroth level (uniform
base grid). The step on level L is then a L = a0 /2 L .
What code is the best? Which one to choose? There is no unique answer—
everything depends on the problem, which we are addressing. If you intend to
study the structure of individual galaxies in the large-scale environment, you must
have a code with very high resolution, variable time stepping and multiple masses.
In this case the TREE or ART codes should be the choice.
15.2.5 Effects of resolution
As the resolution of the simulations improves and the range of their applications
broaden, it becomes increasingly important to understand their limits. The effects
of resolution and convergence studies were studied in a number of publications
(e.g. Moore et al 1998, Frenk et al 1999, Knebe et al 2000, Ghigna et al 2000,
Klypin et al 2001). Knebe et al (2000) made a detailed comparison of realistic
simulations done with three codes: ART, AP3 M and PM. Here we present some
of their results and main conclusions. The simulations were done for the standard
CDM model with the dimensionless Hubble constant h = 0.5 and 0 = 1. The
simulation box of 15h −1 Mpc had 643 equal-mass particles, which gives the mass
resolution (mass per particle) of 3.55×109h −1 M . Because of the low resolution
of the PM runs, we show results only for the other two codes. For the ART code

434

Numerical simulations in cosmology

Figure 15.4. An example of a refinement structure constructed by the (hydro)ART code
for spherical strong explosion (courtesy of A Kravtsov).

the force resolution is practically fixed by the number of particles. The only free
parameter is the number of steps on the lowest (zero) level of resolution. In the
case of the AP3 M, besides the number of steps, one can also request the force
resolution. Parameters from two runs with the ART code and five simulations
with the AP3 M are given in table 15.2.
Figure 15.5 shows the correlation function for the dark matter down to the
scale of 5h −1 kpc, which is close to the force resolution of all our high-resolution
simulations. The correlation function in the AP3 M1 and ART2 runs are similar
to those of AP3 M5 and ART1 respectively and are not shown for clarity. We can
see that the AP3 M5 and the ART1 runs agree to .10% over the whole range of
scales. The correlation amplitudes of runs AP3 M2−4 , however, are systematically
lower at r . 50–60h −1 kpc (i.e. the scale corresponding to ≈15–20 resolutions),
with the AP3 M3 run exhibiting the lowest amplitude. The fact that the AP3 M2

Methods

435

Table 15.2. Parameters of the numerical simulations.
Simulation

Softening
(h −1 kpc)

Dyn. range

Steps
(min–max)

Nsteps /dyn. range

AP3 M1
AP3 M2
AP3 M3
AP3 M4
AP3 M5
ART1
ART2

3.5
2.3
1.8
3.5
7.0
3.7
3.7

4267
6400
8544
4267
2133
4096
4096

8000
6000
6000
2000
8000
660–21 120
330–10 560

1.87
0.94
0.70
0.47
3.75
2.58
5.16

Figure 15.5. The correlation function of dark matter particles. Note that the range of
correlation amplitudes is different in the inset panel.

correlation amplitude deviates less than that of the AP3 M3 run indicates that the
effect is very sensitive to the force resolution.
Note that the AP3 M3 run has formally the best force resolution. Thus, one
would naively expect that it would give the largest correlation function. At scales
.30h −1 kpc the deviations of the AP3M3 from the ART1 or the AP3M5 runs
are ≈100–200%. We attribute these deviations to the numerical effects: the high
force resolution in AP3 M3 was not adequately supported by the time integration.
In other words, the AP3 M3 had too few time steps. Note that it had quite a large

436

Numerical simulations in cosmology

Figure 15.6. Density profiles of four largest halos in simulations of Knebe et al (1999).
Note that the AP3 M3 run has formally the best force resolution, but its actual resolution
was much lower because of an insufficient number of steps.

number of steps (6000), not much smaller than the AP3 M5 (8000). But for its
force resolution, it should have many more steps. The lack of the number of steps
was devastating.
Figure 15.6 presents the density profiles of four of the most massive halos
in our simulations. We have not shown the profile of the most massive halo
because it appears to have undergone a recent major merger and is not very
relaxed. In this figure, we present only profiles of halos in the high-resolution
runs. Not surprisingly, the inner density of the PM halos is much smaller than
in the high-resolution runs and their profiles deviate strongly from the profiles of
high-resolution halos at the scales shown in figure 15.6. A glance at figure 15.6
shows that all profiles agree well at r & 30h −1 kpc. This scale is about eight times
smaller than the mean inter-particle separation. Thus, despite the very different

Methods

437

resolutions, time steps and numerical techniques used for the simulations, the
convergence is observed at a scale much lower than the mean inter-particle
separation, argued by Splinter et al (1998) to be the smallest trustworthy scale.
Nevertheless, there are systematic differences between the runs. The profiles
in two ART runs are identical within the errors indicating convergence (we have
run an additional simulation with time steps twice as small as those in the ART1
finding no difference in the density profiles). Among the AP3 M runs, the profiles
of the AP3 M1 and AP3 M5 are closer to the density profiles of the ART halos than
the rest. The AP3 M2 , AP3 M3 and AP3 M4 , despite the higher force resolution,
exhibit lower densities in the halo cores, the AP3 M3 and AP3 M4 runs being the
most deviant.
These results can be interpreted, if we examine the trend of the central
density, as a function of the ratio of the number of time steps to the dynamic range
of the simulations (see table 15.2). The ratio is smaller when either the number
of steps is smaller or the force resolution is higher. The agreement in the density
profiles is observed when this ratio is & 2. This suggests that for a fixed number of
time steps, there should be a limit on the force resolution. Conversely, for a given
force resolution, there is a lower limit on the required number of time steps. The
exact requirements would probably depend on the code type and the integration
scheme. For the AP3 M code our results suggest that the ratio of the number of
time steps to the dynamic range should be no less than one. It is interesting that
the deviations in the density profiles are similar to and are observed at the same
scales as the deviations in the DM correlation function (figure 15.5), suggesting
that the correlation function is sensitive to the central density distribution of dark
matter halos.
15.2.6 Halo identification
Finding halos in dense environments is a challenge. Some of the problems that
any halo-finding algorithm faces are not numerical. They exist in the real universe.
We select a few typical difficult situations.
(1) A large galaxy with a small satellite. Examples: LMC and the Milky Way or
the M51 system. Assuming that the satellite is bound, do we have to include
the mass of the satellite in the mass of the large galaxy? If we do, then we
count the mass of the satellite twice: once when we find the satellite and then
when we find the large galaxy. This does not seem reasonable. If we do not
include the satellite, then the mass of the large galaxy is underestimated. For
example, the binding energy of a particle at the distance of the satellite will
be wrong. The problem arises when we try to assign particles to different
halos in an effort to find the masses of halos. This is very difficult to do
for particles moving between halos. Even if a particle at some moment has
negative energy relative to one of the halos, it is not guaranteed that it belongs
to the halo. The gravitational potential changes with time, and the particle
may end up falling onto another halo. This is not just a precaution. This

438

Numerical simulations in cosmology

actually was found very often in real halos when we compared the contents of
halos at different redshifts. Interacting halos exchange mass and lose mass.
We try to avoid the situation: instead of
√assigning mass to halos, we find the
maximum of the ‘rotational velocity’, G M/R, which, observationally, is a
more meaningful quantity.
(2) A satellite of a large galaxy. The previous situation is now viewed from a
different angle. How can we estimate the mass or the rotational velocity of
the satellite? The formal virial radius of the satellite is large: the big galaxy
is within the radius. The rotational velocity may rise all the way to the centre
of the large galaxy. In order to find the outer radius of the satellite, we
analyse the density profile. At small distances from the centre of the satellite
the density steeply declines, but then it flattens out and may even increase.
This means that we have reached the outer border of the satellite. We use
the radius at which the density starts to flatten out as the first approximation
for the radius of the halo. This approximation can be improved by removing
unbound particles and checking the steepness of the density profile in the
outer part.
(3) Tidal stripping. Peripheral parts of galaxies, responsible for extended flat
rotation curves outside of clusters, are very likely tidally stripped and lost
when the galaxies fall into a cluster. The same happens with halos: a
large fraction of the halo mass may be lost due to stripping in dense cluster
environments. Thus, if an algorithm finds that 90% of the mass of a halo
identified at an early epoch is lost, it does not mean that the halo was
destroyed. This is not a numerical effect and is not due to ‘lack of physics’.
This is a normal situation. What is left of the halo, given that it still has a
large enough mass and radius, is a ‘galaxy’.
There are different methods of identifying collapsed objects (halos) in
numerical simulations.
The Friends-Of-Friends (FOF) algorithm was used a lot and still has its
adepts. If we imagine that each particle is surrounded by a sphere of radius
bd/2, then every connected group of particles is identified as a halo. Here d
is the mean distance between particles, and b is called the linking parameter,
which typically is 0.2. The dependence of groups on b is extremely strong.
The method stems from an old idea of using percolation theory to discriminate
between cosmological models. Because of this, FOF is also called the percolation
method, which is wrong because the percolation is about groups spanning the
whole box, not collapsed and compact objects. FOF was criticized for failing to
find separate groups in cases when those groups were obviously present (Gelb
1992). The problem originates from the tendency of FOF to ‘percolate’ through
bridges connecting interacting galaxies or galaxies in high-density backgrounds.
DENMAX tried to overcome the problems of FOF by dealing with density
maxima (Gelb 1992, Bertschinger and Gelb 1991). It finds the maxima of density
and then tries to identify particles, which belong to each maximum (halo). The

Spatial and velocity biases

439

procedure is quite complicated. First, the density field is constructed. Second,
the density (with a negative sign) is treated as a potential in which particles start
to move as in a viscous fluid. Eventually, particles sink to the bottom of the
potentials (which are also maxima density). Third, only particles with negative
energy (relative to their group) are retained. Just as in the case of FOF, we can
easily imagine situations when (this time) DENMAX should fail; for example,
two colliding galaxies in a cluster of galaxies. They should just pass each other
because of large relative velocity. In the moment of collision DENMAX ceases to
‘see’ both galaxies because all particles have positive energies. This is probably
a quite unlikely situation. The method is definitely one of the best at present.
The only problem is that it seems to be too complicated for the present state of
simulations. DENMAX has two siblings—SKID (Stadel et al) and BDM (Klypin
and Holtzman 1997)—which are frequently used.
‘Overdensity 200’. There is no name for this method, but it is often used.
Find the density maximum, place a sphere and find the radius, within which the
sphere has the mean overdensity 200 (or 177 if you really want to follow the
top-hat model of nonlinear collapse).

15.3 Spatial and velocity biases
15.3.1 Introduction
The distribution of galaxies is probably biased with respect to the dark matter.
Therefore, galaxies can be used to probe the matter distribution only if we
understand the bias. Although the problem of bias has been studied extensively
in the past (e.g. Kaiser 1984, Davis et al 1985, Dekel and Silk 1986), new data
on high redshift clustering and the anticipation of coming measurements have
recently generated substantial theoretical progress in the field. The breakthrough
in an analytical treatment of the bias was the paper by Mo and White (1996),
who showed how bias can be predicted in the framework of the extended
Press–Schechter approximation. A more elaborate analytical treatment has been
developed by Catelan et al (1998a, b), Porciani et al (1998) and Sheth and Lemson
(1999). The effects of nonlinearity and stochasticity were considered in Dekel and
Lahav (1999) (see also Taruya and Suto 2000).
Valuable results are produced by ‘hybrid’ numerical methods in which lowresolution N-body simulations (typical resolution ∼20 kpc) are combined with
semi-analytical models of galaxy formation (e.g. Diaferio et al 1999, Benson et al
2000, Somerville et al 2001). Typically, the results of these studies are very close
to those obtained with brute-force approach of high-resolution (.2 kpc) N-body
simulations (e.g. Colı́n et al 1999, Ghigna et al 1998). This agreement is quite
remarkable because the methods are very different. It may indicate that the biases
of galaxy-size objects are controlled by the random nature of the clustering and
merging of galaxies and by dynamical effects, which cause the merging, because
those are the only common effects in those two approaches.

440

Numerical simulations in cosmology

Direct N-body simulations can be used for studies of the biases only if
they have very high mass and force resolution. Because of numerous numerical
effects, halos in low-resolution simulations do not survive in dense environments
of clusters and groups (e.g. Moore et al 1996, Tormen et al 1998, Klypin et al
1999a). Estimates of the necessary resolution are given in Klypin et al (1999a).
Indeed, recent simulations, which have sufficient resolution, have found hundreds
of galaxy-size halos moving inside clusters (Ghigna et al 1998, Colı́n et al 1999,
Moore et al 1999, Okamoto and Habe 1999).
It is very difficult to make accurate and trustworthy predictions of
luminosities for galaxies, which should be hosted by dark matter halos. Instead
of luminosities or virial masses we suggest using circular velocities Vc for
both numerical and observational data. For a real galaxy its luminosity tightly
correlates with the circular velocity. So, one has a good idea what the circular
velocity of the galaxy is. Nevertheless, direct measurements of circular velocities
of a large complete sample of galaxies are extremely important because it will
provide a direct way of comparing theory and observations. This chapter is mostly
based on results presented in Colı́n et al (1999, 2000) and Kravtsov and Klypin
(1999).
15.3.2 Oh, bias, bias
There are numerous aspects and notions related with the bias. One should be
really careful to understand what type of bias is used. Results can be dramatically
different. We start by introducing the overdensity field. If ρ̄ is the mean density of
some component (e.g. the dark matter or halos), then for each point x in space we
have δ(x) ≡ [ρ(x) − ρ̄]/ρ̄. The overdensity can be decomposed into the Fourier
spectrum, for which we can find the power spectrum P(k) = |δk |2 . We can then
find the correlation function ξ(r ) and the rms fluctuation of δ(R) smoothed on a
given scale R. We can construct the statistics for each component: dark matter,
galaxies or halos with given properties. Each statistics gives its own definition of
bias b:
Ph (k) = b2P Ph (k),

ξh (r ) = bξ2 ξdm (r ),

δh (R) = bδ δdm (R).

(15.15)

The three estimates of the bias b are related. In a special case, when the
bias is linear, local, and scale independent all three forms of bias are all equal.
In general case they are different and they are complicated nonlinear functions of
scale, mass of the halos or galaxies and redshift. The dependence on the scale
is not local in the sense that the bias in a given position in space may depend
on environment (e.g. density and velocity dispersion) on a larger scale. Bias has
memory: it depends on the local history of the fluctuations. There is another
complication: bias very likely is not a deterministic function. One source of this
stochasticity is that it is non-local. Dependence on the history of clustering may
also introduce some random effect.

Spatial and velocity biases

441

There are some processes which we know create and affect the bias. At
high redshifts there is statistical bias: in a Gaussian correlated field, high-density
regions are more clustered than the field itself (Kaiser 1984). Mo and White
(1996) showed how the extended Press–Schechter formalism can be used to derive
of the bias of the dark matter halos. In the limit of small perturbations on large
scales the bias is (Catelan et al 1998b, Taruya and Suto 2000)
b(M, z, z f ) = 1 +

ν2 − 1
.
δc (z, z f )

(15.16)

Here ν = δc (z, z f )/σ (M, z) is the relative amplitude of a fluctuation on scale
M in units of the rms fluctuation σ (M, z) of the density field at redshift z.
The parameter z f is the redshift of halo formation. The critical threshold of
the top-hat model is δc (z, z f ) = δc,0 D(z)/D(z f ), where D is the growth factor
of perturbations and δc,0 = 1.69. At high redshifts, parameter ν for galaxysize fluctuations is very large and δc is small. As a result, galaxy-size halos are
expected to be more clustered (strongly biases) compared to the dark matter. The
bias is larger for more massive objects. As the fluctuations grow, newly formed
galaxy-size halos do not have such high peaks as at large redshifts and the bias
tends to decrease. It also loses its sensitivity.
At later stages another process starts to change the bias. In group and cluster
progenitors the merging and destruction of halos reduces the number of halos.
This does not happen in the field where the number of halos of given mass may
only increase with time. As a result, the number of halos inside groups and cluster
progenitors is reduced relative to the field. This produces (anti)bias: there is a
relatively smaller number of halos compared with the dark matter. This merging
bias does not depend on the mass of halos and it has a tendency to slow down
once a group becomes a cluster with a large relative velocity of halos (Kravtsov
and Klypin 1999).
Here is a list of different types of bias. We classify them into three groups:
(1) measures of bias, (2) terms related with the description of biases and (3)
physical processes, which produce or change the bias.
15.3.2.1 Measures of bias
(i) Bias measured in a statistical sense (e.g. ratio of correlation functions
ξh (r ) = b 2 ξdm (r )).
(ii) Bias measured point-by-point (e.g. δh (x) − δdm (x) diagrams).
15.3.2.2 Description of biases
(i) Local and non-local bias. For example, b(R) = σh (R)/σm (R) is the local
bias. If b = b(R; R̃), the bias is non-local, where R̃ is some other scale or
scales.

442

Numerical simulations in cosmology

(ii) Linear and nonlinear bias. If in ξh (r ) = b 2 ξdm (r ) the bias b does not depend
on ξdm , it is the linear bias.
(iii) Scale-dependent and scale-independent bias. If b does not depend on the
scale at which the bias is estimated, the bias is scale independent. Note that,
in general, the bias can be nonlinear and scale independent, but this highly
unlikely.
(iv) Stochastic and deterministic.
15.3.2.3 Physical processes, which produce or change the bias
(i) Statistical bias. This arises when a specific subset of points is selected from
a Gaussian field.
(ii) Merging bias. This is produced due to merging and destruction of halos.
(iii) Physical bias. This includes any bias due to physical processes inside
forming galaxies.
15.3.3 Spatial bias
Colı́n et al (1999) have simulated different cosmological models and, using
the simulations, have studied halo biases. Most of the results presented here
are for the currently favoured CDM model with the following parameters:
0 = 1 −  = 0.3, h = 0.7, b = 0.032, σ8 = 1. The model was simulated
with 2563 particles in a 60h −1 Mpc box. The formal mass and force resolutions
are m 1 = 1.1 × 109 h −1 M and 2h −1 kpc. The Bound Density Maximum halo
finder was used to identify halos with at least√30 bound particles. For each halo
we find the maximum circular velocity Vc = G M(< r )/r.
In figure 15.7 we compare the evolution of the correlation functions of the
dark matter and halos. There are remarkable differences between the halos and the
dark matter. The correlation functions of the dark matter always increases with
time (but the rate is different on different scales) and it is never a power law. The
correlation function of the halos at redshifts decreases and then starts to increase
again. It is accurately described by a power law with slope γ = (1.5–1.7).
Figure 15.8 presents a comparison of the theoretical and observational data on
correlation functions and power spectra. The dark matter clearly predicts much
too high a clustering amplitude. The halos are much closer to the observational
points and predict antibias. For the correlation function the antibias appears on
scales r < 5h −1 Mpc; for the power spectrum the scales are k > 0.2h Mpc−1 .
One may get an impression that the antibias starts at longer waves in the power
spectrum λ = 2π/k ≈ 30h −1 Mpc compared with r ≈ 5h −1 Mpc in the
correlation function. There is no contradiction: sharp bias at small distances in the
correlation function when Fourier transformed to the power spectrum produces
antibias at very small wavenumbers. Thus, the bias should be taken into account
at long waves when dealing with the power spectra. There is an inflection point
in the power spectrum where the nonlinear power spectrum start to go upward (if

Spatial and velocity biases

443

Figure 15.7. Evolution of the correlation function of the dark matter and halos. The
correlation function of the dark matter increases monotonically with time. At any given
moment it is not a power law. The correlation function of halos is a power law, but it is not
monotonic in time.

one moves from low to high k) compared with the prediction of the linear theory.
The exact position of this point may have been affected by the finite size of the
simulation box kmin = 0.105h −1 Mpc, but the effect is expected to be small.
At z = 0 the bias hardly depends on the mass limit of the halos. There is
a tendency of more massive halos to be more clustered at very small distances
r < 200h −1 kpc, but at this stage it is not clear that this is not due to residual
numerical effects around centres of clusters. The situation is different at high
redshift. At very high redshifts z > 3 galaxy-size halos are very strongly
(positively) biased. For example, at z = 5 the correlation function of halos

444

Numerical simulations in cosmology

Figure 15.8. The correlation function and the power spectrum of halos with different
limiting circular velocities in the CDM model. The results are compared with the
observational data from the APM and Stromlo–APM surveys. The bias is scale dependent
but it does not depend much on the halo mass.

Spatial and velocity biases

445

Figure 15.9. Top panel: The evolution of bias at comoving scale of 0.54h −1 Mpc for halos
with different circular velocities. Bottom panel: Dependence of the bias on the scale for
halos with the same circular velocity.

with vc > 150 km s−1 was 15 times larger than that of the dark matter at
r = 0.5h −1 Mpc (see figure 8 in Colı́n et al (1999). The bias was also very
strongly mass-dependent with more massive halos being more clustered. At
smaller redshifts the bias was declining quickly. Around z = 1–2 (the exact
value depends on the halo circular velocity) the bias crossed unity and became
less than unity (antibias) at later redshifts.
The evolution of bias is illustrated by figure 15.10. The figure shows that,
at all epochs, the overdensity of halos tightly correlates with the overdensity of
the dark matter. The slope of the relation depends on the dark matter density and
evolves with time. At z > 1 halos are biased (δh > δdm ) in overdense regions with

446

Numerical simulations in cosmology

Figure 15.10. Overdensity of halos δh versus the overdensity of the dark matter δdm .
The overdensities are estimated in spheres of radius RTH = 5h −1 Mpc. The intensity
of the grey shade corresponds to the natural logarithm of the number of spheres in a
two-dimensional grid in δh –δdm space. The full curves show the average relation. The
chain curve is a prediction of an analytical model, which assumes that formation redshift
z f of halos coincides with observation redshift (typical assumption for the Press–Schechter
approximation). The long-dashed curve is for a model, which assumes that the substructure
survives for some time after it falls into a larger object: z f = z + 1.

δdm > 1 and antibiased in underdense regions with δdm < −0.5 At low redshifts
there is an antibias at large overdensities and almost no bias at low densities.
Figure 15.11 shows the density profiles for a cluster with mass 2.5 ×
1014h −1 M . There is antibias on scales below 300h −1 kpc. This is an example
of the merging and destruction bias. Some of the halos have merged or were
destroyed by the central cD halo of the cluster. As the result, there is a smaller
number of halos in the central part compared with what we would expect if the
number density of halos had followed the density of the dark matter (the full curve

Spatial and velocity biases

447

Figure 15.11. Density profiles for a cluster with mass 2.5 × 1014 h −1 M . Top panel:
Dark matter density in units of the mean matter density at z = 0 (full curve) and at z = 1
(chain curve). The Navarro–Frenk–White profile (broken curve) provides a very good fit at
z = 0. The z = 1 profile is given in proper (not comoving) units. Bottom panel: Number
density profiles of halos in the cluster at z = 0 (full circles) and at z = 1 (open circles)
compared with the z = 0 dark matter profile (full curve). There is antibias on scales below
300h −1 kpc.

in the bottom panel). Note that, in the outer parts of the cluster, the halos closely
follow the dark matter.
15.3.4 Velocity bias
There are two statistics, which measure velocity biases—differences in velocities
of the galaxies (halos) and the dark matter. For a review of the results and

448

Numerical simulations in cosmology

(a)

(b)
Figure 15.12. (a) Two-point velocity bias. (b) Top panel: 3D rms velocity for halos
(circles) and for dark matter (full curve) in the 12 largest clusters. Bottom panel: velocity
bias in the clusters. The bias in the first point increases to 1.2 if the central cD halos
are excluded from analysis. Errors correspond to 1-sigma errors of the mean obtained by
averaging over 12 clusters at two moments of time. Fluctuations for individual clusters are
larger.

Spatial and velocity biases

449

references see Colı́n et al (2000). Two-particle or pairwise velocity bias (PVB)
measures the relative velocity dispersion in pairs of objects with given separation
r : b12 = σh−h (r )/σdm−dm (r ). Figure 15.12 (left-hand panel) shows this bias. It
is very sensitive to the number of pairs inside clusters of galaxies, where relative
velocities are largest. Removal of a few pairs can substantially change the value
of the bias. This ‘removal’ happens when halos merge or are destroyed by central
cluster halos.
The one-point velocity bias is estimated as a ratio of the rms velocity of
halos to that of the dark matter: b1 = σh /σdm . It is typically applied to clusters
of galaxies where it is measured at different distances from the cluster centre.
For an analysis of the velocity bias in clusters, Colı́n et al (2000) have selected
the 12 most massive clusters in a simulation of the CDM model. The most
massive cluster had virial mass 6.5 × 1014h −1 M comparable to that of the
Coma cluster. The cluster had 246 halos with circular velocities larger than
90 km s−1 . There were three Virgo-type clusters with virial masses in the range
(1.6–2.4) × 1014 h −1 M and with approximately 100 halos in each cluster. Just
like the spatial bias, the PVB is positive at large redshifts (except for the very
small scales) and decreases with the redshift. At lower redshifts it does not
evolve much and stays below unity (antibias) at scales below 5h −1 Mpc on level
b12 ≈ (0.6–0.8).
Figure 15.13 shows the one-point velocity bias in clusters at z = 0. Note
that the sign of the bias is now different: the halos move slightly faster than the
dark matter. The bias is stronger in the central parts (b1 = 1.2–1.3) and goes
to almost no bias (b1 ≈ 1) at the virial radius and above. Both the antibias
in the pairwise velocities and positive one-point bias are produced by the same
physical process—merging and destruction of halos in the central parts of groups
and clusters. The difference is in the different weighting of halos in these two
statistics. A smaller number of high-velocity pairs significantly changes the PVB,
but it only slightly affects the one-point bias because it is normalized to the
number of halos at a given distance from the cluster centre. At the same time,
merging preferentially happens for halos, which move with a smaller velocity at a
given distance from the cluster centre. Slower halos have shorter dynamical times
and have smaller apocentres. Thus, they have a better chance to be destroyed
and merge with the central cD halo. Because low-velocity halos are eaten up by
the central cD, the velocity dispersion of those which survive is larger. Another
way of addressing the issue of velocity bias is to use the Jeans equations. If we
have a tracer population, which is in equilibrium in a potential produced by mass
M(< r ), then


d ln σr2 (r ) d ln ρ(r )
2
+
+ 2β(r ) = G M(< r ),
(15.17)
−r σr (r )
d ln r
d ln r
where ρ is the number density of the tracer, β is the velocity anisotropy, and σr
is the rms radial velocity. The right-hand side of the equation is the same for

450

Numerical simulations in cosmology

Figure 15.13. One-point velocity bias for three Virgo-type clusters in the simulation.
Central cD halos are not included. Fluctuations in the bias are very large because each
cluster has only ∼100 halos with Vc > 90 km s−1 and because of substantial substructure
in the clusters.

the dark matter and the halos. If the term in the brackets were to be the same,
there would be no velocity bias. But there is systematic difference between the
halos and the dark matter: the slope of the distribution halos in a cluster d dlnlnρ(r)
r is
smaller than that of the dark matter (see Colı́n et al 1999, Ghigna et al 2000). The
reason for the difference in the slopes is the same—merging with the central cD.
Other terms in the equation also have small differences but the main contribution
comes from the slope of the density. Thus, as long as we have spatial antibias of
the halos, there should be a small positive one-point velocity bias in the clusters
and a very strong antibias in the pairwise velocity. The exact values of the biases
are still under debate, but one thing seems to be certain: the two biases go hand
in hand.

Dark matter halos

451

The velocity bias in clusters is difficult to measure because it is small.
Figure 15.12 may be misleading because it shows the average trend but it does not
give the level of fluctuations for a single cluster. Note that the errors in the plots
correspond to the error of the mean obtained by averaging over 12 clusters and
two close moments of time. The fluctuations for a single cluster are much larger.
Figure 15.12 shows results for three Virgo-type clusters in the simulation. The
noise is very large both because of poor statistics (small number of halos) and the
noise produced by residual non-equilibrium effects (substructure). A comparable
(but slightly smaller) value of bv was recently found in simulations by Ghigna et al
(1999) for a cluster in the same mass range as that in figure 15.12. Unfortunately,
it is difficult to make a detailed comparison with their results because Ghigna et
al (1999) use only one hand-picked cluster for a different cosmological model.
Very likely their results are dominated by the noise due to residual substructure.
The results of another high-resolution simulation by Okamoto and Habe (1999)
are consistent with our results.
15.3.5 Conclusions
There are a number of physical processes which can contribute to the biases.
In this contribution we explore the dynamical effects in the dark matter itself,
which result in differences in the spatial and velocity distribution of the halos and
the dark matter. Other effects related to the formation of the luminous parts of
galaxies can also produce or change biases. At this stage it is not clear how strong
these biases are. Because there is a tight correlation between the luminosity and
circular velocity of galaxies, any additional biases are limited by the fact that
galaxies ‘know’ how much dark matter they have.
Biases in the halos are reasonably well understood and can be approximated
on a few megaparsec scales by analytical models. We find that the biases in the
distribution of the halos are sufficient to explain within the framework of standard
cosmological models the clustering properties of galaxies on a vast ranges of
scales from 100 kpc to dozens of megaparsecs. Thus, there is neither need nor
much room for additional biases in the standard cosmological model.
In any case, biases in the halos should be treated as benchmarks for more
complicated models, which include non-gravitational physics. If a model cannot
reproduce the biases of halos or it does not have enough halos, it should be
rejected, because it fails to give the correct dynamics for the main component
of the universe—the dark matter.

15.4 Dark matter halos
15.4.1 Introduction
During the last decade there has been an increasing interest in testing the
predictions of variants of the cold dark matter (CDM) models at sub-galactic

452

Numerical simulations in cosmology

(.100 kpc) scales. This interest was first induced by indications that the observed
rotation curves in the central regions of dark-matter-dominated dwarf galaxies
are at odds with predictions of hierarchical models. Specifically, it was argued
(Moore 1994, Flores and Primack 1994) that the circular velocities, vc (r ) ≡
[G M(< r )/r ]1/2, at small galactocentric radii predicted by the models are too
high and increase too rapidly with increasing radius compared to the observed
rotation curves. The steeper than expected rise in vc (r ) implies that the shape
of the predicted halo density distribution is incorrect and/or that the DM halos
formed in CDM models are too concentrated (i.e. have too much of their mass
concentrated in the inner regions).
In addition to the density profiles, there is an alarming mismatch in the
predicted abundance of small-mass (.108 –109h −1 M ) galactic satellites and the
observed number of satellites in the Local Group (Kauffmann et al 1993, Klypin
et al 1999b, Moore et al 1999). Although this discrepancy may well be due
to feedback processes such as photoionization that prevent gas collapse and star
formation in the majority of the small-mass satellites (e.g. Bullock et al 2000),
the mass scale at which the problem sets in is similar to the scale in the spectrum
of primordial fluctuations that may be responsible for the problems with density
profiles. In the age of precision cosmology that the forthcoming MAP and Planck
cosmic microwave background anisotropy satellite missions are expected to bring
about, tests of the cosmological models at small scales may prove to be the final
frontier and the ultimate challenge to our understanding of the cosmology and
structure formation in the universe. However, this obviously requires detailed
predictions and checks from the theoretical side and higher resolution/quality
observations and thorough understanding of their implications and associated
caveats from the observational side. In this section we focus on the theoretical
predictions of the density distribution of DM halos and some problems with
comparing these predictions to observations.
A systematic study of halo density profiles for a wide range of halo masses
and cosmologies was carried out by Navarro et al (1996, 1997; hereafter NFW),
who argued that an analytical profile of the form ρ(r ) = ρs (r/rs )−1 (1 + r/rs )−2
provides a good description of halo profiles in their simulations for all halo
masses and in all cosmologies. Here, rs is the scale radius which, for this profile
corresponds to the scale at which d log ρ(r )/d log r |r=rs = −2. The parameters
of the profile are determined by the halo’s virial mass Mvir and concentration
defined as c ≡ rvir /rs . NFW argued that there is a tight correlation between c
and Mvir , which implies that the density distributions of halos of different masses
can, in fact, be described by a one-parameter family of analytical profiles. Further
studies by Kravtsov et al (1997, 1999), Jing (2000) and Bullock et al (2001),
although confirming the c(Mvir ) correlation, indicated that there is a significant
scatter in the density profiles and concentrations for DM halos of a given mass.
Following the initial studies by Moore (1994) and Flores and Primack
(1994), Kravtsov et al (1999) presented a systematic comparison of the results
of numerical simulations with rotation curves of a sample of 17 DM-dominated

Dark matter halos

453

dwarf and low-surface-brightness (LSB) galaxies. Based on these comparisons,
we argued that there does not seem to be a significant discrepancy in the shape
of the density profiles at the scales probed by the numerical simulations (&0.02–
0.03rvir, where rvir is the halo’s virial radius). However, these conclusions were
subject to several caveats and had to be tested. First, the observed galactic rotation
curves had to be re-examined more carefully and with higher resolution. The fact
that all of the observed rotation curves used in earlier analyses were obtained
using relatively low-resolution HI observations, required checks of the possible
beam smearing effects. Also, the possibility of non-circular random motions
in the central regions that could modify the rotation velocity of the gas (e.g.
Binney and Tremain 1987, p 198) had to be considered. Second, the theoretical
predictions had to be tested for convergence and extended to scales .0.01rvir.
Moore et al (1998; see also a more recent convergence study by Ghigna
et al 2000) presented a convergence study and argued that the mass resolution
has a significant impact on the central density distribution of halos. They argued
that at least several million particles per halo are required to model the density
profiles at scales .0.01rvir reliably. Based on these results, Moore et al (1999)
advocated a density profile of the form ρ(r ) ∝ (r/r0 )−1.5 [1 + (r/r0 )1.5 ]−1 ,
that behaves similarly (ρ ∝ r −3 ) to the NFW profile at large radii, but is
steeper at small r : ρ ∝ r −1.5 . Most recently, Jing and Suto (2000) presented a
systematic study of density profiles for halo masses ranging from 2×1012h −1 M
to 5 × 1014 h −1 M . The study was uniform in mass and force resolution
featuring ∼(5–10) × 105 particles per halo and a force resolution of ∼0.004rvir.
They found that the galaxy-mass halos in their simulations are well fitted by
profile† ρ(r ) ∝ (r/r0 )−1.5 [1 + r/r0 ]−1.5 , but that cluster-mass halos are well
described by the NFW profile, with a logarithmic slope of the density profiles
at r = 0.01rvir changing from ≈ − 1.5 for Mvir ∼ 1012h −1 M to ≈ − 1.1 for
Mvir ∼ 5 × 1014h −1 M . Jing and Suto interpreted these results as evidence that
the profiles of DM halos are not universal.
The rotation curves of a number of dwarf and LSB galaxies have recently
been reconsidered using Hα observations (e.g. Swaters et al 2000, van den Bosch
et al 2000). The results show that, for some galaxies, Hα rotation curves are
significantly different in their central regions than the rotation curves derived from
HI observations. This indicates that the HI rotation curves are affected by beam
smearing (Swaters et al 2000). It is also possible that some of the difference
may be due to real differences in the kinematics of the two tracer gas components
(ionized and neutral hydrogen). Preliminary comparisons of the new Hα rotation
curves with model predictions show that the NFW density profiles are consistent
with the observed shapes of the rotation curves (van den Bosch et al 2000).
Moreover, cusp density profiles with inner logarithmic slopes as steep as ∼ − 1.5
also seem to be consistent with the data (van den Bosch et al 2000). Nevertheless,
† Note that their profile is somewhat different from the profile advocated by Moore et al, but behaves
similarly to the latter at small radii.

454

Numerical simulations in cosmology

CDM halos appear to be too concentrated (Navarro and Swaters 2000, McGaugh
et al 2000) compared to galactic halos and therefore the problem remains.
New observational and theoretical developments show that a comparison of
model predictions to the data is not straightforward. Decisive comparisons require
the convergence of theoretical predictions and understanding the kinematics of
the gas in the central regions of the observed galaxies. In this section we present
convergence tests designed to test the effects of mass resolution on the density
profiles of halos formed in the currently popular CDM model with cosmological
constant (CDM) and simulated using the multiple mass resolution version of
the Adaptive Refinement Tree code (ART). We also discuss some caveats in
drawing conclusions about the density profiles from the fits of analytical functions
to numerical results and their comparisons to the data.
15.4.2 Dark matter halos: the NFW and the Moore et al profiles
Before we fit the analytical profiles to real dark matter halos or compare them
with observed rotational curves, it is instructive to compare different analytical
approximations. Although the NFW and Moore et al profiles predict different
behaviour for ρ(r ) in the central regions of a halo, the scale where this difference
becomes significant depends on the specific values of the halo’s characteristic
density and radius. Table 15.3 presents the different parameters and statistics
associated with the two analytical profiles. For the NFW profile more information
can be found in Klypin et al (1999a, b, 2001), Lokas and Mamon (2000) and in
Widrow (2000).
Each profile is set by two independent parameters. We choose these to be
the characteristic density ρ0 and radius rs . In this case all expressions describing
the different properties of the profiles have a simple form and do not depend
on the concentration. The concentration or the virial mass appears only in the
normalization of the expressions. The choice of the virial radius (e.g. Lokas and
Mamon 2000) as a scale unit results in more complicated expressions with an
explicit dependence on the concentration. In this case, one also has to be careful
about the definition of the virial radius, as there are several different definitions in
the literature. For example, it is often defined as the radius, r200 , within which the
average density is 200 times the critical density. In this section the virial radius
is defined as the radius within which the average density is equal to the density
predicted by the top-hat model: it is δTH times the average matter density in the
universe. For the 0 = 1 case the two existing definitions are equivalent. In the
case of 0 = 0.3 models, however, the virial radius is about 30% larger than r200 .
There is no unique way of defining a consistent concentration for the
different analytical profiles. Again, it is natural to use the characteristic radius
rs to define the concentration: c ≡ rvir /rs . This simplifies the expressions. At
the same time, if we fit the same dark matter halo with the two profiles, we will
get different concentrations because the values of the corresponding rs will be
different. Alternatively, if we choose to match the outer parts of the profiles

Dark matter halos

455

Table 15.3. Comparison of the NFW and Moore et al profiles.
Parameter
Density
x = r/rs

NFW

Moore et al

ρ0
ρ=
x(1+x)2
ρ ∝ x −3 for x  1

ρ=

ρ0
x 1.5 (1+x)1.5
−3
ρ ∝ x for x  1

ρ ∝ x −1 for x  1

ρ ∝ x −1.5 for x  1

ρ/ρ0 = 1/4.00 at x = 1

ρ/ρ0 = 1/2.00 at x = 1

ρ/ρ0 = 1/21.3 at x = 2.15

ρ/ρ0 = 1/3.35 at x = 1.25

Mass
M = 4πρ0 rs3 f (x)

x
f (x) = ln(1 + x) − 1+x

f (x) = 23 ln(1 + x 3/2 )

CNFW = 1.72CMoore

CMoore = CNFW /1.72

M = Mvir f (x)/ f (C)
3
Mvir = 43 πρcr 0 δTH rvir

Concentration
C = rvir /rs

(for the same Mvir and rmax )
C

C1/5 ≈ 0.86 f (C NFW)+0.1363
NFW

C1/5 =

(error <3% for CNFW = 5 − 30) C0.0 ≈
Cγ =−2 = CNFW

CMoore

3/2

[(1+CMoore )1/5 −1]2/3
CMoore
3/10

[CMoore −1]2/3
Cγ =−2 = 23/2 CMoore

Cγ =−2 =≈ 2.83CMoore
Circular velocity
(x)
vc2 = r vir Cx ff (C)
vir
2 xmax f (x)
vc2 = vmax
x f (xmax )
2 = G Mvir
vvir
r
GM

xmax ≈ 2.15

xmax ≈ 1.25

C
2
2
vmax
≈ 0.216vvir
f (C)

C
2
2
vmax
≈ 0.466vvir
f (C)

vir

(say, r > rs ) as closely as possible, we may choose to change the ratio of the
characteristic radii rs,NFW /rs,Moore in such a way that both profiles reach the
maximum circular velocity vc at the same physical radius rmax . In this case, the
formal concentration of the Moore et al profile is 1.72 times smaller than that of
the NFW profile. Indeed, with this normalization the profiles look very similar
in the outer parts as one finds in figure 15.14. Table 15.3 also gives two other
‘concentrations’. The concentration C1/5 is defined as the ratio of virial radius
to the radius, which encompasses one-fifth of the virial mass (Avila-Reese et al
1999). For halos with CNFW ≈ 5.5 this one-fifth mass concentration is equal to
CNFW . One can also define the concentration as the ratio of the virial radius to
the radius at which the logarithmic slope of the density profile is equal to −2.
This scale corresponds to rs for the NFW profile and ≈0.35rs for the Moore et al
profile.

456

Numerical simulations in cosmology

Figure 15.14. Comparison of the Moore et al and NFW profiles. Each profile is
normalized to have the same virial mass and the same radius of the maximum circular
velocity. Left panels: High-concentration halo with concentrations typical for small
galaxies CNFW = 17. Right panels: Low-concentration halo with concentrations typical
for clusters of galaxies. The deviations are very small (<3%) for radii r > 1/2rs . The top
panels show the local logarithmic slope of the profiles. Note that for the high concentration
halo the slope of the profile is significantly larger than the asymptotic value −1 even at very
small radii r ≈ 0.01/rvir .

Figure 15.14 presents a comparison of the analytic profiles normalized to
have the same virial mass and the same radius rmax . We show the results for
halos with low and high concentration values which are representative of clusterand low-mass galaxy halos, respectively. The bottom panels show the profiles,
while the top panels show the corresponding logarithmic slope as a function of
the radius. The figure shows that the two profiles are very similar throughout the
main body of the halos. Only in the very central region do the differences become
significant. The difference is more apparent in the logarithmic slope than in the

Dark matter halos

457

actual density profiles. Moreover, for galaxy-mass halos the difference sets in
at a rather small radius (.0.01rvir), which would correspond to scales less than
1 kpc for the typical DM-dominated dwarf and LSB galaxies. In most analyses
involving galaxy-size halos, the differences between the NFW and Moore et al
profiles are irrelevant, and the NFW profile should provide an accurate description
of the density distribution.
Note also that for galaxy-size (e.g. high-concentration) halos the logarithmic
slope of the NFW profile does not reach its asymptotic inner value of −1 at scales
as small as 0.01rvir. For ∼ 1012h −1 M halos the logarithmic slope of the NFW
profile is ≈ − 1.4–1.5, while for cluster-size halos this slope is ≈ − 1.2. This
dependence of slope at a given fraction of the virial radius on the virial mass
of the halo is very similar to the results plotted in figure 3 of Jing and Suto
(2000). They interpreted it as evidence that the halo profiles are not universal.
It is obvious, however, that their results are consistent with the NFW profiles and
the dependence of the slope on mass can simply be a manifestation of the wellstudied cvir (M) relation.
To summarize, we find that the differences between the NFW and Moore et
al profiles are very small (ρ/ρ < 10%) for radii above 1% of the virial radius.
The differences are larger for halos with smaller concentrations. For the NFW
profile, the asymptotic value of the central slope γ = −1 is not achieved even at
radii as small as 1–2% of the virial radius.
15.4.3 Properties of dark matter halos
Some properties of halos depend on the large-scale environment in which the
halos are found. We will call a halo distinct if it is not inside a virial radius
of another (larger) halo. A halo is called a sub-halo if it is inside another halo.
The number of sub-halos depends on the mass resolution—the deeper we go,
the more sub-halos we will find. Most of the results given here are based on a
simulation, which was complete to masses down to 1011h −1 M or, equivalently,
to the maximum circular velocity of 100 km s−1 .
15.4.3.1 Mass and velocity distribution functions
The halo mass and velocity function has been extensively analysed by Sigad et
al (2000) for halos in the CDM model. Additional results can also be found in
Ghigna et al (1999), Moore et al (1999), Klypin et al (1999b) and Gottlöber et al
(1998). Figure 15.15 compares the mass function of sub-halos and distinct halos.
The Press–Schechter approximation overestimates the mass function by a factor
of twofor M < 5 × 1012h −1 M and it somewhat underestimates it at larger
masses. A more advanced approximation given by Sheth and Tormen is more
accurate. On scales below 1014h −1 M the mass function is close to a power law
with slope α ≈ −1.8. There is no visible difference in the slope for sub-halos and
for the distinct halos.

458

Numerical simulations in cosmology

Figure 15.15. The mass function for distinct halos (top) and for sub-halos bottom). Raw
counts are marked by symbols with error bars. The curves are Schechter-function fits. The
Press–Schechter (dotted) and Sheth–Tormen (dashes) predictions for distinct halos are also
shown. On scales below 1014 h −1 M the mass function is close to a power law with slope
α ≈ −1.8. There is no visible difference in the slope for sub-halos and that for distinct
halos. (After Sigad et al 2000.)

For each halo one can measure the maximum circular velocity Vmax . In many
cases (especially for sub-halos) Vmax is a better measure of the size of the halo.
It is also related more closely with the observed properties of galaxies hosted
by halos. Figure 15.16 presents the velocity distribution functions of different
types of halo. In addition to distinct halos and sub-halos, we also show isolated
halos and halos in groups and clusters. Here isolated halos are defined as halos
with a mass less than 1013h −1 M , which are not inside a larger halo and which
do not have sub-halos more massive than 1011h −1 M . The velocity function is
β
approximated by a power law dn = ∗ Vmax dVmax with slope β ≈ −3.8 for
distinct halos. The slope depends on the environment: β ≈ −3.1 for halos in
groups and β ≈ −4 for isolated halos. Klypin et al (1999b) and Ghigna et al
(1999) found that the slope β ≈ −3.8–4 of the velocity function extends to much
smaller halos with velocities down to 10 km s−1 .

Dark matter halos

459

Figure 15.16. Velocity functions for isolated halos (squares) and for halos in groups and
clusters. Halos with mass less than 1013 h −1 M are used for the plots. (After Sigad et al
2000.)

15.4.3.2 Correlation between characteristic density and radius
The halo density profiles are approximated by the NFW profile:
ρ=

ρ0
.
(r/r0 )[1 + r/r0 ]2

(15.18)

Kravtsov et al (1999) found the correlation between the two parameters of halos:
ρ0 and rs . Figure 15.17 compares the results for the DM halos with those for
DM-dominated, LSB galaxies and dwarf galaxies. The halos are consistent with
observational data: smaller halos are denser.
15.4.3.3 Correlations between mass, concentration and redshift
Navarro et al (1997) argued that the halo profiles have a universal shape in the
sense that the profile is uniquely defined by the virial mass of the halo. Bullock
et al (2001) analysed concentrations of thousands of halos at different redshifts.
To some degree they confirm the conclusions of Navarro et al (1997): halo
concentration correlates with its mass. However, some significant deviations
were also found. There is no one-to-one relation between concentration and
mass. It appears the the universal profile should only be treated as a trend:
the halo concentration does increase as the halo mass decreases, but there are
large deviations for individual halos from that ‘universal’ shape. Halos have an
intrinsic scatter of concentration: at the 1σ level halos with the same mass have
(log cvir ) = 0.18 or, equivalently, Vmax /Vmax = 0.12.

460

Numerical simulations in cosmology

Figure 15.17. Correlation of the characteristic density ρ0 and radius r0 for the dwarf and
LSB galaxies (full and open circles) and for DM halos (crosses) in different cosmological
models. The halos are consistent with observational data: smaller halos are denser. (After
Kravtsov et al 1999.)

15.4.3.4 Velocity anisotropy
Inside a large halo, sub-halos or DM particles do not move on either circular or
radial orbits. A velocity ellipsoid can be measured at each position inside a halo. It
can be characterized by an anisotropy parameter defined as β(r ) = 1 − V⊥2 /2Vr2 .
Here V⊥2 is the velocity dispersion perpendicular to the radial direction and Vr2
is the radial velocity dispersion. For pure radial motions β = 1. For isotropic
velocities β = 0. The function β(r ) was estimated for halos in different
cosmological models (see Colı́n et al 1999 for references). By studying 12 rich
clusters with many sub-halos inside each of them, Colı́n et al (1999) found that
both the sub-halos and DM particles can be described by the same anisotropy

Dark matter halos

461

(a)

(b)
Figure 15.18. (a) Dependence of concentration with mass for distinct halos. The bold full
curve is the median value. The errors are errors of the mean due to sampling. The outer
chain curves encompass 68% of halos in the simulations. The broken curves and arrows
indicate values corrected for the noise in halo profiles. Thin curves are different analytical
models. (b) Median halo concentration as a function of mass for different redshifts. The
thin lines show the predictions of an analytical model. (After Bullock et al 2001.)

462

Numerical simulations in cosmology

parameter
β(r ) = 0.15 +

2x
,
x2 + 4

x = r/rvir .

(15.19)

15.4.4 Halo profiles: convergence study
The following results are based on Klypin et al (2001).
15.4.4.1 Numerical simulations
Using the ART code (Kravtsov et al 1997, Kravtsov 1999), we simulate a flat
low-density cosmological model (CDM) with 0 = 1 −  = 0.3, the
Hubble parameter (in units of 100 km s−1 Mpc−1 ) h = 0.7, and the spectrum
normalization σ8 = 0.9. We run two sets of simulations with 30h −1 Mpc and
25h −1 Mpc computational box. The first simulations were run to the present
moment z = 0. The second set of simulations had higher mass resolution and
therefore produced more halos but were run only to z = 1.
In all our simulations the step in the expansion parameter was chosen to be
a0 = 2 × 10−3 on the zero level of resolution. This gives about 500 steps for
an entire run to z = 0. A test run was done with a time step twice as small as that
for a halo of comparable mass (but with a smaller number of particles) as studied
in this chapter. We did not find any visible deviations in the halo profile. In the
first set of simulations, the highest refinement level was ten, which corresponds
to 500 × 210 ≈ 500 000 time steps at the tenth level. For the second set of
simulations, nine levels of refinement were reached which corresponds to 128 000
steps at the ninth level.
In the following sections we present the results for four halos. The first halo
(A) was the only halo selected for re-simulation in the first set of simulations. In
this case the selected halo was relatively quiescent at z = 0 and had no massive
neighbours. The halo was located in a long filament bordering a large void. It was
about 10 Mpc away from the nearest cluster-size halo. After the high-resolution
simulation was completed we found that the nearest galaxy-size halo was about
5 Mpc away. The halo had a fairly typical merging history with an M(t) track
slightly lower than the average mass growth predicted using extended Press–
Schechter model. The last major merger event occurred at z ≈ 2.5; at lower
redshifts the mass growth (the mass in this time interval has grown by a factor of
three) was due to slow and steady mass accretion.
The second set of simulations was done in a different way. In the lowresolution run we selected three halos in a well-pronounced filament. Two of
the halos were neighbours located at about 0.5 Mpc from each other. The third
halo was 2 Mpc away from this pair. Thus, the halos were not selected to be
too isolated as was the case in the first set of runs. Moreover, the simulation
was analysed at an earlier moment (z = 1) where halos are more likely to be
unrelaxed. Therefore, we consider the halo A from the first set as an example

Dark matter halos

463

Table 15.4. Parameters of halos.
z

Mvir
M /h
(1) (2) (3)
A1
A2
A3
B
C
D

0
0
0
1
1
1

1.97 × 1012
2.05 × 1012
1.98 × 1012
8.5 × 1011
6.8 × 1011
9.6 × 1011

Rvir
Vmax
Npart
kpc h−1 km s−1
(4)
(5)
(6)
257
261
256
241
208
245

247.0
248.5
250.5
195.4
165.7
202.4

1.2 × 105
1.5 × 104
1.9 × 103
7.1 × 105
5.0 × 105
7.9 × 105

m part
M /h
(7)

Form. res. CNFW RelEr RelEr
kpc h−1
NFW Moore
(8)
(9)
(10) (11)

1.6 × 107
1.3 × 108
1.1 × 109
1.2 × 106
1.2 × 106
1.2 × 106

0.23
0.91
3.66
0.19
0.19
0.19

17.4
16.0
16.6
12.3
11.9
9.5

0.17
0.13
0.16
0.23
0.37
0.25

0.20
0.16
0.10
0.16
0.20
0.60

of a rather isolated well-relaxed halo. In many respects, this halo is similar
to halos simulated by other research groups that used multiple mass resolution
techniques. The three halos from the second set of simulations can be viewed
as being representative of more typical halos, not necessarily well relaxed and
located in more crowded environments.
The parameters of the simulated DM halos are listed in table 15.4. Columns
represent:
(1) the halo ‘name’ (halos A1 , A2 , A3 are halo A re-simulated with different
resolutions);
(2) the redshift at which the halo was analysed;
(3)–(5) the virial mass, comoving virial radius and maximum circular velocity.
At z = 0 (z = 1) the virial radius was estimated as the radius within which
the average overdensity of matter is 340 (180) times larger than the mean
cosmological density of matter at that redshift;
(6) the number of particles within the virial radius;
(7) the smallest particle mass in the simulation;
(8) formal force resolution achieved in the simulation. As we will show later,
convergent results are expected at scales larger than four times the formal
resolution;
(9) the halo concentration as estimated from NFW profile fits to halo density
profiles;
(10) the maximum relative error of the NFW fit: ρNFW /ρh − 1 (the error was
estimated inside 50h −1 kpc radius);
(11) the same as in the previous column, but for the fits of profile advocated by
Moore et al.
Halo A in the first set of simulations was re-simulated three times with
increasing mass resolution. For each simulation, we considered outputs at four
moments in the interval to z = 0–0.03. The parameters of the halos in these
simulations averaged over the four moments are presented in the first three rows
of table 15.4. We did not find any systematic change with resolution in the values

464

Numerical simulations in cosmology

of the halo parameters either on the virial radius scale or around the maximum of
the circular velocity (r = (30–40)h −1 kpc).
The top panel in figure 15.19 shows the central region of halo A1 (see
table 15.4). This plot is similar to figure 1(a) in Moore et al (1998) in that all
profiles are drawn to the formal force resolution. The straight lines indicate the
slopes of two power laws: γ = −1 and γ = −1.4. The figure indeed shows
that, at around 1% of the virial radius, the slope is steeper than −1 and the central
slope increases as we increase the mass resolution. Moore et al (1998) interpreted
this behaviour as evidence that the profiles are steeper than those predicted by
the NFW profile. We also note that the results of our highest resolution run A1
are qualitatively consistent with the results from Kravtsov et al (1999). Indeed,
if the profiles are considered down to the scale of two formal resolutions, the
density profile slope in the very central part of the profile r . 0.01rvir is close to
γ = −0.5.
The profiles in figure 15.19 reflect the density distribution in the cores
of simulated halos. However, the interpretation of these profiles is not
straightforward because it requires an assessment of the numerical effects. The
formal resolution does not usually even correspond to the scale where the
numerical force is fully Newtonian (usually it is still considerably ‘softer’ than
the Newtonian value). In the ART code, the inter-particle force reaches (on
average) the Newtonian value at about two formal resolutions (see Kravtsov et
al 1997). The effects of force resolution can be studied by re-simulating the same
objects with higher force resolution and comparing the density profiles. Such a
convergence study was done in Kravtsov et al (1998) where it was found that
for a fixed mass resolution the halo density profiles converge at scales above two
formal resolutions. Second, the local dynamical time for particles moving in the
core of a halo is very short. For example, particles on the circular orbit of the
radius 1h −1 kpc from the centre of halo A makes about 200 revolutions over the
Hubble time. Therefore, if the time step is insufficiently small, numerical errors
in these regions will tend to grow especially fast. The third possible source of
numerical error is the mass resolution. Poor mass resolution in simulations with
good force resolution may, for example, lead to two-body effects (e.g. Knebe et al
2000). An insufficient number of particles may also result in a ‘grainy’ potential
in halo cores and thereby affect the accuracy of the orbit integration. In these
effects, the mass resolution may be closely inter-related with the force resolution.
It is clear thus that, in order to draw conclusions unaffected by numerical
errors, one has to determine the range of trustworthy scales using convergence
analysis. The bottom panel in figure 15.19 shows that, for the halo A simulations,
the convergence for vastly different mass and force resolution is reached for scales
greater than or approximately equal to four formal force resolutions (all profiles in
this figure are plotted down to the radius of four formal force resolutions). For all
resolutions, there are more than 200 particles within the radius of four resolutions
from the halo centre. For the highest resolution simulation (halo A1 ) convergence
is reached at scales &0.005rvir.

Dark matter halos

465

(a)

(b)
Figure 15.19. (a) Density profiles of halo A simulated with different mass and force
resolutions. The profiles are plotted down to the formal force resolution of each simulation.
(b) The profiles plotted down to four formal resolutions. It is clear that for vastly
different mass (from 2000 to 120 000 particles in the halo) and force (from 3.66h −1 kpc to
0.23h −1 kpc) resolutions, convergence is reached at these scales.

466

Numerical simulations in cosmology

Figure 15.20. Fits of the NFW and Moore et al halo profiles to the profile of halo A1
(bottom panel). The top panel shows the fractional deviations of the analytic fits from the
numerical profile. Note that both analytical profiles fit the numerical profile equally well:
fractional deviations are smaller than 20% over almost three decades in the radius.

In order to judge which profile provides a better description of the simulated
profiles we fitted the NFW and Moore et al analytical profiles. Figure 15.20
presents the results of the fits and shows that both profiles fit the numerical profile
equally well: fractional deviations of the fitted profiles from the numerical one are
smaller than 20% over almost three decades in the radius. It is thus clear that the
fact that the numerical profile has a slope steeper than −1 at the scale of ∼0.01rvir
does not mean that a good fit of the NFW profile (or even analytical profiles with
shallower asymptotic slopes) cannot be obtained.
There is certainly a certain degree of degeneracy in fitting various analytic
profiles to the numerical results. Figure 15.21 illustrates this further by showing
results of fitting profiles (full curves) of the form ρ(r ) ∝ (r/r0 )−γ [1 +
(r/r0 )α ]−(β−α)/γ to the same (halo A1 ) simulated halo profile shown as full

Dark matter halos

467

Figure 15.21. Analytical fits to the density profile of halo A1 (see table 15.4) from our
set of simulations. The fits are of the form ρ(r ) ∝ (r/r0 )−γ [1 + (r/r0 )α ]−(β−α)/γ . The
legend in each panel indicates the corresponding values of α, β and γ of the fit; the digit
in parentheses indicates whether the parameter was kept fixed (0) or not (1) during the fit.
Note that various sets of parameters, α, β, γ , provide equally good fits to the simulated
halo profile in the whole resolved range of scales ≈(0.005–1)rvir . This indicates a large
degree of degeneracy in the parameters α, β and γ .

circles. The legend in each panel indicates the corresponding values of α, β and
γ of the fit; the digit in parentheses indicates whether the parameter was kept
fixed (0) or not (1) during the fit. The two right-hand panels show the fits of the
NFW and Moore et al profiles; the bottom left-hand panel shows fit of the profiles
used by Jing and Suto (2000). The top left-hand panel shows a fit in which the
inner slope was fixed but α and β were fitted. The figure shows that all four
analytic profiles can provide a nice fit to the numerical profile in the whole range
(0.005–1)rvir .

468

Numerical simulations in cosmology

15.4.4.2 Halo profiles at z = 1
As we have mentioned, the halo A analysed in the previous section is somewhat
special because it was selected as an isolated relaxed halo. In order to reach
unbiased conclusions, in this section we will present an analysis of halos from
the second set of simulations (halos B, C and D in table 15.4) which were not
selected to be relaxed or isolated. Based on the results of the convergence study
presented in the previous section, we will consider profiles of these halos only at
scales above four formal resolutions using results starting only from four formal
resolutions and not less than 200 particles. Note that these conditions are probably
more stringent than necessary because these halos were simulated with five to
seven times more particles per halo. There is an advantage in analysing halos at
a relatively high redshift. Halos of a given mass will have a lower concentration
(see Bullock et al 2001). A lower concentration implies a large scale at which
the asymptotic inner slope is reached. Profiles of the high-redshift halos should,
therefore, be more useful in discriminating between the analytic models with
different inner slopes.
We found that a substantial substructure is present inside the virial radius in
all three halos. Figure 15.22 shows the profiles of these halos at z = 1. There
profiles are not as smooth as that of halo A1 due to their substructure. Note that
bumps and depressions visible in the profiles cannot have a significantly larger
amplitude than the shot noise. Halo C appeared to be the most relaxed of the
three halos. It also had its last major merger somewhat earlier than the other two.
Halo D had a major merger event at z ≈ 2. Remnants of the merger are still
visible as a hump at radii around 100h −1 kpc. Non-uniformities in the profiles
caused by the substructure may substantially bias the analytic fits to the entire
range of scales below the virial radius. Therefore, we used only the central,
presumably more relaxed, regions in the analytic fits: r < 50h −1 kpc for halo D
and r < 100h −1 kpc for halos B and C (fits using only central 50h −1 kpc did not
change the results).
The best-fit parameters were obtained by minimizing the maximum
fractional deviation of the fit: max(abs(log ρfit ) − log ρh ). Minimizing the sum of
the squares of deviations (χ 2 ), as is often done, can result in larger errors at small
radii with the false impression that the fit fails because it has a wrong central slope.
The fit that minimizes the maximum deviations improves the NFW fit for points
in the range of radii (5–20)h −1 kpc, where the NFW fit would appear to be below
the data points if the fit was done by χ 2 minimization. This improvement comes
at the expense of a few points around 1h −1 kpc. For example, if we fit halo B
by using χ 2 minimization, the concentration decreases from 12.3 (see table 15.4)
to 11.8. We also made a fit for halo B assuming even more stringent limits on
the effects of numerical resolution. By minimizing the maximum deviation we
fitted the halo starting at six times the formal resolution. Inside this radius there
were about 900 particles. The resulting parameters of the fit were close to those
in table 15.4: CNFW = 11.8, and the maximum error of the NFW fit was 17%.

Dark matter halos

469

Figure 15.22. Profiles of halos B, C and D at z = 1. The profiles of halos C and D were
offset downwards by factors of 10 and 100 for clarity. The full curves show simulated
profiles, while the dotted and chain curves show the NFW and Moore et al fits, respectively.
The halo profiles in the simulations are plotted down to four formal resolutions. Each halo
had more than 200 particles inside the smallest plotted scale.

We found that the errors in the Moore et al fits were systematically smaller
than those of the NFW fits, though the differences were not dramatic. The Moore
et al fit failed for halo D. It formally gave very small errors, but this was done for
a fit with an unreasonably small concentration (C = 2). When we constrained
the approximation to have a concentration twice as large compared with the best
NFW fit, we were able to obtain a reasonable fit (this fit is shown in figure 15.22).
Nevertheless, the central part was fitted poorly in this case.
Our analysis therefore failed to determine which analytic profile provides a
better description of the density distribution in simulated halos. Despite the larger
number of particles per halo and lower concentrations of halos, the results are
still inconclusive. The Moore et al profile is a better fit to the profile of halo C;
the NFW profile is a better fit to the central part of halo D. Halo B represents

470

Numerical simulations in cosmology

an intermediate case where both profiles provide equally good fits (similar to the
analysis of halo A).
Note that there seems to be real deviations in the parameters of halos of
the same mass. Halos B and D have the same virial radii and nearly the same
circular velocities, yet their concentrations are different by 30%. We find the
same differences in estimates of C1/5 concentrations, which do not depend on the
specifics of an analytic fit. The central slope at around 1 kpc also changes from
halo to halo.
15.4.4.3 Summary
In this section we have given a review of some of the internal properties of DM
halos focusing mostly on their profiles and concentrations. Our results are mostly
based on simulations done with the ART code, which is capable of handling
particles with different masses, variable force and time resolution. In runs with the
highest resolution, the code achieved (formal) dynamical range of 217 = 131 072
with 500 000 steps for particles at the highest level of resolution.
Our conclusions regarding the convergence of the profiles differ from those
of Moore et al (1998). If we take into account only the radii, at which we believe
the numerical effects (the force resolution, the resolution of initial perturbations
and two-body scattering) to be small, then we find that the slope and amplitude
of the density do not change when we change the force and mass resolution. This
result is consistent with what was found in simulations of the ‘Santa Barbara’
cluster (Frenk et al 1999): at a fixed resolved scale the results do not change as
the resolution increases. For the ART code the results converged at four times
the formal force resolution and more than 200 particles. These convergence
limits very likely depend on the particular code used and on the duration of the
integration.
We reproduce Moore et al’s results regarding convergence and the results
from Kravtsov et al (1998) regarding shallow central profiles, but only when
we considered points inside unresolved scales. We conclude that those results
followed from an overly optimistic interpretation of the numerical accuracy of the
simulations.
For the galaxy-size halos considered in this section with masses Mvir =
7 × 1011h −1 M to 2 × 1012h −1 M and concentrations C = 9–17 both the NFW
profile, ρ ∝ r −1 (1 + r )−2 , and the Moore et al profile, ρ ∝ r −1.5 (1 + r 1.5 )−1 ,
give good fits with an accuracy of about 10% for radii not smaller than 1% of the
virial radius. None of the profiles is significantly better than the other.
Halos with the same mass may have different profiles. No matter what profile
is used—NFW or Moore et al—there is no universal profile: halo mass does not
yet define the density profile. Nevertheless, the universal profile is an extremely
useful notion which should be interpreted as the general trend C(M) of halos with
a larger mass to have a lower concentration. Deviations from the general C(M)
are real and significant (Bullock et al 2001). It is not yet clear but it seems very

References

471

likely that the central slopes of halos also have real fluctuations. The fluctuations
in the concentration and central slopes are important for interpreting the central
parts of rotation curves.

References
Aarseth S J 1963 Mon. Not. R. Astron. Soc. 126 223
——1985 Multiple Time Scales ed J W Brackbill and B J Cohen (New York: Academic
Press) p 377
Aarseth S J, Gott J R and Turner E L 1979 Astrophys. J. 228 664
Appel A 1985 SIAM J. Sci. Stat. Comput. 6 85
Avila-Reese et al 1999 Mon. Not. R. Astron. Soc. 310 527
Barnes J and Hut P 1986 Nature 324 446
Benson A J, Cole S, Frenk C S, Baugh C M and Lacey C D 2000 Mon. Not. R. Astron. Soc.
311 793 (astro-ph/9903343)
Bertschinger E 1998 Annu. Rev. Astron. Astrophys. 36 599
——2001 Preprint astro-ph/0103301
Bertschinger E and Gelb J 1991 Comput. Phys. 5 164
Binney J and Tremaine S 1987 Galactic Dynamics (Princeton, NJ: Princeton University
Press)
Bouchet F R and Hernquist L 1988 Astrophys. J. Suppl. 68 521
Bullock J S, Kolatt T S, Sigad Y, Somerville R S, Kravtsov A V, Klypin A, Primack J P
and Dekel A 2001 Mon. Not. R. Astron. Soc. 321 559 (astro-ph/9908159)
Bullock J S, Kravtsov A V and Weinberg D H 2000 Astrophys. J. 539 517 (astroph/0002214)
Catelan P, Lucchin F, Matarrese S and Porciani C 1998a Mon. Not. R. Astron. Soc. 297 692
Catelan P, Matarrese S and Porciani C 1998b Astrophys. J. Lett. 502 1
Colı́n P, Klypin A and Kravtsov A 2000 Astrophys. J. 539 561 (astro-ph/9907337)
Colı́n P, Klypin A A, Kravtsov A V and Khokhlov A M 1999 Astrophys. J. 523 32 (astroph/9809202)
Couchman H M P 1991 Astrophys. J. 368 23
Davis M, Efstathiou G, Frenk C S and White S D M 1985 Astrophys. J. 292 371
Dekel A and Lahav O 1999 Astrophys. J. 520 24 (astro-ph/9806193)
Dekel A and Silk J 1986 Astrophys. J. 303 39
Diaferio A, Kauffmann G, Colberg J M and White S D M 1999 Mon. Not. R. Astron. Soc.
307 537 (astro-ph/9812009)
Doroshkevich A G, Kotok E V, Novikov I D, Polyudov A N and Sigov Yu S 1980 Mon.
Not. R. Astron. Soc. 192 321
Efstathiou G, Davis M, Frenk C S and White S D M 1985 Astrophys. J. Suppl. 57 241
Flores R A and Primack J R 1994 Astrophys. J. 427 L1
Frenk C et al 1999 Astrophys. J. 525 630
Gelb J 1992 PhD Thesis MIT
Ghigna S, Moore B, Governato F, Lake G, Quinn T and Stadel J 1998 Mon. Not. R. Astron.
Soc. 300 146
——Observational Cosmology: The Development of Galaxy Systems ed G Giuricin,
M Mezzetti and P Solucci (San Francisco, CA: ASP) p 140
——2000 Astrophys. J. 544 616

472

Numerical simulations in cosmology

Gottlöber S, Klypin A and Kravtsov A V 1998 Observational Cosmology: The
Development of Galaxy Systems (ASP Conf. Series 176, 1999) ed G Giuricin,
M Mezetti and P Salucci (San Francisco, CA: ASP) p 418
Gross M 1997 PhD Thesis University of California, Santa Cruz
Gunn J E and Gott J R 1972 Astrophys. J. 176 1
Hernquist L 1987 Astrophys. J. Suppl. 64 715
Hernquist L, Bouchet F R and Suto Y 1991 Astrophys. J. Suppl. 75 231
Hockney R W and Eastwood J W 1981 Numerical Simulations Using Particles (New York:
McGraw-Hill)
Hu W and Sugiyama 1996 Astrophys. J. 471 542
Jing Y P 2000 Astrophys. J. 535 30 (astro-ph/9901340)
Jing Y P and Suto Y 2000 Astrophys. J. 529 L69
Kaiser N 1984 Astrophys. J. 284 L9
Kauffmann G, White S D M and Guiderdoni B 1993 Mon. Not. R. Astron. Soc. 264 201
Klypin A, Gotlöber S, Kravtsov A V and Khokhlov A 1999a Astrophys. J. 516 530
(KGKK)
Klypin A and Holtzman J 1997 Preprint astro-ph/9712217
Klypin A, Holtzman J, Primack J and Regos E 1993 Astrophys. J. 416 1
Klypin A, Kravtsov A V, Bullock J S and Primack J P 2001 Astrophys. J. 554 903
Klypin A, Kravtsov A V, Valenzuela O and Prada F 1999b Astrophys. J. 522 82
Klypin A and Shandarin S F 1983 Mon. Not. R. Astron. Soc. 204 891
Knebe A, Green A and Binney J 2001 Mon. Not. R. Astron. Soc. 325 845
Knebe A, Kravtsov A V, Gottlöber S and Klypin A 2000 Mon. Not. R. Astron. Soc. 317
630
Kravtsov A V 1999 PhD Thesis New Mexico State University
Kravtsov A V and Klypin A 1999 Astrophys. J. 520 437
Kravtsov A V, Klypin A, Bullock J S and Primack J P 1999 Astrophys. J. 502 48
Kravtsov A V, Klypin A and Khokhlov A 1997 Astrophys. J. Suppl. 111 73
Lokas Elanol Mammon G A 2001 Mon. Not. R. Astron. Soc. 321 155
Mo H J and White S D M 1996 Mon. Not. R. Astron. Soc. 282 347
Moore B 1994 Nature 370 629
Moore B, Ghigna S, Governato F, Lake G, Quinn T, Stadel J and Tozzi P 1999 Astrophys.
J. Lett. 524 L19 (astro-ph/9907411)
Moore B, Governato F, Quinn T, Stadel J and Lake G 1998, Astrophys. J. 499 L5
Moore B, Katz N and Lake G 1996 Astrophys. J. 456 455
Moore B, Quinn T, Governato F, Stadel J and Lake G 1999 Mon. Not. R. Astron. Soc. 310
1147
Navarro J F, Frenk C S and White S D M 1996 Astrophys. J. 462 563
——1997 Astrophys. J. 490 493
Okamoto T and Habe A 1999 Astrophys. J. 516 591
Peebles P J E 1970 Astron. J. 75 13
Porciani C, Matarrese S, Lucchin F and Catelan P 1998 Mon. Not. R. Astron. Soc. 298 1097
(astro-ph/9801290)
Press W H and Schechter P L 1974 Astrophys. J. 187 425
Quinn T, Katz N and Stadel J and Lake G 1997 Preprint astro-ph/9710043
Sahni V and Coles P 1995 Phys. Rep. 262 2
Sellwood J A 1987 Annu. Rev. Astron. Astrophys. 25 151
Sheth R K and Lemson G 1999 Mon. Not. R. Astron. Soc. 305 946 (astro-ph/9808138)

References

473

Sigad Y, Kolatt T S, Bullock J S, Kravtsov A V, Klypin A, Primack J R and Dekel A 2000
in preparation
Somerville R, Lemson G, Sigad Y, Dekel A, Kauffmann G and White S D M 2001 Mon.
Not. R. Astron. Soc. 320 289 (astro-ph/9912073)
Splinter R et al 1998 Astrophys. J. 498 38
Swaters R A, Madore B F and Trewhella M 2000 Astrophys. J. 531 L107
Taruya A and Suto Y 2000 Astrophys. J. 542 559 (astro-ph/0004288)
Tormen G, Diaferio A and Syer D 1998 Mon. Not. R. Astron. Soc. 299 728
van den Bosch F C, Robertson B E, Dalcanton J J and de Blok W J G 2000 Astrophys. J.
119 1579
White S D M 1976 Mon. Not. R. Astron. Soc. 177 717
White S D M and Rees M J 1978 Mon. Not. R. Astron. Soc. 183 341
Widrow L 2000 Astrophys. J. 131 39
Zeldovich Ya B 1970 Astron. Astrophys. 5 84

Index

m , 
measurement, 340, 341
m , 
definition, 313
measurement, 339
2dF survey, 345
3-sphere, 20
Abel integral equation, 76
acceleration vector, 113
acoustic
peak, 96
peaks, 93, 237
acoustic oscillations, 234
defect models, 238
adiabatic, 87
adiathermal, 86
almost-EGS theorem, 144
angular correlation function, 345
angular diameter
distance, 29
angular diameter distance, 242, 315
anthropic principle, 171
antigravity, 18
associated Legendre polynomials,
69
average density, 370
for a fractal, 370
axial symmetry, 393
baryon signature, 238, 249
beam width, 244
Bianchi
classification, 133
identities, 110, 119
474

universes, 132
bias, 353, 361, 440
spatial, 442
velocity, 447
Birkhoff’s theorem, 22
blackbody spectrum, 219
bolometric, 28
BOOMERanG, 247
CBI microwave interferometer, 251
CDMS, 274, 276
chiral fermion, 214
Christoffel relations, 123
cluster mass function, 337
clusters
clustering of, 359
CMBFAST, 88
COBE, 88
satellite, 222
codes, 429
ART, 429
DENMAX, 438
FOF, 438
GADGET, 432
TREE, 432
coherent oscillations, 237
from inflation, 260
tests of, 254
collapse, 58
commutation functions, 121
comoving coordinate, 20
Compton scattering, 224
conditional density, 354, 369
fractal dimension, 373

Index
shape in a VL sample, 373
cone diagrams, 344
conformal
anomaly, 160
time, 51
connection components, 122
conservation equations, 110, 115
consistency checks, 146
constituents of the early universe,
224
continuity equation, 54
convective derivative, 14
coordinate basis, 122
coordinates
Eulerian, 48
correlation function, 59, 345, 367
as tool for test homogeneity, 368
canonical shape, 371
discussion on the shape, 374
exponent
standard value, 371
for a fractal, 370
shape for a ML sample, 371
shape for a VL sample, 371
correlation length, 368
as a measure for the scale of
fluctuactions, 368
discussion, 374
exponent
for a fractal, 370
for a VL sample, 372
for a fractal, 370
for a ML sample, 371
standard value, 371
cosmic background radiation, 111
cosmic microwave background, 87,
219
discovery of, 220
measurements, current, 247
oscillator equation, 235
polarization of, 229
search for fluctuations, 222
temperature fluctuations, 223
unrecognized discoveries of, 221

475

cosmic variance, 63, 242, 245
cosmological
4-velocity, 111, 112
Boltzmann equation, 225
constant, 18, 23, 110, 205, 240
CDM models, 205
fluid equations, 225
initial conditions, 236
linear perturbation theory, 224
model space, 239
models, 239
parameters, 239, 259
degeneracies, 242
symmetries, 127
units, 222
cosmological models
symmetries of, 124
cosmologies
isotropic, 127
locally rotationally symmetric,
127, 129, 130
self-similar, 136
covariant, 10
curvature
length, 23
of the spacetime, 109
scalar, 16
DAMA, 276, 278, 285
damping scale, 242, 253
dark matter (DM), 192–193, 240
baryonic, 193
cold (CDM), 55, 198–203
hot (HDM), 55, 194–198
mixed (MDM), 56
warm (WDM), 203–204
data compression, 65
de Sitter spacetime, 33
deflection angle, 382, 383
density
critical, 22, 240
parameter, 23
perturbation, 168
perturbation field, 58

476

Index

profiles, 436
deprojection, 76
detector noise, 244
deuterium measurements, 249
de Sitter spacetime, 166
diffusion damping, 227
dimensionality of symmetry
groups, 126
distance measure, 315
distant-observer approximation, 74
Doppler peak, 96
double readout, 273, 274, 280
dynamical systems approach, 134

from inflation, 259
problem, 24
tests of, 252
fractal, 61
fractal dimension, 354, 369
fractal galaxy distribution, 353
free-streaming, 55, 227
Friedmann, 108
equation, 22, 131
models, 313, 314
Friedmann–Lemaı̂tre models, 130
FRW metric, 224
fundamental observers, 19
FWHM, 94

e-foldings, 25
Einstein, 379
field equations, 109, 123
cosmological, 225
tensor, 16
Einstein radius, 392, 393
Einstein–de Sitter, 25
electromagnetic field, 114
energy condition, 111
energy–momentum tensor, 14, 110,
114
ensemble average power, 94
equation of state, 114
γ -law, 115
equilibrium points, 136
ergodic, 59
Euler’s equation, 15
evolution equations for fluctuations,
423
exclusion plot, 269, 276
expansion scalar, 113

gauge
freedom of metric perturbations,
225
transformation, 10
gauge invariant formalism, 225
gaussianity
from inflation, 260
tests of, 255
general relativity, 379
goldstino, 216
gravitational lens, 384
gravitational lensing, 247, 378, 379
gravitational potentials, 242
gravitino, 203, 213
non-thermal, 213
production, 215
group
of isometries, 126
of isotropy, 126
of symmetries, 127

false vacuum, 160
Faraday rotation, 257
fermion mass, 191
Dirac, 191
Majorana, 191
fingers of God, 80
Fisher information matrix, 246
flatness

halo, 427
Hankel transform, 69
harmonic boundary conditions, 59
harmonic expansions, 232
microwave background, 228
high resolution, 427
high-baryon models, 361
homogeneity, 359

Index
homogeneous, 19
hot big bang, 219
Hubble
drag, 53
generalized law, 113
parameter, 113, 240
inflation, 108, 159, 259
chaotic, 161
consistency check, 260
eternal, 169
new, 161
old, 160
tests of, 258
inflationary consistency equation,
102
inflaton field, 161, 213
initial conditions
explanation of, 150
showing irrelevance of, 147
invariants, 10
irremovable problem, 153
isotropic, 19
isotropisation properties, 138
Jacobi identities, 123
Jeans length, 50
Kamiokande, 296
Killing vectors, 125
kinematic quantities, 113
large-scale structures, 47
last-scattering shell, 91
Lemaı̂tre, 108
Lemaı̂tre–Tolman–Bondi models,
130
lens
equation, 384
plane, 383
Schwarzschild, 393, 394
lepton number, 188
Lie derivative, 125
Limber’s equation, 71
linearized equations of motion, 47

477

Lorentz group symmetry, 10
luminosity
distance, 29, 315
luminosity function
galaxy, 326, 327
x-ray cluster, 338, 339
luminosity segregation effect, 376
Lyman-break technique, 328, 330
M theory, 211
magnetic fields
primordial
tests of, 257
MAP satellite, 251
mass refinement, 428
mass–length relation, 369
matter–radiation equality, 87, 242
MAXIMA, 247
mean streaming, 351
Metropolis algorithm, 246
microlensing, 394
microwave foreground emission,
247
Minimal Supersymmetric Standard
Model (MSSM), 199
ML sample, 371
model independent constraints, 251
monopole problem, 36
multigrid, 432
negative
curvature, 20
Neutralino, 265
neutralino, 199, 283
thermal history, 200
neutrino, 189, 296–310
beyond SM, 195
effective species, 241, 243
mass, 194, 241, 243
mixing and oscillations, 297
thermal history, 196
Newton, 379
nucleosynthesis, 282
number counts, 319, 320, 325, 328

478

Index

numerical simulations, 421
observational variables, 139
observations
anisotropic and inhomogeneous
Models, 142
Bianchi models, 142
Occam’s Razor, 252
occupation numbers, 166
orthogonal spatial derivative, 112
Ostriker–Vishniac effect, 247
pairwise velocity dispersion, 348
parameter curvature matrix, 246
particle horizon, 27, 141
perfect fluid, 114
perturbations
adiabatic, 237
from inflation, 260
tests of, 254
isocurvature, 237
outside horizon, 260
primordial, 240
scale-invariant, 260
tensor
tests of, 255
phase planes, 131, 137
phonon-mediated detection, 271,
272
pixelization noise, 244
Planck density, 172
Planck satellite, 251
point-like lens, 382
polarization tensor, 232
power spectrum, 60, 357, 361
isotropic, 60
microwave background
variance, 245
microwave background
polarization, 234
microwave background
temperature, 228
scale invariant, 61
power spectrum

microwave background
estimator, 228
pre-heating, 173, 176
precision cosmology, 219, 248
prewhitening, 66
primary anisotropies, 89
primordial nucleosynthesis, 240,
249
principal components analysis, 65
prior probability distribution, 250
projected correlation function, 75
projection tensor, 112
proof of almost-FL Geometry, 143
quadrupole polarization source, 231
quantum fluctuations, 168
quenching factor, 271, 273
quintessence, 205–207, 240
R parity, 198, 283
radiation-dominated, 87
Raychaudhuri equation, 116, 131
recombination, 87, 224
redshift, 22, 140, 314
photometric, 331–333
space distortions, 347
redshift–space correlation function,
79
REFLEX survey, 359
refraction index, 382
reheating, 36, 172
reionization, 241
tests of, 257
relativistic enthalpy, 15
resolution, 433
Ricci
identities, 116
tensor, 16, 109
Riemann tensor, 16
Robertson, 108
rotation coefficients, 122
RW metric, 21
Sachs–Wolfe effect, 12, 223

Index
integrated, 223, 242
sampling variance, 63
scalar field, 111
scale factor, 20
scaling test, 76
Schwarzschild radius, 383
seasonal modulation, 267, 269
secondary anisotropies, 89
see-saw mechanism, 196
selection effects, 140
selection function, 70
shear
propagation equation, 118
tensor, 113
shear-free dust, 120
signal-to-noise eigenmodes, 67
Silk damping, 55, 227
singularity theorem, 117
slow-rolling approximation, 35
small universes, 141
Soldner, 379
sound horizon, 242, 253
source plane, 383
spatial topology, 131
spatially closed, 22
spatially homogeneous universes,
129, 130
spatially open, 22
specific intensity, 28
speed of light, 382
spherical Bessel functions, 70, 95
spherical harmonics, 69
stable clustering, 72
Standard Model (SM), 188–192
fermion masses, 191
gauge boson masses, 190
Higgs mechanism in, 189
star formation
history, 335
rate, 331, 334
Stokes parameters, 230
stress tensor, 110
string theory, 211
Sunyaev–Zeldovich effect, 247

super-Higgs effect, 216
superclusters, 345
superconformal symmetry, 212
supergravity, 211
superconformal, 213
superstring theory, 211
supersymmetry, 212
spontaneously broken, 216
surface brightness, 28, 317
surveys
clusters, 337, 339
galaxies, 322
selection methods, 322–324
strategies, 322
surveys, 345
temperature fluctuation
Doppler, 223
gravitational, 223
intrinsic, 223
tensor spherical harmonics, 233
tensor–scalar ratio, 240
tetrad
components, 122
formalism, 121
tight coupling, 226
tilt, 41, 101
time
delay, 382
derivative, 112
lookback, 314
Tolman–Bondi models, 130
topology of the universe
tests of, 257
tracker field, 45
transfer function, 55
Turnround, 58
universe
closed, 20
open, 21
virial theorem, 58
virialization, 58

479

480

Index

visual horizons, 141
VL sample, 371
Vlasov equation, 424
voids, 345
volume
comoving element, 316
element, 112
vorticity
modes, 54
propagation equation, 117
tensor, 113

vanishing, 117
Walker, 108
Weyl tensor, 109, 115
WIMP wind, 284
annual modulation, 284
WIMPs, 283
x-ray clusters, 359
Zwicky, 379, 380

A Brief History of Time - Stephen Hawking

future (or have we?) but I discuss a possible explanation for this.
I also describe the progress that has been made recently in finding “dualities” or correspondences between
apparently different theories of physics. These correspondences are a strong indication that there is a complete
unified theory of physics, but they also suggest that it may not be possible to express this theory in a single
fundamental formulation. Instead, we may have to use different reflections of the underlying theory in different
situations. It might be like our being unable to represent the surface of the earth on a single map and having to
use different maps in different regions. This would be a revolution in our view of the unification of the laws of
science but it would not change the most important point: that the universe is governed by a set of rational laws
that we can discover and understand.
On the observational side, by far the most important development has been the measurement of fluctuations in
the cosmic microwave background radiation by COBE (the Cosmic Background Explorer satellite) and other
collaborations. These fluctuations are the finger-prints of creation, tiny initial irregularities in the otherwise
smooth and uniform early universe that later grew into galaxies, stars, and all the structures we see around us.
Their form agrees with the predictions of the proposal that the universe has no boundaries or edges in the
imaginary time direction; but further observations will be necessary to distinguish this proposal from other
possible explanations for the fluctuations in the background. However, within a few years we should know
whether we can believe that we live in a universe that is completely self-contained and without beginning or
end.
Stephen Hawking

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/A Brief History in Time.html (2 of 2) [2/20/2001 3:13:58 AM]

A Brief History of Time - Stephen Hawking... Chapter 1

CHAPTER 1
OUR PICTURE OF THE UNIVERSE

A well-known scientist (some say it was Bertrand Russell) once gave a public lecture on astronomy. He
described how the earth orbits around the sun and how the sun, in turn, orbits around the center of a vast
collection of stars called our galaxy. At the end of the lecture, a little old lady at the back of the room got up and
said: “What you have told us is rubbish. The world is really a flat plate supported on the back of a giant
tortoise.” The scientist gave a superior smile before replying, “What is the tortoise standing on.” “You’re very
clever, young man, very clever,” said the old lady. “But it’s turtles all the way down!”
Most people would find the picture of our universe as an infinite tower of tortoises rather ridiculous, but why do
we think we know better? What do we know about the universe, and how do we know it? Where did the
universe come from, and where is it going? Did the universe have a beginning, and if so, what happened before
then? What is the nature of time? Will it ever come to an end? Can we go back in time? Recent breakthroughs
in physics, made possible in part by fantastic new technologies, suggest answers to some of these
longstanding questions. Someday these answers may seem as obvious to us as the earth orbiting the sun – or
perhaps as ridiculous as a tower of tortoises. Only time (whatever that may be) will tell.
As long ago as 340 BC the Greek philosopher Aristotle, in his book On the Heavens, was able to put forward
two good arguments for believing that the earth was a round sphere rather than a Hat plate. First, he realized
that eclipses of the moon were caused by the earth coming between the sun and the moon. The earth’s
shadow on the moon was always round, which would be true only if the earth was spherical. If the earth had
been a flat disk, the shadow would have been elongated and elliptical, unless the eclipse always occurred at a
time when the sun was directly under the center of the disk. Second, the Greeks knew from their travels that
the North Star appeared lower in the sky when viewed in the south than it did in more northerly regions. (Since
the North Star lies over the North Pole, it appears to be directly above an observer at the North Pole, but to
someone looking from the equator, it appears to lie just at the horizon. From the difference in the apparent
position of the North Star in Egypt and Greece, Aristotle even quoted an estimate that the distance around the
earth was 400,000 stadia. It is not known exactly what length a stadium was, but it may have been about 200
yards, which would make Aristotle’s estimate about twice the currently accepted figure. The Greeks even had a
third argument that the earth must be round, for why else does one first see the sails of a ship coming over the
horizon, and only later see the hull?
Aristotle thought the earth was stationary and that the sun, the moon, the planets, and the stars moved in
circular orbits about the earth. He believed this because he felt, for mystical reasons, that the earth was the
center of the universe, and that circular motion was the most perfect. This idea was elaborated by Ptolemy in
the second century AD into a complete cosmological model. The earth stood at the center, surrounded by eight
spheres that carried the moon, the sun, the stars, and the five planets known at the time, Mercury, Venus,
Mars, Jupiter, and Saturn.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/n.html (1 of 7) [2/20/2001 3:14:06 AM]

A Brief History of Time - Stephen Hawking... Chapter 1

Figure 1:1
The planets themselves moved on smaller circles attached to their respective spheres in order to account for
their rather complicated observed paths in the sky. The outermost sphere carried the so-called fixed stars,
which always stay in the same positions relative to each other but which rotate together across the sky. What
lay beyond the last sphere was never made very clear, but it certainly was not part of mankind’s observable
universe.
Ptolemy’s model provided a reasonably accurate system for predicting the positions of heavenly bodies in the
sky. But in order to predict these positions correctly, Ptolemy had to make an assumption that the moon
followed a path that sometimes brought it twice as close to the earth as at other times. And that meant that the
moon ought sometimes to appear twice as big as at other times! Ptolemy recognized this flaw, but nevertheless
his model was generally, although not universally, accepted. It was adopted by the Christian church as the
picture of the universe that was in accordance with Scripture, for it had the great advantage that it left lots of
room outside the sphere of fixed stars for heaven and hell.
A simpler model, however, was proposed in 1514 by a Polish priest, Nicholas Copernicus. (At first, perhaps for
fear of being branded a heretic by his church, Copernicus circulated his model anonymously.) His idea was that
the sun was stationary at the center and that the earth and the planets moved in circular orbits around the sun.
Nearly a century passed before this idea was taken seriously. Then two astronomers – the German, Johannes
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/n.html (2 of 7) [2/20/2001 3:14:06 AM]

A Brief History of Time - Stephen Hawking... Chapter 1

Kepler, and the Italian, Galileo Galilei – started publicly to support the Copernican theory, despite the fact that
the orbits it predicted did not quite match the ones observed. The death blow to the Aristotelian/Ptolemaic
theory came in 1609. In that year, Galileo started observing the night sky with a telescope, which had just been
invented. When he looked at the planet Jupiter, Galileo found that it was accompanied by several small
satellites or moons that orbited around it. This implied that everything did not have to orbit directly around the
earth, as Aristotle and Ptolemy had thought. (It was, of course, still possible to believe that the earth was
stationary at the center of the universe and that the moons of Jupiter moved on extremely complicated paths
around the earth, giving the appearance that they orbited Jupiter. However, Copernicus’s theory was much
simpler.) At the same time, Johannes Kepler had modified Copernicus’s theory, suggesting that the planets
moved not in circles but in ellipses (an ellipse is an elongated circle). The predictions now finally matched the
observations.
As far as Kepler was concerned, elliptical orbits were merely an ad hoc hypothesis, and a rather repugnant one
at that, because ellipses were clearly less perfect than circles. Having discovered almost by accident that
elliptical orbits fit the observations well, he could not reconcile them with his idea that the planets were made to
orbit the sun by magnetic forces. An explanation was provided only much later, in 1687, when Sir Isaac Newton
published his Philosophiae Naturalis Principia Mathematica, probably the most important single work ever
published in the physical sciences. In it Newton not only put forward a theory of how bodies move in space and
time, but he also developed the complicated mathematics needed to analyze those motions. In addition,
Newton postulated a law of universal gravitation according to which each body in the universe was attracted
toward every other body by a force that was stronger the more massive the bodies and the closer they were to
each other. It was this same force that caused objects to fall to the ground. (The story that Newton was inspired
by an apple hitting his head is almost certainly apocryphal. All Newton himself ever said was that the idea of
gravity came to him as he sat “in a contemplative mood” and “was occasioned by the fall of an apple.”) Newton
went on to show that, according to his law, gravity causes the moon to move in an elliptical orbit around the
earth and causes the earth and the planets to follow elliptical paths around the sun.
The Copernican model got rid of Ptolemy’s celestial spheres, and with them, the idea that the universe had a
natural boundary. Since “fixed stars” did not appear to change their positions apart from a rotation across the
sky caused by the earth spinning on its axis, it became natural to suppose that the fixed stars were objects like
our sun but very much farther away.
Newton realized that, according to his theory of gravity, the stars should attract each other, so it seemed they
could not remain essentially motionless. Would they not all fall together at some point? In a letter in 1691 to
Richard Bentley, another leading thinker of his day, Newton argued that this would indeed happen if there were
only a finite number of stars distributed over a finite region of space. But he reasoned that if, on the other hand,
there were an infinite number of stars, distributed more or less uniformly over infinite space, this would not
happen, because there would not be any central point for them to fall to.
This argument is an instance of the pitfalls that you can encounter in talking about infinity. In an infinite
universe, every point can be regarded as the center, because every point has an infinite number of stars on
each side of it. The correct approach, it was realized only much later, is to consider the finite situation, in which
the stars all fall in on each other, and then to ask how things change if one adds more stars roughly uniformly
distributed outside this region. According to Newton’s law, the extra stars would make no difference at all to the
original ones on average, so the stars would fall in just as fast. We can add as many stars as we like, but they
will still always collapse in on themselves. We now know it is impossible to have an infinite static model of the
universe in which gravity is always attractive.
It is an interesting reflection on the general climate of thought before the twentieth century that no one had
suggested that the universe was expanding or contracting. It was generally accepted that either the universe
had existed forever in an unchanging state, or that it had been created at a finite time in the past more or less
as we observe it today. In part this may have been due to people’s tendency to believe in eternal truths, as well
as the comfort they found in the thought that even though they may grow old and die, the universe is eternal
and unchanging.
Even those who realized that Newton’s theory of gravity showed that the universe could not be static did not
think to suggest that it might be expanding. Instead, they attempted to modify the theory by making the

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/n.html (3 of 7) [2/20/2001 3:14:06 AM]

A Brief History of Time - Stephen Hawking... Chapter 1

gravitational force repulsive at very large distances. This did not significantly affect their predictions of the
motions of the planets, but it allowed an infinite distribution of stars to remain in equilibrium – with the attractive
forces between nearby stars balanced by the repulsive forces from those that were farther away. However, we
now believe such an equilibrium would be unstable: if the stars in some region got only slightly nearer each
other, the attractive forces between them would become stronger and dominate over the repulsive forces so
that the stars would continue to fall toward each other. On the other hand, if the stars got a bit farther away
from each other, the repulsive forces would dominate and drive them farther apart.
Another objection to an infinite static universe is normally ascribed to the German philosopher Heinrich Olbers,
who wrote about this theory in 1823. In fact, various contemporaries of Newton had raised the problem, and the
Olbers article was not even the first to contain plausible arguments against it. It was, however, the first to be
widely noted. The difficulty is that in an infinite static universe nearly every line of sight would end on the
surface of a star. Thus one would expect that the whole sky would be as bright as the sun, even at night.
Olbers’ counter-argument was that the light from distant stars would be dimmed by absorption by intervening
matter. However, if that happened the intervening matter would eventually heat up until it glowed as brightly as
the stars. The only way of avoiding the conclusion that the whole of the night sky should be as bright as the
surface of the sun would be to assume that the stars had not been shining forever but had turned on at some
finite time in the past. In that case the absorbing matter might not have heated up yet or the light from distant
stars might not yet have reached us. And that brings us to the question of what could have caused the stars to
have turned on in the first place.
The beginning of the universe had, of course, been discussed long before this. According to a number of early
cosmologies and the Jewish/Christian/Muslim tradition, the universe started at a finite, and not very distant,
time in the past. One argument for such a beginning was the feeling that it was necessary to have “First Cause”
to explain the existence of the universe. (Within the universe, you always explained one event as being caused
by some earlier event, but the existence of the universe itself could be explained in this way only if it had some
beginning.) Another argument was put forward by St. Augustine in his book The City of God. He pointed out
that civilization is progressing and we remember who performed this deed or developed that technique. Thus
man, and so also perhaps the universe, could not have been around all that long. St. Augustine accepted a
date of about 5000 BC for the Creation of the universe according to the book of Genesis. (It is interesting that
this is not so far from the end of the last Ice Age, about 10,000 BC, which is when archaeologists tell us that
civilization really began.)
Aristotle, and most of the other Greek philosophers, on the other hand, did not like the idea of a creation
because it smacked too much of divine intervention. They believed, therefore, that the human race and the
world around it had existed, and would exist, forever. The ancients had already considered the argument about
progress described above, and answered it by saying that there had been periodic floods or other disasters that
repeatedly set the human race right back to the beginning of civilization.
The questions of whether the universe had a beginning in time and whether it is limited in space were later
extensively examined by the philosopher Immanuel Kant in his monumental (and very obscure) work Critique of
Pure Reason, published in 1781. He called these questions antinomies (that is, contradictions) of pure reason
because he felt that there were equally compelling arguments for believing the thesis, that the universe had a
beginning, and the antithesis, that it had existed forever. His argument for the thesis was that if the universe did
not have a beginning, there would be an infinite period of time before any event, which he considered absurd.
The argument for the antithesis was that if the universe had a beginning, there would be an infinite period of
time before it, so why should the universe begin at any one particular time? In fact, his cases for both the thesis
and the antithesis are really the same argument. They are both based on his unspoken assumption that time
continues back forever, whether or not the universe had existed forever. As we shall see, the concept of time
has no meaning before the beginning of the universe. This was first pointed out by St. Augustine. When asked:
“What did God do before he created the universe?” Augustine didn’t reply: “He was preparing Hell for people
who asked such questions.” Instead, he said that time was a property of the universe that God created, and
that time did not exist before the beginning of the universe.
When most people believed in an essentially static and unchanging universe, the question of whether or not it
had a beginning was really one of metaphysics or theology. One could account for what was observed equally
well on the theory that the universe had existed forever or on the theory that it was set in motion at some finite
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/n.html (4 of 7) [2/20/2001 3:14:06 AM]

A Brief History of Time - Stephen Hawking... Chapter 1

time in such a manner as to look as though it had existed forever. But in 1929, Edwin Hubble made the
landmark observation that wherever you look, distant galaxies are moving rapidly away from us. In other words,
the universe is expanding. This means that at earlier times objects would have been closer together. In fact, it
seemed that there was a time, about ten or twenty thousand million years ago, when they were all at exactly
the same place and when, therefore, the density of the universe was infinite. This discovery finally brought the
question of the beginning of the universe into the realm of science.
Hubble’s observations suggested that there was a time, called the big bang, when the universe was
infinitesimally small and infinitely dense. Under such conditions all the laws of science, and therefore all ability
to predict the future, would break down. If there were events earlier than this time, then they could not affect
what happens at the present time. Their existence can be ignored because it would have no observational
consequences. One may say that time had a beginning at the big bang, in the sense that earlier times simply
would not be defined. It should be emphasized that this beginning in time is very different from those that had
been considered previously. In an unchanging universe a beginning in time is something that has to be
imposed by some being outside the universe; there is no physical necessity for a beginning. One can imagine
that God created the universe at literally any time in the past. On the other hand, if the universe is expanding,
there may be physical reasons why there had to be a beginning. One could still imagine that God created the
universe at the instant of the big bang, or even afterwards in just such a way as to make it look as though there
had been a big bang, but it would be meaningless to suppose that it was created before the big bang. An
expanding universe does not preclude a creator, but it does place limits on when he might have carried out his
job!
In order to talk about the nature of the universe and to discuss questions such as whether it has a beginning or
an end, you have to be clear about what a scientific theory is. I shall take the simpleminded view that a theory
is just a model of the universe, or a restricted part of it, and a set of rules that relate quantities in the model to
observations that we make. It exists only in our minds and does not have any other reality (whatever that might
mean). A theory is a good theory if it satisfies two requirements. It must accurately describe a large class of
observations on the basis of a model that contains only a few arbitrary elements, and it must make definite
predictions about the results of future observations. For example, Aristotle believed Empedocles’s theory that
everything was made out of four elements, earth, air, fire, and water. This was simple enough, but did not make
any definite predictions. On the other hand, Newton’s theory of gravity was based on an even simpler model, in
which bodies attracted each other with a force that was proportional to a quantity called their mass and
inversely proportional to the square of the distance between them. Yet it predicts the motions of the sun, the
moon, and the planets to a high degree of accuracy.
Any physical theory is always provisional, in the sense that it is only a hypothesis: you can never prove it. No
matter how many times the results of experiments agree with some theory, you can never be sure that the next
time the result will not contradict the theory. On the other hand, you can disprove a theory by finding even a
single observation that disagrees with the predictions of the theory. As philosopher of science Karl Popper has
emphasized, a good theory is characterized by the fact that it makes a number of predictions that could in
principle be disproved or falsified by observation. Each time new experiments are observed to agree with the
predictions the theory survives, and our confidence in it is increased; but if ever a new observation is found to
disagree, we have to abandon or modify the theory.
At least that is what is supposed to happen, but you can always question the competence of the person who
carried out the observation.
In practice, what often happens is that a new theory is devised that is really an extension of the previous theory.
For example, very accurate observations of the planet Mercury revealed a small difference between its motion
and the predictions of Newton’s theory of gravity. Einstein’s general theory of relativity predicted a slightly
different motion from Newton’s theory. The fact that Einstein’s predictions matched what was seen, while
Newton’s did not, was one of the crucial confirmations of the new theory. However, we still use Newton’s theory
for all practical purposes because the difference between its predictions and those of general relativity is very
small in the situations that we normally deal with. (Newton’s theory also has the great advantage that it is much
simpler to work with than Einstein’s!)
The eventual goal of science is to provide a single theory that describes the whole universe. However, the

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/n.html (5 of 7) [2/20/2001 3:14:06 AM]

A Brief History of Time - Stephen Hawking... Chapter 1

approach most scientists actually follow is to separate the problem into two parts. First, there are the laws that
tell us how the universe changes with time. (If we know what the universe is like at any one time, these physical
laws tell us how it will look at any later time.) Second, there is the question of the initial state of the universe.
Some people feel that science should be concerned with only the first part; they regard the question of the
initial situation as a matter for metaphysics or religion. They would say that God, being omnipotent, could have
started the universe off any way he wanted. That may be so, but in that case he also could have made it
develop in a completely arbitrary way. Yet it appears that he chose to make it evolve in a very regular way
according to certain laws. It therefore seems equally reasonable to suppose that there are also laws governing
the initial state.
It turns out to be very difficult to devise a theory to describe the universe all in one go. Instead, we break the
problem up into bits and invent a number of partial theories. Each of these partial theories describes and
predicts a certain limited class of observations, neglecting the effects of other quantities, or representing them
by simple sets of numbers. It may be that this approach is completely wrong. If everything in the universe
depends on everything else in a fundamental way, it might be impossible to get close to a full solution by
investigating parts of the problem in isolation. Nevertheless, it is certainly the way that we have made progress
in the past. The classic example again is the Newtonian theory of gravity, which tells us that the gravitational
force between two bodies depends only on one number associated with each body, its mass, but is otherwise
independent of what the bodies are made of. Thus one does not need to have a theory of the structure and
constitution of the sun and the planets in order to calculate their orbits.
Today scientists describe the universe in terms of two basic partial theories – the general theory of relativity
and quantum mechanics. They are the great intellectual achievements of the first half of this century. The
general theory of relativity describes the force of gravity and the large-scale structure of the universe, that is,
the structure on scales from only a few miles to as large as a million million million million (1 with twenty-four
zeros after it) miles, the size of the observable universe. Quantum mechanics, on the other hand, deals with
phenomena on extremely small scales, such as a millionth of a millionth of an inch. Unfortunately, however,
these two theories are known to be inconsistent with each other – they cannot both be correct. One of the
major endeavors in physics today, and the major theme of this book, is the search for a new theory that will
incorporate them both – a quantum theory of gravity. We do not yet have such a theory, and we may still be a
long way from having one, but we do already know many of the properties that it must have. And we shall see,
in later chapters, that we already know a fair amount about the predications a quantum theory of gravity must
make.
Now, if you believe that the universe is not arbitrary, but is governed by definite laws, you ultimately have to
combine the partial theories into a complete unified theory that will describe everything in the universe. But
there is a fundamental paradox in the search for such a complete unified theory. The ideas about scientific
theories outlined above assume we are rational beings who are free to observe the universe as we want and to
draw logical deductions from what we see.
In such a scheme it is reasonable to suppose that we might progress ever closer toward the laws that govern
our universe. Yet if there really is a complete unified theory, it would also presumably determine our actions.
And so the theory itself would determine the outcome of our search for it! And why should it determine that we
come to the right conclusions from the evidence? Might it not equally well determine that we draw the wrong
conclusion.? Or no conclusion at all?
The only answer that I can give to this problem is based on Darwin’s principle of natural selection. The idea is
that in any population of self-reproducing organisms, there will be variations in the genetic material and
upbringing that different individuals have. These differences will mean that some individuals are better able
than others to draw the right conclusions about the world around them and to act accordingly. These individuals
will be more likely to survive and reproduce and so their pattern of behavior and thought will come to dominate.
It has certainly been true in the past that what we call intelligence and scientific discovery have conveyed a
survival advantage. It is not so clear that this is still the case: our scientific discoveries may well destroy us all,
and even if they don’t, a complete unified theory may not make much difference to our chances of survival.
However, provided the universe has evolved in a regular way, we might expect that the reasoning abilities that
natural selection has given us would be valid also in our search for a complete unified theory, and so would not
lead us to the wrong conclusions.
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/n.html (6 of 7) [2/20/2001 3:14:06 AM]

A Brief History of Time - Stephen Hawking... Chapter 1

Because the partial theories that we already have are sufficient to make accurate predictions in all but the most
extreme situations, the search for the ultimate theory of the universe seems difficult to justify on practical
grounds. (It is worth noting, though, that similar arguments could have been used against both relativity and
quantum mechanics, and these theories have given us both nuclear energy and the microelectronics
revolution!) The discovery of a complete unified theory, therefore, may not aid the survival of our species. It
may not even affect our lifestyle. But ever since the dawn of civilization, people have not been content to see
events as unconnected and inexplicable. They have craved an understanding of the underlying order in the
world. Today we still yearn to know why we are here and where we came from. Humanity’s deepest desire for
knowledge is justification enough for our continuing quest. And our goal is nothing less than a complete
description of the universe we live in.

PREVIOUS

NEXT

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/n.html (7 of 7) [2/20/2001 3:14:06 AM]

A Brief History of Time - Stephen Hawking... Chapter 2

CHAPTER 2
SPACE AND TIME

Our present ideas about the motion of bodies date back to Galileo and Newton. Before them people believed
Aristotle, who said that the natural state of a body was to be at rest and that it moved only if driven by a force or
impulse. It followed that a heavy body should fall faster than a light one, because it would have a greater pull
toward the earth.
The Aristotelian tradition also held that one could work out all the laws that govern the universe by pure
thought: it was not necessary to check by observation. So no one until Galileo bothered to see whether bodies
of different weight did in fact fall at different speeds. It is said that Galileo demonstrated that Aristotle’s belief
was false by dropping weights from the leaning tower of Pisa. The story is almost certainly untrue, but Galileo
did do something equivalent: he rolled balls of different weights down a smooth slope. The situation is similar to
that of heavy bodies falling vertically, but it is easier to observe because the Speeds are smaller. Galileo’s
measurements indicated that each body increased its speed at the same rate, no matter what its weight. For
example, if you let go of a ball on a slope that drops by one meter for every ten meters you go along, the ball
will be traveling down the slope at a speed of about one meter per second after one second, two meters per
second after two seconds, and so on, however heavy the ball. Of course a lead weight would fall faster than a
feather, but that is only because a feather is slowed down by air resistance. If one drops two bodies that don’t
have much air resistance, such as two different lead weights, they fall at the same rate. On the moon, where
there is no air to slow things down, the astronaut David R. Scott performed the feather and lead weight
experiment and found that indeed they did hit the ground at the same time.
Galileo’s measurements were used by Newton as the basis of his laws of motion. In Galileo’s experiments, as a
body rolled down the slope it was always acted on by the same force (its weight), and the effect was to make it
constantly speed up. This showed that the real effect of a force is always to change the speed of a body, rather
than just to set it moving, as was previously thought. It also meant that whenever a body is not acted on by any
force, it will keep on moving in a straight line at the same speed. This idea was first stated explicitly in Newton’s
Principia Mathematica, published in 1687, and is known as Newton’s first law. What happens to a body when a
force does act on it is given by Newton’s second law. This states that the body will accelerate, or change its
speed, at a rate that is proportional to the force. (For example, the acceleration is twice as great if the force is
twice as great.) The acceleration is also smaller the greater the mass (or quantity of matter) of the body. (The
same force acting on a body of twice the mass will produce half the acceleration.) A familiar example is
provided by a car: the more powerful the engine, the greater the acceleration, but the heavier the car, the
smaller the acceleration for the same engine. In addition to his laws of motion, Newton discovered a law to
describe the force of gravity, which states that every body attracts every other body with a force that is
proportional to the mass of each body. Thus the force between two bodies would be twice as strong if one of
the bodies (say, body A) had its mass doubled. This is what you might expect because one could think of the
new body A as being made of two bodies with the original mass. Each would attract body B with the original
force. Thus the total force between A and B would be twice the original force. And if, say, one of the bodies had
twice the mass, and the other had three times the mass, then the force would be six times as strong. One can
now see why all bodies fall at the same rate: a body of twice the weight will have twice the force of gravity
pulling it down, but it will also have twice the mass. According to Newton’s second law, these two effects will
exactly cancel each other, so the acceleration will be the same in all cases.
Newton’s law of gravity also tells us that the farther apart the bodies, the smaller the force. Newton’s law of
gravity says that the gravitational attraction of a star is exactly one quarter that of a similar star at half the
distance. This law predicts the orbits of the earth, the moon, and the planets with great accuracy. If the law
were that the gravitational attraction of a star went down faster or increased more rapidly with distance, the
orbits of the planets would not be elliptical, they would either spiral in to the sun or escape from the sun.
The big difference between the ideas of Aristotle and those of Galileo and Newton is that Aristotle believed in a
preferred state of rest, which any body would take up if it were not driven by some force Or impulse. In
particular, he thought that the earth was at rest. But it follows from Newton’s laws that there is no unique

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/a.html (1 of 12) [2/20/2001 3:14:15 AM]

A Brief History of Time - Stephen Hawking... Chapter 2

standard of rest. One could equally well say that body A was at rest and body B was moving at constant speed
with respect to body A, or that body B was at rest and body A was moving. For example, if one sets aside for a
moment the rotation of the earth and its orbit round the sun, one could say that the earth was at rest and that a
train on it was traveling north at ninety miles per hour or that the train was at rest and the earth was moving
south at ninety miles per hour. If one carried out experiments with moving bodies on the train, all Newton’s laws
would still hold. For instance, playing Ping-Pong on the train, one would find that the ball obeyed Newton’s laws
just like a ball on a table by the track. So there is no way to tell whether it is the train or the earth that is moving.
The lack of an absolute standard of rest meant that one could not determine whether two events that took place
at different times occurred in the same position in space. For example, suppose our Ping-Pong ball on the train
bounces straight up and down, hitting the table twice on the same spot one second apart. To someone on the
track, the two bounces would seem to take place about forty meters apart, because the train would have
traveled that far down the track between the bounces. The nonexistence of absolute rest therefore meant that
one could not give an event an absolute position in space, as Aristotle had believed. The positions of events
and the distances between them would be different for a person on the train and one on the track, and there
would be no reason to prefer one person’s position to the other’s.
Newton was very worried by this lack of absolute position, or absolute space, as it was called, because it did
not accord with his idea of an absolute God. In fact, he refused to accept lack of absolute space, even though it
was implied by his laws. He was severely criticized for this irrational belief by many people, most notably by
Bishop Berkeley, a philosopher who believed that all material objects and space and time are an illusion. When
the famous Dr. Johnson was told of Berkeley’s opinion, he cried, “I refute it thus!” and stubbed his toe on a
large stone.
Both Aristotle and Newton believed in absolute time. That is, they believed that one could unambiguously
measure the interval of time between two events, and that this time would be the same whoever measured it,
provided they used a good clock. Time was completely separate from and independent of space. This is what
most people would take to be the commonsense view. However, we have had to change our ideas about space
and time. Although our apparently commonsense notions work well when dealing with things like apples, or
planets that travel comparatively slowly, they don’t work at all for things moving at or near the speed of light.
The fact that light travels at a finite, but very high, speed was first discovered in 1676 by the Danish astronomer
Ole Christensen Roemer. He observed that the times at which the moons of Jupiter appeared to pass behind
Jupiter were not evenly spaced, as one would expect if the moons went round Jupiter at a constant rate. As the
earth and Jupiter orbit around the sun, the distance between them varies. Roemer noticed that eclipses of
Jupiter’s moons appeared later the farther we were from Jupiter. He argued that this was because the light from
the moons took longer to reach us when we were farther away. His measurements of the variations in the
distance of the earth from Jupiter were, however, not very accurate, and so his value for the speed of light was
140,000 miles per second, compared to the modern value of 186,000 miles per second. Nevertheless,
Roemer’s achievement, in not only proving that light travels at a finite speed, but also in measuring that speed,
was remarkable – coming as it did eleven years before Newton’s publication of Principia Mathematica. A proper
theory of the propagation of light didn’t come until 1865, when the British physicist James Clerk Maxwell
succeeded in unifying the partial theories that up to then had been used to describe the forces of electricity and
magnetism. Maxwell’s equations predicted that there could be wavelike disturbances in the combined
electromagnetic field, and that these would travel at a fixed speed, like ripples on a pond. If the wavelength of
these waves (the distance between one wave crest and the next) is a meter or more, they are what we now call
radio waves. Shorter wavelengths are known as microwaves (a few centimeters) or infrared (more than a
ten-thousandth of a centimeter). Visible light has a wavelength of between only forty and eighty millionths of a
centimeter. Even shorter wavelengths are known as ultraviolet, X rays, and gamma rays.
Maxwell’s theory predicted that radio or light waves should travel at a certain fixed speed. But Newton’s theory
had got rid of the idea of absolute rest, so if light was supposed to travel at a fixed speed, one would have to
say what that fixed speed was to be measured relative to.
It was therefore suggested that there was a substance called the "ether" that was present everywhere, even in
"empty" space. Light waves should travel through the ether as sound waves travel through air, and their speed
should therefore be relative to the ether. Different observers, moving relative to the ether, would see light

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/a.html (2 of 12) [2/20/2001 3:14:15 AM]

A Brief History of Time - Stephen Hawking... Chapter 2

coming toward them at different speeds, but light's speed relative to the ether would remain fixed. In particular,
as the earth was moving through the ether on its orbit round the sun, the speed of light measured in the
direction of the earth's motion through the ether (when we were moving toward the source of the light) should
be higher than the speed of light at right angles to that motion (when we are not moving toward the source). In
1887Albert Michelson (who later became the first American to receive the Nobel Prize for physics) and Edward
Morley carried out a very careful experiment at the Case School of Applied Science in Cleveland. They
compared the speed of light in the direction of the earth's motion with that at right angles to the earth's motion.
To their great surprise, they found they were exactly the same!
Between 1887 and 1905 there were several attempts, most notably by the Dutch physicist Hendrik Lorentz, to
explain the result of the Michelson-Morley experiment in terms of objects contracting and clocks slowing down
when they moved through the ether. However, in a famous paper in 1905, a hitherto unknown clerk in the
Swiss patent office, Albert Einstein, pointed out that the whole idea of an ether was unnecessary, providing one
was willing to abandon the idea of absolute time. A similar point was made a few weeks later by a leading
French mathematician, Henri Poincare. Einstein’s arguments were closer to physics than those of Poincare,
who regarded this problem as mathematical. Einstein is usually given the credit for the new theory, but
Poincare is remembered by having his name attached to an important part of it.
The fundamental postulate of the theory of relativity, as it was called, was that the laws of science should be
the same for all freely moving observers, no matter what their speed. This was true for Newton’s laws of
motion, but now the idea was extended to include Maxwell’s theory and the speed of light: all observers should
measure the same speed of light, no matter how fast they are moving. This simple idea has some remarkable
consequences. Perhaps the best known are the equivalence of mass and energy, summed up in Einstein’s
famous equation E=mc2 (where E is energy, m is mass, and c is the speed of light), and the law that nothing
may travel faster than the speed of light. Because of the equivalence of energy and mass, the energy which an
object has due to its motion will add to its mass. In other words, it will make it harder to increase its speed. This
effect is only really significant for objects moving at speeds close to the speed of light. For example, at 10
percent of the speed of light an object’s mass is only 0.5 percent more than normal, while at 90 percent of the
speed of light it would be more than twice its normal mass. As an object approaches the speed of light, its mass
rises ever more quickly, so it takes more and more energy to speed it up further. It can in fact never reach the
speed of light, because by then its mass would have become infinite, and by the equivalence of mass and
energy, it would have taken an infinite amount of energy to get it there. For this reason, any normal object is
forever confined by relativity to move at speeds slower than the speed of light. Only light, or other waves that
have no intrinsic mass, can move at the speed of light.
An equally remarkable consequence of relativity is the way it has revolutionized our ideas of space and time. In
Newton’s theory, if a pulse of light is sent from one place to another, different observers would agree on the
time that the journey took (since time is absolute), but will not always agree on how far the light traveled (since
space is not absolute). Since the speed of the light is just the distance it has traveled divided by the time it has
taken, different observers would measure different speeds for the light. In relativity, on the other hand, all
observers must agree on how fast light travels. They still, however, do not agree on the distance the light has
traveled, so they must therefore now also disagree over the time it has taken. (The time taken is the distance
the light has traveled – which the observers do not agree on – divided by the light’s speed – which they do
agree on.) In other words, the theory of relativity put an end to the idea of absolute time! It appeared that each
observer must have his own measure of time, as recorded by a clock carried with him, and that identical clocks
carried by different observers would not necessarily agree.
Each observer could use radar to say where and when an event took place by sending out a pulse of light or
radio waves. Part of the pulse is reflected back at the event and the observer measures the time at which he
receives the echo. The time of the event is then said to be the time halfway between when the pulse was sent
and the time when the reflection was received back: the distance of the event is half the time taken for this
round trip, multiplied by the speed of light. (An event, in this sense, is something that takes place at a single
point in space, at a specified point in time.) This idea is shown here, which is an example of a space-time
diagram...

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/a.html (3 of 12) [2/20/2001 3:14:16 AM]

A Brief History of Time - Stephen Hawking... Chapter 2

Figure 2:1
Using this procedure, observers who are moving relative to each other will assign different times and positions
to the same event. No particular observer’s measurements are any more correct than any other observer’s, but
all the measurements are related. Any observer can work out precisely what time and position any other
observer will assign to an event, provided he knows the other observer’s relative velocity.
Nowadays we use just this method to measure distances precisely, because we can measure time more
accurately than length. In effect, the meter is defined to be the distance traveled by light in
0.000000003335640952 second, as measured by a cesium clock. (The reason for that particular number is that
it corresponds to the historical definition of the meter – in terms of two marks on a particular platinum bar kept
in Paris.) Equally, we can use a more convenient, new unit of length called a light-second. This is simply
defined as the distance that light travels in one second. In the theory of relativity, we now define distance in
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/a.html (4 of 12) [2/20/2001 3:14:16 AM]

A Brief History of Time - Stephen Hawking... Chapter 2

terms of time and the speed of light, so it follows automatically that every observer will measure light to have
the same speed (by definition, 1 meter per 0.000000003335640952 second). There is no need to introduce the
idea of an ether, whose presence anyway cannot be detected, as the Michelson-Morley experiment showed.
The theory of relativity does, however, force us to change fundamentally our ideas of space and time. We must
accept that time is not completely separate from and independent of space, but is combined with it to form an
object called space-time.
It is a matter of common experience that one can describe the position of a point in space by three numbers, or
coordinates. For instance, one can say that a point in a room is seven feet from one wall, three feet from
another, and five feet above the floor. Or one could specify that a point was at a certain latitude and longitude
and a certain height above sea level. One is free to use any three suitable coordinates, although they have only
a limited range of validity. One would not specify the position of the moon in terms of miles north and miles
west of Piccadilly Circus and feet above sea level. Instead, one might describe it in terms of distance from the
sun, distance from the plane of the orbits of the planets, and the angle between the line joining the moon to the
sun and the line joining the sun to a nearby star such as Alpha Centauri. Even these coordinates would not be
of much use in describing the position of the sun in our galaxy or the position of our galaxy in the local group of
galaxies. In fact, one may describe the whole universe in terms of a collection of overlapping patches. In each
patch, one can use a different set of three coordinates to specify the position of a point.
An event is something that happens at a particular point in space and at a particular time. So one can specify it
by four numbers or coordinates. Again, the choice of coordinates is arbitrary; one can use any three
well-defined spatial coordinates and any measure of time. In relativity, there is no real distinction between the
space and time coordinates, just as there is no real difference between any two space coordinates. One could
choose a new set of coordinates in which, say, the first space coordinate was a combination of the old first and
second space coordinates. For instance, instead of measuring the position of a point on the earth in miles north
of Piccadilly and miles west of Piccadilly, one could use miles northeast of Piccadilly, and miles north-west of
Piccadilly. Similarly, in relativity, one could use a new time coordinate that was the old time (in seconds) plus
the distance (in light-seconds) north of Piccadilly.
It is often helpful to think of the four coordinates of an event as specifying its position in a four-dimensional
space called space-time. It is impossible to imagine a four-dimensional space. I personally find it hard enough
to visualize three-dimensional space! However, it is easy to draw diagrams of two-dimensional spaces, such as
the surface of the earth. (The surface of the earth is two-dimensional because the position of a point can be
specified by two coordinates, latitude and longitude.) I shall generally use diagrams in which time increases
upward and one of the spatial dimensions is shown horizontally. The other two spatial dimensions are ignored
or, sometimes, one of them is indicated by perspective. (These are called space-time diagrams, like Figure
2:1.)

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/a.html (5 of 12) [2/20/2001 3:14:16 AM]

A Brief History of Time - Stephen Hawking... Chapter 2

Figure 2:2
For example, in Figure 2:2 time is measured upward in years and the distance along the line from the sun to
Alpha Centauri is measured horizontally in miles. The paths of the sun and of Alpha Centauri through
space-time are shown as the vertical lines on the left and right of the diagram. A ray of light from the sun
follows the diagonal line, and takes four years to get from the sun to Alpha Centauri.
As we have seen, Maxwell’s equations predicted that the speed of light should be the same whatever the
speed of the source, and this has been confirmed by accurate measurements. It follows from this that if a pulse
of light is emitted at a particular time at a particular point in space, then as time goes on it will spread out as a
sphere of light whose size and position are independent of the speed of the source. After one millionth of a
second the light will have spread out to form a sphere with a radius of 300 meters; after two millionths of a
second, the radius will be 600 meters; and so on. It will be like the ripples that spread out on the surface of a
pond when a stone is thrown in. The ripples spread out as a circle that gets bigger as time goes on. If one
stacks snapshots of the ripples at different times one above the other, the expanding circle of ripples will mark
out a cone whose tip is at the place and time at which the stone hit the water Figure 2:3.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/a.html (6 of 12) [2/20/2001 3:14:16 AM]

A Brief History of Time - Stephen Hawking... Chapter 2

Figure 2:3
Similarly, the light spreading out from an event forms a (three-dimensional) cone in (the four-dimensional)
space-time. This cone is called the future light cone of the event. In the same way we can draw another cone,
called the past light cone, which is the set of events from which a pulse of light is able to reach the given event
Figure 2:4.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/a.html (7 of 12) [2/20/2001 3:14:16 AM]

A Brief History of Time - Stephen Hawking... Chapter 2

Figure 2:4
Given an event P, one can divide the other events in the universe into three classes. Those events that can be
reached from the event P by a particle or wave traveling at or below the speed of light are said to be in the
future of P. They will lie within or on the expanding sphere of light emitted from the event P. Thus they will lie
within or on the future light cone of P in the space-time diagram. Only events in the future of P can be affected
by what happens at P because nothing can travel faster than light.
Similarly, the past of P can be defined as the set of all events from which it is possible to reach the event P
traveling at or below the speed of light. It is thus the set of events that can affect what happens at P. The
events that do not lie in the future or past of P are said to lie in the elsewhere of P Figure 2:5.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/a.html (8 of 12) [2/20/2001 3:14:16 AM]

A Brief History of Time - Stephen Hawking... Chapter 2

Figure 2:5
What happens at such events can neither affect nor be affected by what happens at P. For example, if the sun
were to cease to shine at this very moment, it would not affect things on earth at the present time because they
would be in the elsewhere of the event when the sun went out Figure 2:6.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/a.html (9 of 12) [2/20/2001 3:14:16 AM]

A Brief History of Time - Stephen Hawking... Chapter 2

Figure 2:6
We would know about it only after eight minutes, the time it takes light to reach us from the sun. Only then
would events on earth lie in the future light cone of the event at which the sun went out. Similarly, we do not
know what is happening at the moment farther away in the universe: the light that we see from distant galaxies
left them millions of years ago, and in the case of the most distant object that we have seen, the light left some
eight thousand million years ago. Thus, when we look at the universe, we are seeing it as it was in the past.
If one neglects gravitational effects, as Einstein and Poincare did in 1905, one has what is called the special
theory of relativity. For every event in space-time we may construct a light cone (the set of all possible paths of
light in space-time emitted at that event), and since the speed of light is the same at every event and in every
direction, all the light cones will be identical and will all point in the same direction. The theory also tells us that
nothing can travel faster than light. This means that the path of any object through space and time must be
represented by a line that lies within the light cone at each event on it (Fig. 2.7). The special theory of relativity
was very successful in explaining that the speed of light appears the same to all observers (as shown by the
Michelson-Morley experiment) and in describing what happens when things move at speeds close to the speed
of light. However, it was inconsistent with the Newtonian theory of gravity, which said that objects attracted
each other with a force that depended on the distance between them. This meant that if one moved one of the
objects, the force on the other one would change instantaneously. Or in other gravitational effects should travel
with infinite velocity, instead of at or below the speed of light, as the special theory of relativity required.
Einstein made a number of unsuccessful attempts between 1908 and 1914 to find a theory of gravity that was
consistent with special relativity. Finally, in 1915, he proposed what we now call the general theory of relativity.
Einstein made the revolutionary suggestion that gravity is not a force like other forces, but is a consequence of
the fact that space-time is not flat, as had been previously assumed: it is curved, or “warped,” by the distribution

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/a.html (10 of 12) [2/20/2001 3:14:16 AM]

A Brief History of Time - Stephen Hawking... Chapter 2

of mass and energy in it. Bodies like the earth are not made to move on curved orbits by a force called gravity;
instead, they follow the nearest thing to a straight path in a curved space, which is called a geodesic. A
geodesic is the shortest (or longest) path between two nearby points. For example, the surface of the earth is a
two-dimensional curved space. A geodesic on the earth is called a great circle, and is the shortest route
between two points (Fig. 2.8). As the geodesic is the shortest path between any two airports, this is the route
an airline navigator will tell the pilot to fly along. In general relativity, bodies always follow straight lines in
four-dimensional space-time, but they nevertheless appear to us to move along curved paths in our
three-dimensional space. (This is rather like watching an airplane flying over hilly ground. Although it follows a
straight line in three-dimensional space, its shadow follows a curved path on the two-dimensional ground.)
The mass of the sun curves space-time in such a way that although the earth follows a straight path in
four-dimensional space-time, it appears to us to move along a circular orbit in three-dimensional space.
Fact, the orbits of the planets predicted by general relativity are almost exactly the same as those predicted by
the Newtonian theory of gravity. However, in the case of Mercury, which, being the nearest planet to the sun,
feels the strongest gravitational effects, and has a rather elongated orbit, general relativity predicts that the long
axis of the ellipse should rotate about the sun at a rate of about one degree in ten thousand years. Small
though this effect is, it had been noticed before 1915 and served as one of the first confirmations of Einstein’s
theory. In recent years the even smaller deviations of the orbits of the other planets from the Newtonian
predictions have been measured by radar and found to agree with the predictions of general relativity.
Light rays too must follow geodesics in space-time. Again, the fact that space is curved means that light no
longer appears to travel in straight lines in space. So general relativity predicts that light should be bent by
gravitational fields. For example, the theory predicts that the light cones of points near the sun would be slightly
bent inward, on account of the mass of the sun. This means that light from a distant star that happened to pass
near the sun would be deflected through a small angle, causing the star to appear in a different position to an
observer on the earth (Fig. 2.9). Of course, if the light from the star always passed close to the sun, we would
not be able to tell whether the light was being deflected or if instead the star was really where we see it.
However, as the earth orbits around the sun, different stars appear to pass behind the sun and have their light
deflected. They therefore change their apparent position relative to other stars. It is normally very difficult to see
this effect, because the light from the sun makes it impossible to observe stars that appear near to the sun the
sky. However, it is possible to do so during an eclipse of the sun, when the sun’s light is blocked out by the
moon. Einstein’s prediction of light deflection could not be tested immediately in 1915, because the First World
War was in progress, and it was not until 1919 that a British expedition, observing an eclipse from West Africa,
showed that light was indeed deflected by the sun, just as predicted by the theory. This proof of a German
theory by British scientists was hailed as a great act of reconciliation between the two countries after the war. It
is ionic, therefore, that later examination of the photographs taken on that expedition showed the errors were as
great as the effect they were trying to measure. Their measurement had been sheer luck, or a case of knowing
the result they wanted to get, not an uncommon occurrence in science. The light deflection has, however, been
accurately confirmed by a number of later observations.
Another prediction of general relativity is that time should appear to slower near a massive body like the earth.
This is because there is a relation between the energy of light and its frequency (that is, the number of waves of
light per second): the greater the energy, the higher frequency. As light travels upward in the earth’s
gravitational field, it loses energy, and so its frequency goes down. (This means that the length of time between
one wave crest and the next goes up.) To someone high up, it would appear that everything down below was
making longer to happen. This prediction was tested in 1962, using a pair of very accurate clocks mounted at
the top and bottom of a water tower. The clock at the bottom, which was nearer the earth, was found to run
slower, in exact agreement with general relativity. The difference in the speed of clocks at different heights
above the earth is now of considerable practical importance, with the advent of very accurate navigation
systems based on signals from satellites. If one ignored the predictions of general relativity, the position that
one calculated would be wrong by several miles!
Newton’s laws of motion put an end to the idea of absolute position in space. The theory of relativity gets rid of
absolute time. Consider a pair of twins. Suppose that one twin goes to live on the top of a mountain while the
other stays at sea level. The first twin would age faster than the second. Thus, if they met again, one would be
older than the other. In this case, the difference in ages would be very small, but it would be much larger if one
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/a.html (11 of 12) [2/20/2001 3:14:16 AM]

A Brief History of Time - Stephen Hawking... Chapter 2

of the twins went for a long trip in a spaceship at nearly the speed of light. When he returned, he would be
much younger than the one who stayed on earth. This is known as the twins paradox, but it is a paradox only if
one has the idea of absolute time at the back of one’s mind. In the theory of relativity there is no unique
absolute time, but instead each individual has his own personal measure of time that depends on where he is
and how he is moving.
Before 1915, space and time were thought of as a fixed arena in which events took place, but which was not
affected by what happened in it. This was true even of the special theory of relativity. Bodies moved, forces
attracted and repelled, but time and space simply continued, unaffected. It was natural to think that space and
time went on forever.
The situation, however, is quite different in the general theory of relativity. Space and time are now dynamic
quantities: when a body moves, or a force acts, it affects the curvature of space and time – and in turn the
structure of space-time affects the way in which bodies move and forces act. Space and time not only affect but
also are affected by everything that happens in the universe. Just as one cannot talk about events in the
universe without the notions of space and time, so in general relativity it became meaningless to talk about
space and time outside the limits of the universe.
In the following decades this new understanding of space and time was to revolutionize our view of the
universe. The old idea of an essentially unchanging universe that could have existed, and could continue to
exist, forever was replaced by the notion of a dynamic, expanding universe that seemed to have begun a finite
time ago, and that might end at a finite time in the future. That revolution forms the subject of the next chapter.
And years later, it was also to be the starting point for my work in theoretical physics. Roger Penrose and I
showed that Einstein’s general theory of relativity implied that the universe must have a beginning and,
possibly, an end.

PREVIOUS

NEXT

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/a.html (12 of 12) [2/20/2001 3:14:16 AM]

A Brief History of Time - Stephen Hawking... Chapter 3

CHAPTER 3
THE EXPANDING UNIVERSE

If one looks at the sky on a clear, moonless night, the brightest objects one sees are likely to be the planets
Venus, Mars, Jupiter, and Saturn. There will also be a very large number of stars, which are just like our own
sun but much farther from us. Some of these fixed stars do, in fact, appear to change very slightly their
positions relative to each other as earth orbits around the sun: they are not really fixed at all! This is because
they are comparatively near to us. As the earth goes round the sun, we see them from different positions
against the background of more distant stars. This is fortunate, because it enables us to measure directly the
distance of these stars from us: the nearer they are, the more they appear to move. The nearest star, called
Proxima Centauri, is found to be about four light-years away (the light from it takes about four years to reach
earth), or about twenty-three million million miles. Most of the other stars that are visible to the naked eye lie
within a few hundred light-years of us. Our sun, for comparison, is a mere light-minutes away! The visible stars
appear spread all over the night sky, but are particularly concentrated in one band, which we call the Milky
Way. As long ago as 1750, some astronomers were suggesting that the appearance of the Milky Way could be
explained if most of the visible stars lie in a single disklike configuration, one example of what we now call a
spiral galaxy. Only a few decades later, the astronomer Sir William Herschel confirmed this idea by
painstakingly cataloging the positions and distances of vast numbers of stars. Even so, the idea gained
complete acceptance only early this century.
Our modern picture of the universe dates back to only 1924, when the American astronomer Edwin Hubble
demonstrated that ours was not the only galaxy. There were in fact many others, with vast tracts of empty
space between them. In order to prove this, he needed to determine the distances to these other galaxies,
which are so far away that, unlike nearby stars, they really do appear fixed. Hubble was forced, therefore, to
use indirect methods to measure the distances. Now, the apparent brightness of a star depends on two factors:
how much light it radiates (its luminosity), and how far it is from us. For nearby stars, we can measure their
apparent brightness and their distance, and so we can work out their luminosity. Conversely, if we knew the
luminosity of stars in other galaxies, we could work out their distance by measuring their apparent brightness.
Hubble noted that certain types of stars always have the same luminosity when they are near enough for us to
measure; therefore, he argued, if we found such stars in another galaxy, we could assume that they had the
same luminosity – and so calculate the distance to that galaxy. If we could do this for a number of stars in the
same galaxy, and our calculations always gave the same distance, we could be fairly confident of our estimate.
In this way, Edwin Hubble worked out the distances to nine different galaxies. We now know that our galaxy is
only one of some hundred thousand million that can be seen using modern telescopes, each galaxy itself
containing some hundred thousand million stars. Figure 3:1 shows a picture of one spiral galaxy that is similar
to what we think ours must look like to someone living in another galaxy.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/b.html (1 of 9) [2/20/2001 3:14:24 AM]

A Brief History of Time - Stephen Hawking... Chapter 3

Figure 3:1
We live in a galaxy that is about one hundred thousand light-years across and is slowly rotating; the stars in its
spiral arms orbit around its center about once every several hundred million years. Our sun is just an ordinary,
average-sized, yellow star, near the inner edge of one of the spiral arms. We have certainly come a long way
since Aristotle and Ptolemy, when thought that the earth was the center of the universe!
Stars are so far away that they appear to us to be just pinpoints of light. We cannot see their size or shape. So
how can we tell different types of stars apart? For the vast majority of stars, there is only one characteristic
feature that we can observe – the color of their light. Newton discovered that if light from the sun passes
through a triangular-shaped piece of glass, called a prism, it breaks up into its component colors (its spectrum)
as in a rainbow. By focusing a telescope on an individual star or galaxy, one can similarly observe the spectrum
of the light from that star or galaxy. Different stars have different spectra, but the relative brightness of the
different colors is always exactly what one would expect to find in the light emitted by an object that is glowing
red hot. (In fact, the light emitted by any opaque object that is glowing red hot has a characteristic spectrum
that depends only on its temperature – a thermal spectrum. This means that we can tell a star’s temperature
from the spectrum of its light.) Moreover, we find that certain very specific colors are missing from stars’
spectra, and these missing colors may vary from star to star. Since we know that each chemical element
absorbs a characteristic set of very specific colors, by matching these to those that are missing from a star’s
spectrum, we can determine exactly which elements are present in the star’s atmosphere.
In the 1920s, when astronomers began to look at the spectra of stars in other galaxies, they found something
most peculiar: there were the same characteristic sets of missing colors as for stars in our own galaxy, but they
were all shifted by the same relative amount toward the red end of the spectrum. To understand the
implications of this, we must first understand the Doppler effect. As we have seen, visible light consists of
fluctuations, or waves, in the electromagnetic field. The wavelength (or distance from one wave crest to the
next) of light is extremely small, ranging from four to seven ten-millionths of a meter. The different wavelengths
of light are what the human eye sees as different colors, with the longest wavelengths appearing at the red end
of the spectrum and the shortest wavelengths at the blue end. Now imagine a source of light at a constant
distance from us, such as a star, emitting waves of light at a constant wavelength. Obviously the wavelength of

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/b.html (2 of 9) [2/20/2001 3:14:24 AM]

A Brief History of Time - Stephen Hawking... Chapter 3

the waves we receive will be the same as the wavelength at which they are emitted (the gravitational field of the
galaxy will not be large enough to have a significant effect). Suppose now that the source starts moving toward
us. When the source emits the next wave crest it will be nearer to us, so the distance between wave crests will
be smaller than when the star was stationary. This means that the wavelength of the waves we receive is
shorter than when the star was stationary. Correspondingly, if the source is moving away from us, the
wavelength of the waves we receive will be longer. In the case of light, therefore, means that stars moving
away from us will have their spectra shifted toward the red end of the spectrum (red-shifted) and those moving
toward us will have their spectra blue-shifted. This relationship between wavelength and speed, which is called
the Doppler effect, is an everyday experience. Listen to a car passing on the road: as the car is approaching, its
engine sounds at a higher pitch (corresponding to a shorter wavelength and higher frequency of sound waves),
and when it passes and goes away, it sounds at a lower pitch. The behavior of light or radio waves is similar.
Indeed, the police make use of the Doppler effect to measure the speed of cars by measuring the wavelength
of pulses of radio waves reflected off them.
ln the years following his proof of the existence of other galaxies, Rubble spent his time cataloging their
distances and observing their spectra. At that time most people expected the galaxies to be moving around
quite randomly, and so expected to find as many blue-shifted spectra as red-shifted ones. It was quite a
surprise, therefore, to find that most galaxies appeared red-shifted: nearly all were moving away from us! More
surprising still was the finding that Hubble published in 1929: even the size of a galaxy’s red shift is not random,
but is directly proportional to the galaxy’s distance from us. Or, in other words, the farther a galaxy is, the faster
it is moving away! And that meant that the universe could not be static, as everyone previously had thought, is
in fact expanding; the distance between the different galaxies is changing all the time.
The discovery that the universe is expanding was one of the great intellectual revolutions of the twentieth
century. With hindsight, it is easy wonder why no one had thought of it before. Newton, and others should have
realized that a static universe would soon start to contract under the influence of gravity. But suppose instead
that the universe is expanding. If it was expanding fairly slowly, the force of gravity would cause it eventually to
stop expanding and then to start contracting. However, if it was expanding at more than a certain critical rate,
gravity would never be strong enough to stop it, and the universe would continue to expand forever. This is a bit
like what happens when one fires a rocket upward from the surface of the earth. If it has a fairly low speed,
gravity will eventually stop the rocket and it will start falling back. On the other hand, if the rocket has more than
a certain critical speed (about seven miles per second), gravity will not be strong enough to pull it back, so it will
keep going away from the earth forever. This behavior of the universe could have been predicted from
Newton’s theory of gravity at any time in the nineteenth, the eighteenth, or even the late seventeenth century.
Yet so strong was the belief in a static universe that it persisted into the early twentieth century. Even Einstein,
when he formulated the general theory of relativity in 1915, was so sure that the universe had to be static that
he modified his theory to make this possible, introducing a so-called cosmological constant into his equations.
Einstein introduced a new “antigravity” force, which, unlike other forces, did not come from any particular
source but was built into the very fabric of space-time. He claimed that space-time had an inbuilt tendency to
expand, and this could be made to balance exactly the attraction of all the matter in the universe, so that a
static universe would result. Only one man, it seems, was willing to take general relativity at face value, and
while Einstein and other physicists were looking for ways of avoiding general relativity’s prediction of a
nonstatic universe, the Russian physicist and mathematician Alexander Friedmann instead set about explaining
it.
Friedmann made two very simple assumptions about the universe: that the universe looks identical in
whichever direction we look, and that this would also be true if we were observing the universe from anywhere
else. From these two ideas alone, Friedmann showed that we should not expect the universe to be static. In
fact, in 1922, several years before Edwin Hubble’s discovery, Friedmann predicted exactly what Hubble found!
The assumption that the universe looks the same in every direction is clearly not true in reality. For example, as
we have seen, the other stars in our galaxy form a distinct band of light across the night sky, called the Milky
Way. But if we look at distant galaxies, there seems to be more or less the same number of them. So the
universe does seem to be roughly the same in every direction, provided one views it on a large scale compared
to the distance between galaxies, and ignores the differences on small scales. For a long time, this was
sufficient justification for Friedmann’s assumption – as a rough approximation to the real universe. But more
recently a lucky accident uncovered the fact that Friedmann’s assumption is in fact a remarkably accurate
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/b.html (3 of 9) [2/20/2001 3:14:24 AM]

A Brief History of Time - Stephen Hawking... Chapter 3

description of our universe.
In 1965 two American physicists at the Bell Telephone Laboratories in New Jersey, Arno Penzias and Robert
Wilson, were testing a very sensitive microwave detector. (Microwaves are just like light waves, but with a
wavelength of around a centimeter.) Penzias and Wilson were worried when they found that their detector was
picking up more noise than it ought to. The noise did not appear to be coming from any particular direction.
First they discovered bird droppings in their detector and checked for other possible malfunctions, but soon
ruled these out. They knew that any noise from within the atmosphere would be stronger when the detector
was not pointing straight up than when it was, because light rays travel through much more atmosphere when
received from near the horizon than when received from directly overhead. The extra noise was the same
whichever direction the detector was pointed, so it must come from outside the atmosphere. It was also the
same day and night and throughout the year, even though the earth was rotating on its axis and orbiting around
the sun. This showed that the radiation must come from beyond the Solar System, and even from beyond the
galaxy, as otherwise it would vary as the movement of earth pointed the detector in different directions.
In fact, we know that the radiation must have traveled to us across most of the observable universe, and since
it appears to be the same in different directions, the universe must also be the same in every direction, if only
on a large scale. We now know that whichever direction we look, this noise never varies by more than a tiny
fraction: so Penzias and Wilson had unwittingly stumbled across a remarkably accurate confirmation of
Friedmann’s first assumption. However, because the universe is not exactly the same in every direction, but
only on average on a large scale, the microwaves cannot be exactly the same in every direction either. There
have to be slight variations between different directions. These were first detected in 1992 by the Cosmic
Background Explorer satellite, or COBE, at a level of about one part in a hundred thousand. Small though these
variations are, they are very important, as will be explained in Chapter 8.
At roughly the same time as Penzias and Wilson were investigating noise in their detector, two American
physicists at nearby Princeton University, Bob Dicke and Jim Peebles, were also taking an interest in
microwaves. They were working on a suggestion, made by George Gamow (once a student of Alexander
Friedmann), that the early universe should have been very hot and dense, glowing white hot. Dicke and
Peebles argued that we should still be able to see the glow of the early universe, because light from very
distant parts of it would only just be reaching us now. However, the expansion of the universe meant that this
light should be so greatly red-shifted that it would appear to us now as microwave radiation. Dicke and Peebles
were preparing to look for this radiation when Penzias and Wilson heard about their work and realized that they
had already found it. For this, Penzias and Wilson were awarded the Nobel Prize in 1978 (which seems a bit
hard on Dicke and Peebles, not to mention Gamow!).
Now at first sight, all this evidence that the universe looks the same whichever direction we look in might seem
to suggest there is something special about our place in the universe. In particular, it might seem that if we
observe all other galaxies to be moving away from us, then we must be at the center of the universe. There is,
however, an alternate explanation: the universe might look the same in every direction as seen from any other
galaxy too. This, as we have seen, was Friedmann’s second assumption. We have no scientific evidence for, or
against, this assumption. We believe it only on grounds of modesty: it would be most remarkable if the universe
looked the same in every direction around us, but not around other points in the universe! In Friedmann’s
model, all the galaxies are moving directly away from each other. The situation is rather like a balloon with a
number of spots painted on it being steadily blown up. As the balloon expands, the distance between any two
spots increases, but there is no spot that can be said to be the center of the expansion. Moreover, the farther
apart the spots are, the faster they will be moving apart. Similarly, in Friedmann’s model the speed at which any
two galaxies are moving apart is proportional to the distance between them. So it predicted that the red shift of
a galaxy should be directly proportional to its distance from us, exactly as Hubble found. Despite the success of
his model and his prediction of Hubble’s observations, Friedmann’s work remained largely unknown in the West
until similar models were discovered in 1935 by the American physicist Howard Robertson and the British
mathematician Arthur Walker, in response to Hubble’s discovery of the uniform expansion of the universe.
Although Friedmann found only one, there are in fact three different kinds of models that obey Friedmann’s two
fundamental assumptions. In the first kind (which Friedmann found) the universe is expanding sufficiently
slowly that the gravitational attraction between the different galaxies causes the expansion to slow down and
eventually to stop. The galaxies then start to move toward each other and the universe contracts.
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/b.html (4 of 9) [2/20/2001 3:14:24 AM]

A Brief History of Time - Stephen Hawking... Chapter 3

Figure 3:2
Figure 3:2 shows how the distance between two neighboring galaxies changes as time increases. It starts at
zero, increases to a maximum, and then decreases to zero again. In the second kind of solution, the universe is
expanding so rapidly that the gravitational attraction can never stop it, though it does slow it down a bit.

Figure 3:3
Figure 3:3 Shows the Separation between neighboring galaxies in this model. It starts at zero and eventually
the galaxies are moving apart at a steady speed. Finally, there is a third kind of solution, in which the universe
is expanding only just fast enough to avoid recollapse.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/b.html (5 of 9) [2/20/2001 3:14:24 AM]

A Brief History of Time - Stephen Hawking... Chapter 3

Figure 3:4
In this case the separation, shown in Figure 3:4, also starts at zero and increases forever. However, the speed
at which the galaxies are moving apart gets smaller and smaller, although it never quite reaches zero.
A remarkable feature of the first kind of Friedmann model is that in it the universe is not infinite in space, but
neither does space have any boundary. Gravity is so strong that space is bent round onto itself, making it rather
like the surface of the earth. If one keeps traveling in a certain direction on the surface of the earth, one never
comes up against an impassable barrier or falls over the edge, but eventually comes back to where one
started.
In the first kind of Friedmann model, space is just like this, but with three dimensions instead of two for the
earth’s surface. The fourth dimension, time, is also finite in extent, but it is like a line with two ends or
boundaries, a beginning and an end. We shall see later that when one combines general relativity with the
uncertainty principle of quantum mechanics, it is possible for both space and time to be finite without any edges
or boundaries.
The idea that one could go right round the universe and end up where one started makes good science fiction,
but it doesn’t have much practical significance, because it can be shown that the universe would recollapse to
zero size before one could get round. You would need to travel faster than light in order to end up where you
started before the universe came to an end – and that is not allowed!
In the first kind of Friedmann model, which expands and recollapses, space is bent in on itself, like the surface
of the earth. It is therefore finite in extent. In the second kind of model, which expands forever, space is bent
the other way, like the surface of a saddle. So in this case space is infinite. Finally, in the third kind of
Friedmann model, with just the critical rate of expansion, space is flat (and therefore is also infinite).
But which Friedmann model describes our universe? Will the universe eventually stop expanding and start
contracting, or will it expand forever? To answer this question we need to know the present rate of expansion of
the universe and its present average density. If the density is less than a certain critical value, determined by
the rate of expansion, the gravitational attraction will be too weak to halt the expansion. If the density is greater
than the critical value, gravity will stop the expansion at some time in the future and cause the universe to
recollapse.
We can determine the present rate of expansion by measuring the velocities at which other galaxies are
moving away from us, using the Doppler effect. This can be done very accurately. However, the distances to
the galaxies are not very well known because we can only measure them indirectly. So all we know is that the
universe is expanding by between 5 percent and 10 percent every thousand million years. However, our
uncertainty about the present average density of the universe is even greater. If we add up the masses of all
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/b.html (6 of 9) [2/20/2001 3:14:24 AM]

A Brief History of Time - Stephen Hawking... Chapter 3

the stars that we can see in our galaxy and other galaxies, the total is less than one hundredth of the amount
required to halt the expansion of the universe, even for the lowest estimate of the rate of expansion. Our galaxy
and other galaxies, however, must contain a large amount of “dark matter” that we cannot see directly, but
which we know must be there because of the influence of its gravitational attraction on the orbits of stars in the
galaxies. Moreover, most galaxies are found in clusters, and we can similarly infer the presence of yet more
dark matter in between the galaxies in these clusters by its effect on the motion of the galaxies. When we add
up all this dark matter, we still get only about one tenth of the amount required to halt the expansion. However,
we cannot exclude the possibility that there might be some other form of matter, distributed almost uniformly
throughout the universe, that we have not yet detected and that might still raise the average density of the
universe up to the critical value needed to halt the expansion. The present evidence therefore suggests that the
universe will probably expand forever, but all we can really be sure of is that even if the universe is going to
recollapse, it won’t do so for at least another ten thousand million years, since it has already been expanding
for at least that long. This should not unduly worry us: by that time, unless we have colonized beyond the Solar
System, mankind will long since have died out, extinguished along with our sun!
All of the Friedmann solutions have the feature that at some time in the past (between ten and twenty thousand
million years ago) the distance between neighboring galaxies must have been zero. At that time, which we call
the big bang, the density of the universe and the curvature of space-time would have been infinite. Because
mathematics cannot really handle infinite numbers, this means that the general theory of relativity (on which
Friedmann’s solutions are based) predicts that there is a point in the universe where the theory itself breaks
down. Such a point is an example of what mathematicians call a singularity. In fact, all our theories of science
are formulated on the assumption that space-time is smooth and nearly fiat, so they break down at the big bang
singularity, where the curvature of space-time is infinite. This means that even if there were events before the
big bang, one could not use them to determine what would happen afterward, because predictability would
break down at the big bang.
Correspondingly, if, as is the case, we know only what has happened since the big bang, we could not
determine what happened beforehand. As far as we are concerned, events before the big bang can have no
consequences, so they should not form part of a scientific model of the universe. We should therefore cut them
out of the model and say that time had a beginning at the big bang.
Many people do not like the idea that time has a beginning, probably because it smacks of divine intervention.
(The Catholic Church, on the other hand, seized on the big bang model and in 1951officially pronounced it to
be in accordance with the Bible.) There were therefore a number of attempts to avoid the conclusion that there
had been a big bang. The proposal that gained widest support was called the steady state theory. It was
suggested in 1948 by two refugees from Nazi-occupied Austria, Hermann Bondi and Thomas Gold, together
with a Briton, Fred Hoyle, who had worked with them on the development of radar during the war. The idea was
that as the galaxies moved away from each other, new galaxies were continually forming in the gaps in
between, from new matter that was being continually created. The universe would therefore look roughly the
same at all times as well as at all points of space. The steady state theory required a modification of general
relativity to allow for the continual creation of matter, but the rate that was involved was so low (about one
particle per cubic kilometer per year) that it was not in conflict with experiment. The theory was a good scientific
theory, in the sense described in Chapter 1: it was simple and it made definite predictions that could be tested
by observation. One of these predictions was that the number of galaxies or similar objects in any given volume
of space should be the same wherever and whenever we look in the universe. In the late 1950s and early
1960s a survey of sources of radio waves from outer space was carried out at Cambridge by a group of
astronomers led by Martin Ryle (who had also worked with Bondi, Gold, and Hoyle on radar during the war).
The Cambridge group showed that most of these radio sources must lie outside our galaxy (indeed many of
them could be identified with other galaxies) and also that there were many more weak sources than strong
ones. They interpreted the weak sources as being the more distant ones, and the stronger ones as being
nearer. Then there appeared to be less common sources per unit volume of space for the nearby sources than
for the distant ones. This could mean that we are at the center of a great region in the universe in which the
sources are fewer than elsewhere. Alternatively, it could mean that the sources were more numerous in the
past, at the time that the radio waves left on their journey to us, than they are now. Either explanation
contradicted the predictions of the steady state theory. Moreover, the discovery of the microwave radiation by
Penzias and Wilson in 1965 also indicated that the universe must have been much denser in the past. The
steady state theory therefore had to be abandoned.
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/b.html (7 of 9) [2/20/2001 3:14:24 AM]

A Brief History of Time - Stephen Hawking... Chapter 3

Another attempt to avoid the conclusion that there must have been a big bang, and therefore a beginning of
time, was made by two Russian scientists, Evgenii Lifshitz and Isaac Khalatnikov, in 1963. They suggested that
the big bang might be a peculiarity of Friedmann’s models alone, which after all were only approximations to
the real universe. Perhaps, of all the models that were roughly like the real universe, only Friedmann’s would
contain a big bang singularity. In Friedmann’s models, the galaxies are all moving directly away from each
other – so it is not surprising that at some time in the past they were all at the same place. In the real universe,
however, the galaxies are not just moving directly away from each other – they also have small sideways
velocities. So in reality they need never have been all at exactly the same place, only very close together.
Perhaps then the current expanding universe resulted not from a big bang singularity, but from an earlier
contracting phase; as the universe had collapsed the particles in it might not have all collided, but had flown
past and then away from each other, producing the present expansion of the the universe that were roughly like
Friedmann’s models but took account of the irregularities and random velocities of galaxies in the real universe.
They showed that such models could start with a big bang, even though the galaxies were no longer always
moving directly away from each other, but they claimed that this was still only possible in certain exceptional
models in which the galaxies were all moving in just the right way. They argued that since there seemed to be
infinitely more Friedmann-like models without a big bang singularity than there were with one, we should
conclude that there had not in reality been a big bang. They later realized, however, that there was a much
more general class of Friedmann-like models that did have singularities, and in which the galaxies did not have
to be moving any special way. They therefore withdrew their claim in 1970.
The work of Lifshitz and Khalatnikov was valuable because it showed that the universe could have had a
singularity, a big bang, if the general theory of relativity was correct. However, it did not resolve the crucial
question: Does general relativity predict that our universe should have had a big bang, a beginning of time?
The answer to this carne out of a completely different approach introduced by a British mathematician and
physicist, Roger Penrose, in 1965. Using the way light cones behave in general relativity, together with the fact
that gravity is always attractive, he showed that a star collapsing under its own gravity is trapped in a region
whose surface eventually shrinks to zero size. And, since the surface of the region shrinks to zero, so too must
its volume. All the matter in the star will be compressed into a region of zero volume, so the density of matter
and the curvature of space-time become infinite. In other words, one has a singularity contained within a region
of space-time known as a black hole.
At first sight, Penrose’s result applied only to stars; it didn’t have anything to say about the question of whether
the entire universe had a big bang singularity in its past. However, at the time that Penrose produced his
theorem, I was a research student desperately looking for a problem with which to complete my Ph.D. thesis.
Two years before, I had been diagnosed as suffering from ALS, commonly known as Lou Gehrig’s disease, or
motor neuron disease, and given to understand that I had only one or two more years to live. In these
circumstances there had not seemed much point in working on my Ph.D.– I did not expect to survive that long.
Yet two years had gone by and I was not that much worse. In fact, things were going rather well for me and I
had gotten engaged to a very nice girl, Jane Wilde. But in order to get married, I needed a job, and in order to
get a job, I needed a Ph.D.
In 1965 I read about Penrose’s theorem that any body undergoing gravitational collapse must eventually form a
singularity. I soon realized that if one reversed the direction of time in Penrose’s theorem, so that the collapse
became an expansion, the conditions of his theorem would still hold, provided the universe were roughly like a
Friedmann model on large scales at the present time. Penrose’s theorem had shown that any collapsing star
must end in a singularity; the time-reversed argument showed that any Friedmann-like expanding universe
must have begun with a singularity. For technical reasons, Penrose’s theorem required that the universe be
infinite in space. So I could in fact, use it to prove that there should be a singularity only if the universe was
expanding fast enough to avoid collapsing again (since only those Friedmann models were infinite in space).
During the next few years I developed new mathematical techniques to remove this and other technical
conditions from the theorems that proved that singularities must occur. The final result was a joint paper by
Penrose and myself in 1970, which at last proved that there must have been a big bang singularity provided
only that general relativity is correct and the universe contains as much matter as we observe. There was a lot
of opposition to our work, partly from the Russians because of their Marxist belief in scientific determinism, and
partly from people who felt that the whole idea of singularities was repugnant and spoiled the beauty of
Einstein’s theory. However, one cannot really argue with a mathematical theorem. So in the end our work
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/b.html (8 of 9) [2/20/2001 3:14:24 AM]

A Brief History of Time - Stephen Hawking... Chapter 3

became generally accepted and nowadays nearly everyone assumes that the universe started with a big bang
singularity. It is perhaps ironic that, having changed my mind, I am now trying to convince other physicists that
there was in fact no singularity at the beginning of the universe – as we shall see later, it can disappear once
quantum effects are taken into account.
We have seen in this chapter how, in less than half a century, man’s view of the universe formed over millennia
has been transformed. Hubble’s discovery that the universe was expanding, and the realization of the
insignificance of our own planet in the vastness of the universe, were just the starting point. As experimental
and theoretical evidence mounted, it became more and more clear that the universe must have had a
beginning in time, until in 1970 this was finally proved by Penrose and myself, on the basis of Einstein’s general
theory of relativity. That proof showed that general relativity is only an incomplete theory: it cannot tell us how
the universe started off, because it predicts that all physical theories, including itself, break down at the
beginning of the universe. However, general relativity claims to be only a partial theory, so what the singularity
theorems really show is that there must have been a time in the very early universe when the universe was so
small that one could no longer ignore the small-scale effects of the other great partial theory of the twentieth
century, quantum mechanics. At the start of the 1970s, then, we were forced to turn our search for an
understanding of the universe from our theory of the extraordinarily vast to our theory of the extraordinarily tiny.
That theory, quantum mechanics, will be described next, before we turn to the efforts to combine the two partial
theories into a single quantum theory of gravity.

PREVIOUS

NEXT

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/b.html (9 of 9) [2/20/2001 3:14:24 AM]

A Brief History of Time - Stephen Hawking... Chapter 4

CHAPTER 4
THE UNCERTAINTY PRINCIPLE

The success of scientific theories, particularly Newton’s theory of gravity, led the French scientist the Marquis de Laplace
at the beginning of the nineteenth century to argue that the universe was completely deterministic. Laplace suggested
that there should be a set of scientific laws that would allow us to predict everything that would happen in the universe, if
only we knew the complete state of the universe at one time. For example, if we knew the positions and speeds of the
sun and the planets at one time, then we could use Newton’s laws to calculate the state of the Solar System at any other
time. Determinism seems fairly obvious in this case, but Laplace went further to assume that there were similar laws
governing everything else, including human behavior.
The doctrine of scientific determinism was strongly resisted by many people, who felt that it infringed God’s freedom to
intervene in the world, but it remained the standard assumption of science until the early years of this century. One of the
first indications that this belief would have to be abandoned came when calculations by the British scientists Lord
Rayleigh and Sir James Jeans suggested that a hot object, or body, such as a star, must radiate energy at an infinite
rate. According to the laws we believed at the time, a hot body ought to give off electromagnetic waves (such as radio
waves, visible light, or X rays) equally at all frequencies. For example, a hot body should radiate the same amount of
energy in waves with frequencies between one and two million million waves a second as in waves with frequencies
between two and three million million waves a second. Now since the number of waves a second is unlimited, this would
mean that the total energy radiated would be infinite.
In order to avoid this obviously ridiculous result, the German scientist Max Planck suggested in 1900 that light, X rays,
and other waves could not be emitted at an arbitrary rate, but only in certain packets that he called quanta. Moreover,
each quantum had a certain amount of energy that was greater the higher the frequency of the waves, so at a high
enough frequency the emission of a single quantum would require more energy than was available. Thus the radiation at
high frequencies would be reduced, and so the rate at which the body lost energy would be finite.
The quantum hypothesis explained the observed rate of emission of radiation from hot bodies very well, but its
implications for determinism were not realized until 1926, when another German scientist, Werner Heisenberg,
formulated his famous uncertainty principle. In order to predict the future position and velocity of a particle, one has to be
able to measure its present position and velocity accurately. The obvious way to do this is to shine light on the particle.
Some of the waves of light will be scattered by the particle and this will indicate its position. However, one will not be able
to determine the position of the particle more accurately than the distance between the wave crests of light, so one needs
to use light of a short wavelength in order to measure the position of the particle precisely. Now, by Planck’s quantum
hypothesis, one cannot use an arbitrarily small amount of light; one has to use at least one quantum. This quantum will
disturb the particle and change its velocity in a way that cannot be predicted. moreover, the more accurately one
measures the position, the shorter the wavelength of the light that one needs and hence the higher the energy of a single
quantum. So the velocity of the particle will be disturbed by a larger amount. In other words, the more accurately you try
to measure the position of the particle, the less accurately you can measure its speed, and vice versa. Heisenberg
showed that the uncertainty in the position of the particle times the uncertainty in its velocity times the mass of the
particle can never be smaller than a certain quantity, which is known as Planck’s constant. Moreover, this limit does not
depend on the way in which one tries to measure the position or velocity of the particle, or on the type of particle:
Heisenberg’s uncertainty principle is a fundamental, inescapable property of the world.
The uncertainty principle had profound implications for the way in which we view the world. Even after more than seventy
years they have not been fully appreciated by many philosophers, and are still the subject of much controversy. The
uncertainty principle signaled an end to Laplace’s dream of a theory of science, a model of the universe that would be
completely deterministic: one certainly cannot predict future events exactly if one cannot even measure the present state
of the universe precisely! We could still imagine that there is a set of laws that determine events completely for some
supernatural being, who could observe the present state of the universe without disturbing it. However, such models of
the universe are not of much interest to us ordinary mortals. It seems better to employ the principle of economy known as
Occam’s razor and cut out all the features of the theory that cannot be observed. This approach led Heisenberg, Erwin
Schrodinger, and Paul Dirac in the 1920s to reformulate mechanics into a new theory called quantum mechanics, based
on the uncertainty principle. In this theory particles no longer had separate, well-defined positions and velocities that
could not be observed, Instead, they had a quantum state, which was a combination of position and velocity.
In general, quantum mechanics does not predict a single definite result for an observation. Instead, it predicts a number
of different possible outcomes and tells us how likely each of these is. That is to say, if one made the same measurement
on a large number of similar systems, each of which started off in the same way, one would find that the result of the

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/c.html (1 of 5) [2/20/2001 3:14:40 AM]

A Brief History of Time - Stephen Hawking... Chapter 4

measurement would be A in a certain number of cases, B in a different number, and so on. One could predict the
approximate number of times that the result would be A or B, but one could not predict the specific result of an individual
measurement. Quantum mechanics therefore introduces an unavoidable element of unpredictability or randomness into
science. Einstein objected to this very strongly, despite the important role he had played in the development of these
ideas. Einstein was awarded the Nobel Prize for his contribution to quantum theory. Nevertheless, Einstein never
accepted that the universe was governed by chance; his feelings were summed up in his famous statement “God does
not play dice.” Most other scientists, however, were willing to accept quantum mechanics because it agreed perfectly with
experiment. Indeed, it has been an outstandingly successful theory and underlies nearly all of modern science and
technology. It governs the behavior of transistors and integrated circuits, which are the essential components of
electronic devices such as televisions and computers, and is also the basis of modern chemistry and biology. The only
areas of physical science into which quantum mechanics has not yet been properly incorporated are gravity and the
large-scale structure of the universe.
Although light is made up of waves, Planck’s quantum hypothesis tells us that in some ways it behaves as if it were
composed of particles: it can be emitted or absorbed only in packets, or quanta. Equally, Heisenberg’s uncertainty
principle implies that particles behave in some respects like waves: they do not have a definite position but are “smeared
out” with a certain probability distribution. The theory of quantum mechanics is based on an entirely new type of
mathematics that no longer describes the real world in terms of particles and waves; it is only the observations of the
world that may be described in those terms. There is thus a duality between waves and particles in quantum mechanics:
for some purposes it is helpful to think of particles as waves and for other purposes it is better to think of waves as
particles. An important consequence of this is that one can observe what is called interference between two sets of
waves or particles. That is to say, the crests of one set of waves may coincide with the troughs of the other set. The two
sets of waves then cancel each other out rather than adding up to a stronger wave as one might expect Figure 4:1.

Figure 4:1
A familiar example of interference in the case of light is the colors that are often seen in soap bubbles. These are caused
by reflection of light from the two sides of the thin film of water forming the bubble. White light consists of light waves of
all different wavelengths, or colors, For certain wavelengths the crests of the waves reflected from one side of the soap
film coincide with the troughs reflected from the other side. The colors corresponding to these wavelengths are absent
from the reflected light, which therefore appears to be colored. Interference can also occur for particles, because of the
duality introduced by quantum mechanics. A famous example is the so-called two-slit experiment Figure 4:2.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/c.html (2 of 5) [2/20/2001 3:14:40 AM]

A Brief History of Time - Stephen Hawking... Chapter 4

Figure 4:2
Consider a partition with two narrow parallel slits in it. On one side of the partition one places a source of fight of a
particular color (that is, of a particular wavelength). Most of the light will hit the partition, but a small amount will go
through the slits. Now suppose one places a screen on the far side of the partition from the light. Any point on the screen
will receive waves from the two slits. However, in general, the distance the light has to travel from the source to the
screen via the two slits will be different. This will mean that the waves from the slits will not be in phase with each other
when they arrive at the screen: in some places the waves will cancel each other out, and in others they will reinforce
each other. The result is a characteristic pattern of light and dark fringes.
The remarkable thing is that one gets exactly the same kind of fringes if one replaces the source of light by a source of
particles such as electrons with a definite speed (this means that the corresponding waves have a definite length). It
seems the more peculiar because if one only has one slit, one does not get any fringes, just a uniform distribution of
electrons across the screen. One might therefore think that opening another slit would just increase the number of
electrons hitting each point of the screen, but, because of interference, it actually decreases it in some places. If
electrons are sent through the slits one at a time, one would expect each to pass through one slit or the other, and so
behave just as if the slit it passed through were the only one there – giving a uniform distribution on the screen. In reality,
however, even when the electrons are sent one at a time, the fringes still appear. Each electron, therefore, must be

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/c.html (3 of 5) [2/20/2001 3:14:40 AM]

A Brief History of Time - Stephen Hawking... Chapter 4

passing through both slits at the same time!
The phenomenon of interference between particles has been crucial to our understanding of the structure of atoms, the
basic units of chemistry and biology and the building blocks out of which we, and everything around us, are made. At the
beginning of this century it was thought that atoms were rather like the planets orbiting the sun, with electrons (particles
of negative electricity) orbiting around a central nucleus, which carried positive electricity. The attraction between the
positive and negative electricity was supposed to keep the electrons in their orbits in the same way that the gravitational
attraction between the sun and the planets keeps the planets in their orbits. The trouble with this was that the laws of
mechanics and electricity, before quantum mechanics, predicted that the electrons would lose energy and so spiral
inward until they collided with the nucleus. This would mean that the atom, and indeed all matter, should rapidly collapse
to a state of very high density. A partial solution to this problem was found by the Danish scientist Niels Bohr in 1913. He
suggested that maybe the electrons were not able to orbit at just any distance from the central nucleus but only at certain
specified distances. If one also supposed that only one or two electrons could orbit at any one of these distances, this
would solve the problem of the collapse of the atom, because the electrons could not spiral in any farther than to fill up
the orbits with e least distances and energies.
This model explained quite well the structure of the simplest atom, hydrogen, which has only one electron orbiting around
the nucleus. But it was not clear how one ought to extend it to more complicated atoms. Moreover, the idea of a limited
set of allowed orbits seemed very arbitrary. The new theory of quantum mechanics resolved this difficulty. It revealed that
an electron orbiting around the nucleus could be thought of as a wave, with a wavelength that depended on its velocity.
For certain orbits, the length of the orbit would correspond to a whole number (as opposed to a fractional number) of
wavelengths of the electron. For these orbits the wave crest would be in the same position each time round, so the
waves would add up: these orbits would correspond to Bohr’s allowed orbits. However, for orbits whose lengths were not
a whole number of wavelengths, each wave crest would eventually be canceled out by a trough as the electrons went
round; these orbits would not be allowed.
A nice way of visualizing the wave/particle duality is the so-called sum over histories introduced by the American scientist
Richard Feynman. In this approach the particle is not supposed to have a single history or path in space-time, as it would
in a classical, nonquantum theory. Instead it is supposed to go from A to B by every possible path. With each path there
are associated a couple of numbers: one represents the size of a wave and the other represents the position in the cycle
(i.e., whether it is at a crest or a trough). The probability of going from A to B is found by adding up the waves for all the
paths. In general, if one compares a set of neighboring paths, the phases or positions in the cycle will differ greatly. This
means that the waves associated with these paths will almost exactly cancel each other out. However, for some sets of
neighboring paths the phase will not vary much between paths. The waves for these paths will not cancel out Such paths
correspond to Bohr’s allowed orbits.
With these ideas, in concrete mathematical form, it was relatively straightforward to calculate the allowed orbits in more
complicated atoms and even in molecules, which are made up of a number of atoms held together by electrons in orbits
that go round more than one nucleus. Since the structure of molecules and their reactions with each other underlie all of
chemistry and biology, quantum mechanics allows us in principle to predict nearly everything we see around us, within
the limits set by the uncertainty principle. (In practice, however, the calculations required for systems containing more
than a few electrons are so complicated that we cannot do them.)
Einstein’s general theory of relativity seems to govern the large-scale structure of the universe. It is what is called a
classical theory; that is, it does not take account of the uncertainty principle of quantum mechanics, as it should for
consistency with other theories. The reason that this does not lead to any discrepancy with observation is that all the
gravitational fields that we normally experience are very weak. How-ever, the singularity theorems discussed earlier
indicate that the gravitational field should get very strong in at least two situations, black holes and the big bang. In such
strong fields the effects of quantum mechanics should be important. Thus, in a sense, classical general relativity, by
predicting points of infinite density, predicts its own downfall, just as classical (that is, nonquantum) mechanics predicted
its downfall by suggesting that atoms should collapse to infinite density. We do not yet have a complete consistent theory
that unifies general relativity and quantum mechanics, but we do know a number of the features it should have. The
consequences that these would have for black holes and the big bang will be described in later chapters. For the
moment, however, we shall turn to the recent attempts to bring together our understanding of the other forces of nature
into a single, unified quantum theory.

PREVIOUS

NEXT

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/c.html (4 of 5) [2/20/2001 3:14:40 AM]

A Brief History of Time - Stephen Hawking... Chapter 4

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/c.html (5 of 5) [2/20/2001 3:14:40 AM]

A Brief History of Time - Stephen Hawking... Chapter 5

CHAPTER 5
ELEMENTARY PARTICLES AND THE FORCES OF NATURE

Aristotle believed that all the matter in the universe was made up of four basic elements – earth, air, fire, and water.
These elements were acted on by two forces: gravity, the tendency for earth and water to sink, and levity, the tendency
for air and fire to rise. This division of the contents of the universe into matter and forces is still used today. Aristotle
believed that matter was continuous, that is, one could divide a piece of matter into smaller and smaller bits without any
limit: one never came up against a grain of matter that could not be divided further. A few Greeks, however, such as
Democritus, held that matter was inherently grainy and that everything was made up of large numbers of various different
kinds of atoms. (The word atom means “indivisible” in Greek.) For centuries the argument continued without any real
evidence on either side, but in 1803 the British chemist and physicist John Dalton pointed out that the fact that chemical
compounds always combined in certain proportions could be explained by the grouping together of atoms to form units
called molecules. However, the argument between the two schools of thought was not finally settled in favor of the
atomists until the early years of this century. One of the important pieces of physical evidence was provided by Einstein.
In a paper written in 1905, a few weeks before the famous paper on special relativity, Einstein pointed out that what was
called Brownian motion – the irregular, random motion of small particles of dust suspended in a liquid – could be
explained as the effect of atoms of the liquid colliding with the dust particles.
By this time there were already suspicions that these atoms were not, after all, indivisible. Several years previously a
fellow of Trinity College, Cambridge, J. J. Thomson, had demonstrated the existence of a particle of matter, called the
electron, that had a mass less than one thousandth of that of the lightest atom. He used a setup rather like a modern TV
picture tube: a red-hot metal filament gave off the electrons, and because these have a negative electric charge, an
electric field could be used to accelerate them toward a phosphor-coated screen. When they hit the screen, flashes of
light were generated. Soon it was realized that these electrons must be coming from within the atoms themselves, and in
1911 the New Zealand physicist Ernest Rutherford finally showed that the atoms of matter do have internal structure:
they are made up of an extremely tiny, positively charged nucleus, around which a number of electrons orbit. He deduced
this by analyzing the way in which alpha-particles, which are positively charged particles given off by radioactive atoms,
are deflected when they collide with atoms.
At first it was thought that the nucleus of the atom was made up of electrons and different numbers of a positively
charged particle called the proton, from the Greek word meaning “first,” because it was believed to be the fundamental
unit from which matter was made. However, in 1932 a colleague of Rutherford’s at Cambridge, James Chadwick,
discovered that the nucleus contained another particle, called the neutron, which had almost the same mass as a proton
but no electrical charge. Chadwick received the Nobel Prize for his discovery, and was elected Master of Gonville and
Caius College, Cambridge (the college of which I am now a fellow). He later resigned as Master because of
disagreements with the Fellows. There had been a bitter dispute in the college ever since a group of young Fellows
returning after the war had voted many of the old Fellows out of the college offices they had held for a long time. This
was before my time; I joined the college in 1965 at the tail end of the bitterness, when similar disagreements forced
another Nobel Prize – winning Master, Sir Nevill Mott, to resign.
Up to about thirty years ago, it was thought that protons and neutrons were “elementary” particles, but experiments in
which protons were collided with other protons or electrons at high speeds indicated that they were in fact made up of
smaller particles. These particles were named quarks by the Caltech physicist Murray Gell-Mann, who won the Nobel
Prize in 1969 for his work on them. The origin of the name is an enigmatic quotation from James Joyce: “Three quarks for
Muster Mark!” The word quark is supposed to be pronounced like quart, but with a k at the end instead of a t, but is
usually pronounced to rhyme with lark.
There are a number of different varieties of quarks: there are six “flavors,” which we call up, down, strange, charmed,
bottom, and top. The first three flavors had been known since the 1960s but the charmed quark was discovered only in
1974, the bottom in 1977, and the top in 1995. Each flavor comes in three “colors,” red, green, and blue. (It should be
emphasized that these terms are just labels: quarks are much smaller than the wavelength of visible light and so do not
have any color in the normal sense. It is just that modern physicists seem to have more imaginative ways of naming new
particles and phenomena – they no longer restrict themselves to Greek!) A proton or neutron is made up of three quarks,
one of each color. A proton contains two up quarks and one down quark; a neutron contains two down and one up. We
can create particles made up of the other quarks (strange, charmed, bottom, and top), but these all have a much greater
mass and decay very rapidly into protons and neutrons.
We now know that neither the atoms nor the protons and neutrons within them are indivisible. So the question is: what
are the truly elementary particles, the basic building blocks from which everything is made? Since the wavelength of light

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/d.html (1 of 8) [2/20/2001 3:14:54 AM]

A Brief History of Time - Stephen Hawking... Chapter 5

is much larger than the size of an atom, we cannot hope to “look” at the parts of an atom in the ordinary way. We need to
use something with a much smaller wave-length. As we saw in the last chapter, quantum mechanics tells us that all
particles are in fact waves, and that the higher the energy of a particle, the smaller the wavelength of the corresponding
wave. So the best answer we can give to our question depends on how high a particle energy we have at our disposal,
because this determines on how small a length scale we can look. These particle energies are usually measured in units
called electron volts. (In Thomson’s experiments with electrons, we saw that he used an electric field to accelerate the
electrons. The energy that an electron gains from an electric field of one volt is what is known as an electron volt.) In the
nineteenth century, when the only particle energies that people knew how to use were the low energies of a few electron
volts generated by chemical reactions such as burning, it was thought that atoms were the smallest unit. In Rutherford’s
experiment, the alpha-particles had energies of millions of electron volts. More recently, we have learned how to use
electromagnetic fields to give particles energies of at first millions and then thousands of millions of electron volts. And so
we know that particles that were thought to be “elementary” thirty years ago are, in fact, made up of smaller particles.
May these, as we go to still higher energies, in turn be found to be made from still smaller particles? This is certainly
possible, but we do have some theoretical reasons for believing that we have, or are very near to, a knowledge of the
ultimate building blocks of nature.
Using the wave/particle duality discussed in the last chapter, every-thing in the universe, including light and gravity, can
be described in terms of particles. These particles have a property called spin. One way of thinking of spin is to imagine
the particles as little tops spinning about an axis. However, this can be misleading, because quantum mechanics tells us
that the particles do not have any well-defined axis. What the spin of a particle really tells us is what the particle looks like
from different directions. A particle of spin 0 is like a dot: it looks the same from every direction Figure 5:1-i. On the other
hand, a particle of spin 1 is like an arrow: it looks different from different directions Figure 5:1-ii. Only if one turns it round
a complete revolution (360 degrees) does the particle look the same. A particle of spin 2 is like a double-headed arrow
Figure 5:1-iii: it looks the same if one turns it round half a revolution (180 degrees). Similarly, higher spin particles look
the same if one turns them through smaller fractions of a complete revolution. All this seems fairly straightforward, but the
remark-able fact is that there are particles that do not look the same if one turns them through just one revolution: you
have to turn them through two complete revolutions! Such particles are said to have spin ½.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/d.html (2 of 8) [2/20/2001 3:14:54 AM]

A Brief History of Time - Stephen Hawking... Chapter 5

Figure 5:1
All the known particles in the universe can be divided into two groups: particles of spin ½, which make up the matter in
the universe, and particles of spin 0, 1, and 2, which, as we shall see, give rise to forces between the matter particles.
The matter particles obey what is called Pauli’s exclusion principle. This was discovered in 1925 by an Austrian physicist,
Wolfgang Pauli – for which he received the Nobel Prize in 1945. He was the archetypal theoretical physicist: it was said
of him that even his presence in the same town would make experiments go wrong! Pauli’s exclusion principle says that
two similar particles can-not exist in the same state; that is, they cannot have both the same position and the same
velocity, within the limits given by the uncertainty principle. The exclusion principle is crucial because it explains why
matter particles do not collapse to a state of very high density under the influence of the forces produced by the particles
of spin 0, 1, and 2: if the matter particles have very nearly the same positions, they must have different velocities, which
means that they will not stay in the same position for long. If the world had been created without the exclusion principle,
quarks would not form separate, well-defined protons and neutrons. Nor would these, together with electrons, form
separate, well-defined atoms. They would all collapse to form a roughly uniform, dense “soup.”
A proper understanding of the electron and other spin-½ particles did not come until 1928, when a theory was proposed
by Paul Dirac, who later was elected to the Lucasian Professorship of Mathematics at Cambridge (the same
professorship that Newton had once held and that I now hold). Dirac’s theory was the first of its kind that was consistent
with both quantum mechanics and the special theory of relativity. It explained mathematically why the electron had
spin-½; that is, why it didn’t look the same if you turned it through only one complete revolution, but did if you turned it
through two revolutions. It also predicted that the electron should have a partner: an anti-electron, or positron. The
discovery of the positron in 1932 confirmed Dirac’s theory and led to his being awarded the Nobel Prize for physics in
1933. We now know that every particle has an antiparticle, with which it can annihilate. (In the case of the force-carrying
particles, the antiparticles are the same as the particles themselves.) There could be whole antiworlds and antipeople
made out of antiparticles. However, if you meet your antiself, don’t shake hands! You would both vanish in a great flash
of light. The question of why there seem to be so many more particles than antiparticles around us is extremely
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/d.html (3 of 8) [2/20/2001 3:14:54 AM]

A Brief History of Time - Stephen Hawking... Chapter 5

important, and I shall return to it later in the chapter.
In quantum mechanics, the forces or interactions between matter particles are all supposed to be carried by particles of
integer spin – 0, 1, or 2. What happens is that a matter particle, such as an electron or a quark, emits a force-carrying
particle. The recoil from this emission changes the velocity of the matter particle. The force-carrying particle then collides
with another matter particle and is absorbed. This collision changes the velocity of the second particle, just as if there had
been a force between the two matter particles. It is an important property of ' the force-carrying particles that they do not
obey the exclusion principle. This means that there is no limit to the number that can be exchanged, and so they can give
rise to a strong force. However, if the force-carrying particles have a high mass, it will be difficult to produce and
exchange them over a large distance. So the forces that they carry will have only a short range. On the other hand, if the
force-carrying particles have no mass of their own, the forces will be long range. The force-carrying particles exchanged
between matter particles are said to be virtual particles because, unlike “real” particles, they cannot be directly detected
by a particle detector. We know they exist, however, because they do have a measurable effect: they give rise to forces
between matter particles. Particles of spin 0, 1, or 2 do also exist in some circumstances as real particles, when they can
be directly detected. They then appear to us as what a classical physicist would call waves, such as waves of light or
gravitational waves. They may sometimes be emitted when matter particles interact with each other by exchanging virtual
force-carrying particles. (For example, the electric repulsive force between two electrons is due to the exchange of virtual
photons, which can never be directly detected; but if one electron moves past another, real photons may be given off,
which we detect as light waves.)
Force-carrying particles can be grouped into four categories according to the strength of the force that they carry and the
particles with which they interact. It should be emphasized that this division into four classes is man-made; it is
convenient for the construction of partial theories, but it may not correspond to anything deeper. Ultimately, most
physicists hope to find a unified theory that will explain all four forces as different aspects of a single force. Indeed, many
would say this is the prime goal of physics today. Recently, successful attempts have been made to unify three of the
four categories of force – and I shall describe these in this chapter. The question of the unification of the remaining
category, gravity, we shall leave till later.
The first category is the gravitational force. This force is universal, that is, every particle feels the force of gravity,
according to its mass or energy. Gravity is the weakest of the four forces by a long way; it is so weak that we would not
notice it at all were it not for two special properties that it has: it can act over large distances, and it is always attractive.
This means that the very weak gravitational forces between the individual particles in two large bodies, such as the earth
and the sun, can all add up to produce a significant force. The other three forces are either short range, or are sometimes
attractive and some-times repulsive, so they tend to cancel out. In the quantum mechanical way of looking at the
gravitational field, the force between two matter particles is pictured as being carried by a particle of spin 2 called the
graviton. This has no mass of its own, so the force that it carries is long range. The gravitational force between the sun
and the earth is ascribed to the exchange of gravitons between the particles that make up these two bodies. Although the
exchanged particles are virtual, they certainly do produce a measurable effect – they make the earth orbit the sun! Real
gravitons make up what classical physicists would call gravitational waves, which are very weak – and so difficult to
detect that they have not yet been observed.
The next category is the electromagnetic force, which interacts with electrically charged particles like electrons and
quarks, but not with uncharged particles such as gravitons. It is much stronger than the gravitational force: the
electromagnetic force between two electrons is about a million million million million million million million (1 with forty-two
zeros after it) times bigger than the gravitational force. However, there are two kinds of electric charge, positive and
negative. The force between two positive charges is repulsive, as is the force between two negative charges, but the
force is attractive between a positive and a negative charge. A large body, such as the earth or the sun, contains nearly
equal numbers of positive and negative charges. Thus the attractive and repulsive forces between the individual particles
nearly cancel each other out, and there is very little net electromagnetic force. However, on the small scales of atoms
and molecules, electromagnetic forces dominate. The electromagnetic attraction between negatively charged electrons
and positively charged protons in the nucleus causes the electrons to orbit the nucleus of the atom, just as gravitational
attraction causes the earth to orbit the sun. The electromagnetic attraction is pictured as being caused by the exchange
of large numbers of virtual massless particles of spin 1, called photons. Again, the photons that are exchanged are virtual
particles. However, when an electron changes from one allowed orbit to another one nearer to the nucleus, energy is
released and a real photon is emitted – which can be observed as visible light by the human eye, if it has the right
wave-length, or by a photon detector such as photographic film. Equally, if a real photon collides with an atom, it may
move an electron from an orbit nearer the nucleus to one farther away. This uses up the energy of the photon, so it is
absorbed.
The third category is called the weak nuclear force, which is responsible for radioactivity and which acts on all matter
particles of spin-½, but not on particles of spin 0, 1, or 2, such as photons and gravitons. The weak nuclear force was not
well understood until 1967, when Abdus Salam at Imperial College, London, and Steven Weinberg at Harvard both
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/d.html (4 of 8) [2/20/2001 3:14:54 AM]

A Brief History of Time - Stephen Hawking... Chapter 5

proposed theories that unified this interaction with the electromagnetic force, just as Maxwell had unified electricity and
magnetism about a hundred years earlier. They suggested that in addition to the photon, there were three other spin-1
particles, known collectively as massive vector bosons, that carried the weak force. These were called W+ (pronounced
W plus), W- (pronounced W minus), and Zº (pronounced Z naught), and each had a mass of around 100 GeV (GeV
stands for gigaelectron-volt, or one thousand million electron volts). The Weinberg-Salam theory exhibits a property
known as spontaneous symmetry breaking. This means that what appear to be a number of completely different particles
at low energies are in fact found to be all the same type of particle, only in different states. At high energies all these
particles behave similarly. The effect is rather like the behavior of a roulette ball on a roulette wheel. At high energies
(when the wheel is spun quickly) the ball behaves in essentially only one way – it rolls round and round. But as the wheel
slows, the energy of the ball decreases, and eventually the ball drops into one of the thirty-seven slots in the wheel. In
other words, at low energies there are thirty-seven different states in which the ball can exist. If, for some reason, we
could only observe the ball at low energies, we would then think that there were thirty-seven different types of ball!
In the Weinberg-Salam theory, at energies much greater than 100 GeV, the three new particles and the photon would all
behave in a similar manner. But at the lower particle energies that occur in most normal situations, this symmetry
between the particles would be broken. WE, W, and Zº would acquire large masses, making the forces they carry have a
very short range. At the time that Salam and Weinberg proposed their theory, few people believed them, and particle
accelerators were not powerful enough to reach the energies of 100 GeV required to produce real W+, W-, or Zº particles.
However, over the next ten years or so, the other predictions of the theory at lower energies agreed so well with
experiment that, in 1979, Salam and Weinberg were awarded the Nobel Prize for physics, together with Sheldon
Glashow, also at Harvard, who had suggested similar unified theories of the electromagnetic and weak nuclear forces.
The Nobel committee was spared the embarrassment of having made a mistake by the discovery in 1983 at CERN
(European Centre for Nuclear Research) of the three massive partners of the photon, with the correct predicted masses
and other properties. Carlo Rubbia, who led the team of several hundred physicists that made the discovery, received the
Nobel Prize in 1984, along with Simon van der Meer, the CERNengineer who developed the antimatter storage system
employed. (It is very difficult to make a mark in experimental physics these days unless you are already at the top! )
The fourth category is the strong nuclear force, which holds the quarks together in the proton and neutron, and holds the
protons and neutrons together in the nucleus of an atom. It is believed that this force is carried by another spin-1 particle,
called the gluon, which interacts only with itself and with the quarks. The strong nuclear force has a curious property
called confinement: it always binds particles together into combinations that have no color. One cannot have a single
quark on its own because it would have a color (red, green, or blue). Instead, a red quark has to be joined to a green and
a blue quark by a “string” of gluons (red + green + blue = white). Such a triplet constitutes a proton or a neutron. Another
possibility is a pair consisting of a quark and an antiquark (red + antired, or green + antigreen, or blue + antiblue = white).
Such combinations make up the particles known as mesons, which are unstable because the quark and antiquark can
annihilate each other, producing electrons and other particles. Similarly, confinement prevents one having a single gluon
on its own, because gluons also have color. Instead, one has to have a collection of gluons whose colors add up to white.
Such a collection forms an unstable particle called a glueball.
The fact that confinement prevents one from observing an isolated quark or gluon might seem to make the whole notion
of quarks and gluons as particles somewhat metaphysical. However, there is another property of the strong nuclear
force, called asymptotic freedom, that makes the concept of quarks and gluons well defined. At normal energies, the
strong nuclear force is indeed strong, and it binds the quarks tightly together. However, experiments with large particle
accelerators indicate that at high energies the strong force becomes much weaker, and the quarks and gluons behave
almost like free particles.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/d.html (5 of 8) [2/20/2001 3:14:54 AM]

A Brief History of Time - Stephen Hawking... Chapter 5

Figure 5:2
Figure 5:2 shows a photograph of a collision between a high-energy proton and antiproton. The success of the unification
of the electromagnetic and weak nuclear forces led to a number of attempts to combine these two forces with the strong
nuclear force into what is called a grand unified theory (or GUT). This title is rather an exaggeration: the resultant theories
are not all that grand, nor are they fully unified, as they do not include gravity. Nor are they really complete theories,
because they contain a number of parameters whose values cannot be predicted from the theory but have to be chosen
to fit in with experiment. Nevertheless, they may be a step toward a complete, fully unified theory. The basic idea of
GUTs is as follows: as was mentioned above, the strong nuclear force gets weaker at high energies. On the other hand,
the electromagnetic and weak forces, which are not asymptotically free, get stronger at high energies. At some very high
energy, called the grand unification energy, these three forces would all have the same strength and so could just be
different aspects of a single force. The GUTs also predict that at this energy the different spin-½ matter particles, like
quarks and electrons, would also all be essentially the same, thus achieving another unification.
The value of the grand unification energy is not very well known, but it would probably have to be at least a thousand
million million GeV. The present generation of particle accelerators can collide particles at energies of about one hundred
GeV, and machines are planned that would raise this to a few thousand GeV. But a machine that was powerful enough to
accelerate particles to the grand unification energy would have to be as big as the Solar System – and would be unlikely
to be funded in the present economic climate. Thus it is impossible to test grand unified theories directly in the laboratory.
However, just as in the case of the electromagnetic and weak unified theory, there are low-energy consequences of the
theory that can be tested.
The most interesting of these is the prediction that protons, which make up much of the mass of ordinary matter, can
spontaneously decay into lighter particles such as antielectrons. The reason this is possible is that at the grand
unification energy there is no essential difference between a quark and an antielectron. The three quarks inside a proton
normally do not have enough energy to change into antielectrons, but very occasionally one of them may acquire

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/d.html (6 of 8) [2/20/2001 3:14:54 AM]

A Brief History of Time - Stephen Hawking... Chapter 5

sufficient energy to make the transition because the uncertainty principle means that the energy of the quarks inside the
proton cannot be fixed exactly. The proton would then decay. The probability of a quark gaining sufficient energy is so
low that one is likely to have to wait at least a million million million million million years (1 followed by thirty zeros). This is
much longer than the time since the big bang, which is a mere ten thousand million years or so (1 followed by ten zeros).
Thus one might think that the possibility of spontaneous proton decay could not be tested experimentally. However, one
can increase one’s chances of detecting a decay by observing a large amount of matter containing a very large number
of protons. (If, for example, one observed a number of protons equal to 1 followed by thirty-one zeros for a period of one
year, one would expect, according to the simplest GUT, to observe more than one proton decay.)
A number of such experiments have been carried out, but none have yielded definite evidence of proton or neutron
decay. One experiment used eight thousand tons of water and was performed in the Morton Salt Mine in Ohio (to avoid
other events taking place, caused by cosmic rays, that might be confused with proton decay). Since no spontaneous
proton decay had been observed during the experiment, one can calculate that the probable life of the proton must be
greater than ten million million million million million years (1 with thirty-one zeros). This is longer than the lifetime
predicted by the simplest grand unified theory, but there are more elaborate theories in which the predicted lifetimes are
longer. Still more sensitive experiments involving even larger quantities of matter will be needed to test them.
Even though it is very difficult to observe spontaneous proton decay, it may be that our very existence is a consequence
of the reverse process, the production of protons, or more simply, of quarks, from an initial situation in which there were
no more quarks than antiquarks, which is the most natural way to imagine the universe starting out. Matter on the earth is
made up mainly of protons and neutrons, which in turn are made up of quarks. There are no antiprotons or antineutrons,
made up from antiquarks, except for a few that physicists produce in large particle accelerators. We have evidence from
cosmic rays that the same is true for all the matter in our galaxy: there are no antiprotons or antineutrons apart from a
small number that are produced as particle/ antiparticle pairs in high-energy collisions. If there were large regions of
antimatter in our galaxy, we would expect to observe large quantities of radiation from the borders between the regions of
matter and antimatter, where many particles would be colliding with their anti-particles, annihilating each other and giving
off high-energy radiation.
We have no direct evidence as to whether the matter in other galaxies is made up of protons and neutrons or antiprotons
and anti-neutrons, but it must be one or the other: there cannot be a mixture in a single galaxy because in that case we
would again observe a lot of radiation from annihilations. We therefore believe that all galaxies are composed of quarks
rather than antiquarks; it seems implausible that some galaxies should be matter and some antimatter.
Why should there be so many more quarks than antiquarks? Why are there not equal numbers of each? It is certainly
fortunate for us that the numbers are unequal because, if they had been the same, nearly all the quarks and antiquarks
would have annihilated each other in the early universe and left a universe filled with radiation but hardly any matter.
There would then have been no galaxies, stars, or planets on which human life could have developed. Luckily, grand
unified theories may provide an explanation of why the universe should now contain more quarks than antiquarks, even if
it started out with equal numbers of each. As we have seen, GUTs allow quarks to change into antielectrons at high
energy. They also allow the reverse processes, antiquarks turning into electrons, and electrons and antielectrons turning
into antiquarks and quarks. There was a time in the very early universe when it was so hot that the particle energies
would have been high enough for these transformations to take place. But why should that lead to more quarks than
antiquarks? The reason is that the laws of physics are not quite the same for particles and antiparticles.
Up to 1956 it was believed that the laws of physics obeyed each of three separate symmetries called C, P, and T. The
symmetry C means that the laws are the same for particles and antiparticles. The symmetry P means that the laws are
the same for any situation and its mirror image (the mirror image of a particle spinning in a right-handed direction is one
spinning in a left-handed direction). The symmetry T means that if you reverse the direction of motion of all particles and
antiparticles, the system should go back to what it was at earlier times; in other words, the laws are the same in the
forward and backward directions of time. In 1956 two American physicists, Tsung-Dao Lee and Chen Ning Yang,
suggested that the weak force does not in fact obey the symmetry P. In other words, the weak force would make the
universe develop in a different way from the way in which the mirror image of the universe would develop. The same
year, a colleague, Chien-Shiung Wu, proved their prediction correct. She did this by lining up the nuclei of radioactive
atoms in a magnetic field, so that they were all spinning in the same direction, and showed that the electrons were given
off more in one direction than another. The following year, Lee and Yang received the Nobel Prize for their idea. It was
also found that the weak force did not obey the symmetry C. That is, it would cause a universe composed of antiparticles
to behave differently from our universe. Nevertheless, it seemed that the weak force did obey the combined symmetry
CP. That is, the universe would develop in the same way as its mirror image if, in addition, every particle was swapped
with its antiparticle! However, in 1964 two more Americans, J. W. Cronin and Val Fitch, discovered that even the CP
symmetry was not obeyed in the decay of certain particles called K-mesons. Cronin and Fitch eventually received the
Nobel Prize for their work in 1980. (A lot of prizes have been awarded for showing that the universe is not as simple as
we might have thought!)
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/d.html (7 of 8) [2/20/2001 3:14:54 AM]

A Brief History of Time - Stephen Hawking... Chapter 5

There is a mathematical theorem that says that any theory that obeys quantum mechanics and relativity must always
obey the combined symmetry CPT. In other words, the universe would have to behave the same if one replaced particles
by antiparticles, took the mirror image, and also reversed the direction of time. But Cronin and Fitch showed that if one
replaces particles by antiparticles and takes the mirror image, but does not reverse the direction of time, then the
universe does not behave the same. The laws of physics, therefore, must change if one reverses the direction of time –
they do not obey the symmetry T.
Certainly the early universe does not obey the symmetry T: as time runs forward the universe expands – if it ran
backward, the universe would be contracting. And since there are forces that do not obey the symmetry T, it follows that
as the universe expands, these forces could cause more antielectrons to turn into quarks than electrons into antiquarks.
Then, as the universe expanded and cooled, the antiquarks would annihilate with the quarks, but since there would be
more quarks than antiquarks, a small excess of quarks would remain. It is these that make up the matter we see today
and out of which we ourselves are made. Thus our very existence could be regarded as a confirmation of grand unified
theories, though a qualitative one only; the uncertainties are such that one cannot predict the numbers of quarks that will
be left after the annihilation, or even whether it would be quarks or antiquarks that would remain. (Had it been an excess
of antiquarks, however, we would simply have named antiquarks quarks, and quarks antiquarks.)
Grand unified theories do not include the force of gravity. This does not matter too much, because gravity is such a weak
force that its effects can usually be neglected when we are dealing with elementary particles or atoms. However, the fact
that it is both long range and always attractive means that its effects all add up. So for a sufficiently large number of
matter particles, gravitational forces can dominate over all other forces. This is why it is gravity that determines the
evolution of the universe. Even for objects the size of stars, the attractive force of gravity can win over all the other forces
and cause the star to collapse. My work in the 1970s focused on the black holes that can result from such stellar collapse
and the intense gravitational fields around them. It was this that led to the first hints of how the theories of quantum
mechanics and general relativity might affect each other – a glimpse of the shape of a quantum theory of gravity yet to
come.

PREVIOUS

NEXT

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/d.html (8 of 8) [2/20/2001 3:14:54 AM]

A Brief History of Time - Stephen Hawking... Chapter 6

CHAPTER 6
BLACK HOLES

The term black hole is of very recent origin. It was coined in 1969 by the American scientist John Wheeler as a graphic
description of an idea that goes back at least two hundred years, to a time when there were two theories about light:
one, which Newton favored, was that it was composed of particles; the other was that it was made of waves. We now
know that really both theories are correct. By the wave/particle duality of quantum mechanics, light can be regarded as
both a wave and a particle. Under the theory that light is made up of waves, it was not clear how it would respond to
gravity. But if light is composed of particles, one might expect them to be affected by gravity in the same way that
cannonballs, rockets, and planets are. At first people thought that particles of light traveled infinitely fast, so gravity
would not have been able to slow them down, but the discovery by Roemer that light travels at a finite speed meant that
gravity might have an important effect.
On this assumption, a Cambridge don, John Michell, wrote a paper in 1783 in the Philosophical Transactions of the
Royal Society of London in which he pointed out that a star that was sufficiently massive and compact would have such
a strong gravitational field that light could not escape: any light emitted from the surface of the star would be dragged
back by the star’s gravitational attraction before it could get very far. Michell suggested that there might be a large
number of stars like this. Although we would not be able to see them because the light from them would not reach us,
we would still feel their gravitational attraction. Such objects are what we now call black holes, because that is what
they are: black voids in space. A similar suggestion was made a few years later by the French scientist the Marquis de
Laplace, apparently independently of Michell. Interestingly enough, Laplace included it in only the first and second
editions of his book The System of the World, and left it out of later editions; perhaps he decided that it was a crazy
idea. (Also, the particle theory of light went out of favor during the nineteenth century; it seemed that everything could
be explained by the wave theory, and according to the wave theory, it was not clear that light would be affected by
gravity at all.)
In fact, it is not really consistent to treat light like cannonballs in Newton’s theory of gravity because the speed of light is
fixed. (A cannonball fired upward from the earth will be slowed down by gravity and will eventually stop and fall back; a
photon, however, must continue upward at a constant speed. How then can Newtonian grav-ity affect light?) A
consistent theory of how gravity affects light did not come along until Einstein proposed general relativity in 1915. And
even then it was a long time before the implications of the theory for massive stars were understood.
To understand how a black hole might be formed, we first need an understanding of the life cycle of a star. A star is
formed when a large amount of gas (mostly hydrogen) starts to collapse in on itself due to its gravitational attraction. As
it contracts, the atoms of the gas collide with each other more and more frequently and at greater and greater speeds –
the gas heats up. Eventually, the gas will be so hot that when the hydrogen atoms collide they no longer bounce off
each other, but instead coalesce to form helium. The heat released in this reaction, which is like a controlled hydrogen
bomb explosion, is what makes the star shine. This additional heat also increases the pressure of the gas until it is
sufficient to balance the gravitational attraction, and the gas stops contracting. It is a bit like a balloon – there is a
balance between the pressure of the air inside, which is trying to make the balloon expand, and the tension in the
rubber, which is trying to make the balloon smaller. Stars will remain stable like this for a long time, with heat from the
nuclear reactions balancing the gravitational attraction. Eventually, however, the star will run out of its hydrogen and
other nuclear fuels. Paradoxically, the more fuel a star starts off with, the sooner it runs out. This is because the more
massive the star is, the hotter it needs to be to balance its gravitational attraction. And the hotter it is, the faster it will
use up its fuel. Our sun has probably got enough fuel for another five thousand million years or so, but more massive
stars can use up their fuel in as little as one hundred million years, much less than the age of the universe. When a star
runs out of fuel, it starts to cool off and so to contract. What might happen to it then was first understood only at the end
of the 1920s.
In 1928 an Indian graduate student, Subrahmanyan Chandrasekhar, set sail for England to study at Cambridge with the
British astronomer Sir Arthur Eddington, an expert on general relativity. (According to some accounts, a journalist told
Eddington in the early 1920s that he had heard there were only three people in the world who understood general
relativity. Eddington paused, then replied, “I am trying to think who the third person is.”) During his voyage from India,
Chandrasekhar worked out how big a star could be and still support itself against its own gravity after it had used up all
its fuel. The idea was this: when the star becomes small, the matter particles get very near each other, and so
according to the Pauli exclusion principle, they must have very different velocities. This makes them move away from
each other and so tends to make the star expand. A star can therefore maintain itself at a constant radius by a balance
between the attraction of gravity and the repulsion that arises from the exclusion principle, just as earlier in its life

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/e.html (1 of 9) [2/20/2001 3:15:08 AM]

A Brief History of Time - Stephen Hawking... Chapter 6

gravity was balanced by the heat.
Chandrasekhar realized, however, that there is a limit to the repulsion that the exclusion principle can provide. The
theory of relativity limits the maximum difference in the velocities of the matter particles in the star to the speed of light.
This means that when the star got sufficiently dense, the repulsion caused by the exclusion principle would be less than
the attraction of gravity. Chandrasekhar calculated that a cold star of more than about one and a half times the mass of
the sun would not be able to support itself against its own gravity. (This mass is now known as the Chandrasekhar
limit.) A similar discovery was made about the same time by the Russian scientist Lev Davidovich Landau.
This had serious implications for the ultimate fate of massive stars. If a star’s mass is less than the Chandrasekhar limit,
it can eventually stop contracting and settle down to a possible final state as a “white dwarf” with a radius of a few
thousand miles and a density of hundreds of tons per cubic inch. A white dwarf is supported by the exclusion principle
repulsion between the electrons in its matter. We observe a large number of these white dwarf stars. One of the first to
be discovered is a star that is orbiting around Sirius, the brightest star in the night sky.
Landau pointed out that there was another possible final state for a star, also with a limiting mass of about one or two
times the mass of the sun but much smaller even than a white dwarf. These stars would be supported by the exclusion
principle repulsion between neutrons and protons, rather than between electrons. They were therefore called neutron
stars. They would have a radius of only ten miles or so and a density of hundreds of millions of tons per cubic inch. At
the time they were first predicted, there was no way that neutron stars could be observed. They were not actually
detected until much later.
Stars with masses above the Chandrasekhar limit, on the other hand, have a big problem when they come to the end of
their fuel. In some cases they may explode or manage to throw off enough matter to reduce their mass below the limit
and so avoid catastrophic gravitational collapse, but it was difficult to believe that this always happened, no matter how
big the star. How would it know that it had to lose weight? And even if every star managed to lose enough mass to
avoid collapse, what would happen if you added more mass to a white dwarf 'or neutron star to take it over the limit?
Would it collapse to infinite density? Eddington was shocked by that implication, and he refused to believe
Chandrasekhar’s result. Eddington thought it was simply not possible that a star could collapse to a point. This was the
view of most scientists: Einstein himself wrote a paper in which he claimed that stars would not shrink to zero size. The
hostility of other scientists, particularly Eddington, his former teacher and the leading authority on the structure of stars,
persuaded Chandrasekhar to abandon this line of work and turn instead to other problems in astronomy, such as the
motion of star clusters. However, when he was awarded the Nobel Prize in 1983, it was, at least in part, for his early
work on the limiting mass of cold stars.
Chandrasekhar had shown that the exclusion principle could not halt the collapse of a star more massive than the
Chandrasekhar limit, but the problem of understanding what would happen to such a star, according to general
relativity, was first solved by a young American, Robert Oppenheimer, in 1939. His result, however, suggested that
there would be no observational consequences that could be detected by the telescopes of the day. Then World War II
intervened and Oppenheimer himself became closely involved in the atom bomb project. After the war the problem of
gravitational collapse was largely forgotten as most scientists became caught up in what happens on the scale of the
atom and its nucleus. In the 1960s, however, interest in the large-scale problems of astronomy and cosmology was
revived by a great increase in the number and range of astronomical observations brought about by the application of
modern technology. Oppenheimer’s work was then rediscovered and extended by a number of people.
The picture that we now have from Oppenheimer’s work is as follows. The gravitational field of the star changes the
paths of light rays in space-time from what they would have been had the star not been present. The light cones, which
indicate the paths followed in space and time by flashes of light emitted from their tips, are bent slightly inward near the
surface of the star. This can be seen in the bending of light from distant stars observed during an eclipse of the sun. As
the star contracts, the gravitational field at its surface gets stronger and the light cones get bent inward more. This
makes it more difficult for light from the star to escape, and the light appears dimmer and redder to an observer at a
distance. Eventually, when the star has shrunk to a certain critical radius, the gravitational field at the surface becomes
so strong that the light cones are bent inward so much that light can no longer escape Figure 6:1.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/e.html (2 of 9) [2/20/2001 3:15:08 AM]

A Brief History of Time - Stephen Hawking... Chapter 6

Figure 6:1
According to the theory of relativity, nothing can travel faster than light. Thus if light cannot escape, neither can anything
else; everything is dragged back by the gravitational field. So one has a set of events, a region of space-time, from
which it is not possible to escape to reach a distant observer. This region is what we now call a black hole. Its boundary
is called the event horizon and it coincides with the paths of light rays that just fail to escape from the black hole.
In order to understand what you would see if you were watching a star collapse to form a black hole, one has to
remember that in the theory of relativity there is no absolute time. Each observer has his own measure of time. The time
for someone on a star will be different from that for someone at a distance, because of the gravitational field of the star.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/e.html (3 of 9) [2/20/2001 3:15:08 AM]

A Brief History of Time - Stephen Hawking... Chapter 6

Suppose an intrepid astronaut on the surface of the collapsing star, collapsing inward with it, sent a signal every
second, according to his watch, to his spaceship orbiting about the star. At some time on his watch, say 11:00, the star
would shrink below the critical radius at which the gravitational field becomes so strong nothing can escape, and his
signals would no longer reach the spaceship. As 11:00 approached his companions watching from the spaceship would
find the intervals between successive signals from the astronaut getting longer and longer, but this effect would be very
small before 10:59:59. They would have to wait only very slightly more than a second between the astronaut’s 10:59:58
signal and the one that he sent when his watch read 10:59:59, but they would have to wait forever for the 11:00 signal.
The light waves emitted from the surface of the star between 10:59:59 and 11:00, by the astronaut’s watch, would be
spread out over an infinite period of time, as seen from the spaceship. The time interval between the arrival of
successive waves at the spaceship would get longer and longer, so the light from the star would appear redder and
redder and fainter and fainter. Eventually, the star would be so dim that it could no longer be seen from the spaceship:
all that would be left would be a black hole in space. The star would, however, continue to exert the same gravitational
force on the spaceship, which would continue to orbit the black hole. This scenario is not entirely realistic, however,
because of the following problem. Gravity gets weaker the farther you are from the star, so the gravitational force on our
intrepid astronaut’s feet would always be greater than the force on his head. This difference in the forces would stretch
our astronaut out like spaghetti or tear him apart before the star had contracted to the critical radius at which the event
horizon formed! However, we believe that there are much larger objects in the universe, like the central regions of
galaxies, that can also undergo gravitational collapse to produce black holes; an astronaut on one of these would not be
torn apart before the black hole formed. He would not, in fact, feel anything special as he reached the critical radius,
and could pass the point of no return without noticing it However, within just a few hours, as the region continued to
collapse, the difference in the gravitational forces on his head and his feet would become so strong that again it would
tear him apart.
The work that Roger Penrose and I did between 1965 and 1970 showed that, according to general relativity, there must
be a singularity of infinite density and space-time curvature within a black hole. This is rather like the big bang at the
beginning of time, only it would be an end of time for the collapsing body and the astronaut. At this singularity the laws
of science and our ability to predict the future would break down. However, any observer who remained outside the
black hole would not be affected by this failure of predictability, because neither light nor any other signal could reach
him from the singularity. This remarkable fact led Roger Penrose to propose the cosmic censorship hypothesis, which
might be paraphrased as “God abhors a naked singularity.” In other words, the singularities produced by gravitational
collapse occur only in places, like black holes, where they are decently hidden from outside view by an event horizon.
Strictly, this is what is known as the weak cosmic censorship hypothesis: it protects observers who remain outside the
black hole from the consequences of the breakdown of predictability that occurs at the singularity, but it does nothing at
all for the poor unfortunate astronaut who falls into the hole.
There are some solutions of the equations of general relativity in which it is possible for our astronaut to see a naked
singularity: he may be able to avoid hitting the singularity and instead fall through a "wormhole” and come out in another
region of the universe. This would offer great possibilities for travel in space and time, but unfortunately it seems that
these solutions may all be highly unstable; the least disturbance, such as the presence of an astronaut, may change
them so that the astronaut could not see the singularity until he hit it and his time came to an end. In other words, the
singularity would always lie in his future and never in his past. The strong version of the cosmic censorship hypothesis
states that in a realistic solution, the singularities would always lie either entirely in the future (like the singularities of
gravitational collapse) or entirely in the past (like the , big bang). I strongly believe in cosmic censorship so I bet Kip
Thorne and John Preskill of Cal Tech that it would always hold. I lost the bet on a technicality because examples were
produced of solutions with a singularity that was visible from a long way away. So I had to pay up, which according to
the terms of the bet meant I had to clothe their nakedness. But I can claim a moral victory. The naked singularities were
unstable: the least disturbance would cause them either to disappear or to be hidden behind an event horizon. So they
would not occur in realistic situations.
The event horizon, the boundary of the region of space-time from which it is not possible to escape, acts rather like a
one-way membrane around the black hole: objects, such as unwary astronauts, can fall through the event horizon into
the black hole, but nothing can ever get out of the black hole through the event horizon. (Remember that the event
horizon is the path in space-time of light that is trying to escape from the black hole, and nothing can travel faster than
light.) One could well say of the event horizon what the poet Dante said of the entrance to Hell: “All hope abandon, ye
who enter here.” Anything or anyone who falls through the event horizon will soon reach the region of infinite density
and the end of time.
General relativity predicts that heavy objects that are moving will cause the emission of gravitational waves, ripples in
the curvature of space that travel at the speed of light. These are similar to light waves, which are ripples of the
electromagnetic field, but they are much harder to detect. They can be observed by the very slight change in separation
they produce between neighboring freely moving objects. A number of detectors are being built in the United States,
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/e.html (4 of 9) [2/20/2001 3:15:08 AM]

A Brief History of Time - Stephen Hawking... Chapter 6

Europe, and Japan that will measure displacements of one part in a thousand million million million (1 with twenty-one
zeros after it), or less than the nucleus of an atom over a distance of ten miles.
Like light, gravitational waves carry energy away from the objects that emit them. One would therefore expect a system
of massive objects to settle down eventually to a stationary state, because the energy in any movement would be
carried away by the emission of gravitational waves. (It is rather like dropping a cork into water: at first it bobs up and
down a great deal, but as the ripples carry away its energy, it eventually settles down to a stationary state.) For
example, the movement of the earth in its orbit round the sun produces gravitational waves. The effect of the energy
loss will be to change the orbit of the earth so that gradually it gets nearer and nearer to the sun, eventually collides with
it, and settles down to a stationary state. The rate of energy loss in the case of the earth and the sun is very low – about
enough to run a small electric heater. This means it will take about a thousand million million million million years for the
earth to run into the sun, so there’s no immediate cause for worry! The change in the orbit of the earth is too slow to be
observed, but this same effect has been observed over the past few years occurring in the system called PSR 1913 +
16 (PSR stands for “pulsar,” a special type of neutron star that emits regular pulses of radio waves). This system
contains two neutron stars orbiting each other, and the energy they are losing by the emission of gravitational waves is
causing them to spiral in toward each other. This confirmation of general relativity won J. H. Taylor and R. A. Hulse the
Nobel Prize in 1993. It will take about three hundred million . years for them to collide. Just before they do, they will be
orbiting so fast that they will emit enough gravitational waves for detectors like LIGO to pick up.
During the gravitational collapse of a star to form a black hole, the movements would be much more rapid, so the rate at
which energy is carried away would be much higher. It would therefore not be too long ' before it settled down to a
stationary state. What would this final stage look like? One might suppose that it would depend on all the complex
features of the star from which it had formed – not only its mass and rate of rotation, but also the different densities of
various parts of the star, and the complicated movements of the gases within the star. And if black holes were as varied
as the objects that collapsed to form them, it might be very difficult to make any predictions about black holes in
general.
In 1967, however, the study of black holes was revolutionized by Werner Israel, a Canadian scientist (who was born in
Berlin, brought up in South Africa, and took his doctoral degree in Ireland). Israel showed that, according to general
relativity, non-rotating black holes must be very simple; they were perfectly spherical, their size depended only on their
mass, and any two such black holes with the same mass were identical. They could, in fact, be described by a
particular solution of Einstein’s equations that had been known since 1917, found by Karl Schwarzschild shortly after
the discovery of general relativity. At first many people, including Israel himself, argued that since black holes had to be
perfectly spherical, a black hole could only form from the collapse of a perfectly spherical object. Any real star – which
would never be perfectly spherical – could therefore only collapse to form a naked singularity.
There was, however, a different interpretation of Israel’s result, which was advocated by Roger Penrose and John
Wheeler in particular. They argued that the rapid movements involved in a star’s collapse would mean that the
gravitational waves it gave off would make it ever more spherical, and by the time it had settled down to a stationary
state, it would be precisely spherical. According to this view, any non-rotating star, however complicated its shape and
internal structure, would end up after gravitational collapse as a perfectly spherical black hole, whose size would
depend only on its mass. Further calculations supported this view, and it soon came to be adopted generally.
Israel’s result dealt with the case of black holes formed from non-rotating bodies only. In 1963, Roy Kerr, a New
Zealander, found a set of solutions of the equations of general relativity that described rotating black holes. These
“Kerr” black holes rotate at a constant rate, their size and shape depending only on their mass and rate of rotation. If the
rotation is zero, the black hole is perfectly round and the solution is identical to the Schwarzschild solution. If the
rotation is non-zero, the black hole bulges outward near its equator (just as the earth or the sun bulge due to their
rotation), and the faster it rotates, the more it bulges. So, to extend Israel’s result to include rotating bodies, it was
conjectured that any rotating body that collapsed to form a black hole would eventually settle down to a stationary state
described by the Kerr solution. In 1970 a colleague and fellow research student of mine at Cambridge, Brandon Carter,
took the first step toward proving this conjecture. He showed that, provided a stationary rotating black hole had an axis
of symmetry, like a spinning top, its size and shape would depend only on its mass and rate of rotation. Then, in 1971, I
proved that any stationary rotating black hole would indeed have such an axis of symmetry. Finally, in 1973, David
Robinson at Kings College, London, used Carter’s and my results to show that the conjecture had been correct: such a
black hole had indeed to be the Kerr solution. So after gravitational collapse a black hole must settle down into a state
in which it could be rotating, but not pulsating. Moreover, its size and shape would depend only on its mass and rate of
rotation, and not on the nature of the body that had collapsed to form it. This result became known by the maxim: “A
black hole has no hair.” The “no hair” theorem is of great practical importance, because it so greatly restricts the
possible types of black holes. One can therefore make detailed models of objects that might contain black holes and
compare the predictions of the models with observations. It also means that a very large amount of information about

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/e.html (5 of 9) [2/20/2001 3:15:08 AM]

A Brief History of Time - Stephen Hawking... Chapter 6

the body that has collapsed must be lost when a black hole is formed, because afterward all we can possibly measure
about the body is its mass and rate of rotation. The significance of this will be seen in the next chapter.
Black holes are one of only a fairly small number of cases in the history of science in which a theory was developed in
great detail as a mathematical model before there was any evidence from observations that it was correct. Indeed, this
used to be the main argument of opponents of black holes: how could one believe in objects for which the only
evidence was calculations based on the dubious theory of general relativity? In 1963, however, Maarten Schmidt, an
astronomer at the Palomar Observatory in California, measured the red shift of a faint starlike object in the direction of
the source of radio waves called 3C273 (that is, source number 273 in the third Cambridge catalogue of radio sources).
He found it was too large to be caused by a gravitational field: if it had been a gravitational red shift, the object would
have to be so massive and so near to us that it would disturb the orbits of planets in the Solar System. This suggested
that the red shift was instead caused by the expansion of the universe, which, in turn, meant that the object was a very
long distance away. And to be visible at such a great distance, the object must be very bright, must, in other words, be
emitting a huge amount of energy. The only mechanism that people could think of that would produce such large
quantities of energy seemed to be the gravitational collapse not just of a star but of a whole central region of a galaxy. A
number of other similar “quasi-stellar objects,” or quasars, have been discovered, all with large red shifts. But they are
all too far away and therefore too difficult to observe to provide conclusive evidence of black holes.
Further encouragement for the existence of black holes came in 1967 with the discovery by a research student at
Cambridge, Jocelyn Bell-Burnell, of objects in the sky that were emitting regular pulses of radio waves. At first Bell and
her supervisor, Antony Hewish, thought they might have made contact with an alien civilization in the galaxy! Indeed, at
the seminar at which they announced their discovery, I remember that they called the first four sources to be found
LGM 1 – 4, LGM standing for “Little Green Men.” In the end, however, they and everyone else came to the less
romantic conclusion that these objects, which were given the name pulsars, were in fact rotating neutron stars that were
emitting pulses of radio waves because of a complicated interaction between their magnetic fields and surrounding
matter. This was bad news for writers of space westerns, but very hopeful for the small number of us who believed in
black holes at that time: it was the first positive evidence that neutron stars existed. A neutron star has a radius of about
ten miles, only a few times the critical radius at which a star becomes a black hole. If a star could collapse to such a
small size, it is not unreasonable to expect that other stars could collapse to even smaller size and become black holes.
How could we hope to detect a black hole, as by its very definition it does not emit any light? It might seem a bit like
looking for a black cat in a coal cellar. Fortunately, there is a way. As John Michell pointed out in his pioneering paper in
1783, a black hole still exerts a gravitational fierce on nearby objects. Astronomers have observed many systems in
which two stars orbit around each other, attracted toward each other by gravity. They also observe systems in which
there is only one visible star that is orbiting around some unseen companion. One cannot, of course, immediately
conclude that the companion is a black hole: it might merely be a star that is too faint to be seen. However, some of
these systems, like the one called Cygnus X-1 Figure 6:2, are also strong sources of X-rays.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/e.html (6 of 9) [2/20/2001 3:15:08 AM]

A Brief History of Time - Stephen Hawking... Chapter 6

Figure 6:2
The best explanation for this phenomenon is that matter has been blown off the surface of the visible star. As it falls
toward the unseen companion, it develops a spiral motion (rather like water running out of a bath), and it gets very hot,
emitting X-rays Figure 6:3.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/e.html (7 of 9) [2/20/2001 3:15:08 AM]

A Brief History of Time - Stephen Hawking... Chapter 6

Figure 6:3
For this mechanism to work, the unseen object has to be very small, like a white dwarf, neutron star, or black hole.
From the observed orbit of the visible star, one can determine the lowest possible mass of the unseen object. In the
case of Cygnus X-l, this is about six times the mass of the sun, which, according to Chandrasekhar’r result, is too great
for the unseen object to be a white dwarf. It is also too large a mass to be a neutron star. It seems, therefore, that it
must be a black hole.
There are other models to explain Cygnus X-1 that do not include a black hole, but they are all rather far-fetched. A
black hole seems to be the only really natural explanation of the observations. Despite this, I had a bet with Kip Thorne
of the California Institute of Technology that in fact Cygnus X-1 does not contain a black hole! This was a form f
insurance policy for me. I have done a lot of work on black holes, and it would all be wasted if it turned out that black
holes do not exist. But in that case, I would have the consolation of winning my bet, which would bring me four years of
the magazine Private Eye. In fact, although the situation with Cygnus X-1 has not changed much since we made the bet
in 1975, there is now so much other observational evidence in favor of black holes that I have conceded the bet. I paid
the specified penalty, which was a one-year subscription to Penthouse, to the outrage of Kip’s liberated wife.
We also now have evidence for several other black holes in systems like Cygnus X-1 in our galaxy and in two
neighboring galaxies called the Magellanic Clouds. The number of black holes, however, is almost certainly very much
higher; in the long history of the universe, many stars must have burned all their nuclear fuel and have had to collapse.
The number of black holes may well be greater even than the number of visible stars, which totals about a hundred
thousand million in our galaxy alone. The extra gravitational attraction of such a large number of black holes could
explain why our galaxy rotates at the rate it does: the mass of the visible stars is insufficient to account for this. We also

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/e.html (8 of 9) [2/20/2001 3:15:08 AM]

A Brief History of Time - Stephen Hawking... Chapter 6

have some evidence that there is a much larger black hole, with a mass of about a hundred thousand times that of the
sun, at the center of our galaxy. Stars in the galaxy that come too near this black hole will be torn apart by the
difference in the gravitational forces on their near and far sides. Their remains and gas that is thrown off other stars, will
fall toward the black hole. As in the case of Cygnus X-l, the gas will spiral inward and will heat up, though not as much
as in that case. It will not get hot enough to emit X rays, but it could account for the very compact source of radio waves
and infrared rays that is observed at the galactic center.
It is thought that similar but even larger black holes, with masses of about a hundred million times the mass of the sun,
occur at the centers of quasars. For example, observations with the Hubble telescope of the galaxy known as M87
reveal that it contains a disk of gas 130 light-years across rotating about a central object two thousand million times the
mass of the sun. This can only be a black hole. Matter falling into such a supermassive black hole would provide the
only source of power great enough to explain the enormous amounts of energy that these objects are emitting. As the
matter spirals into the black hole, it would make the black hole rotate in the same direction, causing it to develop a
magnetic field rather like that of the earth. Very high-energy particles would be generated near the black hole by the
in-falling matter. The magnetic field would be so strong that it could focus these particles into jets ejected outward along
the axis of rotation of the black hole, that is, in the directions of its north and south poles. Such jets are indeed observed
in a number of galaxies and quasars. One can also consider the possibility that there might be black holes with masses
much less than that of the sun. Such black holes could not be formed by gravitational collapse, because their masses
are below the Chandrasekhar mass limit: stars of this low mass can support themselves against the force of gravity
even when they have exhausted their nuclear fuel. Low-mass black holes could form only if matter was compressed to
enormous densities by very large external pressures. Such conditions could occur in a very big hydrogen bomb: the
physicist John Wheeler once calculated that if one took all the heavy water in all the oceans of the world, one could
build a hydrogen bomb that would compress matter at the center so much that a black hole would be created. (Of
course, there would be no one left to observe it!) A more practical possibility is that such low-mass black holes might
have been formed in the high temperatures and pressures of the very early universe. Black holes would have been
formed only if the early universe had not been perfectly smooth and uniform, because only a small region that was
denser than average could be compressed in this way to form a black hole. But we know that there must have been
some irregularities, because otherwise the matter in the universe would still be perfectly uniformly distributed at the
present epoch, instead of being clumped together in stars and galaxies.
Whether the irregularities required to account for stars and galaxies would have led to the formation of a significant
number of “primordial” black holes clearly depends on the details of the conditions in the early universe. So if we could
determine how many primordial black holes there are now, we would learn a lot about the very early stages of the
universe. Primordial black holes with masses more than a thousand million tons (the mass of a large mountain) could
be detected only by their gravitational influence on other, visible matter or on the expansion of the universe. However,
as we shall learn in the next chapter, black holes are not really black after all: they glow like a hot body, and the smaller
they are, the more they glow. So, paradoxically, smaller black holes might actually turn out to be easier to detect than
large ones!

PREVIOUS

NEXT

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/e.html (9 of 9) [2/20/2001 3:15:08 AM]

A Brief History of Time - Stephen Hawking... Chapter 7

CHAPTER 7
BLACK HOLES AIN’T SO BLACK

Before 1970, my research on general relativity had concentrated mainly on the question of whether or not there had
been a big bang singularity. However, one evening in November that year, shortly after the birth of my daughter, Lucy,
I started to think about black holes as I was getting into bed. My disability makes this rather a slow process, so I had
plenty of time. At that date there was no precise definition of which points in space-time lay inside a black hole and
which lay outside. I had already discussed with Roger Penrose the idea of defining a black hole as the set of events
from which it was not possible to escape to a large distance, which is now the generally accepted definition. It means
that the boundary of the black hole, the event horizon, is formed by the light rays that just fail to escape from the black
hole, hovering forever just on the edge Figure 7:1. It is a bit like running away from the police and just managing to
keep one step ahead but not being able to get clear away!

Figure 7:1
Suddenly I realized that the paths of these light rays could never approach one another. If they did they must
eventually run into one another. It would be like meeting someone else running away from the police in the opposite
direction – you would both be caught! (Or, in this case, fall into a black hole.) But if these light rays were swallowed up
by the black hole, then they could not have been on the boundary of the black hole. So the paths of light rays in the

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/f.html (1 of 8) [2/20/2001 3:15:18 AM]

A Brief History of Time - Stephen Hawking... Chapter 7

event horizon had always to be moving parallel to, or away from, each other. Another way of seeing this is that the
event horizon, the boundary of the black hole, is like the edge of a shadow – the shadow of impending doom. If you
look at the shadow cast by a source at a great distance, such as the sun, you will see that the rays of light in the edge
are not approaching each other.
If the rays of light that form the event horizon, the boundary of the black hole, can never approach each other, the area
of the event horizon might stay the same or increase with time, but it could never decrease because that would mean
that at least some of the rays of light in the boundary would have to be approaching each other. In fact, the area would
increase whenever matter or radiation fell into the black hole Figure 7:2.

Figures 7:2 & 7:3
Or if two black holes collided and merged together to form a single black hole, the area of the event horizon of the final
black hole would be greater than or equal to the sum of the areas of the event horizons of the original black holes
Figure 7:3. This nondecreasing property of the event horizon’s area placed an important restriction on the possible
behavior of black holes. I was so excited with my discovery that I did not get much sleep that night. The next day I rang
up Roger Penrose. He agreed with me. I think, in fact, that he had been aware of this property of the area. However,
he had been using a slightly different definition of a black hole. He had not realized that the boundaries of the black
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/f.html (2 of 8) [2/20/2001 3:15:18 AM]

A Brief History of Time - Stephen Hawking... Chapter 7

hole according to the two definitions would be the same, and hence so would their areas, provided the black hole had
settled down to a state in which it was not changing with time.
The nondecreasing behavior of a black hole’s area was very reminiscent of the behavior of a physical quantity called
entropy, which measures the degree of disorder of a system. It is a matter of common experience that disorder will
tend to increase if things are left to themselves. (One has only to stop making repairs around the house to see that!)
One can create order out of disorder (for example, one can paint the house), but that requires expenditure of effort or
energy and so decreases the amount of ordered energy available.
A precise statement of this idea is known as the second law of thermodynamics. It states that the entropy of an isolated
system always increases, and that when two systems are joined together, the entropy of the combined system is
greater than the sum of the entropies of the individual systems. For example, consider a system of gas molecules in a
box. The molecules can be thought of as little billiard balls continually colliding with each other and bouncing off the
walls of the box. The higher the temperature of the gas, the faster the molecules move, and so the more frequently and
harder they collide with the walls of the box and the greater the outward pressure they exert on the walls. Suppose that
initially the molecules are all confined to the left-hand side of the box by a partition. If the partition is then removed, the
molecules will tend to spread out and occupy both halves of the box. At some later time they could, by chance, all be in
the right half or back in the left half, but it is overwhelmingly more probable that there will be roughly equal numbers in
the two halves. Such a state is less ordered, or more disordered, than the original state in which all the molecules were
in one half. One therefore says that the entropy of the gas has gone up. Similarly, suppose one starts with two boxes,
one containing oxygen molecules and the other containing nitrogen molecules. If one joins the boxes together and
removes the intervening wall, the oxygen and the nitrogen molecules will start to mix. At a later time the most probable
state would be a fairly uniform mixture of oxygen and nitrogen molecules throughout the two boxes. This state would
be less ordered, and hence have more entropy, than the initial state of two separate boxes.
The second law of thermodynamics has a rather different status than that of other laws of science, such as Newton's
law of gravity, for example, because it does not hold always, just in the vast majority of cases. The probability of all the
gas molecules in our first box
found in one half of the box at a later time is many millions of millions to one, but it can happen. However, if one has a
black hole around there seems to be a rather easier way of violating the second law: just throw some matter with a lot
of entropy such as a box of gas, down the black hole. The total entropy of matter outside the black hole would go
down. One could, of course, still say that the total entropy, including the entropy inside the black hole, has not gone
down - but since there is no way to look inside the black hole, we cannot see how much entropy the matter inside it
has. It would be nice, then, if there was some feature of the black hole by which observers outside the black hole could
tell its entropy, and which would increase whenever matter carrying entropy fell into the black hole. Following the
discovery, described above, that the area of the event horizon increased whenever matter fell into a black hole, a
research student at Princeton named Jacob Bekenstein suggested that the area of the event horizon was a measure of
the entropy of the black hole. As matter carrying entropy fell into a black hole, the area of its event horizon would go
up, so that the sum of the entropy of matter outside black holes and the area of the horizons would never go down.
This suggestion seemed to prevent the second law of thermodynamics from being violated in most situations.
However, there was one fatal flaw. If a black hole has entropy, then it ought to also have a temperature. But a body
with a particular temperature must emit radiation at a certain rate. It is a matter of common experience that if one heats
up a poker in a fire it glows red hot and emits radiation, but bodies at lower temperatures emit radiation too; one just
does not normally notice it because the amount is fairly small. This radiation is required in order to prevent violation of
the second law. So black holes ought to emit radiation. But by their very definition, black holes are objects that are not
supposed to emit anything. It therefore seemed that the area of the event horizon of a black hole could not be regarded
as its entropy. In 1972 I wrote a paper with Brandon Carter and an American colleague, Jim Bardeen, in which we
pointed out that although there were many similarities between entropy and the area of the event horizon, there was
this apparently fatal difficulty. I must admit that in writing this paper I was motivated partly by irritation with Bekenstein,
who, I felt, had misused my discovery of the increase of the area of the event horizon. However, it turned out in the end
that he was basically correct, though in a manner he had certainly not expected.
In September 1973, while I was visiting Moscow, I discussed black holes with two leading Soviet experts, Yakov
Zeldovich and Alexander Starobinsky. They convinced me that, according to the quantum mechanical uncertainty
principle, rotating black holes should create and emit particles. I believed their arguments on physical grounds, but I did
not like the mathematical way in which they calculated the emission. I therefore set about devising a better
mathematical treatment, which I described at an informal seminar in Oxford at the end of November 1973. At that time I
had not done the calculations to find out how much would actually be emitted. I was expecting to discover just the
radiation that Zeldovich and Starobinsky had predicted from rotating black holes. However, when I did the calculation, I
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/f.html (3 of 8) [2/20/2001 3:15:18 AM]

A Brief History of Time - Stephen Hawking... Chapter 7

found, to my surprise and annoyance, that even non-rotating black holes should apparently create and emit particles at
a steady rate. At first I thought that this emission indicated that one of the approximations I had used was not valid. I
was afraid that if Bekenstein found out about it, he would use it as a further argument to support his ideas about the
entropy of black holes, which I still did not like. However, the more I thought about it, the more it seemed that the
approximations really ought to hold. But what finally convinced me that the emission was real was that the spectrum of
the emitted particles was exactly that which would be emitted by a hot body, and that the black hole was emitting
particles at exactly the correct rate to prevent violations of the second law. Since then the calculations have been
repeated in a number of different forms by other people. They all confirm that a black hole ought to emit particles and
radiation as if it were a hot body with a temperature that depends only on the black hole’s mass: the higher the mass,
the lower the temperature.
How is it possible that a black hole appears to emit particles when we know that nothing can escape from within its
event horizon? The answer, quantum theory tells us, is that the particles do not come from within the black hole, but
from the “empty” space just outside the black hole’s event horizon! We can understand this in the following way: what
we think of as “empty” space cannot be completely empty because that would mean that all the fields, such as the
gravitational and electromagnetic fields, would have to be exactly zero. However, the value of a field and its rate of
change with time are like the position and velocity of a particle: the uncertainty principle implies that the more
accurately one knows one of these quantities, the less accurately one can know the other. So in empty space the field
cannot be fixed at exactly zero, because then it would have both a precise value (zero) and a precise rate of change
(also zero). There must be a certain minimum amount of uncertainty, or quantum fluctuations, in the value of the field.
One can think of these fluctuations as pairs of particles of light or gravity that appear together at some time, move
apart, and then come together again and annihilate each other. These particles are virtual particles like the particles
that carry the gravitational force of the sun: unlike real particles, they cannot be observed directly with a particle
detector. However, their indirect effects, such as small changes in the energy of electron orbits in atoms, can be
measured and agree with the theoretical predictions to a remarkable degree of accuracy. The uncertainty principle also
predicts that there will be similar virtual pairs of matter particles, such as electrons or quarks. In this case, however,
one member of the pair will be a particle and the other an antiparticle (the antiparticles of light and gravity are the same
as the particles).
Because energy cannot be created out of nothing, one of the partners in a particle/antiparticle pair will have positive
energy, and the other partner negative energy. The one with negative energy is condemned to be a short-lived virtual
particle because real particles always have positive energy in normal situations. It must therefore seek out its partner
and annihilate with it. However, a real particle close to a massive body has less energy than if it were far away,
because it would take energy to lift it far away against the gravitational attraction of the body. Normally, the energy of
the particle is still positive, but the gravitational field inside a black hole is so strong that even a real particle can have
negative energy there. It is therefore possible, if a black hole is present, for the virtual particle with negative energy to
fall into the black hole and become a real particle or antiparticle. In this case it no longer has to annihilate with its
partner. Its forsaken partner may fall into the black hole as well. Or, having positive energy, it might also escape from
the vicinity of the black hole as a real particle or antiparticle Figure 7:4.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/f.html (4 of 8) [2/20/2001 3:15:18 AM]

A Brief History of Time - Stephen Hawking... Chapter 7

Figure 7:4
To an observer at a distance, it will appear to have been emitted from the black hole. The smaller the black hole, the
shorter the distance the particle with negative energy will have to go before it becomes a real particle, and thus the
greater the rate of emission, and the apparent temperature, of the black hole.
The positive energy of the outgoing radiation would be balanced by a flow of negative energy particles into the black
hole. By Einstein’s equation E = mc2 (where E is energy, m is mass, and c is the speed of light), energy is proportional
to mass. A flow of negative energy into the black hole therefore reduces its mass. As the black hole loses mass, the
area of its event horizon gets smaller, but this decrease in the entropy of the black hole is more than compensated for
by the entropy of the emitted radiation, so the second law is never violated.
Moreover, the lower the mass of the black hole, the higher its temperature. So as the black hole loses mass, its
temperature and rate of emission increase, so it loses mass more quickly. What happens when the mass of the black
hole eventually becomes extremely small is not quite clear, but the most reasonable guess is that it would disappear
completely in a tremendous final burst of emission, equivalent to the explosion of millions of H-bombs.
A black hole with a mass a few times that of the sun would have a temperature of only one ten millionth of a degree
above absolute zero. This is much less than the temperature of the microwave radiation that fills the universe (about
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/f.html (5 of 8) [2/20/2001 3:15:18 AM]

A Brief History of Time - Stephen Hawking... Chapter 7

2.7º above absolute zero), so such black holes would emit even less than they absorb. If the universe is destined to go
on expanding forever, the temperature of the microwave radiation will eventually decrease to less than that of such a
black hole, which will then begin to lose mass. But, even then, its temperature would be so low that it would take about
a million million million million million million million million million million million years (1 with sixty-six zeros after it) to
evaporate completely. This is much longer than the age of the universe, which is only about ten or twenty thousand
million years (1 or 2 with ten zeros after it). On the other hand, as mentioned in Chapter 6, there might be primordial
black holes with a very much smaller mass that were made by the collapse of irregularities in the very early stages of
the universe. Such black holes would have a much higher temperature and would be emitting radiation at a much
greater rate. A primordial black hole with an initial mass of a thousand million tons would have a lifetime roughly equal
to the age of the universe. Primordial black holes with initial masses less than this figure would already have
completely evaporated, but those with slightly greater masses would still be emitting radiation in the form of X rays and
gamma rays. These X rays and gamma rays are like waves of light, but with a much shorter wavelength. Such holes
hardly deserve the epithet black: they really are white hot and are emitting energy at a rate of about ten thousand
megawatts.
One such black hole could run ten large power stations, if only we could harness its power. This would be rather
difficult, however: the black hole would have the mass of a mountain compressed into less than a million millionth of an
inch, the size of the nucleus of an atom! If you had one of these black holes on the surface of the earth, there would be
no way to stop it from falling through the floor to the center of the earth. It would oscillate through the earth and back,
until eventually it settled down at the center. So the only place to put such a black hole, in which one might use the
energy that it emitted, would be in orbit around the earth – and the only way that one could get it to orbit the earth
would be to attract it there by towing a large mass in front of it, rather like a carrot in front of a donkey. This does not
sound like a very practical proposition, at least not in the immediate future.
But even if we cannot harness the emission from these primordial black holes, what are our chances of observing
them? We could look for the gamma rays that the primordial black holes emit during most of their lifetime. Although the
radiation from most would be very weak because they are far away, the total from all of them might be detectable. We
do observe such a background of gamma rays: Figure 7:5 shows how the observed intensity differs at different
frequencies (the number of waves per second). However, this background could have been, and probably was,
generated by processes other than primordial black holes. The dotted line in Figure 7:5 shows how the intensity should
vary with frequency for gamma rays given off by primordial black holes, if there were on average 300 per cubic
light-year. One can therefore say that the observations of the gamma ray background do not provide any positive
evidence for primordial black holes, but they do tell us that on average there cannot be more than 300 in every cubic
light-year in the universe. This limit means that primordial black holes could make up at most one millionth of the
matter in the universe.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/f.html (6 of 8) [2/20/2001 3:15:18 AM]

A Brief History of Time - Stephen Hawking... Chapter 7

Figure 7:5
With primordial black holes being so scarce, it might seem unlikely that there would be one near enough for us to
observe as an individual source of gamma rays. But since gravity would draw primordial black holes toward any matter,
they should be much more common in and around galaxies. So although the gamma ray background tells us that there
can be no more than 300 primordial black holes per cubic light-year on average, it tells us nothing about how common
they might be in our own galaxy. If they were, say, a million times more common than this, then the nearest black hole
to us would probably be at a distance of about a thousand million kilometers, or about as far away as Pluto, the farthest
known planet. At this distance it would still be very difficult to detect the steady emission of a black hole, even if it was
ten thousand megawatts. In order to observe a primordial black hole one would have to detect several gamma ray
quanta coming from the same direction within a reasonable space of time, such as a week. Otherwise, they might
simply be part of the background. But Planck’s quantum principle tells us that each gamma ray quantum has a very
high energy, because gamma rays have a very high frequency, so it would not take many quanta to radiate even ten
thousand megawatts. And to observe these few coming from the distance of Pluto would require a larger gamma ray
detector than any that have been constructed so far. Moreover, the detector would have to be in space, because
gamma rays cannot penetrate the atmosphere.
Of course, if a black hole as close as Pluto were to reach the end of its life and blow up, it would be easy to detect the
final burst of emission. But if the black hole has been emitting for the last ten or twenty thousand million years, the
chance of it reaching the end of its life within the next few years, rather than several million years in the past or future,
is really rather small! So in order to have a reasonable chance of seeing an explosion before your research grant ran
out, you would have to find a way to detect any explosions within a distance of about one light-year. In fact bursts of
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/f.html (7 of 8) [2/20/2001 3:15:18 AM]

A Brief History of Time - Stephen Hawking... Chapter 7

gamma rays from space have been detected by satellites originally constructed to look for violations of the Test Ban
Treaty. These seem to occur about sixteen times a month and to be roughly uniformly distributed in direction across
the sky. This indicates that they come from outside the Solar System since otherwise we would expect them to be
concentrated toward the plane of the orbits of the planets. The uniform distribution also indicates that the sources are
either fairly near to us in our galaxy or right outside it at cosmological distances because otherwise, again, they would
be concentrated toward the plane of the galaxy. In the latter case, the energy required to account for the bursts would
be far too high to have been produced by tiny black holes, but if the sources were close in galactic terms, it might be
possible that they were exploding black holes. I would very much like this to be the case but I have to recognize that
there are other possible explanations for the gamma ray bursts, such as colliding neutron stars. New observations in
the next few years, particularly by gravitational wave detectors like LIGO, should enable us to discover the origin of the
gamma ray bursts.
Even if the search for primordial black holes proves negative, as it seems it may, it will still give us important
information about the very early stages of the universe. If the early universe had been chaotic or irregular, or if the
pressure of matter had been low, one would have expected it to produce many more primordial black holes than the
limit already set by our observations of the gamma ray background. Only if the early universe was very smooth and
uniform, with a high pressure, can one explain the absence of observable numbers of primordial black holes.
The idea of radiation from black holes was the first example of a prediction that depended in an essential way on both
the great theories of this century, general relativity and quantum mechanics. It aroused a lot of opposition initially
because it upset the existing viewpoint: “How can a black hole emit anything?” When I first announced the results of
my calculations at a conference at the Rutherford-Appleton Laboratory near Oxford, I was greeted with general
incredulity. At the end of my talk the chairman of the session, John G. Taylor from Kings College, London, claimed it
was all nonsense. He even wrote a paper to that effect. However, in the end most people, including John Taylor, have
come to the conclusion that black holes must radiate like hot bodies if our other ideas about general relativity and
quantum mechanics are correct. Thus, even though we have not yet managed to find a primordial black hole, there is
fairly general agreement that if we did, it would have to be emitting a lot of gamma rays and X rays.
The existence of radiation from black holes seems to imply that gravitational collapse is not as final and irreversible as
we once thought. If an astronaut falls into a black hole, its mass will increase, but eventually the energy equivalent of
that extra mass will be returned to the universe in the form of radiation. Thus, in a sense, the astronaut will be
“recycled.” It would be a poor sort of immortality, however, because any personal concept of time for the astronaut
would almost certainly come to an end as he was torn apart inside the black hole! Even the types of particles that were
eventually emitted by the black hole would in general be different from those that made up the astronaut: the only
feature of the astronaut that would survive would be his mass or energy.
The approximations I used to derive the emission from black holes should work well when the black hole has a mass
greater than a fraction of a gram. However, they will break down at the end of the black hole’s life when its mass gets
very small. The most likely outcome seems to be that the black hole will just disappear, at least from our region of the
universe, taking with it the astronaut and any singularity there might be inside it, if indeed there is one. This was the
first indication that quantum mechanics might remove the singularities that were predicted by general relativity.
However, the methods that I and other people were using in 1974 were not able to answer questions such as whether
singularities would occur in quantum gravity. From 1975 onward I therefore started to develop a more powerful
approach to quantum gravity based on Richard Feynrnan’s idea of a sum over histories. The answers that this
approach suggests for the origin and fate of the universe and its contents, such as astronauts, will be de-scribed in the
next two chapters. We shall see that although the uncertainty principle places limitations on the accuracy of all our
predictions, it may at the same time remove the fundamental unpredictability that occurs at a space-time singularity.

PREVIOUS

NEXT

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/f.html (8 of 8) [2/20/2001 3:15:18 AM]

A Brief History of Time - Stephen Hawking... Chapter 8

CHAPTER 8
THE ORIGIN AND FATE OF THE UNIVERSE

Einstein’s general theory of relativity, on its own, predicted that space-time began at the big bang singularity and
would come to an end either at the big crunch singularity (if the whole universe recollapsed), or at a singularity inside
a black hole (if a local region, such as a star, were to collapse). Any matter that fell into the hole would be destroyed
at the singularity, and only the gravitational effect of its mass would continue to be felt outside. On the other hand,
when quantum effects were taken into account, it seemed that the mass or energy of the matter would eventually be
returned to the rest of the universe, and that the black hole, along with any singularity inside it, would evaporate
away and finally disappear. Could quantum mechanics have an equally dramatic effect on the big bang and big
crunch singularities? What really happens during the very early or late stages of the universe, when gravitational
fields are so strong that quantum effects cannot be ignored? Does the universe in fact have a beginning or an end?
And if so, what are they like?
Throughout the 1970s I had been mainly studying black holes, but in 1981 my interest in questions about the origin
and fate of the universe was reawakened when I attended a conference on cosmology organized by the Jesuits in
the Vatican. The Catholic Church had made a bad mistake with Galileo when it tried to lay down the law on a
question of science, declaring that the sun went round the earth. Now, centuries later, it had decided to invite a
number of experts to advise it on cosmology. At the end of the conference the participants were granted an audience
with the Pope. He told us that it was all right to study the evolution of the universe after the big bang, but we should
not inquire into the big bang itself because that was the moment of Creation and therefore the work of God. I was
glad then that he did not know the subject of the talk I had just given at the conference – the possibility that
space-time was finite but had no boundary, which means that it had no beginning, no moment of Creation. I had no
desire to share the fate of Galileo, with whom I feel a strong sense of identity, partly because of the coincidence of
having been born exactly 300 years after his death!
In order to explain the ideas that I and other people have had about how quantum mechanics may affect the origin
and fate of the universe, it is necessary first to understand the generally accepted history of the universe, according
to what is known as the “hot big bang model.” This assumes that the universe is described by a Friedmann model,
right back to the big bang. In such models one finds that as the universe expands, any matter or radiation in it gets
cooler. (When the universe doubles in size, its temperature falls by half.) Since temperature is simply a measure of
the average energy – or speed – of the particles, this cooling of the universe would have a major effect on the matter
in it. At very high temperatures, particles would be moving around so fast that they could escape any attraction
toward each other due to nuclear or electromagnetic forces, but as they cooled off one would expect particles that
attract each other to start to clump together. Moreover, even the types of particles that exist in the universe would
depend on the temperature. At high enough temperatures, particles have so much energy that whenever they collide
many different particle/antiparticle pairs would be produced – and although some of these particles would annihilate
on hitting antiparticles, they would be produced more rap-idly than they could annihilate. At lower temperatures,
however, when colliding particles have less energy, particle/antiparticle pairs would be produced less quickly – and
annihilation would become faster than production.
At the big bang itself the universe is thought to have had zero size, and so to have been infinitely hot. But as the
universe expanded, the temperature of the radiation decreased. One second after the big bang, it would have fallen
to about ten thousand million degrees. This is about a thousand times the temperature at the center of the sun, but
temperatures as high as this are reached in H-bomb explosions. At this time the universe would have contained
mostly photons, electrons, and neutrinos (extremely light particles that are affected only by the weak force and
gravity) and their antiparticles, together with some protons and neutrons. As the universe continued to expand and
the temperature to drop, the rate at which electron/antielectron pairs were being produced in collisions would have
fallen below the rate at which they were being destroyed by annihilation. So most of the electrons and antielectrons
would have annihilated with each other to produce more photons, leaving only a few electrons left over. The
neutrinos and antineutrinos, however, would not have annihilated with each other, because these particles interact
with themselves and with other particles only very weakly. So they should still be around today. If we could observe
them, it would provide a good test of this picture of a very hot early stage of the universe. Unfortunately, their
energies nowadays would be too low for us to observe them directly. However, if neutrinos are not massless, but
have a small mass of their own, as suggested by some recent experiments, we might be able to detect them
indirectly: they could be a form of “dark matter,” like that mentioned earlier, with sufficient gravitational attraction to
stop the expansion of the universe and cause it to collapse again.
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/g.html (1 of 11) [2/20/2001 3:15:29 AM]

A Brief History of Time - Stephen Hawking... Chapter 8

About one hundred seconds after the big bang, the temperature would have fallen to one thousand million degrees,
the temperature inside the hottest stars. At this temperature protons and neutrons would no longer have sufficient
energy to escape the attraction of the strong nuclear force, and would have started to combine together to produce
the nuclei of atoms of deuterium (heavy hydrogen), which contain one proton and one neutron. The deuterium nuclei
would then have combined with more protons and neutrons to make helium nuclei, which contain two protons and
two neutrons, and also small amounts of a couple of heavier elements, lithium and beryllium. One can calculate that
in the hot big bang model about a quarter of the protons and neutrons would have been converted into helium nuclei,
along with a small amount of heavy hydrogen and other elements. The remaining neutrons would have decayed into
protons, which are the nuclei of ordinary hydrogen atoms.
This picture of a hot early stage of the universe was first put forward by the scientist George Gamow in a famous
paper written in 1948 with a student of his, Ralph Alpher. Gamow had quite a sense of humor – he persuaded the
nuclear scientist Hans Bethe to add his name to the paper to make the list of authors “Alpher, Bethe, Gamow,” like
the first three letters of the Greek alphabet, alpha, beta, gamma: particularly appropriate for a paper on the beginning
of the universe! In this paper they made the remarkable prediction that radiation (in the form of photons) from the
very hot early stages of the universe should still be around today, but with its temperature reduced to only a few
degrees above absolute zero (–273ºC). It was this radiation that Penzias and Wilson found in 1965. At the time that
Alpher, Bethe, and Gamow wrote their paper, not much was known about the nuclear reactions of protons and
neutrons. Predictions made for the proportions of various elements in the early universe were therefore rather
inaccurate, but these calculations have been repeated in the light of better knowledge and now agree very well with
what we observe. It is, moreover, very difficult to explain in any other way why there should be so much helium in the
universe. We are therefore fairly confident that we have the right picture, at least back to about one second after the
big bang.
Within only a few hours of the big bang, the production of helium and other elements would have stopped. And after
that, for the next million years or so, the universe would have just continued expanding, without anything much
happening. Eventually, once the temperature had dropped to a few thousand degrees, and electrons and nuclei no
longer had enough energy to overcome the electromagnetic attraction between them, they would have started
combining to form atoms. The universe as a whole would have continued expanding and cooling, but in regions that
were slightly denser than average, the expansion would have been slowed down by the extra gravitational attraction.
This would eventually stop expansion in some regions and cause them to start to recollapse. As they were
collapsing, the gravitational pull of matter outside these regions might start them rotating slightly. As the collapsing
region got smaller, it would spin faster – just as skaters spinning on ice spin faster as they draw in their arms.
Eventually, when the region got small enough, it would be spinning fast enough to balance the attraction of gravity,
and in this way disklike rotating galaxies were born. Other regions, which did not happen to pick up a rotation, would
become oval-shaped objects called elliptical galaxies. In these, the region would stop collapsing because individual
parts of the galaxy would be orbiting stably round its center, but the galaxy would have no overall rotation.
As time went on, the hydrogen and helium gas in the galaxies would break up into smaller clouds that would collapse
under their own gravity. As these contracted, and the atoms within them collided with one another, the temperature
of the gas would increase, until eventually it became hot enough to start nuclear fusion reactions. These would
convert the hydrogen into more helium, and the heat given off would raise the pressure, and so stop the clouds from
contracting any further. They would remain stable in this state for a long time as stars like our sun, burning hydrogen
into helium and radiating the resulting energy as heat and light. More massive stars would need to be hotter to
balance their stronger gravitational attraction, making the nuclear fusion reactions proceed so much more rapidly that
they would use up their hydrogen in as little as a hundred million years. They would then contract slightly, and as
they heated up further, would start to convert helium into heavier elements like carbon or oxygen. This, however,
would not release much more energy, so a crisis would occur, as was described in the chapter on black holes. What
happens next is not completely clear, but it seems likely that the central regions of the star would collapse to a very
dense state, such as a neutron star or black hole. The outer regions of the star may sometimes get blown off in a
tremendous explosion called a supernova, which would outshine all the other stars in its galaxy. Some of the heavier
elements produced near the end of the star’s life would be flung back into the gas in the galaxy, and would provide
some of the raw material for the next generation of stars. Our own sun contains about 2 percent of these heavier
elements, because it is a second- or third-generation star, formed some five thousand million years ago out of a
cloud of rotating gas containing the debris of earlier supernovas. Most of the gas in that cloud went to form the sun or
got blown away, but a small amount of the heavier elements collected together to form the bodies that now orbit the
sun as planets like the earth.
The earth was initially very hot and without an atmosphere. In the course of time it cooled and acquired an
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/g.html (2 of 11) [2/20/2001 3:15:29 AM]

A Brief History of Time - Stephen Hawking... Chapter 8

atmosphere from the emission of gases from the rocks. This early atmosphere was not one in which we could have
survived. It contained no oxygen, but a lot of other gases that are poisonous to us, such as hydrogen sulfide (the gas
that gives rotten eggs their smell). There are, however, other primitive forms of life that can flourish under such
conditions. It is thought that they developed in the oceans, possibly as a result of chance combinations of atoms into
large structures, called macromolecules, which were capable of assembling other atoms in the ocean into similar
structures. They would thus have reproduced themselves and multiplied. In some cases there would be errors in the
reproduction. Mostly these errors would have been such that the new macromolecule could not reproduce itself and
eventually would have been destroyed. However, a few of the errors would have produced new macromolecules that
were even better at reproducing themselves. They would have therefore had an advantage and would have tended
to replace the original macromolecules. In this way a process of evolution was started that led to the development of
more and more complicated, self-reproducing organisms. The first primitive forms of life consumed various materials,
including hydrogen sulfide, and released oxygen. This gradually changed the atmosphere to the composition that it
has today, and allowed the development of higher forms of life such as fish, reptiles, mammals, and ultimately the
human race.
This picture of a universe that started off very hot and cooled as it expanded is in agreement with all the
observational evidence that we have today. Nevertheless, it leaves a number of important questions unanswered:
1. Why was the early universe so hot?
2. Why is the universe so uniform on a large scale? Why does it look the same at all points of space and in all
directions? In particular, why is the temperature of the microwave back-ground radiation so nearly the same when we
look in different directions? It is a bit like asking a number of students an exam question. If they all give exactly the
same answer, you can be pretty sure they have communicated with each other. Yet, in the model described above,
there would not have been time since the big bang for light to get from one distant region to another, even though the
regions were close together in the early universe. According to the theory of relativity, if light cannot get from one
region to another, no other information can. So there would be no way in which different regions in the early universe
could have come to have the same temperature as each other, unless for some unexplained reason they happened
to start out with the same temperature.
3. Why did the universe start out with so nearly the critical rate of expansion that separates models that recollapse
from those that go on expanding forever, that even now, ten thousand million years later, it is still expanding at nearly
the critical rate? If the rate of expansion one second after the big bang had been smaller by even one part in a
hundred thousand million million, the universe would have recollapsed before it ever reached its present size.
4. Despite the fact that the universe is so uniform and homogeneous on a large scale, it contains local irregularities,
such as stars and galaxies. These are thought to have developed from small differences in the density of the early
universe from one region to another. What was the origin of these density fluctuations?
The general theory of relativity, on its own, cannot explain these features or answer these questions because of its
prediction that the universe started off with infinite density at the big bang singularity. At the singularity, general
relativity and all other physical laws would break down: one couldn’t predict what would come out of the singularity.
As explained before, this means that one might as well cut the big bang, and any events before it, out of the theory,
because they can have no effect on what we observe. Space-time would have a boundary – a beginning at the big
bang.
Science seems to have uncovered a set of laws that, within the limits set by the uncertainty principle, tell us how the
universe will develop with time, if we know its state at any one time. These laws may have originally been decreed by
God, but it appears that he has since left the universe to evolve according to them and does not now intervene in it.
But how did he choose the initial state or configuration of the universe? What were the “boundary conditions” at the
beginning of time?
One possible answer is to say that God chose the initial configuration of the universe for reasons that we cannot
hope to understand. This would certainly have been within the power of an omnipotent being, but if he had started it
off in such an incomprehensible way, why did he choose to let it evolve according to laws that we could understand?
The whole history of science has been the gradual realization that events do not happen in an arbitrary manner, but
that they reflect a certain underlying order, which may or may not be divinely inspired. It would be only natural to
suppose that this order should apply not only to the laws, but also to the conditions at the boundary of space-time
that specify the initial state of the universe. There may be a large number of models of the universe with different
initial conditions that all obey the laws. There ought to be some principle that picks out one initial state, and hence

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/g.html (3 of 11) [2/20/2001 3:15:29 AM]

A Brief History of Time - Stephen Hawking... Chapter 8

one model, to represent our universe.
One such possibility is what are called chaotic boundary conditions. These implicitly assume either that the universe
is spatially infinite or that there are infinitely many universes. Under chaotic boundary conditions, the probability of
finding any particular region of space in any given configuration just after the big bang is the same, in some sense,
as the probability of finding it in any other configuration: the initial state of the universe is chosen purely randomly.
This would mean that the early universe would have probably been very chaotic and irregular because there are
many more chaotic and disordered configurations for the universe than there are smooth and ordered ones. (If each
configuration is equally probable, it is likely that the universe started out in a chaotic and disordered state, simply
because there are so many more of them.) It is difficult to see how such chaotic initial conditions could have given
rise to a universe that is so smooth and regular on a large scale as ours is today. One would also have expected the
density fluctuations in such a model to have led to the formation of many more primordial black holes than the upper
limit that has been set by observations of the gamma ray background.
If the universe is indeed spatially infinite, or if there are infinitely many universes, there would probably be some large
regions somewhere that started out in a smooth and uniform manner. It is a bit like the well-known horde of monkeys
hammering away on typewriters – most of what they write will be garbage, but very occasionally by pure chance they
will type out one of Shakespeare’s sonnets. Similarly, in the case of the universe, could it be that we are living in a
region that just happens by chance to be smooth and uniform? At first sight this might seem very improbable,
because such smooth regions would be heavily outnumbered by chaotic and irregular regions. However, suppose
that only in the smooth regions were galaxies and stars formed and were conditions right for the development of
complicated self-replicating organisms like ourselves who were capable of asking the question: why is the universe
so smooth.? This is an example of the application of what is known as the anthropic principle, which can be
paraphrased as “We see the universe the way it is because we exist.”
There are two versions of the anthropic principle, the weak and the strong. The weak anthropic principle states that
in a universe that is large or infinite in space and/or time, the conditions necessary for the development of intelligent
life will be met only in certain regions that are limited in space and time. The intelligent beings in these regions
should therefore not be surprised if they observe that their locality in the universe satisfies the conditions that are
necessary for their existence. It is a bit like a rich person living in a wealthy neighborhood not seeing any poverty.
One example of the use of the weak anthropic principle is to “explain” why the big bang occurred about ten thousand
million years ago – it takes about that long for intelligent beings to evolve. As explained above, an early generation of
stars first had to form. These stars converted some of the original hydrogen and helium into elements like carbon and
oxygen, out of which we are made. The stars then exploded as supernovas, and their debris went to form other stars
and planets, among them those of our Solar System, which is about five thousand million years old. The first one or
two thousand million years of the earth’s existence were too hot for the development of anything complicated. The
remaining three thousand million years or so have been taken up by the slow process of biological evolution, which
has led from the simplest organisms to beings who are capable of measuring time back to the big bang.
Few people would quarrel with the validity or utility of the weak anthropic principle. Some, however, go much further
and propose a strong version of the principle. According to this theory, there are either many different universes or
many different regions of a single universe, each with its own initial configuration and, perhaps, with its own set of
laws of science. In most of these universes the conditions would not be right for the development of complicated
organisms; only in the few universes that are like ours would intelligent beings develop and ask the question, “Why is
the universe the way we see it?” The answer is then simple: if it had been different, we would not be here!
The laws of science, as we know them at present, contain many fundamental numbers, like the size of the electric
charge of the electron and the ratio of the masses of the proton and the electron. We cannot, at the moment at least,
predict the values of these numbers from theory – we have to find them by observation. It may be that one day we
shall discover a complete unified theory that predicts them all, but it is also possible that some or all of them vary
from universe to universe or within a single universe. The remarkable fact is that the values of these numbers seem
to have been very finely adjusted to make possible the development of life. For example, if the electric charge of the
electron had been only slightly different, stars either would have been unable to burn hydrogen and helium, or else
they would not have exploded. Of course, there might be other forms of intelligent life, not dreamed of even by
writers of science fiction, that did not require the light of a star like the sun or the heavier chemical elements that are
made in stars and are flung back into space when the stars explode. Nevertheless, it seems clear that there are
relatively few ranges of values for the numbers that would allow the development of any form of intelligent life. Most
sets of values would give rise to universes that, although they might be very beautiful, would contain no one able to
wonder at that beauty. One can take this either as evidence of a divine purpose in Creation and the choice of the
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/g.html (4 of 11) [2/20/2001 3:15:29 AM]

A Brief History of Time - Stephen Hawking... Chapter 8

laws of science or as support for the strong anthropic principle.
There are a number of objections that one can raise to the strong anthropic principle as an explanation of the
observed state of the universe. First, in what sense can all these different universes be said to exist? If they are
really separate from each other, what happens in another universe can have no observable consequences in our
own universe. We should therefore use the principle of economy and cut them out of the theory. If, on the other
hand, they are just different regions of a single universe, the laws of science would have to be the same in each
region, because otherwise one could not move continuously from one region to another. In this case the only
difference between the regions would be their initial configurations and so the strong anthropic principle would
reduce to the weak one.
A second objection to the strong anthropic principle is that it runs against the tide of the whole history of science. We
have developed from the geocentric cosmologies of Ptolemy and his forebears, through the heliocentric cosmology
of Copernicus and Galileo, to the modern picture in which the earth is a medium-sized planet orbiting around an
average star in the outer suburbs of an ordinary spiral galaxy, which is itself only one of about a million million
galaxies in the observable universe. Yet the strong anthropic principle would claim that this whole vast construction
exists simply for our sake. This is very hard to believe. Our Solar System is certainly a prerequisite for our existence,
hand one might extend this to the whole of our galaxy to allow for an earlier generation of stars that created the
heavier elements. But there does not seem to be any need for all those other galaxies, nor for the universe to be so
uniform and similar in every direction on the large scale.
One would feel happier about the anthropic principle, at least in its weak version, if one could show that quite a
number of different initial configurations for the universe would have evolved to produce a universe like the one we
observe. If this is the case, a universe that developed from some sort of random initial conditions should contain a
number of regions that are smooth and uniform and are suitable for the evolution of intelligent life. On the other hand,
if the initial state of the universe had to be chosen extremely carefully to lead to something like what we see around
us, the universe would be unlikely to contain any region in which life would appear. In the hot big bang model
described above, there was not enough time in the early universe for heat to have flowed from one region to another.
This means that the initial state of the universe would have to have had exactly the same temperature everywhere in
order to account for the fact that the microwave back-ground has the same temperature in every direction we look.
The initial rate of expansion also would have had to be chosen very precisely for the rate of expansion still to be so
close to the critical rate needed to avoid recollapse. This means that the initial state of the universe must have been
very carefully chosen indeed if the hot big bang model was correct right back to the beginning of time. It would be
very difficult to explain why the universe should have begun in just this way, except as the act of a God who intended
to create beings like us.
In an attempt to find a model of the universe in which many different initial configurations could have evolved to
something like the present universe, a scientist at the Massachusetts Institute of Technology, Alan Guth, suggested
that the early universe might have gone through a period of very rapid expansion. This expansion is said to be
“inflationary,” meaning that the universe at one time expanded at an increasing rate rather than the decreasing rate
that it does today. According to Guth, the radius of the universe increased by a million million million million million (1
with thirty zeros after it) times in only a tiny fraction of a second.
Guth suggested that the universe started out from the big bang in a very hot, but rather chaotic, state. These high
temperatures would have meant that the particles in the universe would be moving very fast and would have high
energies. As we discussed earlier, one would expect that at such high temperatures the strong and weak nuclear
forces and the electromagnetic force would all be unified into a single force. As the universe expanded, it would cool,
and particle energies would go down. Eventually there would be what is called a phase transition and the symmetry
between the forces would be broken: the strong force would become different from the weak and electromagnetic
forces. One common example of a phase transition is the freezing of water when you cool it down. Liquid water is
symmetrical, the same at every point and in every direction. However, when ice crystals form, they will have definite
positions and will be lined up in some direction. This breaks water’s symmetry.
In the case of water, if one is careful, one can “supercool” it: that is, one can reduce the temperature below the
freezing point (OºC) without ice forming. Guth suggested that the universe might behave in a similar way: the
temperature might drop below the critical value without the symmetry between the forces being broken. If this
happened, the universe would be in an unstable state, with more energy than if the symmetry had been broken. This
special extra energy can be shown to have an antigravitational effect: it would have acted just like the cosmological
constant that Einstein introduced into general relativity when he was trying to construct a static model of the
universe. Since the universe would already be expanding just as in the hot big bang model, the repulsive effect of
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/g.html (5 of 11) [2/20/2001 3:15:29 AM]

A Brief History of Time - Stephen Hawking... Chapter 8

this cosmological constant would therefore have made the universe expand at an ever-increasing rate. Even in
regions where there were more matter particles than average, the gravitational attraction of the matter would have
been outweighed by the repulsion of the effective cosmological constant. Thus these regions would also expand in
an accelerating inflationary manner. As they expanded and the matter particles got farther apart, one would be left
with an expanding universe that contained hardly any particles and was still in the supercooled state. Any
irregularities in the universe would simply have been smoothed out by the expansion, as the wrinkles in a balloon are
smoothed away when you blow it up. Thus the present smooth and uniform state of the universe could have evolved
from many different non-uniform initial states.
In such a universe, in which the expansion was accelerated by a cosmological constant rather than slowed down by
the gravitational attraction of matter, there would be enough time for light to travel from one region to another in the
early universe. This could provide a solution to the problem, raised earlier, of why different regions in the early
universe have the same properties. Moreover, the rate of expansion of the universe would automatically become
very close to the critical rate determined by the energy density of the universe. This could then explain why the rate
of expansion is still so close to the critical rate, without having to assume that the initial rate of expansion of the
universe was very carefully chosen.
The idea of inflation could also explain why there is so much matter in the universe. There are something like ten
million million million million million million million million million million million million million million (1 with eighty
zeros after it) particles in the region of the universe that we can observe. Where did they all come from? The answer
is that, in quantum theory, particles can be created out of energy in the form of particle/antiparticle pairs. But that just
raises the question of where the energy came from. The answer is that the total energy of the universe is exactly
zero. The matter in the universe is made out of positive energy. However, the matter is all attracting itself by gravity.
Two pieces of matter that are close to each other have less energy than the same two pieces a long way apart,
because you have to expend energy to separate them against the gravitational force that is pulling them together.
Thus, in a sense, the gravitational field has negative energy. In the case of a universe that is approximately uniform
in space, one can show that this negative gravitational energy exactly cancels the positive energy represented by the
matter. So the total energy of the universe is zero.
Now twice zero is also zero. Thus the universe can double the amount of positive matter energy and also double the
negative gravitational energy without violation of the conservation of energy. This does not happen in the normal
expansion of the universe in which the matter energy density goes down as the universe gets bigger. It does happen,
however, in the inflationary expansion because the energy density of the supercooled state remains constant while
the universe expands: when the universe doubles in size, the positive matter energy and the negative gravitational
energy both double, so the total energy remains zero. During the inflationary phase, the universe increases its size
by a very large amount. Thus the total amount of energy available to make particles becomes very large. As Guth
has remarked, “It is said that there’s no such thing as a free lunch. But the universe is the ultimate free lunch.”
The universe is not expanding in an inflationary way today. Thus there has to be some mechanism that would
eliminate the very large effective cosmological constant and so change the rate of expansion from an accelerated
one to one that is slowed down by gravity, as we have today. In the inflationary expansion one might expect that
eventually the symmetry between the forces would be broken, just as super-cooled water always freezes in the end.
The extra energy of the unbroken symmetry state would then be released and would reheat the universe to a
temperature just below the critical temperature for symmetry between the forces. The universe would then go on to
expand and cool just like the hot big bang model, but there would now be an explanation of why the universe was
expanding at exactly the critical rate and why different regions had the same temperature.
In Guth’s original proposal the phase transition was supposed to occur suddenly, rather like the appearance of ice
crystals in very cold water. The idea was that “bubbles” of the new phase of broken symmetry would have formed in
the old phase, like bubbles of steam surrounded by boiling water. The bubbles were supposed to expand and meet
up with each other until the whole universe was in the new phase. The trouble was, as I and several other people
pointed out, that the universe was expanding so fast that even if the bubbles grew at the speed of light, they would
be moving away from each other and so could not join up. The universe would be left in a very non-uniform state,
with some regions still having symmetry between the different forces. Such a model of the universe would not
correspond to what we see.
In October 1981, I went to Moscow for a conference on quantum gravity. After the conference I gave a seminar on
the inflationary model and its problems at the Sternberg Astronomical Institute. Before this, I had got someone else
to give my lectures for me, because most people could not understand my voice. But there was not time to prepare
this seminar, so I gave it myself, with one of my graduate students repeating my words. It worked well, and gave me
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/g.html (6 of 11) [2/20/2001 3:15:29 AM]

A Brief History of Time - Stephen Hawking... Chapter 8

much more contact with my audience. In the audience was a young Russian, Andrei Linde, from the Lebedev
Institute in Moscow. He said that the difficulty with the bubbles not joining up could be avoided if the bubbles were so
big that our region of the universe is all contained inside a single bubble. In order for this to work, the change from
symmetry to broken symmetry must have taken place very slowly inside the bubble, but this is quite possible
according to grand unified theories. Linde’s idea of a slow breaking of symmetry was very good, but I later realized
that his bubbles would have to have been bigger than the size of the universe at the time! I showed that instead the
symmetry would have broken everywhere at the same time, rather than just inside bubbles. This would lead to a
uniform universe, as we observe. I was very excited by this idea and discussed it with one of my students, Ian Moss.
As a friend of Linde’s, I was rather embarrassed, however, when I was later sent his paper by a scientific journal and
asked whether it was suitable for publication. I replied that there was this flaw about the bubbles being bigger than
the universe, but that the basic idea of a slow breaking of symmetry was very good. I recommended that the paper ¿
published as it was because it would take Linde several months to correct it, since anything he sent to the West
would have to be passed by Soviet censorship, which was neither very skillful nor very quick with scientific papers.
Instead, I wrote a short paper with Ian Moss in the same journal in which we pointed out this problem with the bubble
and showed how it could be resolved.
The day after I got back from Moscow I set out for Philadelphia, where I was due to receive a medal from the
Franklin Institute. My secretary, Judy Fella, had used her not inconsiderable charm to persuade British Airways to
give herself and me free seats on a Concorde as a publicity venture. However, I .was held up on my way to the
airport by heavy rain and I missed the plane. Nevertheless, I got to Philadelphia in the end and received my medal. I
was then asked to give a seminar on the inflationary universe at Drexel University in Philadelphia. I gave the same
seminar about the problems of the inflationary universe, just as in Moscow.
A very similar idea to Linde’s was put forth independently a few months later by Paul Steinhardt and Andreas
Albrecht of the University of Pennsylvania. They are now given joint credit with Linde for what is called “the new
inflationary model,” based on the idea of a slow breaking of symmetry. (The old inflationary model was Guth’s
original suggestion of fast symmetry breaking with the formation of bubbles.)
The new inflationary model was a good attempt to explain why the universe is the way it is. However, I and several
other people showed that, at least in its original form, it predicted much greater variations in the temperature of the
microwave background radiation than are observed. Later work has also cast doubt on whether there could be a
phase transition in the very early universe of the kind required. In my personal opinion, the new inflationary model is
now dead as a scientific theory, although a lot of people do not seem to have heard of its demise and are still writing
papers as if it were viable. A better model, called the chaotic inflationary model, was put forward by Linde in 1983. In
this there is no phase transition or supercooling. Instead, there is a spin 0 field, which, because of quantum
fluctuations, would have large values in some regions of the early universe. The energy of the field in those regions
would behave like a cosmological constant. It would have a repulsive gravitational effect, and thus make those
regions expand in an inflationary manner. As they expanded, the energy of the field in them would slowly decrease
until the inflationary expansion changed to an expansion like that in the hot big bang model. One of these regions
would become what we now see as the observable universe. This model has all the advantages of the earlier
inflationary models, but it does not depend on a dubious phase transition, and it can moreover give a reasonable size
for the fluctuations in the temperature of the microwave background that agrees with observation.
This work on inflationary models showed that the present state of the universe could have arisen from quite a large
number of different initial configurations. This is important, because it shows that the initial state of the part of the
universe that we inhabit did not have to be chosen with great care. So we may, if we wish, use the weak anthropic
principle to explain why the universe looks the way it does now. It cannot be the case, however, that every initial
configuration would have led to a universe like the one we observe. One can show this by considering a very
different state for the universe at the present time, say, a very lumpy and irregular one. One could use the laws of
science to evolve the universe back in time to determine its configuration at earlier times. According to the singularity
theorems of classical general relativity, there would still have been a big bang singularity. If you evolve such a
universe forward in time according to the laws of science, you will end up with the lumpy and irregular state you
started with. Thus there must have been initial configurations that would not have given rise to a universe like the
one we see today. So even the inflationary model does not tell us why the initial configuration was not such as to
produce something very different from what we observe. Must we turn to the anthropic principle for an explanation?
Was it all just a lucky chance? That would seem a counsel of despair, a negation of all our hopes of understanding
the underlying order of the universe.
In order to predict how the universe should have started off, one needs laws that hold at the beginning of time. If the
classical theory of general relativity was correct, the singularity theorems that Roger Penrose and I proved show that
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/g.html (7 of 11) [2/20/2001 3:15:29 AM]

A Brief History of Time - Stephen Hawking... Chapter 8

the beginning of time would have been a point of infinite density and infinite curvature of space-time. All the known
laws of science would break down at such a point. One might suppose that there were new laws that held at
singularities, but it would be very difficult even to formulate such laws at such badly behaved points, and we would
have no guide from observations as to what those laws might be. However, what the singularity theorems really
indicate is that the gravitational field becomes so strong that quantum gravitational effects become important:
classical theory is no longer a good description of the universe. So one has to use a quantum theory of gravity to
discuss the very early stages of the universe. As we shall see, it is possible in the quantum theory for the ordinary
laws of science to hold everywhere, including at the beginning of time: it is not necessary to postulate new laws for
singularities, because there need not be any singularities in the quantum theory.
We don’t yet have a complete and consistent theory that combines quantum mechanics and gravity. However, we
are fairly certain of some features that such a unified theory should have. One is that it should incorporate
Feynman’s proposal to formulate quantum theory in terms of a sum over histories. In this approach, a particle does
not have just a single history, as it would in a classical theory. Instead, it is supposed to follow every possible path in
space-time, and with each of these histories there are associated a couple of numbers, one represent-ing the size of
a wave and the other representing its position in the cycle (its phase). The probability that the particle, say, passes
through some particular point is found by adding up the waves associated with every possible history that passes
through that point. When one actually tries to perform these sums, however, one runs into severe technical
problems. The only way around these is the following peculiar prescription: one must add up the waves for particle
histories that are not in the “real” time that you and I experience but take place in what is called imaginary time.
Imaginary time may sound like science fiction but it is in fact a well-defined mathematical concept. If we take any
ordinary (or “real”) number and multiply it by itself, the result is a positive number. (For example, 2 times 2 is 4, but
so is – 2 times – 2.) There are, however, special numbers (called imaginary numbers) that give negative numbers
when multiplied by themselves. (The one called i, when multiplied by itself, gives – 1, 2i multiplied by itself gives – 4,
and so on.)
One can picture real and imaginary numbers in the following way: The real numbers can be represented by a line
going from left to right, with zero in the middle, negative numbers like – 1, – 2, etc. on the left, and positive numbers,
1, 2, etc. on the right. Then imaginary numbers are represented by a line going up and down the page, with i, 2i, etc.
above the middle, and – i, – 2i, etc. below. Thus imaginary numbers are in a sense numbers at right angles to
ordinary real numbers.
To avoid the technical difficulties with Feynman’s sum over histories, one must use imaginary time. That is to say, for
the purposes of the calculation one must measure time using imaginary numbers, rather than real ones. This has an
interesting effect on space-time: the distinction between time and space disappears completely. A space-time in
which events have imaginary values of the time coordinate is said to be Euclidean, after the ancient Greek Euclid,
who founded the study of the geometry of two-dimensional surfaces. What we now call Euclidean space-time is very
similar except that it has four dimensions instead of two. In Euclidean space-time there is no difference between the
time direction and directions in space. On the other hand, in real space-time, in which events are labeled by ordinary,
real values of the time coordinate, it is easy to tell the difference – the time direction at all points lies within the light
cone, and space directions lie outside. In any case, as far as everyday quantum mechanics is concerned, we may
regard our use of imaginary time and Euclidean space-time as merely a mathematical device (or trick) to calculate
answers about real space-time.
A second feature that we believe must be part of any ultimate theory is Einstein’s idea that the gravitational field is
represented by curved space-time: particles try to follow the nearest thing to a straight path in a curved space, but
because space-time is not flat their paths appear to be bent, as if by a gravitational field. When we apply Feynman’s
sum over histories to Einstein’s view of gravity, the analogue of the history of a particle is now a complete curved
space-time that represents the history of the whole universe. To avoid the technical difficulties in actually performing
the sum over histories, these curved space-times must be taken to be Euclidean. That is, time is imaginary and is
indistinguishable from directions in space. To calculate the probability of finding a real space-time with some certain
property, such as looking the same at every point and in every direction, one adds up the waves associated with all
the histories that have that property.
In the classical theory of general relativity, there are many different possible curved space-times, each corresponding
to a different initial state of the universe. If we knew the initial state of our universe, we would know its entire history.
Similarly, in the quantum theory of gravity, there are many different possible quantum states for the universe. Again,
if we knew how the Euclidean curved space-times in the sum over histories behaved at early times, we would know
the quantum state of the universe.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/g.html (8 of 11) [2/20/2001 3:15:29 AM]

A Brief History of Time - Stephen Hawking... Chapter 8

In the classical theory of gravity, which is based on real space-time, there are only two possible ways the universe
can behave: either it has existed for an infinite time, or else it had a beginning at a singularity at some finite time in
the past. In the quantum theory of gravity, on the other hand, a third possibility arises. Because one is using
Euclidean space-times, in which the time direction is on the same footing as directions in space, it is possible for
space-time to be finite in extent and yet to have no singularities that formed a boundary or edge. Space-time would
be like the surface of the earth, only with two more dimensions. The surface of the earth is finite in extent but it
doesn’t have a boundary or edge: if you sail off into the sunset, you don’t fall off the edge or run into a singularity. (I
know, because I have been round the world!)
If Euclidean space-time stretches back to infinite imaginary time, or else starts at a singularity in imaginary time, we
have the same problem as in the classical theory of specifying the initial state of the universe: God may know how
the universe began, but we cannot give any particular reason for thinking it began one way rather than another. On
the other hand, the quantum theory of gravity has opened up a new possibility, in which there would be no boundary
to space-time and so there would be no need to specify the behavior at the boundary. There would be no
singularities at which the laws of science broke down, and no edge of space-time at which one would have to appeal
to God or some new law to set the boundary conditions for space-time. One could say: “The boundary condition of
the universe is that it has no boundary.” The universe would be completely self-contained and not affected by
anything outside itself. It would neither be created nor destroyed, It would just BE.
It was at the conference in the Vatican mentioned earlier that I first put forward the suggestion that maybe time and
space together formed a surface that was finite in size but did not have any boundary or edge. My paper was rather
mathematical, however, so its implications for the role of God in the creation of the universe were not generally
recognized at the time (just as well for me). At the time of the Vatican conference, I did not know how to use the “no
boundary” idea to make predictions about the universe. However, I spent the following sum-mer at the University of
California, Santa Barbara. There a friend and colleague of mine, Jim Hartle, worked out with me what conditions the
universe must satisfy if space-time had no boundary. When I returned to Cambridge, I continued this work with two of
my research students, Julian Luttrel and Jonathan Halliwell.
I’d like to emphasize that this idea that time and space should be finite “without boundary” is just a proposal: it cannot
be deduced from some other principle. Like any other scientific theory, it may initially be put forward for aesthetic or
metaphysical reasons, but the real test is whether it makes predictions that agree with observation. This, how-ever, is
difficult to determine in the case of quantum gravity, for two reasons. First, as will be explained in Chapter 11, we are
not yet sure exactly which theory successfully combines general relativity and quantum mechanics, though we know
quite a lot about the form such a theory must have. Second, any model that described the whole universe in detail
would be much too complicated mathematically for us to be able to calculate exact predictions. One therefore has to
make simplifying assumptions and approximations – and even then, the problem of extracting predictions remains a
formidable one.
Each history in the sum over histories will describe not only the space-time but everything in it as well, including any
complicated organisms like human beings who can observe the history of the universe. This may provide another
justification for the anthropic principle, for if all the histories are possible, then so long as we exist in one of the
histories, we may use the anthropic principle to explain why the universe is found to be the way it is. Exactly what
meaning can be attached to the other histories, in which we do not exist, is not clear. This view of a quantum theory
of gravity would be much more satisfactory, however, if one could show that, using the sum over histories, our
universe is not just one of the possible histories but one of the most probable ones. To do this, we must perform the
sum over histories for all possible Euclidean space-times that have no boundary.
Under the “no boundary” proposal one learns that the chance of the universe being found to be following most of the
possible histories is negligible, but there is a particular family of histories that are much more probable than the
others. These histories may be pictured as being like the surface of the earth, with the distance from the North Pole
representing imaginary time and the size of a circle of constant distance from the North Pole representing the spatial
size of the universe. The universe starts at the North Pole as a single point. As one moves south, the circles of
latitude at constant distance from the North Pole get bigger, corresponding to the universe expanding with imaginary
time Figure 8:1. The universe would reach a maximum size at the equator and would contract with increasing
imaginary time to a single point at the South Pole. Ever though the universe would have zero size at the North and
South Poles, these points would not be singularities, any more than the North aid South Poles on the earth are
singular. The laws of science will hold at them, just as they do at the North and South Poles on the earth.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/g.html (9 of 11) [2/20/2001 3:15:29 AM]

A Brief History of Time - Stephen Hawking... Chapter 8

Figure 8:1
The history of the universe in real time, however, would look very different. At about ten or twenty thousand million
years ago, it would have a minimum size, which was equal to the maximum radius of the history in imaginary time. At
later real times, the universe would expand like the chaotic inflationary model proposed by Linde (but one would not
now have to assume that the universe was created somehow in the right sort of state). The universe would expand to
a very large size Figure 8:1 and eventually it would collapse again into what looks like a singularity in real time. Thus,
in a sense, we are still all doomed, even if we keep away from black holes. Only if we could picture the universe in
terms of imaginary time would there be no singularities.
If the universe really is in such a quantum state, there would be no singularities in the history of the universe in
imaginary time. It might seem therefore that my more recent work had completely undone the results of my earlier
work on singularities. But, as indicated above, the real importance of the singularity theorems was that they showed
that the gravitational field must become so strong that quantum gravitational effects could not be ignored. This in turn
led to the idea that the universe could be finite in imaginary time but without boundaries or singularities. When one
goes back to the real time in which we live, however, there will still appear to be singularities. The poor astronaut
who falls into a black hole will still come to a sticky end; only if he lived in imaginary time would he encounter no
singularities.
This might suggest that the so-called imaginary time is really the real time, and that what we call real time is just a
figment of our imaginations. In real time, the universe has a beginning and an end at singularities that form a
boundary to space-time and at which the laws of science break down. But in imaginary time, there are no
singularities or boundaries. So maybe what we call imaginary time is really more basic, and what we call real is just
an idea that we invent to help us describe what we think the universe is like. But according to the approach I
described in Chapter 1, a scientific theory is just a mathematical model we make to describe our observations: it
exists only in our minds. So it is meaningless to ask: which is real, “real” or “imaginary” time? It is simply a matter of
which is the more useful description.
One can also use the sum over histories, along with the no boundary proposal, to find which properties of the
universe are likely to occur together. For example, one can calculate the probability that the universe is expanding at
nearly the same rate in all different directions at a time when the density of the universe has its present value. In the
simplified models that have been examined so far, this probability turns out to be high; that is, the proposed no
boundary condition leads to the prediction that it is extremely probable that the present rate of expansion of the
universe is almost the same in each direction. This is consistent with the observations of the microwave background
radiation, which show that it has almost exactly the same intensity in any direction. If the universe were expanding
faster in some directions than in others, the intensity of the radiation in those directions would be reduced by an

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/g.html (10 of 11) [2/20/2001 3:15:29 AM]

A Brief History of Time - Stephen Hawking... Chapter 8

additional red shift.
Further predictions of the no boundary condition are currently being worked out. A particularly interesting problem is
the size of the small departures from uniform density in the early universe that caused the formation first of the
galaxies, then of stars, and finally of us. The uncertainty principle implies that the early universe cannot have been
completely uniform because there must have been some uncertainties or fluctuations in the positions and velocities
of the particles. Using the no boundary condition, we find that the universe must in fact have started off with just the
minimum possible non-uniformity allowed by the uncertainty principle. The universe would have then undergone a
period of rapid expansion, as in the inflationary models. During this period, the initial non-uniformities would have
been amplified until they were big enough to explain the origin of the structures we observe around us. In 1992 the
Cosmic Background Explorer satellite (COBE) first detected very slight variations in the intensity of the microwave
background with direction. The way these non-uniformities depend on direction seems to agree with the predictions
of the inflationary model and the no boundary proposal. Thus the no boundary proposal is a good scientific theory in
the sense of Karl Popper: it could have been falsified by observations but instead its predictions have been
confirmed. In an expanding universe in which the density of matter varied slightly from place to place, gravity would
have caused the denser regions to slow down their expansion and start contracting. This would lead to the formation
of galaxies, stars, and eventually even insignificant creatures like ourselves. Thus all the complicated structures that
we see in the universe might be explained by the no boundary condition for the universe together with the uncertainty
principle of quantum mechanics.
The idea that space and time may form a closed surface without boundary also has profound implications for the role
of God in the affairs of the universe. With the success of scientific theories in describing events, most people have
come to believe that God allows the universe to evolve according to a set of laws and does not intervene in the
universe to break these laws. However, the laws do not tell us what the universe should have looked like when it
started – it would still be up to God to wind up the clockwork and choose how to start it off. So long as the universe
had a beginning, we could suppose it had a creator. But if the universe is really completely self-contained, having no
boundary or edge, it would have neither beginning nor end: it would simply be. What place, then, for a creator?

PREVIOUS

NEXT

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/g.html (11 of 11) [2/20/2001 3:15:29 AM]

A Brief History of Time - Stephen Hawking... Chapter 9

CHAPTER 9
THE ARROW OF TIME

In previous chapters we have seen how our views of the nature of time have changed over the years. Up to the
beginning of this century people believed in an absolute time. That is, each event could be labeled by a number
called “time” in a unique way, and all good clocks would agree on the time interval between two events.
However, the discovery that the speed of light appeared the same to every observer, no matter how he was
moving, led to the theory of relativity – and in that one had to abandon the idea that there was a unique
absolute time. Instead, each observer would have his own measure of time as recorded by a clock that he
carried: clocks carried by different observers would not necessarily agree. Thus time became a more personal
concept, relative to the observer who measured it.
When one tried to unify gravity with quantum mechanics, one had to introduce the idea of “imaginary” time.
Imaginary time is indistinguishable from directions in space. If one can go north, one can turn around and head
south; equally, if one can go forward in imaginary time, one ought to be able to turn round and go backward.
This means that there can be no important difference between the forward and backward directions of
imaginary time. On the other hand, when one looks at “real” time, there’s a very big difference between the
forward and backward directions, as we all know. Where does this difference between the past and the future
come from? Why do we remember the past but not the future?
The laws of science do not distinguish between the past and the future. More precisely, as explained earlier,
the laws of science are unchanged under the combination of operations (or symmetries) known as C, P, and T.
(C means changing particles for antiparticles. P means taking the mirror image, so left and right are
interchanged. And T means reversing the direction of motion of all particles: in effect, running the motion
backward.) The laws of science that govern the behavior of matter under all normal situations are unchanged
under the combination of the two operations C and P on their own. In other words, life would be just the same
for the inhabitants of another planet who were both mirror images of us and who were made of antimatter,
rather than matter.
If the laws of science are unchanged by the combination of operations C and P, and also by the combination C,
P, and T, they must also be unchanged under the operation T alone. Yet there is a big difference between the
forward and backward directions of real time in ordinary life. Imagine a cup of water falling off a table and
breaking into pieces on the floor. If you take a film of this, you can easily tell whether it is being run forward or
backward. If you run it backward you will see the pieces suddenly gather themselves together off the floor and
jump back to form a whole cup on the table. You can tell that the film is being run backward because this kind
of behavior is never observed in ordinary life. If it were, crockery manufacturers would go out of business.
The explanation that is usually given as to why we don’t see broken cups gathering themselves together off the
floor and jumping back onto the table is that it is forbidden by the second law of thermodynamics. This says that
in any closed system disorder, or entropy, always increases with time. In other words, it is a form of Murphy’s
law: things always tend to go wrong! An intact cup on the table is a state of high order, but a broken cup on the
floor is a disordered state. One can go readily from the cup on the table in the past to the broken cup on the
floor in the future, but not the other way round.
The increase of disorder or entropy with time is one example of what is called an arrow of time, something that
distinguishes the past from the future, giving a direction to time. There are at least three different arrows of
time. First, there is the thermodynamic arrow of time, the direction of time in which disorder or entropy
increases. Then, there is the psychological arrow of time. This is the direction in which we feel time passes, the
direction in which we remember the past but not the future. Finally, there is the cosmological arrow of time. This
is the direction of time in which the universe is expanding rather than contracting.
In this chapter I shall argue that the no boundary condition for the universe, together with the weak anthropic
principle, can explain why all three arrows point in the same direction – and moreover, why a well-defined arrow
of time should exist at all. I shall argue that the psychological arrow is determined by the thermodynamic arrow,

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/h.html (1 of 5) [2/20/2001 3:15:38 AM]

A Brief History of Time - Stephen Hawking... Chapter 9

and that these two arrows necessarily always point in the same direction. If one assumes the no boundary
condition for the universe, we shall see that there must be well-defined thermodynamic and cosmological
arrows of time, but they will not point in the same direction for the whole history of the universe. However, I
shall argue that it is only when they do point in the same direction that conditions are suitable for the
development of intelligent beings who can ask the question: why does disorder increase in the same direction
of time as that in which the universe expands?
I shall discuss first the thermodynamic arrow of time. The second law of thermodynamics results from the fact
that there are always many more disordered states than there are ordered ones. For example, consider the
pieces of a jigsaw in a box. There is one, and. only one, arrangement in which the pieces make a complete
picture. On the other hand, there are a very large number of arrangements in which the pieces are disordered
and don’t make a picture.
Suppose a system starts out in one of the small number of ordered states. As time goes by, the system will
evolve according to the laws of science and its state will change. At a later time, it is more probable that the
system will be in a disordered state than in an ordered one because there are more disordered states. Thus
disorder will tend to increase with time if the system obeys an initial condition of high order.
Suppose the pieces of the jigsaw start off in a box in the ordered arrangement in which they form a picture. If
you shake the box, the pieces will take up another arrangement. This will probably be a disordered
arrangement in which the pieces don’t form a proper picture, simply because there are so many more
disordered arrangements. Some groups of pieces may still form parts of the picture, but the more you shake
the box, the more likely it is that these groups will get broken up and the pieces will be in a completely jumbled
state in which they don’t form any sort of picture. So the disorder of the pieces will probably increase with time if
the pieces obey the initial condition that they start off in a condition of high order.
Suppose, however, that God decided that the universe should finish up in a state of high order but that it didn’t
matter what state it started in. At early times the universe would probably be in a disordered state. This would
mean that disorder would decrease with time. You would see broken cups gathering themselves together and
jumping back onto the table. However, any human beings who were observing the cups would be living in a
universe in which disorder decreased with time. I shall argue that such beings would have a psychological
arrow of time that was backward. That is, they would remember events in the future, and not remember events
in their past. When the cup was broken, they would remember it being on the table, but when it was on the
table, they would not remember it being on the floor.
It is rather difficult to talk about human memory because we don’t know how the brain works in detail. We do,
however, know all about how computer memories work. I shall therefore discuss the psychological arrow of
time for computers. I think it is reasonable to assume that the arrow for computers is the same as that for
humans. If it were not, one could make a killing on the stock exchange by having a computer that would
remember tomorrow’s prices! A computer memory is basically a device containing elements that can exist in
either of two states. A simple example is an abacus. In its simplest form, this consists of a number of wires; on
each wire there are a number of beads that can be put in one of two positions. Before an item is recorded in a
computer’s memory, the memory is in a disordered state, with equal probabilities for the two possible states.
(The abacus beads are scattered randomly on the wires of the abacus.) After the memory interacts with the
system to be remembered, it will definitely be in one state or the other, according to the state of the system.
(Each abacus bead will be at either the left or the right of the abacus wire.) So the memory has passed from a
disordered state to an ordered one. However, in order to make sure that the memory is in the right state, it is
necessary to use a certain amount of energy (to move the bead or to power the computer, for example). This
energy is dissipated as heat, and increases the amount of disorder in the universe. One can show that this
increase in disorder is always greater than the increase in the order of the memory itself. Thus the heat
expelled by the computer’s cooling fan means that when a computer records an item in memory, the total
amount of disorder in the universe still goes up. The direction of time in which a computer remembers the past
is the same as that in which disorder increases.
Our subjective sense of the direction of time, the psychological arrow of time, is therefore determined within our
brain by the thermodynamic arrow of time. Just like a computer, we must remember things in the order in which
entropy increases. This makes the second law of thermodynamics almost trivial. Disorder increases with time

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/h.html (2 of 5) [2/20/2001 3:15:38 AM]

A Brief History of Time - Stephen Hawking... Chapter 9

because we measure time in the direction in which disorder increases You can’t have a safer bet than that!
But why should the thermodynamic arrow of time exist at all? Or, in other words, why should the universe be in
a state of high order at one end of time, the end that we call the past? Why is it not in a state of complete
disorder at all times? After all, this might seem more probable. And why is the direction of time in which
disorder increases the same as that in which the universe expands?
In the classical theory of general relativity one cannot predict how the universe would have begun because all
the known laws of science would have broken down at the big bang singularity. The universe could have
started out in a very smooth and ordered state. This would have led to well-defined thermodynamic and
cosmological arrows of time, as we observe. But it could equally well have started out in a very lumpy and
disordered state. In that case, the universe would already be in a state of complete disorder, so disorder could
not increase with time. It would either stay constant, in which case there would be no well-defined
thermodynamic arrow of time, or it would decrease, in which case the thermodynamic arrow of time would point
in the opposite direction to the cosmological arrow. Neither of these possibilities agrees with what we observe.
However, as we have seen, classical general relativity predicts its own downfall. When the curvature of
space-time becomes large, quantum gravitational effects will become important and the classical theory will
cease to be a good description of the universe. One has to use a quantum theory of gravity to understand how
the universe began.
In a quantum theory of gravity, as we saw in the last chapter, in order to specify the state of the universe one
would still have to say how the possible histories of the universe would behave at the boundary of space-time in
the past. One could avoid this difficulty of having to describe what we do not and cannot know only if the
histories satisfy the no boundary condition: they are finite in extent but have no boundaries, edges, or
singularities. In that case, the beginning of time would be a regular, smooth point of space-time and the
universe would have begun its expansion in a very smooth and ordered state. It could not have been
completely uniform, because that would violate the uncertainty principle of quantum theory. There had to be
small fluctuations in the density and velocities of particles. The no boundary condition, however, implied that
these fluctuations were as small as they could be, consistent with the uncertainty principle.
The universe would have started off with a period of exponential or “inflationary” expansion in which it would
have increased its size by a very large factor. During this expansion, the density fluctuations would have
remained small at first, but later would have started to grow. Regions in which the density was slightly higher
than average would have had their expansion slowed down by the gravitational attraction of the extra mass.
Eventually, such regions would stop expanding and collapse to form galaxies, stars, and beings like us. The
universe would have started in a smooth and ordered state, and would become lumpy and disordered as time
went on. This would explain the existence of the thermodynamic arrow of time.
But what would happen if and when the universe stopped expanding and began to contract? Would the
thermodynamic arrow reverse and disorder begin to decrease with time? This would lead to all sorts of
science-fiction-like possibilities for people who survived from the expanding to the contracting phase. Would
they see broken cups gathering themselves together off the floor and jumping back onto the table? Would they
be able to remember tomorrow’s prices and make a fortune on the stock market? It might seem a bit academic
to worry about what will happen when the universe collapses again, as it will not start to contract for at least
another ten thousand million years. But there is a quicker way to find out what will happen: jump into a black
hole. The collapse of a star to form a black hole is rather like the later stages of the collapse of the whole
universe. So if disorder were to decrease in the contracting phase of the universe, one might also expect it to
decrease inside a black hole. So perhaps an astronaut who fell into a black hole would be able to make money
at roulette by remembering where the ball went before he placed his bet. (Unfortunately, however, he would not
have long to play before he was turned to spaghetti. Nor would he be able to let us know about the reversal of
the thermodynamic arrow, or even bank his winnings, because he would be trapped behind the event horizon
of the black hole.)
At first, I believed that disorder would decrease when the universe recollapsed. This was because I thought that
the universe had to return to a smooth and ordered state when it became small again. This would mean that
the contracting phase would be like the time reverse of the expanding phase. People in the contracting phase
would live their lives backward: they would die before they were born and get younger as the universe

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/h.html (3 of 5) [2/20/2001 3:15:38 AM]

A Brief History of Time - Stephen Hawking... Chapter 9

contracted.
This idea is attractive because it would mean a nice symmetry between the expanding and contracting phases.
However, one cannot adopt it on its own, independent of other ideas about the universe. The question is: is it
implied by the no boundary condition, or is it inconsistent with that condition? As I said, I thought at first that the
no boundary condition did indeed imply that disorder would decrease in the contracting phase. I was misled
partly by the analogy with the surface of the earth. If one took the beginning of the universe to correspond to
the North Pole, then the end of the universe should be similar to the beginning, just as the South Pole is similar
to the North. However, the North and South Poles correspond to the beginning and end of the universe in
imaginary time. The beginning and end in real time can be very different from each other. I was also misled by
work I had done on a simple model of the universe in which the collapsing phase looked like the time reverse of
the expanding phase. However, a colleague of mine, Don Page, of Penn State University, pointed out that the
no boundary condition did not require the contracting phase necessarily to be the time reverse of the expanding
phase. Further, one of my students, Raymond Laflamme, found that in a slightly more complicated model, the
collapse of the universe was very different from the expansion. I realized that I had made a mistake: the no
boundary condition implied that disorder would in fact continue to increase during the contraction. The
thermodynamic and psychological arrows of time would not reverse when the universe begins to recontract, or
inside black holes.
What should you do when you find you have made a mistake like that? Some people never admit that they are
wrong and continue to find new, and often mutually inconsistent, arguments to support their case – as
Eddington did in opposing black hole theory. Others claim to have never really supported the incorrect view in
the first place or, if they did, it was only to show that it was inconsistent. It seems to me much better and less
confusing if you admit in print that you were wrong. A good example of this was Einstein, who called the
cosmological constant, which he introduced when he was trying to make a static model of the universe, the
biggest mistake of his life.
To return to the arrow of time, there remains the question: why do we observe that the thermodynamic and
cosmological arrows point in the same direction? Or in other words, why does disorder increase in the same
direction of time as that in which the universe expands? If one believes that the universe will expand and then
contract again, as the no boundary proposal seems to imply, this becomes a question of why we should be in
the expanding phase rather than the contracting phase.
One can answer this on the basis of the weak anthropic principle. Conditions in the contracting phase would not
be suitable for the existence of intelligent beings who could ask the question: why is disorder increasing in the
same direction of time as that in which the universe is expanding? The inflation in the early stages of the
universe, which the no boundary proposal predicts, means that the universe must be expanding at very close to
the critical rate at which it would just avoid recollapse, and so will not recollapse for a very long time. By then all
the stars will have burned out and the protons and neutrons in them will probably have decayed into light
particles and radiation. The universe would be in a state of almost complete disorder. There would be no strong
thermodynamic arrow of time. Disorder couldn’t increase much because the universe would be in a state of
almost complete disorder already. However, a strong thermodynamic arrow is necessary for intelligent life to
operate. In order to survive, human beings have to consume food, which is an ordered form of energy, and
convert it into heat, which is a disordered form of energy. Thus intelligent life could not exist in the contracting
phase of the universe. This is the explanation of why we observe that the thermodynamic and cosmological
arrows of time point in the same direction. It is not that the expansion of the universe causes disorder to
increase. Rather, it is that the no boundary condition causes disorder to increase and the conditions to be
suitable for intelligent life only in the expanding phase.
To summarize, the laws of science do not distinguish between the forward and backward directions of time.
However, there are at least three arrows of time that do distinguish the past from the future. They are the
thermodynamic arrow, the direction of time in which disorder increases; the psychological arrow, the direction
of time in which we remember the past and not the future; and the cosmological arrow, the direction of time in
which the universe expands rather than contracts. I have shown that the psychological arrow is essentially the
same as the thermodynamic arrow, so that the two would always point in the same direction. The no boundary
proposal for the universe predicts the existence of a well-defined thermodynamic arrow of time because the
universe must start off in a smooth and ordered state. And the reason we observe this thermodynamic arrow to
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/h.html (4 of 5) [2/20/2001 3:15:38 AM]

A Brief History of Time - Stephen Hawking... Chapter 9

agree with the cosmological arrow is that intelligent beings can exist only in the expanding phase. The
contracting phase will be unsuitable because it has no strong thermodynamic arrow of time.
The progress of the human race in understanding the universe has established a small corner of order in an
increasingly disordered universe. If you remember every word in this book, your memory will have recorded
about two million pieces of information: the order in your brain will have increased by about two million units.
However, while you have been reading the book, you will have converted at least a thousand calories of
ordered energy, in the form of food, into disordered energy, in the form of heat that you lose to the air around
you by convection and sweat. This will increase the disorder of the universe by about twenty million million
million million units – or about ten million million million times the increase in order in your brain – and that’s if
you remember everything in this book. In the next chapter but one I will try to increase the order in our neck of
the woods a little further by explaining how people are trying to fit together the partial theories I have described
to form a complete unified theory that would cover everything in the universe.

PREVIOUS

NEXT

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/h.html (5 of 5) [2/20/2001 3:15:38 AM]

A Brief History of Time - Stephen Hawking... Chapter 10

CHAPTER 10
WORMHOLES AND TIME TRAVEL

The last chapter discussed why we see time go forward: why disorder increases and why we remember the
past but not the future. Time was treated as if it were a straight railway line on which one could only go one way
or the other.
But what if the railway line had loops and branches so that a train could keep going forward but come back to a
station it had already passed? In other words, might it be possible for someone to travel into the future or the
past?
H. G. Wells in The Time Machine explored these possibilities as have countless other writers of science fiction.
Yet many of the ideas of science fiction, like submarines and travel to the moon, have become matters of
science fact. So what are the prospects for time travel?
The first indication that the laws of physics might really allow people to travel in time came in 1949 when Kurt
Godel discovered a new space-time allowed by general relativity. Godel was a mathematician who was famous
for proving that it is impossible to prove all true statements, even if you limit yourself to trying to prove all the
true statements in a subject as apparently cut and dried as arithmetic. Like the uncertainty principle, Godel’s
incompleteness theorem may be a fundamental limitation on our ability to understand and predict the universe,
but so far at least it hasn’t seemed to be an obstacle in our search for a complete unified theory.
Godel got to know about general relativity when he and Einstein spent their later years at the Institute for
Advanced Study in Princeton. His space-time had the curious property that the whole universe was rotating.
One might ask: “Rotating with respect to what?” The answer is that distant matter would be rotating with
respect to directions that little tops or gyroscopes point in.
This had the side effect that it would be possible for someone to go off in a rocket ship and return to earth
before he set out. This property really upset Einstein, who had thought that general relativity wouldn’t allow time
travel. However, given Einstein’s record of ill-founded opposition to gravitational collapse and the uncertainty
principle, maybe this was an encouraging sign. The solution Godel found doesn’t correspond to the universe
we live in because we can show that the universe is not rotating. It also had a non-zero value of the
cosmological constant that Einstein introduced when he thought the universe was unchanging. After Hubble
discovered the expansion of the universe, there was no need for a cosmological constant and it is now
generally believed to be zero. However, other more reasonable space-times that are allowed by general
relativity and which permit travel into the past have since been found. One is in the interior of a rotating black
hole. Another is a space-time that contains two cosmic strings moving past each other at high speed. As their
name suggests, cosmic strings are objects that are like string in that they have length but a tiny cross section.
Actually, they are more like rubber bands because they are under enormous tension, something like a million
million million million tons. A cosmic string attached to the earth could accelerate it from 0 to 60 mph in 1/30th
of a second. Cosmic strings may sound like pure science fiction but there are reasons to believe they could
have formed in the early universe as a result of symmetry-breaking of the kind discussed in Chapter 5.
Because they would be under enormous tension and could start in any configuration, they might accelerate to
very high speeds when they straighten out.
The Godel solution and the cosmic string space-time start out so distorted that travel into the past was always
possible. God might have created such a warped universe but we have no reason to believe he did.
Observations of the microwave background and of the abundances of the light elements indicate that the early
universe did not have the kind of curvature required to allow time travel. The same conclusion follows on
theoretical grounds if the no boundary proposal is correct. So the question is: if the universe starts out without
the kind of curvature required for time travel, can we subsequently warp local regions of space-time sufficiently
to allow it?
A closely related problem that is also of concern to writers of science fiction is rapid interstellar or intergalactic

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/i.html (1 of 5) [2/20/2001 3:15:45 AM]

A Brief History of Time - Stephen Hawking... Chapter 10

travel. According to relativity, nothing can travel faster than light. If we therefore sent a spaceship to our nearest
neighboring star, Alpha Centauri, which is about four light-years away, it would take at least eight years before
we could expect the travelers to return and tell us what they had found. If the expedition were to the center of
our galaxy, it would be at least a hundred thousand years before it came back. The theory of relativity does
allow one consolation. This is the so-called twins paradox mentioned in Chapter 2.
Because there is no unique standard of time, but rather observers each have their own time as measured by
clocks that they carry with them, it is possible for the journey to seem to be much shorter for the space travelers
than for those who remain on earth. But there would not be much joy in returning from a space voyage a few
years older to find that everyone you had left behind was dead and gone thousands of years ago. So in order to
have any human interest in their stories, science fiction writers had to suppose that we would one day discover
how to travel faster than light. What most of thee authors don’t seem to have realized is that if you can travel
faster than light, the theory of relativity implies you can also travel back in the, as the following limerick says:
There was a young lady of Wight
Who traveled much faster than light.
She departed one day,
In a relative way,
And arrived on the previous night
The point is that the theory of relativity says hat there is no unique measure of time that all observers will agree
on Rather, each observer has his or her own measure of time. If it is possible for a rocket traveling below the
speed of light to get from event A (say, the final of the 100-meter race of the Olympic Games in 202) to event B
(say, the opening of the 100,004th meeting of the Congress of Alpha Centauri), then all observers will agree
that event A happened before event B according to their times. Suppose, however, that the spaceship would
have to travel faster than light to carry the news of the race to the Congress. Then observers moving at
different speeds can disagree about whether event A occurred before B or vice versa. According to the time of
an observer who is at rest with respect to the earth, it may be that the Congress opened after the race. Thus
this observer would think that a spaceship could get from A to B in time if only it could ignore the speed-of-light
speed limit. However, to an observer at Alpha Centauri moving away from the earth at nearly the speed of light,
it would appear that event B, the opening of the Congress, would occur before event A, the 100-meter race.
The theory of relativity says that the laws of physics appear the same to observers moving at different speeds.
This has been well tested by experiment and is likely to remain a feature even if we find a more advanced
theory to replace relativity Thus the moving observer would say that if faster-than-light travel is possible, it
should be possible to get from event B, the opening of the Congress, to event A, the 100-meter race. If one
went slightly faster, one could even get back before the race and place a bet on it in the sure knowledge that
one would win.
There is a problem with breaking the speed-of-light barrier. The theory of relativity says that the rocket power
needed to accelerate a spaceship gets greater and greater the nearer it gets to the speed of light. We have
experimental evidence for this, not with spaceships but with elementary particles in particle accelerators like
those at Fermilab or CERN (European Centre for Nuclear Research). We can accelerate particles to 99.99
percent of the speed of light, but however much power we feed in, we can’t get them beyond the speed-of-light
barrier. Similarly with spaceships: no matter how much rocket power they have, they can’t accelerate beyond
the speed of light.
That might seem to rule out both rapid space travel and travel back in time. However, there is a possible way
out. It might be that one could warp space-time so that there was a shortcut between A and B One way of doing
this would be to create a wormhole between A and B. As its name suggests, a wormhole is a thin tube of
space-time which can connect two nearly flat regions far apart.
There need be no relation between the distance through the wormhole and the separation of its ends in the
nearly Hat background. Thus one could imagine that one could create or find a wormhole that world lead from
the vicinity of the Solar System to Alpha Centauri. The distance through the wormhole might be only a few
million miles even though earth and Alpha Centauri are twenty million million miles apart in ordinary space. This
would allow news of the 100-meter race to reach the opening of the Congress. But then an observer moving
toward 6e earth should also be able to find another wormhole that would enable him to get from the opening of
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/i.html (2 of 5) [2/20/2001 3:15:45 AM]

A Brief History of Time - Stephen Hawking... Chapter 10

the Congress on Alpha Centauri back to earth before the start of the race. So wormholes, like any other
possible form of travel faster than light, would allow one to travel into the past.
The idea of wormholes between different regions of space-time was not an invention of science fiction writers
but came from a very respectable source.
In 1935, Einstein and Nathan Rosen wrote a paper in which they showed that general relativity allowed what
they called “bridges,” but which are now known as wormholes. The Einstein-Rosen bridges didn’t last long
enough for a spaceship to get through: the ship would run into a singularity as the wormhole pinched off.
However, it has been suggested that it might be possible for an advanced civilization to keep a wormhole open.
To do this, or to warp space-time in any other way so as to permit time travel, one can show that one needs a
region of space-time with negative curvature, like the surface of a saddle. Ordi-nary matter, which has a
positive energy density, gives space-time a positive curvature, like the surface of a sphere. So what one needs,
in order to warp space-time in a way that will allow travel into the past, is matter with negative energy density.
Energy is a bit like money: if you have a positive balance, you can distribute it in various ways, but according to
the classical laws that were believed at the beginning of the century, you weren’t allowed to be overdrawn. So
these classical laws would have ruled out any possibility of time travel. However, as has been described in
earlier chapters, the classical laws were superseded by quantum laws based on the uncertainty principle. The
quantum laws are more liberal and allow you to be overdrawn on one or two accounts provided the total
balance is positive. In other words, quantum theory allows the energy density to be negative in some places,
provided that this is made up for by positive energy densities in other places, so that the total energy re-mains
positive. An example of how quantum theory can allow negative energy densities is provided by what is called
the Casimir effect. As we saw in Chapter 7, even what we think of as “empty” space is filled with pairs of virtual
particles and antiparticles that appear together, move apart, and come back together and annihilate each other.
Now, suppose one has two parallel metal plates a short distance apart. The plates will act like mirrors for the
virtual photons or particles of light. In fact they will form a cavity between them, a bit like an organ pipe that will
resonate only at certain notes. This means that virtual photons can occur in the space between the plates only
if their wavelengths (the distance between the crest of one wave and the next) fit a whole number of times into
the gap between the plates. If the width of a cavity is a whole number of wavelengths plus a fraction of a
wave-length, then after some reflections backward and forward between the plates, the crests of one wave will
coincide with the troughs of another and the waves will cancel out.
Because the virtual photons between the plates can have only the resonant wavelengths, there will be slightly
fewer of them than in the region outside the plates where virtual photons can have any wavelength. Thus there
will be slightly fewer virtual photons hitting the inside surfaces of the plates than the outside surfaces. One
would therefore expect a force on the plates, pushing them toward each other. This force has actually been
detected and has the predicted value. Thus we have experimental evidence that virtual particles exist and have
real effects.
The fact that there are fewer virtual photons between the plates means that their energy density will be less
than elsewhere. But the total energy density in “empty” space far away from the plates must be zero, because
otherwise the energy density would warp the space and it would not be almost flat. So, if the energy density
between the plates is less than the energy density far away, it must be negative.
We thus have experimental evidence both that space-time can be warped (from the bending of light during
eclipses) and that it can be curved in the way necessary to allow time travel (from the Casimir effect). One
might hope therefore that as we advance in science and technology, we would eventually manage to build a
time machine. But if so, why hasn’t anyone come back from the future and told us how to do it? There might be
good reasons why it would be unwise to give us the secret of time travel at our present primitive state of
development, but unless human nature changes radically, it is difficult to believe that some visitor from the
future wouldn’t spill the beans. Of course, some people would claim that sightings of UFOs are evidence that
we are being visited either by aliens or by people from the future. (If the aliens were to get here in reasonable
time, they would need faster-than-light travel, so the two possibilities may be equivalent.)
However, I think that any visit by aliens or people from the future would be much more obvious and, probably,
much more unpleasant. If they are going to reveal themselves at all, why do so only to those who are not

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/i.html (3 of 5) [2/20/2001 3:15:45 AM]

A Brief History of Time - Stephen Hawking... Chapter 10

regarded as reliable witnesses? If they are trying to warn us of some great danger, they are not being very
effective.
A possible way to explain the absence of visitors from the future would be to say that the past is fixed because
we have observed it and seen that it does not have the kind of warping needed to allow travel back from the
future. On the other hand, the future is unknown and open, so it might well have the curvature required. This
would mean that any time travel would be confined to the future. There would be no chance of Captain Kirk and
the Starship Enterprise turning up at the present time.
This might explain why we have not yet been overrun by tourists from the future, but it would not avoid the
problems that would arise if one were able to go back and change history. Suppose, for example, you went
back and killed your great-great-grandfather while he was still a child. There are many versions of this paradox
but they are essentially equivalent: one would get contradictions if one were free to change the past.
There seem to be two possible resolutions to the paradoxes posed by time travel. One I shall call the consistent
histories approach. It says that even if space-time is warped so that it would be possible to travel into the past,
what happens in space-time must be a consistent solution of the laws of physics. According to this viewpoint,
you could not go back in time unless history showed that you had already arrived in the past and, while there,
had not killed your great-great-grandfather or committed any other acts that would conflict with your current
situation in the present. Moreover, when you did go back, you wouldn’t be able to change recorded history.
That means you wouldn’t have free will to do what you wanted. Of course, one could say that free will is an
illusion anyway. If there really is a complete unified theory that governs everything, it presumably also
determines your actions. But it does so in a way that is impossible to calculate for an organism that is as
complicated as a human being. The reason we say that humans have free will is because we can’t predict what
they will do. However, if the human then goes off in a rocket ship and comes back before he or she set off, we
will be able to predict what he or she will do because it will be part of recorded history. Thus, in that situation,
the time traveler would have no free will.
The other possible way to resolve the paradoxes of time travel might be called the alternative histories
hypothesis. The idea here is that when time travelers go back to the past, they enter alternative histories which
differ from recorded history. Thus they can act freely, without the constraint of consistency with their previous
history. Steven Spiel-berg had fun with this notion in the Back to the Future films: Marty McFly was able to go
back and change his parents’ courtship to a more satisfactory history.
The alternative histories hypothesis sounds rather like Richard Feynman’s way of expressing quantum theory
as a sum over histories, which was described in Chapters 4 and 8. This said that the universe didn’t just have a
single history: rather it had every possible history, each with its own probability. However, there seems to be an
important difference between Feynman’s proposal and alternative histories. In Feynman’s sum, each history
comprises a complete space-time and everything in it. The space-time may be so warped that it is possible to
travel in a rocket into the past. But the rocket would remain in the same space-time and therefore the same
history, which would have to be consistent. Thus Feynman’s sum over histories proposal seems to support the
consistent histories hypothesis rather than the alternative histories.
The Feynman sum over histories does allow travel into the past on a microscopic scale. In Chapter 9 we saw
that the laws of science are unchanged by combinations of the operations C, P, and T. This means that an
antiparticle spinning in the anticlockwise direction and moving from A to B can also be viewed as an ordinary
particle spinning clockwise and moving backward in time from B to A. Similarly, an ordinary particle moving
forward in time is equivalent to an antiparticle moving backward in time. As has been discussed in this chapter
and Chapter 7, “empty” space is filled with pairs of virtual particles and antiparticles that appear together, move
apart, and then come back together and annihilate each other.
So, one can regard the pair of particles as a single particle moving on a closed loop in space-time. When the
pair is moving forward in time (from the event at which it appears to that at which it annihilates), it is called a
particle. But when the particle is traveling back in time (from the event at which the pair annihilates to that at
which it appears), it is said to be an antiparticle traveling forward in time.
The explanation of how black holes can emit particles and radiation (given in Chapter 7) was that one member

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/i.html (4 of 5) [2/20/2001 3:15:45 AM]

A Brief History of Time - Stephen Hawking... Chapter 10

of a virtual particle/ antiparticle pair (say, the antiparticle) might fall into the black hole, leaving the other
member without a partner with which to annihilate. The forsaken particle might fall into the hole as well, but it
might also escape from the vicinity of the black hole. If so, to an observer at a distance it would appear to be a
particle emitted by the black hole.
One can, however, have a different but equivalent intuitive picture of the mechanism for emission from black
holes. One can regard the member of the virtual pair that fell into the black hole (say, the antiparticle) as a
particle traveling backward in time out of the hole. When it gets to the point at which the virtual
particle/antiparticle pair appeared together, it is scattered by the gravitational field into a particle traveling
forward in time and escaping from the black hole. If, instead, it were the particle member of the virtual pair that
fell into the hole, one could regard it as an antiparticle traveling back in time and coming out of the black hole.
Thus the radiation by black holes shows that quantum theory allows travel back in time on a microscopic scale
and that such time travel can produce observable effects.
One can therefore ask: does quantum theory allow time travel on a macroscopic scale, which people could
use? At first sight, it seems it should. The Feynman sum over histories proposal is supposed to be over all
histories. Thus it should include histories in which space-time is so warped that it is possible to travel into the
past. Why then aren’t we in trouble with history? Suppose, for example, someone had gone back and given the
Nazis the secret of the atom bomb?
One would avoid these problems if what I call the chronology protection conjecture holds. This says that the
laws of physics conspire to prevent macroscopic bodies from carrying information into the past. Like the cosmic
censorship conjecture, it has not been proved but there are reasons to believe it is true.
The reason to believe that chronology protection operates is that when space-time is warped enough to make
travel into the past possible, virtual particles moving on closed loops in space-time can become real particles
traveling forward in time at or below the speed of light. As these particles can go round the loop any number of
times, they pass each point on their route many times. Thus their energy is counted over and over again and
the energy density will become very large. This could give space-time a positive curvature that would not allow
travel into the past. It is not yet clear whether these particles would cause positive or negative curvature or
whether the curvature produced by some kinds of virtual particles might cancel that produced by other kinds.
Thus the possibility of time travel remains open. But I’m not going to bet on it. My opponent might have the
unfair advantage of knowing the future.

PREVIOUS

NEXT

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/i.html (5 of 5) [2/20/2001 3:15:45 AM]

A Brief History of Time - Stephen Hawking... Chapter 11

CHAPTER 11
THE UNIFICATION OF PHYSICS

As was explained in the first chapter, it would be very difficult to construct a complete unified theory of
everything in the universe all at one go. So instead we have made progress by finding partial theories that
describe a limited range of happenings and by neglecting other effects or approximating them by certain
numbers. (Chemistry, for example, allows us to calculate the interactions of atoms, without knowing the internal
structure of an atom’s nucleus.) Ultimately, however, one would hope to find a complete, consistent, unified
theory that would include all these partial theories as approximations, and that did not need to be adjusted to fit
the facts by picking the values of certain arbitrary numbers in the theory. The quest for such a theory is known
as “the unification of physics.” Einstein spent most of his later years unsuccessfully searching for a unified
theory, but the time was not ripe: there were partial theories for gravity and the electromagnetic force, but very
little was known about the nuclear forces. Moreover, Einstein refused to believe in the reality of quantum
mechanics, despite the important role he had played in its development. Yet it seems that the uncertainty
principle is a fundamental feature of the universe we live in. A successful unified theory must, therefore,
necessarily incorporate this principle.
As I shall describe, the prospects for finding such a theory seem to be much better now because we know so
much more about the universe. But we must beware of overconfidence – we have had false dawns before! At
the beginning of this century, for example, it was thought that everything could be explained in terms of the
properties of continuous matter, such as elasticity and heat conduction. The discovery of atomic structure and
the uncertainty principle put an emphatic end to that. Then again, in 1928, physicist and Nobel Prize winner
Max Born told a group of visitors to Gottingen University, “Physics, as we know it, will be over in six months.”
His confidence was based on the recent discovery by Dirac of the equation that governed the electron. It was
thought that a similar equation would govern the proton, which was the only other particle known at the time,
and that would be the end of theoretical physics. However, the discovery of the neutron and of nuclear forces
knocked that one on the head too. Having said this, I still believe there are grounds for cautious optimism that
we may now be near the end of the search for the ultimate laws of nature.
In previous chapters I have described general relativity, the partial theory of gravity, and the partial theories that
govern the weak, the strong, and the electromagnetic forces. The last three may be combined in so-called
grand unified theories, or GUTs, which are not very satisfactory because they do not include gravity and
because they contain a number of quantities, like the relative masses of different particles, that cannot be
predicted from the theory but have to be chosen to fit observations. The main difficulty in finding a theory that
unifies gravity with the other forces is that general relativity is a “classical” theory; that is, it does not incorporate
the uncertainty principle of quantum mechanics. On the other hand, the other partial theories depend on
quantum mechanics in an essential way. A necessary first step, therefore, is to combine general relativity with
the uncertainty principle. As we have seen, this can produce some remarkable consequences, such as black
holes not being black, and the universe not having any singularities but being completely self-contained and
without a boundary. The trouble is, as explained in Chapter 7, that the uncertainty principle means that even
“empty” space is filled with pairs of virtual particles and antiparticles. These pairs would have an infinite amount
of energy and, therefore, by Einstein’s famous equation E = mc2, they would have an infinite amount of mass.
Their gravitational attraction would thus curve up the universe to infinitely small size.
Rather similar, seemingly absurd infinities occur in the other partial theories, but in all these cases the infinities
can be canceled out by a process called renormalization. This involves canceling the infinities by introducing
other infinities. Although this technique is rather dubious mathematically, it does seem to work in practice, and
has been used with these theories to make predictions that agree with observations to an extraordinary degree
of accuracy. Renormalization, however, does have a serious drawback from the point of view of trying to find a
complete theory, because it means that the actual values of the masses and the strengths of the forces cannot
be predicted from the theory, but have to be chosen to fit the observations.
In attempting to incorporate the uncertainty principle into general relativity, one has only two quantities that can
be adjusted: the strength of gravity and the value of the cosmological constant. But adjusting these is not
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/j.html (1 of 11) [2/20/2001 3:15:56 AM]

A Brief History of Time - Stephen Hawking... Chapter 11

sufficient to remove all the infinities. One therefore has a theory that seems to predict that certain quantities,
such as the curvature of space-time, are really infinite, yet these quantities can be observed and measured to
be perfectly finite! This problem in combining general relativity and the uncertainty principle had been
suspected for some time, but was finally confirmed by detailed calculations in 1972. Four years later, a possible
solution, called “supergravity,” was suggested. The idea was to combine the spin-2 particle called the graviton,
which carries the gravitational force, with certain other particles of spin 3/2, 1, ½, and 0. In a sense, all these
particles could then be regarded as different aspects of the same “superparticle,” thus unifying the matter
particles with spin ½ and 3/2 with the force-carrying particles of spin 0, 1, and 2. The virtual particle/antiparticle
pairs of spin ½ and 3/2 would have negative energy, and so would tend to cancel out the positive energy of the
spin 2, 1, and 0 virtual pairs. This would cause many of the possible infinities to cancel out, but it was
suspected that some infinities might still remain. However, the calculations required to find out whether or not
there were any infinities left uncancelled were so long and difficult that no one was prepared to undertake them.
Even with a computer it was reckoned it would take at least four years, and the chances were very high that
one would make at least one mistake, probably more. So one would know one had the right answer only if
someone else repeated the calculation and got the same answer, and that did not seem very likely!
Despite these problems, and the fact that the particles in the super-gravity theories did not seem to match the
observed particles, most scientists believed that supergravity was probably the right answer to the problem of
the unification of physics. It seemed the best way of unifying gravity with the other forces. However, in 1984
there was a remarkable change of opinion in favor of what are called string theories. In these theories the basic
objects are not particles, which occupy a single point of space, but things that have a length but no other
dimension, like an infinitely thin piece of string. These strings may have ends (the so-called open strings) or
they may be joined up with themselves in closed loops (closed strings) Figure 11:1 and Figure 11:2.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/j.html (2 of 11) [2/20/2001 3:15:56 AM]

A Brief History of Time - Stephen Hawking... Chapter 11

Figures 11:1 & 11:2
A particle occupies one point of space at each instant of time. Thus its history can be represented by a line in
space-time (the “world-line”). A string, on the other hand, occupies a line in space at each moment of time. So
its history in space-time is a two-dimensional surface called the world-sheet. (Any point on such a world-sheet
can be described by two numbers, one specifying the time and the other the position of the point on the string.)
The world-sheet of an open string is a strip: its edges represent the paths through space-time of the ends of the
string Figure 11:1. The world-sheet of a closed string is a cylinder or tube Figure 11:2: a slice through the tube
is a circle, which represents the position of the string at one particular time.
Two pieces of string can join together to form a single string; in the case of open strings they simply join at the
ends Figure 11:3, while in the case of closed strings it is like the two legs joining on a pair of trousers Figure
11:4.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/j.html (3 of 11) [2/20/2001 3:15:56 AM]

A Brief History of Time - Stephen Hawking... Chapter 11

Figure 11:3

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/j.html (4 of 11) [2/20/2001 3:15:56 AM]

A Brief History of Time - Stephen Hawking... Chapter 11

Figure 11:4
Similarly, a single piece of string can divide into two strings. In string theories, what were previously thought of
as particles are now pictured as waves traveling down the string, like waves on a vibrating kite string. The
emission or absorption of one particle by another corresponds to the dividing or joining together of strings. For
example, the gravitational force of the sun on the earth was pictured in particle theories as being caused by the
emission of a graviton by a particle in the sun and its absorption by a particle in the earth Figure 11:5.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/j.html (5 of 11) [2/20/2001 3:15:56 AM]

A Brief History of Time - Stephen Hawking... Chapter 11

Figures 11:5 & 11:6
In string theory, this process corresponds to an H-shaped tube or pipe Figure 11:6 (string theory is rather like
plumbing, in a way). The two vertical sides of the H correspond to the particles in the sun and the earth, and the
horizontal crossbar corresponds to the graviton that travels between them.
String theory has a curious history. It was originally invented in the late 1960s in an attempt to find a theory to
describe the strong force. The idea was that particles like the proton and the neutron could be regarded as
waves on a string. The strong forces between the particles would correspond to pieces of string that went
between other bits of string, as in a spider’s web. For this theory to give the observed value of the strong force
between particles, the strings had to be like rubber bands with a pull of about ten tons.
In 1974 Joel Scherk from Paris and John Schwarz from the California Institute of Technology published a paper
in which they showed that string theory could describe the gravitational force, but only if the tension in the string
were very much higher, about a thousand million million million million million million tons (1 with thirty-nine
zeros after it). The predictions of the string theory would be just the same as those of general relativity on
normal length scales, but they would differ at very small distances, less than a thousand million million million
million millionth of a centimeter (a centimeter divided by 1 with thirty-three zeros after it). Their work did not
receive much attention, however, because at just about that time most people abandoned the original string
theory of the strong force in favor of the theory based on quarks and gluons, which seemed to fit much better
with observations. Scherk died in tragic circumstances (he suffered from diabetes and went into a coma when
no one was around to give him an injection of insulin). So Schwarz was left alone as almost the only supporter
of string theory, but now with the much higher proposed value of the string tension.
In 1984 interest in strings suddenly revived, apparently for two reasons. One was that people were not really
making much progress toward showing that supergravity was finite or that it could explain the kinds of particles
that we observe. The other was the publication of a paper by John Schwarz and Mike Green of Queen Mary
College, London, that showed that string theory might be able to explain the existence of particles that have a
built-in left-handedness, like some of the particles that we observe. Whatever the reasons, a large number of
people soon began to work on string theory and a new version was developed, the so-called heterotic string,
which seemed as if it might be able to explain the types of particles that we observe.
String theories also lead to infinities, but it is thought they will all cancel out in versions like the heterotic string

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/j.html (6 of 11) [2/20/2001 3:15:56 AM]

A Brief History of Time - Stephen Hawking... Chapter 11

(though this is not yet known for certain). String theories, however, have a bigger problem: they seem to be
consistent only if space-time has either ten or twenty-six dimensions, instead of the usual four! Of course, extra
space-time dimensions are a commonplace of science fiction indeed, they provide an ideal way of overcoming
the normal restriction of general relativity that one cannot travel faster than light or back in time (see Chapter
10). The idea is to take a shortcut through the extra dimensions. One can picture this in the following way.
Imagine that the space we live in has only two dimensions and is curved like the surface of an anchor ring or
torus Figure 11:7.

Figure 11:7
If you were on one side of the inside edge of the ring and you wanted to get to a point on the other side, you
would have to go round the inner edge of the ring. However, if you were able to travel in the third dimension,
you could cut straight across.
Why don’t we notice all these extra dimensions, if they are really there? Why do we see only three space
dimensions and one time dimension? The suggestion is that the other dimensions are curved up into a space of
very small size, something like a million million million million millionth of an inch. This is so small that we just
don’t notice it: we see only one time dimension and three space dimensions, in which space-time is fairly flat. It
is like the surface of a straw. If you look at it closely, you see it is two-dimensional (the position of a point on the
straw is described by two numbers, the length along the straw and the distance round the circular direction).
But if you look at it from a distance, you don’t see the thickness of the straw and it looks one-dimensional (the
position of a point is specified only by the length along the straw). So it is with space-time: on a very small scale
it is ten-dimensional and highly curved, but on bigger scales you don’t see the curvature or the extra
dimensions. If this picture is correct, it spells bad news for would-be space travelers: the extra dimensions
would be far too small to allow a spaceship through. However, it raises another major problem. Why should
some, but not all, of the dimensions be curled up into a small ball? Presumably, in the very early universe all
the dimensions would have been very curved. Why did one time dimension and three space dimensions flatten

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/j.html (7 of 11) [2/20/2001 3:15:56 AM]

A Brief History of Time - Stephen Hawking... Chapter 11

out, while the other dimensions remain tightly curled up?
One possible answer is the anthropic principle. Two space dimensions do not seem to be enough to allow for
the development of complicated beings like us. For example, two-dimensional animals living on a
one-dimensional earth would have to climb over each other in order to get past each other. If a two-dimensional
creature ate something it could not digest completely, it would have to bring up the remains the same way it
swallowed them, because if there were a passage right through its body, it would divide the creature into two
separate halves: our two-dimensional being would fall apart Figure 11:8. Similarly, it is difficult to see how there
could be any circulation of the blood in a two-dimensional creature.

Figure 11:8
There would also be problems with more than three space dimensions. The gravitational force between two
bodies would decrease more rapidly with distance than it does in three dimensions. (In three dimensions, the
gravitational force drops to 1/4 if one doubles the distance. In four dimensions it would drop to 1/5, in five
dimensions to 1/6, and so on.) The significance of this is that the orbits of planets, like the earth, around the sun
would be unstable: the least disturbance from a circular orbit (such as would be caused by the gravitational
attraction of other planets) would result in the earth spiraling away from or into the sun. We would either freeze
or be burned up. In fact, the same behavior of gravity with distance in more than three space dimensions
means that the sun would not be able to exist in a stable state with pressure balancing gravity. It would either
fall apart or it would collapse to form a black hole. In either case, it would not be of much use as a source of
heat and light for life on earth. On a smaller scale, the electrical forces that cause the electrons to orbit round
the nucleus in an atom would behave in the same way as gravitational forces. Thus the electrons would either
escape from the atom altogether or would spiral into the nucleus. In either case, one could not have atoms as
we know them.
It seems clear then that life, at least as we know it, can exist only in regions of space-time in which one time
dimension and three space dimensions are not curled up small. This would mean that one could appeal to the
weak anthropic principle, provided one could show that string theory does at least allow there to be such
regions of the universe – and it seems that indeed string theory does. There may well be other regions of the
universe, or other universes (whatever that may mean), in which all the dimensions are curled up small or in
which more than four dimensions are nearly flat, but there would be no intelligent beings in such regions to

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/j.html (8 of 11) [2/20/2001 3:15:56 AM]

A Brief History of Time - Stephen Hawking... Chapter 11

observe the different number of effective dimensions.
Another problem is that there are at least four different string theories (open strings and three different closed
string theories) and millions of ways in which the extra dimensions predicted by string theory could be curled
up. Why should just one string theory and one kind of curling up be picked out? For a time there seemed no
answer, and progress got bogged down. Then, from about 1994, people started discovering what are called
dualities: different string theories and different ways of curling up the extra dimensions could lead to the same
results in four dimensions. Moreover, as well as particles, which occupy a single point of space, and strings,
which are lines, there were found to be other objects called p-branes, which occupied two-dimensional or
higher-dimensional volumes in space. (A particle can be regarded as a 0-brane and a string as a 1-brane but
there were also p-branes for p=2 to p=9.) What this seems to indicate is that there is a sort of democracy
among supergravity, string, and p-brane theories: they seem to fit together but none can be said to be more
fundamental than the others. They appear to be different approximations to some fundamental theory that are
valid in different situations.
People have searched for this underlying theory, but without any success so far. However, I believe there may
not be any single formulation of the fundamental theory any more than, as Godel showed, one could formulate
arithmetic in terms of a single set of axioms. Instead it may be like maps – you can’t use a single map to
describe the surface of the earth or an anchor ring: you need at least two maps in the case of the earth and four
for the anchor ring to cover every point. Each map is valid only in a limited region, but different maps will have a
region of overlap. The collection of maps provides a complete description of the surface. Similarly, in physics it
may be necessary to use different formulations in different situations, but two different formulations would agree
in situations where they can both be applied. The whole collection of different formulations could be regarded
as a complete unified theory, though one that could not be expressed in terms of a single set of postulates.
But can there really be such a unified theory? Or are we perhaps just chasing a mirage? There seem to be
three possibilities:
1. There really is a complete unified theory (or a collection of overlapping formulations), which we will someday
discover if we are smart enough.
2. There is no ultimate theory of the universe, just an infinite sequence of theories that describe the universe
more and more accurately.
3. There is no theory of the universe: events cannot be predicted beyond a certain extent but occur in a random
and arbitrary manner.
Some would argue for the third possibility on the grounds that if there were a complete set of laws, that would
infringe God’s freedom to change his mind and intervene in the world. It’s a bit like the old paradox: can God
make a stone so heavy that he can’t lift it? But the idea that God might want to change his mind is an example
of the fallacy, pointed out by St. Augustine, of imagining God as a being existing in time: time is a property only
of the universe that God created. Presumably, he knew what he intended when he set it up!
With the advent of quantum mechanics, we have come to recognize that events cannot be predicted with
complete accuracy but that there is always a degree of uncertainty. If one likes, one could ascribe this
randomness to the intervention of God, but it would be a very strange kind of intervention: there is no evidence
that it is directed toward any purpose. Indeed, if it were, it would by definition not be random. In modern times,
we have effectively removed the third possibility above by redefining the goal of science: our aim is to formulate
a set of laws that enables us to predict events only up to the limit set by the uncertainty principle.
The second possibility, that there is an infinite sequence of more and more refined theories, is in agreement
with all our experience so far. On many occasions we have increased the sensitivity of our measurements or
made a new class of observations, only to discover new phenomena that were not predicted by the existing
theory, and to account for these we have had to develop a more advanced theory. It would therefore not be
very surprising if the present generation of grand unified theories was wrong in claiming that nothing essentially
new will happen between the electroweak unification energy of about 100 GeV and the grand unification energy
of about a thousand million million GeV. We might indeed expect to find several new layers of structure more

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/j.html (9 of 11) [2/20/2001 3:15:56 AM]

A Brief History of Time - Stephen Hawking... Chapter 11

basic than the quarks and electrons that we now regard as “elementary” particles.
However, it seems that gravity may provide a limit to this sequence of “boxes within boxes.” If one had a
particle with an energy above what is called the Planck energy, ten million million million GeV (1 followed by
nineteen zeros), its mass would be so concentrated that it would cut itself off from the rest of the universe and
form a little black hole. Thus it does seem that the sequence of more and more refined theories should have
some limit as we go to higher and higher energies, so that there should be some ultimate theory of the
universe. Of course, the Planck energy is a very long way from the energies of around a hundred GeV, which
are the most that we can produce in the laboratory at the present time. We shall not bridge that gap with
particle accelerators in the foreseeable future! The very early stages of the universe, however, are an arena
where such energies must have occurred. I think that there is a good chance that the study of the early
universe and the requirements of mathematical consistency will lead us to a complete unified theory within the
lifetime of some of us who are around today, always presuming we don’t blow ourselves up first.
What would it mean if we actually did discover the ultimate theory of the universe? As was explained in Chapter
1, we could never be quite sure that we had indeed found the correct theory, since theories can’t be proved.
But if the theory was mathematically consistent and always gave predictions that agreed with observations, we
could be reasonably confident that it was the right one. It would bring to an end a long and glorious chapter in
the history of humanity’s intellectual struggle to understand the universe. But it would also revolutionize the
ordinary person’s understanding of the laws that govern the universe. In Newton’s time it was possible for an
educated person to have a grasp of the whole of human knowledge, at least in outline. But since then, the pace
of the development of science has made this impossible. Because theories are always being changed to
account for new observations, they are never properly digested or simplified so that ordinary people can
understand them. You have to be a specialist, and even then you can only hope to have a proper grasp of a
small proportion of the scientific theories. Further, the rate of progress is so rapid that what one learns at school
or university is always a bit out of date. Only a few people can keep up with the rapidly advancing frontier of
knowledge, and they have to devote their whole time to it and specialize in a small area. The rest of the
population has little idea of the advances that are being made or the excitement they are generating. Seventy
years ago, if Eddington is to be believed, only two people understood the general theory of relativity. Nowadays
tens of thousands of university graduates do, and many millions of people are at least familiar with the idea. If a
complete unified theory was discovered, it would only be a matter of time before it was digested and simplified
in the same way and taught in schools, at least in outline. We would then all be able to have some
understanding of the laws that govern the universe and are responsible for our existence.
Even if we do discover a complete unified theory, it would not mean that we would be able to predict events in
general, for two reasons. The first is the limitation that the uncertainty principle of quantum mechanics sets on
our powers of prediction. There is nothing we can do to get around that. In practice, however, this first limitation
is less restrictive than the second one. It arises from the fact that we could not solve the equations of the theory
exactly, except in very simple situations. (We cannot even solve exactly for the motion of three bodies in
Newton’s theory of gravity, and the difficulty increases with the number of bodies and the complexity of the
theory.) We already know the laws that govern the behavior of matter under all but the most extreme
conditions. In particular, we know the basic laws that underlie all of chemistry and biology. Yet we have
certainly not reduced these subjects to the status of solved problems: we have, as yet, had little success in
predicting human behavior from mathematical equations! So even if we do find a complete set of basic laws,
there will still be in the years ahead the intellectually challenging task of developing better approximation
methods, so that we can make useful predictions of the probable outcomes in complicated and realistic
situations. A complete, consistent, unified theory is only the first step: our goal is a complete understanding of
the events around us, and of our own existence.

PREVIOUS

NEXT

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/j.html (10 of 11) [2/20/2001 3:15:56 AM]

A Brief History of Time - Stephen Hawking... Chapter 11

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/j.html (11 of 11) [2/20/2001 3:15:56 AM]

A Brief History of Time - Stephen Hawking... Chapter 12

CHAPTER 12
CONCLUSION

We find ourselves in a bewildering world. We want to make sense of what we see around us and to ask: What
is the nature of the universe? What is our place in it and where did it and we come from? Why is it the way it is?
To try to answer these questions we adopt some “world picture.” Just as an infinite tower of tortoises supporting
the fiat earth is such a picture, so is the theory of superstrings. Both are theories of the universe, though the
latter is much more mathematical and precise than the former. Both theories lack observational evidence: no
one has ever seen a giant tortoise with the earth on its back, but then, no one has seen a superstring either.
However, the tortoise theory fails to be a good scientific theory because it predicts that people should be able to
fall off the edge of the world. This has not been found to agree with experience, unless that turns out to be the
explanation for the people who are supposed to have disappeared in the Bermuda Triangle!
The earliest theoretical attempts to describe and explain the universe involved the idea that events and natural
phenomena were controlled by spirits with human emotions who acted in a very humanlike and unpredictable
manner. These spirits inhabited natural objects, like rivers and mountains, including celestial bodies, like the
sun and moon. They had to be placated and their favor sought in order to ensure the fertility of the soil and the
rotation of the seasons. Gradually, however, it must have been noticed that there were certain regularities: the
sun always rose in the east and set in the west, whether or not a sacrifice had been made to the sun god.
Further, the sun, the moon, and the planets followed precise paths across the sky that could be predicted in
advance with considerable accuracy. The sun and the moon might still be gods, but they were gods who
obeyed strict laws, apparently without any exceptions, if one discounts stories like that of the sun stopping for
Joshua.
At first, these regularities and laws were obvious only in astronomy and a few other situations. However, as
civilization developed, and particularly in the last 300 years, more and more regularities and laws were
discovered. The success of these laws led Laplace at the beginning of the nineteenth century to postulate
scientific determinism; that is, he suggested that there would be a set of laws that would determine the
evolution of the universe precisely, given its configuration at one time.
Laplace’s determinism was incomplete in two ways. It did not say how the laws should be chosen and it did not
specify the initial configuration of the universe. These were left to God. God would choose how the universe
began and what laws it obeyed, but he would not intervene in the universe once it had started. In effect, God
was confined to the areas that nineteenth-century science did not understand.
We now know that Laplace’s hopes of determinism cannot be realized, at least in the terms he had in mind.
The uncertainty principle of quantum mechanics implies that certain pairs of quantities, such as the position and
velocity of a particle, cannot both be predicted with complete accuracy. Quantum mechanics deals with this
situation via a class of quantum theories in which particles don’t have well-defined positions and velocities but
are represented by a wave. These quantum theories are deterministic in the sense that they give laws for the
evolution of the wave with time. Thus if one knows the wave at one time, one can calculate it at any other time.
The unpredictable, random element comes in only when we try to interpret the wave in terms of the positions
and velocities of particles. But maybe that is our mistake: maybe there are no particle positions and velocities,
but only waves. It is just that we try to fit the waves to our preconceived ideas of positions and velocities. The
resulting mismatch is the cause of the apparent unpredictability.
In effect, we have redefined the task of science to be the discovery of laws that will enable us to predict events
up to the limits set by the uncertainty principle. The question remains, however: how or why were the laws and
the initial state of the universe chosen?
In this book I have given special prominence to the laws that govern gravity, because it is gravity that shapes
the large-scale structure of the universe, even though it is the weakest of the four categories of forces. The
laws of gravity were incompatible with the view held until quite recently that the universe is unchanging in time:

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/k.html (1 of 4) [2/20/2001 3:16:08 AM]

A Brief History of Time - Stephen Hawking... Chapter 12

the fact that gravity is always attractive implies that the universe must be either expanding or contracting.
According to the general theory of relativity, there must have been a state of infinite density in the past, the big
bang, which would have been an effective beginning of time. Similarly, if the whole universe recollapsed, there
must be another state of infinite density in the future, the big crunch, which would be an end of time. Even if the
whole universe did not recollapse, there would be singularities in any localized regions that collapsed to form
black holes. These singularities would be an end of time for anyone who fell into the black hole. At the big bang
and other singularities, all the laws would have broken down, so God would still have had complete freedom to
choose what happened and how the universe began.
When we combine quantum mechanics with general relativity, there seems to be a new possibility that did not
arise before: that space and time together might form a finite, four-dimensional space without singularities or
boundaries, like the surface of the earth but with more dimensions. It seems that this idea could explain many
of the observed features of the universe, such as its large-scale uniformity and also the smaller-scale
departures from homogeneity, like galaxies, stars, and even human beings. It could even account for the arrow
of time that we observe. But if the universe is completely self-contained, with no singularities or boundaries,
and completely described by a unified theory, that has profound implications for the role of God as Creator.
Einstein once asked the question: “How much choice did God have in constructing the universe?” If the no
boundary proposal is correct, he had no freedom at all to choose initial conditions. He would, of course, still
have had the freedom to choose the laws that the universe obeyed. This, however, may not really have been all
that much of a choice; there may well be only one, or a small number, of complete unified theories, such as the
heterotic string theory, that are self-consistent and allow the existence of structures as complicated as human
beings who can investigate the laws of the universe and ask about the nature of God.
Even if there is only one possible unified theory, it is just a set of rules and equations. What is it that breathes
fire into the equations and makes a universe for them to describe? The usual approach of science of
constructing a mathematical model cannot answer the questions of why there should be a universe for the
model to describe. Why does the universe go to all the bother of existing? Is the unified theory so compelling
that it brings about its own existence? Or does it need a creator, and, if so, does he have any other effect on
the universe? And who created him?
Up to now, most scientists have been too occupied with the development of new theories that describe what
the universe is to ask the question why. On the other hand, the people whose business it is to ask why, the
philosophers, have not been able to keep up with the advance of scientific theories. In the eighteenth century,
philosophers considered the whole of human knowledge, including science, to be their field and discussed
questions such as: did the universe have a beginning? However, in the nineteenth and twentieth centuries,
science became too technical and mathematical for the philosophers, or anyone else except a few specialists.
Philosophers reduced the scope of their inquiries so much that Wittgenstein, the most famous philosopher of
this century, said, “The sole remaining task for philosophy is the analysis of language.” What a comedown from
the great tradition of philosophy from Aristotle to Kant!
However, if we do discover a complete theory, it should in time be understandable in broad principle by
everyone, not just a few scientists. Then we shall all, philosophers, scientists, and just ordinary people, be able
to take part in the discussion of the question of why it is that we and the universe exist. If we find the answer to
that, it would be the ultimate triumph of human reason – for then we would know the mind of God.

ALBERT EINSTEIN
Einstein’s connection with the politics of the nuclear bomb is well known: he signed the famous letter to
President Franklin Roosevelt that persuaded the United States to take the idea seriously, and he engaged in
postwar efforts to prevent nuclear war. But these were not just the isolated actions of a scientist dragged into
the world of politics. Einstein’s life was, in fact, to use his own words, “divided between politics and equations.”
Einstein’s earliest political activity came during the First World War, when he was a professor in Berlin.
Sickened by what he saw as the waste of human lives, he became involved in antiwar demonstrations. His

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/k.html (2 of 4) [2/20/2001 3:16:08 AM]

A Brief History of Time - Stephen Hawking... Chapter 12

advocacy of civil disobedience and public encouragement of people to refuse conscription did little to endear
him to his colleagues. Then, following the war, he directed his efforts toward reconciliation and improving
international relations. This too did not make him popular, and soon his politics were making it difficult for him to
visit the United States, even to give lectures.
Einstein’s second great cause was Zionism. Although he was Jewish by descent, Einstein rejected the biblical
idea of God. However, a growing awareness of anti-Semitism, both before and during the First World War, led
him gradually to identify with the Jewish community, and later to become an outspoken supporter of Zionism.
Once more unpopularity did not stop him from speaking his mind. His theories came under attack; an
anti-Einstein organization was even set up. One man was convicted of inciting others to murder Einstein (and
fined a mere six dollars). But Einstein was phlegmatic. When a book was published entitled 100 Authors
Against Einstein, he retorted, “If I were wrong, then one would have been enough!”
In 1933, Hitler came to power. Einstein was in America, and declared he would not return to Germany. Then,
while Nazi militia raided his house and confiscated his bank account, a Berlin newspaper displayed the
headline “Good News from Einstein – He’s Not Coming Back.” In the face of the Nazi threat, Einstein
renounced pacifism, and eventually, fearing that German scientists would build a nuclear bomb, proposed that
the United States should develop its own. But even before the first atomic bomb had been detonated, he was
publicly warning of the dangers of nuclear war and proposing international control of nuclear weaponry.
Throughout his life, Einstein’s efforts toward peace probably achieved little that would last – and certainly won
him few friends. His vocal support of the Zionist cause, however, was duly recognized in 1952, when he was
offered the presidency of Israel. He declined, saying he thought he was too naive in politics. But perhaps his
real reason was different: to quote him again, “Equations are more important to me, because politics is for the
present, but an equation is something for eternity.”

GALILEO GALILEI
Galileo, perhaps more than any other single person, was responsible for the birth of modern science. His
renowned conflict with the Catholic Church was central to his philosophy, for Galileo was one of the first to
argue that man could hope to understand how the world works, and, moreover, that we could do this by
observing the real world.
Galileo had believed Copernican theory (that the planets orbited the sun) since early on, but it was only when
he found the evidence needed to support the idea that he started to publicly support it. He wrote about
Copernicus’s theory in Italian (not the usual academic Latin), and soon his views became widely supported
outside the universities. This annoyed the Aristotelian professors, who united against him seeking to persuade
the Catholic Church to ban Copernicanism.
Galileo, worried by this, traveled to Rome to speak to the ecclesiastical authorities. He argued that the Bible
was not intended to tell us anything about scientific theories, and that it was usual to assume that, where the
Bible conflicted with common sense, it was being allegorical. But the Church was afraid of a scandal that might
undermine its fight against Protestantism, and so took repressive measures. It declared Copernicanism “false
and erroneous” in 1616, and commanded Galileo never again to “defend or hold” the doctrine. Galileo
acquiesced.
In 1623, a longtime friend of Galileo’s became the Pope. Immediately Galileo tried to get the 1616 decree
revoked. He failed, but he did manage to get permission to write a book discussing both Aristotelian and
Copernican theories, on two conditions: he would not take sides and would come to the conclusion that man
could in any case not determine how the world worked because God could bring about the same effects in
ways unimagined by man, who could not place restrictions on God’s omnipotence.
The book, Dialogue Concerning the Two Chief World Systems, was completed and published in 1632, with the
full backing of the censors – and was immediately greeted throughout Europe as a literary and philosophical
masterpiece. Soon the Pope, realizing that people were seeing the book as a convincing argument in favor of

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/k.html (3 of 4) [2/20/2001 3:16:08 AM]

A Brief History of Time - Stephen Hawking... Chapter 12

Copernicanism, regretted having allowed its publication. The Pope argued that although the book had the
official blessing of the censors, Galileo had nevertheless contravened the 1616 decree. He brought Galileo
before the Inquisition, who sentenced him to house arrest for life and commanded him to publicly renounce
Copernicanism. For a second time, Galileo acquiesced.
Galileo remained a faithful Catholic, but his belief in the independence of science had not been crushed. Four
years before his death in 1642, while he was still under house arrest, the manuscript of his second major book
was smuggled to a publisher in Holland. It was this work, referred to as Two New Sciences, even more than his
support for Copernicus, that was to be the genesis of modern physics.

ISAAC NEWTON
Isaac Newton was not a pleasant man. His relations with other academics were notorious, with most of his later
life spent embroiled in heated disputes. Following publication of Principia Mathematica – surely the most
influential book ever written in physics – Newton had risen rapidly into public prominence. He was appointed
president of the Royal Society and became the first scientist ever to be knighted.
Newton soon clashed with the Astronomer Royal, John Flamsteed, who had earlier provided Newton with
much-needed data for Principia, but was now withholding information that Newton wanted. Newton would not
take no for an answer: he had himself appointed to the governing body of the Royal Observatory and then tried
to force immediate publication of the data. Eventually he arranged for Flamsteed’s work to be seized and
prepared for publication by Flamsteed’s mortal enemy, Edmond Halley. But Flamsteed took the case to court
and, in the nick of time, won a court order preventing distribution of the stolen work. Newton was incensed and
sought his revenge by systematically deleting all references to Flamsteed in later editions of Principia.
A more serious dispute arose with the German philosopher Gottfried Leibniz. Both Leibniz and Newton had
independently developed a branch of mathematics called calculus, which underlies most of modern physics.
Although we now know that Newton discovered calculus years before Leibniz, he published his work much
later. A major row ensued over who had been first, with scientists vigorously defending both contenders. It is
remarkable, however, that most of the articles appearing in defense of Newton were originally written by his
own hand – and only published in the name of friends! As the row grew, Leibniz made the mistake of appealing
to the Royal Society to resolve the dispute. Newton, as president, appointed an “impartial” committee to
investigate, coincidentally consisting entirely of Newton’s friends! But that was not all: Newton then wrote the
committee’s report himself and had the Royal Society publish it, officially accusing Leibniz of plagiarism. Still
unsatisfied, he then wrote an anonymous review of the report in the Royal Society’s own periodical. Following
the death of Leibniz, Newton is reported to have declared that he had taken great satisfaction in “breaking
Leibniz’s heart.”
During the period of these two disputes, Newton had already left Cambridge and academe. He had been active
in anti-Catholic politics at Cambridge, and later in Parliament, and was rewarded eventually with the lucrative
post of Warden of the Royal Mint. Here he used his talents for deviousness and vitriol in a more socially
acceptable way, successfully conducting a major campaign against counterfeiting, even sending several men to
their death on the gallows.

PREVIOUS

NEXT

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/k.html (4 of 4) [2/20/2001 3:16:08 AM]

A Brief History of Time - Stephen Hawking... Glossary

GLOSSARY

Absolute zero: The lowest possible temperature, at which substances contain no heat energy.
Acceleration: The rate at which the speed of an object is changing.
Anthropic principle: We see the universe the way it is because if it were different we would not be here to
observe it.
Antiparticle: Each type of matter particle has a corresponding antiparticle. When a particle collides with its
antiparticle, they annihilate, leaving only energy.
Atom: The basic unit of ordinary matter, made up of a tiny nucleus (consisting of protons and neutrons)
surrounded by orbiting electrons.
Big bang: The singularity at the beginning of the universe.
Big crunch: The singularity at the end of the universe.
Black hole: A region of space-time from which nothing, not even light, can escape, because gravity is so
strong.
Casimir effect: The attractive pressure between two flat, parallel metal plates placed very near to each other in
a vacuum. The pressure is due to a reduction in the usual number of virtual particles in the space between the
plates.
Chandrasekhar limit: The maximum possible mass of a stable cold star, above which it must collapse into a
black hole.
Conservation of energy: The law of science that states that energy (or its equivalent in mass) can neither be
created nor destroyed.
Coordinates: Numbers that specify the position of a point in space and time.
Cosmological constant: A mathematical device used by Einstein to give space-time an inbuilt tendency to
expand.
Cosmology: The study of the universe as a whole.
Dark matter: Matter in galaxies, clusters, and possibly between clusters, that can not be observed directly but
can be detected by its gravitational effect. As much as 90 percent of the mass of the universe may be in the
form of dark matter.
Duality: A correspondence between apparently different theories that lead to the same physical results.
Einstein-Rosen bridge: A thin tube of space-time linking two black holes. Also see Wormhole.
Electric charge: A property of a particle by which it may repel (or attract) other particles that have a charge of
similar (or opposite) sign.
Electromagnetic force: The force that arises between particles with electric charge; the second strongest of
the four fundamental forces.
Electron: A particle with negative electric charge that orbits the nucleus of an atom.
Electroweak unification energy: The energy (around 100 GeV) above which the distinction between the
electromagnetic force and the weak force disappears.
file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/l.html (1 of 4) [2/20/2001 3:16:19 AM]

A Brief History of Time - Stephen Hawking... Glossary

Elementary particle: A particle that, it is believed, cannot be subdivided.
Event: A point in space-time, specified by its time and place.
Event horizon: The boundary of a black hole.
Exclusion principle: The idea that two identical spin-1/2 particles cannot have (within the limits set by the
uncertainty principle) both the same position and the same velocity.
Field: Something that exists throughout space and time, as opposed to a particle that exists at only one point at
a time.
Frequency: For a wave, the number of complete cycles per second.
Gamma rays: Electromagnetic rays of very short wavelength, produced in radio-active decay or by collisions of
elementary particles.
General relativity: Einstein’s theory based on the idea that the laws of science should be the same for all
observers, no matter how they are moving. It explains the force of gravity in terms of the curvature of a
four-dimensional space-time.
Geodesic: The shortest (or longest) path between two points.
Grand unification energy: The energy above which, it is believed, the electro-magnetic force, weak force, and
strong force become indistinguishable from each other.
Grand unified theory (GUT): A theory which unifies the electromagnetic, strong, and weak forces.
Imaginary time: Time measured using imaginary numbers.
Light cone: A surface in space-time that marks out the possible directions for light rays passing through a
given event.
Light-second (light-year): The distance traveled by light in one second (year).
Magnetic field: The field responsible for magnetic forces, now incorporated along with the electric field, into the
electromagnetic field.
Mass: The quantity of matter in a body; its inertia, or resistance to acceleration.
Microwave background radiation: The radiation from the glowing of the hot early universe, now so greatly
red-shifted that it appears not as light but as microwaves (radio waves with a wavelength of a few centimeters).
Also see COBE, on page 145.
Naked singularity: A space-time singularity not surrounded by a black hole.
Neutrino: An extremely light (possibly massless) particle that is affected only by the weak force and gravity.
Neutron: An uncharged particle, very similar to the proton, which accounts for roughly half the particles in an
atomic nucleus.
Neutron star: A cold star, supported by the exclusion principle repulsion between neutrons.
No boundary condition: The idea that the universe is finite but has no boundary (in imaginary time).
Nuclear fusion: The process by which two nuclei collide and coalesce to form a single, heavier nucleus.
Nucleus: The central part of an atom, consisting only of protons and neutrons, held together by the strong

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/l.html (2 of 4) [2/20/2001 3:16:19 AM]

A Brief History of Time - Stephen Hawking... Glossary

force.
Particle accelerator: A machine that, using electromagnets, can accelerate moving charged particles, giving
them more energy.
Phase: For a wave, the position in its cycle at a specified time: a measure of whether it is at a crest, a trough,
or somewhere in between.
Photon: A quantum of light.
Planck’s quantum principle: The idea that light (or any other classical waves) can be emitted or absorbed
only in discrete quanta, whose energy is proportional to their wavelength.
Positron: The (positively charged) antiparticle of the electron.
Primordial black hole: A black hole created in the very early universe.
Proportional: ‘X is proportional to Y’ means that when Y is multiplied by any number, so is X. ‘X is inversely
proportional to Y’ means that when Y is multiplied by any number, X is divided by that number.
Proton: A positively charged particle, very similar to the neutron, that accounts for roughly half the particles in
the nucleus of most atoms.
Pulsar: A rotating neutron star that emits regular pulses of radio waves.
Quantum: The indivisible unit in which waves may be emitted or absorbed.
Quantum chromodynamics (QCD): The theory that describes the interactions of quarks and gluons.
Quantum mechanics: The theory developed from Planck’s quantum principle and Heisenberg’s uncertainty
principle.
Quark: A (charged) elementary particle that feels the strong force. Protons and neutrons are each composed of
three quarks.
Radar: A system using pulsed radio waves to detect the position of objects by measuring the time it takes a
single pulse to reach the object and be reflected back.
Radioactivity: The spontaneous breakdown of one type of atomic nucleus into another.
Red shift: The reddening of light from a star that is moving away from us, due to the Doppler effect.
Singularity: A point in space-time at which the space-time curvature becomes infinite.
Singularity theorem: A theorem that shows that a singularity must exist under certain circumstances – in
particular, that the universe must have started with a singularity.
Space-time: The four-dimensional space whose points are events.
Spatial dimension: Any of the three dimensions that are spacelike – that is, any except the time dimension.
Special relativity: Einstein’s theory based on the idea that the laws of science should be the same for all
observers, no matter how they are moving, in the absence of gravitational phenomena.
Spectrum: The component frequencies that make up a wave. The visible part of the sun’s spectrum can be
seen in a rainbow.
Spin: An internal property of elementary particles, related to, but not identical to, the everyday concept of spin.

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/l.html (3 of 4) [2/20/2001 3:16:19 AM]

A Brief History of Time - Stephen Hawking... Glossary

Stationary state: One that is not changing with time: a sphere spinning at a constant rate is stationary because
it looks identical at any given instant.
String theory: A theory of physics in which particles are described as waves on strings. Strings have length but
no other dimension.
Strong force: The strongest of the four fundamental forces, with the shortest range of all. It holds the quarks
together within protons and neutrons, and holds the protons and neutrons together to form atoms.
Uncertainty principle: The principle, formulated by Heisenberg, that one can never be exactly sure of both the
position and the velocity of a particle; the more accurately one knows the one, the less accurately one can
know the other.
Virtual particle: In quantum mechanics, a particle that can never be directly detected, but whose existence
does have measurable effects.
Wave/particle duality: The concept in quantum mechanics that there is no distinction between waves and
particles; particles may sometimes behave like waves, and waves like particles.
Wavelength: For a wave, the distance between two adjacent troughs or two adjacent crests.
Weak force: The second weakest of the four fundamental forces, with a very short range. It affects all matter
particles, but not force-carrying particles.
Weight: The force exerted on a body by a gravitational field. It is proportional to, but not the same as, its mass.
White dwarf: A stable cold star, supported by the exclusion principle repulsion between electrons.
Wormhole: A thin tube of space-time connecting distant regions of the universe. Wormholes might also link to
parallel or baby universes and could provide the possibility of time travel.

PREVIOUS

NEXT

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/l.html (4 of 4) [2/20/2001 3:16:19 AM]

A Brief History of Time - Stephen Hawking... Acknowledgments

ACKNOWLEDGMENTS

Many people have helped me in writing this book. My scientific colleagues have without exception been
inspiring. Over the years my principal associates and collaborators were Roger Penrose, Robert Geroch,
Brandon Carter, George Ellis, Gary Gibbons, Don Page, and Jim Hartle. I owe a lot to them, and to my
research students, who have always given me help when needed.
One of my students, Brian Whitt, gave me a lot of help writing the first edition of this book. My editor at Bantam
Books, Peter Guzzardi, made innumerable comments which improved the book considerably. In addition, for
this edition, I would like to thank Andrew Dunn, who helped me revise the text.
I could not have written this book without my communication system. The software, called Equalizer, was
donated by Walt Waltosz of Words Plus Inc., in Lancaster, California. My speech synthesizer was donated by
Speech Plus, of Sunnyvale, California. The synthesizer and laptop computer were mounted on my wheelchair
by David Mason, of Cambridge Adaptive Communication Ltd. With this system I can communicate better now
than before I lost my voice.
I have had a number of secretaries and assistants over the years in which I wrote and revised this book. On the
secretarial side, I’m very grateful to Judy Fella, Ann Ralph, Laura Gentry, Cheryl Billington, and Sue Masey. My
assistants have been Colin Williams, David Thomas, and Raymond Laflamme, Nick Phillips, Andrew Dunn,
Stuart Jamieson, Jonathan Brenchley, Tim Hunt, Simon Gill, Jon Rogers, and Tom Kendall. They, my nurses,
colleagues, friends, and family have enabled me to live a very full life and to pursue my research despite my
disability.
Stephen Hawking

ABOUT THE AUTHOR
Stephen Hawking, who was born in 1942 on the anniversary of Galileo’s death, holds Isaac Newton’s chair as
Lucasian Professor of Mathematics at the University of Cambridge. Widely regarded as the most brilliant
theoretical physicist since Einstein, he is also the author of Black Holes and Baby Universes, published in 1993,
as well as numerous scientific papers and books.

PREVIOUS

file:///C|/WINDOWS/Desktop/blahh/Stephen Hawking - A brief history of time/m.html [2/20/2001 3:16:30 AM]


	</blockquote>
</div>
</body>
</html>